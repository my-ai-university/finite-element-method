{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading functions from the scripts\n",
    "\"\"\"\n",
    "Mostafa:\n",
    "I used the new structured output for question generation.\n",
    "It's a beta version, but it works on my end (10/23/2024).\n",
    "https://platform.openai.com/docs/guides/structured-outputs/structured-outputs\n",
    "\n",
    "For answer generation, I had some issues, so I used the standard API.\"\n",
    "\n",
    "Please upgrade before running this notebook: pip install --upgrade openai\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Get parent directory of the notebook \n",
    "sys.path.append(parent_dir)  #  to the Python path\n",
    "\n",
    "from scripts.chunking import process_latex_files\n",
    "from scripts.embedding import get_embeddings, fixed_knn_retrieval\n",
    "from scripts.prompts import gen_questions, gen_questions_s, gen_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting API and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I suggest using 'gpt-4o' for production runs, but it is more expensive.\n",
    "For embeddings, I recommend 'text-embedding-3-large.' We only need to run it once, but it also costs more.\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "# https://openai.com/index/new-embedding-models-and-api-updates/\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "\"\"\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "production_mode = True\n",
    "chunk_by_section = True\n",
    "chpt_for_quest_answ = 10\n",
    "\n",
    "if production_mode == False:\n",
    "    llm_model_questions = \"gpt-4o-mini\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "elif production_mode == True:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "\n",
    "embedding_model = f\"text-embedding-3-{embedding_size}\"  # NOTE: this must be the same for all embeddings. \n",
    "\n",
    "# Setting path for root data folder\n",
    "main_dir = '../data/hughes_latex_Q_then_A_use_context'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Context Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "Space size: (331, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I used fixed size chunks (512) with a 25% overlap.\n",
    "Make sure environment_sensitive is set to False for fixed size.\n",
    "\n",
    "We should embed all chapters to generate the embedding space. For the demo, I only included two chapters.\n",
    "please update the paths in latex_file_paths.\n",
    "\"\"\"\n",
    "\n",
    "# add all book chapters paths\n",
    "latex_file_paths = [ \n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter5.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter6.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "    '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex'\n",
    "]\n",
    "\n",
    "tokens_per_chunk = 4096                         # was 512\n",
    "token_overlap = int(0.25 * tokens_per_chunk)    # 25% overlap\n",
    "environment_sensitive = False                   # If False, equations can split between two chunks, but chunk lengths remain fixed.\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    embedding_space_file_name = f'{main_dir}/hughes_latex_embedding_space_tpc{tokens_per_chunk}_o{token_overlap}_{embedding_size}.json'\n",
    "elif chunk_by_section == True:\n",
    "    embedding_space_file_name = f'{main_dir}/hughes_latex_embedding_space_by_sections_tpc{tokens_per_chunk}_{embedding_size}.json'\n",
    "    token_overlap = 0\n",
    "\n",
    "space = {}\n",
    "if not os.path.exists(embedding_space_file_name):\n",
    "    \n",
    "    chunks = process_latex_files(latex_file_paths, \n",
    "                                 tokens_per_chunk, \n",
    "                                 token_overlap, \n",
    "                                 environment_sensitive, \n",
    "                                 chunk_by_section = chunk_by_section)\n",
    "    \n",
    "    chunk_length = []\n",
    "    char_length = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        print(f\"chunk word length: {len(chunk.split(\" \"))}, chunk char length: {len(chunk)}, chunk = {chunk}\")\n",
    "        chunk_length.append(len(chunk.split(\" \")))\n",
    "        char_length.append(len(chunk))\n",
    "    print(f\"max chunk length in words = {np.max(chunk_length)}\")\n",
    "    print(f\"max chunk length in char = {np.max(char_length)}\")\n",
    "    #print(f\"chunk lengths = {chunk_length}\")\n",
    "\n",
    "    # using api\n",
    "    embedding_space = get_embeddings(client, chunks, model=embedding_model)\n",
    "    \n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'w') as json_file:\n",
    "        json.dump({'embedding_model': embedding_model, 'chunks': chunks, 'embedding_space': embedding_space}, json_file)\n",
    "\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'r') as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "\n",
    "    chunks = loaded_data['chunks']\n",
    "    embedding_space = np.array(loaded_data['embedding_space'])\n",
    "    print(\"loaded\")\n",
    "\n",
    "chunks = np.array(chunks)\n",
    "embedding_space = np.array(embedding_space)\n",
    "print(\"Space size:\", embedding_space.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Questions and Their Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk word length: 1, chunk char length: 10, chunk = \\maketitle\n",
      "chunk word length: 1310, chunk char length: 10335, chunk = \\subsection*{10.1 THE GENERALIZED EIGENPROBLEM} In this chapter we are concerned with algorithms for solving the so-called generalized eigenproblem arising from finite element discretizations. Recall that this takes the form:\\footnote{Problems of this type are motivated and formulated in Chapter 7. To simplify subsequent writing we omit the superscript $h$ on $\\lambda$.} \\begin{equation*} (K-\\lambda M) \\psi=0 \\tag{10.1.1} \\end{equation*} where $K$ is symmetric and positive-semidefinite, $M$ is symmetric and positive-definite, and both matrices possess typical band/profile structure. For convenience we recall some basic properties of the solution of (10.1.1). We assume as always that the dimensions of $K$ and $M$ are $n_{e q} \\times n_{e q}$. There exist $n_{e q}$ eigenvalues and corresponding eigenvectors\\footnote{The eigenvectors may be nonunique.} that satisfy (10.1.1). That is, \\begin{equation*} \\left(K-\\lambda_{l} M\\right) \\psi_{l}=0, \\quad \\text { (no sum) } \\tag{10.1.2} \\end{equation*} where $l=1,2, \\ldots, n_{e q}$ denotes the mode number. Furthermore, \\begin{equation*} 0 \\leq \\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n_{\\text {eq }}} \\tag{10.1.3} \\end{equation*} and \\[ \\begin{array}{lll} \\Psi_{k}^{T} M \\Psi_{l}=\\delta_{k l} & \\text { (M orthonormality) } \\\\ \\Psi_{k}^{T} K \\Psi_{l}=\\lambda_{l} \\delta_{k l} & \\text { (no sum) } & \\text { (K orthogonality) } \\tag{10.1.5} \\end{array} \\] The eigenvectors $\\left\\{\\psi_{l}\\right\\}_{l=1}^{n_{e q}}$ constitute a basis for $\\mathbb{R}^{n_{e e}}$. If, additionally, $\\boldsymbol{K}$ is positivedefinite, as is often the case, then (10.1.3) may be strengthened as follows: \\begin{equation*} 0<\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n_{\\text {eq }}} \\tag{10.1.6} \\end{equation*} In practice, we are typically interested only in the lower modes. These are the most important from a physical standpoint. For example, the lowest mode is the most important in a structural stability (i.e., \"buckling\") calculation and the lower frequencies and corresponding mode shapes are usually the most important in considerations of dynamic response. Additionally, the higher modes of finite element discretizations are not accurate renditions of physical behavior but rather spurious artifacts of the discretization process. Consequently, they are of no engineering interest. For these reasons, we are interested in economical computational algorithms for extracting $\\left\\{\\lambda_{l}, \\Psi_{l}\\right\\}, 1 \\leq l \\leq n_{\\text {modes }}$, where $n_{\\text {modes }} \\ll n_{e q}$ is the number of desired eigenpairs. In practice $n_{\\text {eq }}$ may be very large, and thus solution of the eigenvalue problem, even for only a few eigenpairs, may entail an extensive and costly calculation. Most procedures used for solving the large-scale generalized eigenproblem, (10.1.1), involve a reduced system of the form \\begin{equation*} \\left(\\boldsymbol{K}^{*}-\\lambda^{*} \\boldsymbol{M}^{*}\\right) \\Psi^{*}=0 \\tag{10.1.7} \\end{equation*} where $K^{*}$ and $M^{*}$ are small, full, symmetric matrices. Algorithms, such as the generalized Jacobi method [1], are available for directly solving (10.1.7). Systems such as (10.1.7) may also be solved by first transforming to standard form: \\begin{equation*} \\left(\\bar{K}^{*}-\\lambda^{*} \\boldsymbol{I}\\right) \\bar{\\Psi}^{*}=\\mathbf{0} \\tag{10.1.8} \\end{equation*} where \\begin{align*} & \\overline{\\mathbf{K}}^{*}=\\overline{\\boldsymbol{U}}^{-T} \\mathbf{K}^{*} \\bar{U}^{-1} \\tag{10.1.9}\\\\ & \\overline{\\boldsymbol{\\Psi}}^{*}=\\overline{\\boldsymbol{U}} \\boldsymbol{\\Psi}^{*} \\tag{10.1.10} \\end{align*} and $\\boldsymbol{U}$ is the upper-triangular Cholesky factor of $M^{*}$, i.e., \\begin{equation*} \\boldsymbol{M}^{*}=\\overline{\\boldsymbol{U}}^{T} \\boldsymbol{\\overline{U}} \\tag{10.1.11} \\end{equation*} (The Cholesky factor is related to the Crout factor by $\\boldsymbol{\\overline{U}}=D^{1 / 2} \\boldsymbol{U}$, where $\\boldsymbol{M}^{*}=$ $\\boldsymbol{U}^{\\boldsymbol{T}} \\boldsymbol{D} \\boldsymbol{U}, \\boldsymbol{U}$ is upper-triangular with ones on the diagonal and $\\boldsymbol{D}$ is a diagonal matrix of positive numbers; see Chapter 11 for further information on the Crout factorization.) Observe that the eigenvalues of the standard form, (10.1.8), are identical to the generalized form, (10.1.7). However, the eigenvectors need to be transformed as indicated by (10.1.10) \\textbf{Exercise 1.} Derive (10.1.7) from (10.1.8) through (10.1.11). There are many classical and widely available procedures for solving (10.1.8). For example, the Jacobi, Givens, and Householder-QR methods may be mentioned in this regard (e.g., see Bathe [1] and Noble [2]). For general matrices, it is currently felt that the most efficient strategy for solving the generalized eigenproblem, (10.1.7), is to first transform to standard form, (10.1.8), and then use the Householder-QR algorithm. However, under certain circumstances, such as when the subspace iteration procedure is employed (see Sec. 10.5), for example, direct use of the generalized Jacobi method proves very effective. It is very difficult to make sweeping statements about efficiency because the type of computer (e.g., sequential, vector, or parallel) strongly influences the performance of algorithms. \\textbf{Exercise 2.} Consider the undamped equation of motion, \\begin{equation*} M \\Ddot{d}+K d=F \\tag{10.1.12} \\end{equation*} subject to zero initial displacement and velocity. Expand the solution in terms of the eigenvectors of the associated eigenproblem. Diagonalize the system and exactly solve the individual modal equations. Show that \\begin{equation*} d(t)=\\sum_{l=1}^{n_{e q}}\\left\\{\\frac{1}{\\omega_{l}} \\int_{0}^{t} F_{(l)}(\\tau) \\sin \\omega_{l}(t-\\tau) d \\tau \\Psi_{l}\\right\\} \\tag{10.1.13} \\end{equation*} The $1 / \\omega_{l}$ factor in the expansion illustrates the diminishing influence of the higher modes. This analysis reveals why low mode response is viewed as \"most important\" and therefore, why, in practical calculations the summation in (10.1.13) is truncated at $n_{\\text {modes }} \\ll n_{\\text {eq }}$. \\textbf{Exercise 3.} Obtain an exact solution for the static problem, \\begin{equation*} \\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F} \\tag{10.1.14} \\end{equation*} by way of an eigenvector expansion. Discuss the relative importance of low and high modes for this case. \\textbf{Exercise 4.} Generalize Exercise 2 to account for Rayleigh damping and non-zero initial conditions. Discuss the influence of low and high modes. \\subsection*{10.2 STATIC CONDENSATION} In practice one often encounters eigenvalue problems which can be written in the following partitioned form: \\[ \\left(\\left[\\begin{array}{ll} K_{11} & K_{12} \\tag{10.2.1}\\\\ K_{21} & K_{22} \\end{array}\\right]-\\lambda\\left[\\begin{array}{cc} M_{11} & 0 \\\\ 0 & 0 \\end{array}\\right]\\right)\\left\\{\\begin{array}{l} \\Psi_{1} \\\\ \\Psi_{2} \\end{array}\\right\\}=0 \\] where $M_{11}$ is symmetric and positive-definite. That is, many degrees of freedom are \"massless.\" Problems of this type arise naturally when a relatively light structure is used to support heavy nonstructural masses which can be \"lumped\" at a few degrees of freedom. In such situations the structural mass is often insignificant and may be neglected, resulting in a system like (10.2.1). As it stands, (10.2.1) is ill-posed in the sense that the zero-diagonal masses give rise to infinite eigenvalues. The mass matrix can be \"regularized\" by the addition of small positive nonzero diagonal masses, in which case we return to the format of the generalized eigenvalue problem originally considered, or, alternatively, the zero-mass degrees of freedom can be eliminated by static condensation. To arrive at the statically condensed form, we expand (10.2.1): \\[ \\begin{array}{r} K_{11} \\Psi_{1}+K_{12} \\Psi_{2}-\\lambda M_{11} \\Psi_{1}=0 \\\\ K_{21} \\Psi_{1}+K_{22} \\Psi_{2}=0 \\tag{10.2.3} \\end{array} \\] The next step is to solve (10.2.3) for $\\Psi_{2}$ and then substitute in (10.2.2), which results in \\begin{equation*} \\left(\\boldsymbol{K}_{11}^{*}-\\lambda M_{11}\\right) \\Psi_{1}=0 \\tag{10.2.4} \\end{equation*} where \\begin{equation*} \\boldsymbol{K}_{11}^{*}=\\boldsymbol{K}_{11}-K_{12} K_{22}^{-1} K_{21} \\quad \\text { (statically condensed stiffness) } \\tag{10.2.5} \\end{equation*} The advantage of transforming to statically condensed form is that the problem size is reduced. However, $K_{11}^{*}$ tends to be full. Thus, unless the size of the nonzero-mass matrix is rather small, the reduction to statically condensed form may be uneconomical because the profile structure of $\\boldsymbol{K}$ is lost. Note that to calculate the statically condensed stiffness (10.2.5) efficiently, $\\boldsymbol{K}_{22}$ is never actually inverted. The following steps may be used in practice: \\begin{equation*} K_{22}=\\boldsymbol{U}^{T} D \\boldsymbol{U} \\quad \\text { (Crout factorization) } \\tag{10.2.6} \\end{equation*} \\begin{align*} \\boldsymbol{U} D^{1 / 2} \\boldsymbol{Z} & =\\boldsymbol{K}_{21} \\quad \\text { (solve for } \\boldsymbol{Z} \\text { ) } \\tag{10.2.7}\\\\ \\boldsymbol{K}_{12} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21} & =\\boldsymbol{Z}^{T} \\boldsymbol{Z} \\tag{10.2.8} \\end{align*} Equation (10.2.7) amounts to solution of an equation system with upper triangular coefficient array and multiple right-hand sides (i.e., the columns of $\\boldsymbol{K}_{21}$ ). For further computational considerations regarding static condensation, see [3]. \\subsection*{10.3 DISCRETE RAYLEIGH-RITZ REDUCTION} In the discrete Rayleigh-Ritz approach, static load patterns, $\\boldsymbol{P}$, are selected and corresponding displacement vectors, $\\boldsymbol{R}$, are calculated from:\\\\ where $n_{l p}$ refers to the number of load patterns. The displacements $R$, referred to as the trial vectors, are used to form the reduced eigenproblem, i.e., (10.1.7) by defining \\begin{align*} K^{*} & =\\boldsymbol{R}^{T} \\boldsymbol{K} \\boldsymbol{R} \\tag{10.3.2}\\\\ M^{*} & =\\boldsymbol{R}^{T} \\boldsymbol{M R} \\tag{10.3.3} \\end{align*} The load patterns, $\\boldsymbol{P}$, are usually subject to the following criteria:\\\\ i. The columns of $\\boldsymbol{P}$ should be linearly independent.\\\\ ii. The columns of $\\textbf{P}$ should be selected to arouse the low modes by activating the heaviest masses and most flexible areas of the model.\n",
      "chunk word length: 682, chunk char length: 6001, chunk = \\begin{enumerate} \\item If $n_{l p}$ is small, $K^{*}$ and $M^{*}$ will be full, but small. \\item Equation (10.3.1) amounts to solution of a multiple right-hand-side system with profile coefficient matrix. \\item The discrete Rayleigh-Ritz procedure is a general formalism for obtaining the reduced system. However, the guidelines for selecting $\\boldsymbol{P}$ are somewhat vague and thus a more systematic strategy is required for practical use. \\item The discrete Rayleigh-Ritz reduction is often referred to as a \"projection method.\" \\item The eigenvector approximations are defined by $\\boldsymbol{\\Psi \\cong R \\Psi^*}$. \\item By virtue of the fact that the calculation of trial vectors from load patterns requires solution of (10.3.1), $\\boldsymbol{K}$ must be nonsingular. This precludes application to cases in which there are zero eigenvalues (e.g., structures which possess rigid body modes). A simple reformulation of the original problem involving a positive shifting of the eigenvalues allows us to handle this case: Note that by adding and subtracting $\\boldsymbol{\\alpha M} \\boldsymbol{\\Psi}$, where $\\boldsymbol{\\alpha}$ is a positive number, we produce an eigenproblem \\end{enumerate} \\begin{align*} (\\boldsymbol{\\overline{K}}-\\bar{\\lambda} \\boldsymbol{M}) \\Psi & =0 \\tag{10.3.4}\\\\ \\boldsymbol{\\overline{K}} & =\\boldsymbol{K}+\\alpha \\boldsymbol{M} \\tag{10.3.5}\\\\ \\bar{\\lambda} & =\\lambda+\\alpha \\tag{10.3.6} \\end{align*} in which $\\overline{\\boldsymbol{K}}$ is positive-definite, and the eigenvectors are unchanged. The eigenvalues are related by (10.3.6). This stratagem may also be employed if some of the $\\lambda$ 's are negative. Suppose an estimate of the smallest eigenvalue, $\\lambda_{1}$, is available. Then select $\\alpha>0$ such that $-\\alpha<\\lambda_{1} \\leq 0$. The $\\bar{\\lambda}$ 's are then all positive and solution may proceed as usual.\\\\ 7. Shifting is a particular example of a general class of spectral transformations. Let \\begin{equation*} S=\\left[\\Psi_{1}, \\Psi_{2}, \\ldots, \\Psi_{n_{e q}}\\right] \\tag{10.3.7} \\end{equation*} and \\begin{equation*} \\mathbf{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n_{\\text {eq }}}\\right) \\tag{10.3.8} \\end{equation*} Then it is easily verified that \\begin{equation*} \\boldsymbol{K}=\\boldsymbol{S} \\boldsymbol{\\Lambda} \\boldsymbol{S}^{\\boldsymbol{T}} \\tag{10.3.9} \\end{equation*} and \\begin{equation*} \\boldsymbol{M=S S^{T}} \\tag{10.3.10} \\end{equation*} Let $f=f(\\lambda)$ represent a scalar-valued function of $\\lambda$. Define a matrix-valued function $\\boldsymbol{f}=\\boldsymbol{f}(\\boldsymbol{K})$ by way of \\begin{equation*} f(K)=S \\operatorname{diag}\\left(f\\left(\\lambda_{1}\\right), f\\left(\\lambda_{2}\\right), \\ldots, f\\left(\\lambda_{n_{\\text {eq }}}\\right)\\right) S^{T} \\tag{10.3.11} \\end{equation*} Then it is readily established that solutions of the eigenproblem \\begin{equation*} (f(K)-\\bar{\\lambda} M) \\phi=0 \\tag{10.3.12} \\end{equation*} are related to solutions of \\begin{equation*} (\\boldsymbol{K}-\\lambda M) \\boldsymbol{\\Psi}=\\mathbf{0} \\tag{10.3.13} \\end{equation*} by \\begin{align*} \\bar{\\lambda} & =f(\\lambda) \\tag{10.3.14}\\\\ \\phi & =\\psi \\tag{10.3.15} \\end{align*} \\textbf{Exercise 1.} Consider the partitioned eigenproblem defined by (10.2.1). Show that the statically condensed eigenproblem, (10.2.4) and (10.2.5), can be obtained from the discrete Rayleigh-Ritz approach by selecting the trial vectors to be \\[ R=\\left[\\begin{array}{ccc} \\quad \\quad \\quad \\boldsymbol{I} & \\tag{10.3.16}\\\\ \\hdashline-\\boldsymbol{K}_{22}^{1} & \\boldsymbol{K}_{21} \\end{array}\\right] \\] \\subsection*{10.4 IRONS-GUYAN REDUCTION} The Irons-Guyan reduction [4-6] fits into the discrete Rayleigh-Ritz framework described in the previous section. To understand the method it is helpful to rewrite the generalized eigenproblem in partitioned form: \\[ \\left(\\left[\\begin{array}{ll} K_{11} & K_{12} \\tag{10.4.1}\\\\ K_{21} & K_{22} \\end{array}\\right]-\\lambda\\left[\\begin{array}{ll} M_{11} & M_{12} \\\\ M_{21} & M_{22} \\end{array}\\right]\\right)\\left\\{\\begin{array}{l} \\psi_{1} \\\\ \\psi_{2} \\end{array}\\right\\}=0 \\] The degrees of freedom in $\\psi_{1}$ are to be retained in the reduced eigenproblem, whereas those in $\\psi_{2}$ are to be eliminated. For these purposes $R$ is defined by (10.3.16), which leads to the definitions of the reduced arrays (verify!): \\begin{align*} & \\boldsymbol{K}^{*}=\\boldsymbol{R}^{T} \\boldsymbol{K} \\boldsymbol{R}=\\boldsymbol{K}_{11}-\\boldsymbol{K}_{12} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21} \\tag{10.4.2}\\\\ & \\boldsymbol{M}^{*}=\\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{R}=\\boldsymbol{M}_{11}-\\boldsymbol{M}_{12} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21}-\\boldsymbol{K}_{12} \\boldsymbol{K}_{22}^{-1}\\left(\\boldsymbol{M}_{21}-\\boldsymbol{M}_{22} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21}\\right) \\tag{10.4.3} \\end{align*} Note that (10.4.2) is the statically condensed stiffness and, if $M_{22}=0$ and $M_{12}=$ $\\boldsymbol{M}_{21}^{T}=0$, then $\\boldsymbol{M}^{*}=\\boldsymbol{M}_{11}$ as in the statically condensed problem of Sec. 10.2. The Irons-Guyan reduction may be thought of as invoking a \"static relation,\" namely \\begin{equation*} \\psi_{2}=-K_{22}^{-1} K_{21} \\psi_{1} \\tag{10.4.4} \\end{equation*} to eliminate the unwanted variables. Sometimes $\\psi_{1}$ and $\\psi_{2}$ are referred to as the \"master\" and \"slave\" degrees of freedom, respectively. In practice, the following guidelines are suggested:\\\\ i. Determine the number of eigenvalues and eigenvectors required, say $\\boldsymbol{n}_{\\text {modes }}$.\\\\ ii. Retain a multiple of $n_{\\text {modes }}$ degrees of freedom (e.g., $5 \\times n_{\\text {modes }}$ degrees of freedom).\\\\ iii. The degrees of freedom to be retained are identified by calculating the ratios of diagonal elements in $M$ and $K$, namely, $K_{p p} / M_{p p} 1 \\leq p \\leq n_{e q}$. The degrees of freedom with the smallest ratios are retained (i.e., they are the degrees of freedom in $\\psi_{1} ;$ [7]).\n",
      "chunk word length: 195, chunk char length: 1433, chunk = The calculation of the reduced mass, (10.4.3), is costly! \\subsection*{10.5 SUBSPACE ITERATION} A disadvantage of reduction techniques such as the Irons-Guyan procedure is that there is no guarantee that the eigenvalues and eigenvectors of the reduced problem,\\\\ namely, $\\lambda_l^*$ and $R \\psi_l^*$, will be good approximations of those of the original problem, $\\lambda_{l}$ and $\\psi_{l}$, respectively. Consequently, methods have been developed in which a reduced problem is used along with an iterative strategy to obtain exactly the lower modes of the generalized eigenproblem. This is the underlying idea of the subspace iteration, or block power, method, which is widely used for large-scale finite element calculations. Roughly speaking, the procedure is as follows: Load patterns are selected and trial vectors are calculated. The trial vectors are used to form a reduced problem, which is solved. New load patterns are calculated from the \"inertial\" forces engendered by the eigenvectors of the reduced problem, namely, \\begin{equation*} \\underbrace{\\boldsymbol{P}}_{n_{e q} \\times n_{lp}}= \\boldsymbol{MR}\\left[\\psi_{1}^{*}, \\psi_{2}^{*}, \\ldots, \\psi_{n_{lp}}^{*}\\right] \\tag{10.5.1} \\end{equation*} With these load patterns the process is repeated until convergence is achieved. The calculations are summarized in the flowchart contained in Table 10.5.1. Extensive discussion and further details are presented in [8].\n",
      "chunk word length: 128, chunk char length: 1303, chunk = I. Initialization \\begin{enumerate} \\item Form $K$ and $M$. \\item Factorize $\\boldsymbol{K}=\\boldsymbol{U}^{\\boldsymbol{T}} \\boldsymbol{D} \\boldsymbol{U}$. \\item Specify initial load patterns, $P$.\\\\ II. Iteration \\item Solve for trial vectors: \\end{enumerate} $$ \\left(\\boldsymbol{U}^{\\boldsymbol{T}} \\boldsymbol{D} \\boldsymbol{U}\\right) \\boldsymbol{R}=\\boldsymbol{P} $$ \\begin{enumerate} \\setcounter{enumi}{1} \\item Compute reduced matrices: \\end{enumerate} $$ \\begin{aligned} K^{*} & =R^{T} \\boldsymbol{K R}=R^{T} P \\\\ M^{*} & =R^{T} M R \\end{aligned} $$ \\begin{enumerate} \\setcounter{enumi}{2} \\item Solve the reduced eigenproblem: \\end{enumerate} $$ \\left(K^{*}-\\lambda_{k}^{*} M^{*}\\right) \\psi_{k}^{*}=0, \\quad k=1,2, \\ldots, n_{lp} $$ \\begin{enumerate} \\setcounter{enumi}{3} \\item Calculate approximations to the eigenvectors of the original system: \\end{enumerate} $$ R\\left[\\psi_{1}^{*}, \\psi_{2}^{*}, \\ldots, \\psi_{n_{lp}}^{*}\\right] $$ \\begin{enumerate} \\setcounter{enumi}{4} \\item Perform convergence checks. If the desired eigenvalues have converged, stop. Otherwise continue. \\item Calculate improved load patterns: \\end{enumerate} $$ P=M R\\left[\\psi_{1}^{*}, \\psi_{2}^{*}, \\ldots, \\psi_{n_{lp}}^{*}\\right] $$ \\begin{enumerate} \\setcounter{enumi}{6} \\item Go to Step II-1. \\end{enumerate}\n",
      "chunk word length: 410, chunk char length: 2804, chunk = \\begin{enumerate} \\item It is recommended from practical experience [8] that:\\\\ i. The number of load patterns employed should be calculated from\\\\ $n_{l p}=\\min \\left\\{2 n_{\\text {modes }}, n_{\\text {modes }}+8\\right\\}$, where $n_{\\text {modes }}$ is the number of eigenpairs desired.\\\\ ii. In the first load pattern (i.e., first column of $P$ ) a 1 should be placed in the entry corresponding to the minimum value of $K_{p p} / M_{p p}$, and 0 in the remaining entries. Likewise, a 1 should be placed in the entry of the second column corresponding to the next smallest value of $K_{p p} / M_{p p}$, and so forth. \\item Wilson [9] recommends that one load pattern be generated from random numbers in the interval $[0,1]$. \\item The convergence condition may be specified in terms of consecutive iterates: \\end{enumerate} $$ \\frac{\\left(\\lambda_{k}^{*}\\right)_{l+1}-\\left(\\lambda_{k}^{*}\\right)_{l}}{\\left(\\lambda_{k}^{*}\\right)_{l}} \\leq \\epsilon, \\quad k=1,2, \\ldots, n_{\\text {modes }} $$ where $I$ is the iteration number and $\\epsilon$ is a preassigned error tolerance.\\\\ 4. Solution of the generalized eigenproblem in the subspace iteration algorithm (Step II-3 in Table 10.5.1) is efficiently carried out by way of the generalized Jacobi method [8]. The reason for this is that after a number of iteration steps, the generalized eigenproblem possesses diagonally dominant matrices for which the generalized Jacobi method proves very effective. \\subsection*{10.5.1 Spectrum Slicing} The convergence of a procedure such as subspace iteration does not necessarily guarantee that the first $n_{\\text {modes }}$ eigenpairs of the original system have been found. An eigenvalue can be missed if the original trial vectors are orthogonal to the corresponding eigenvector. To ascertain whether or not this has occurred, a spectrum slicing may be performed.\\footnote[3]{Spectrum slicing is sometimes referred to as a Sturm sequence check [8]. } The steps involved are as follows: Perform a Crout factorization of the matrix $K-\\alpha M$, where $\\alpha=(1+\\delta) \\lambda_{n_{\\text {modes }}}$ and $\\delta$ is a small positive number such that $(1+\\delta) \\lambda_{n_{\\text {modes }}}<\\lambda_{n_{\\text {modes }+1}}$. Let $D_{\\alpha}$ denote the diagonal matrix of pivots obtained. It follows from Sylvester's inertia theorem that the number of eigenvalues smaller than $\\alpha$ will equal the number of negative entries in $D_{\\alpha}$. If this number, say $n_{\\alpha}$, is greater than $n_{\\text {modes }}$, then $n_{\\alpha}-n_{\\text {modes }}$ eigenvalues smaller than $\\alpha$ were missed in the calculation. If this is the case, a revised set of load patterns must be employed and the eigensolution repeated. If $n_{\\alpha}=n_{\\text {modes }}$, then a degree of confidence in the solution is attained.\n",
      "chunk word length: 325, chunk char length: 2390, chunk = Spectrum slicing can be used to determine the number of eigenvalues in an interval $] \\alpha, \\beta\\left[\\right.$, where $\\beta>\\alpha$. Factorize $K-\\alpha M$ and $K-\\beta M$. If $D_{\\alpha}$ and $D_{\\beta}$ are the corresponding pivots and $n_{\\alpha}$ and $n_{\\beta}$ are the number of negative entries of $D_{\\alpha}$ and $D_{\\beta}$, respectively, then $n_{\\beta}-n_{\\alpha}$ is the number of eigenvalues in $] \\alpha, \\beta[$. \\subsection*{10.6.2 Inverse Iteration} When the number of load patterns used in the subspace iteration procedure is one, the process is called inverse iteration. In this case convergence to the lowest eigenvalue occurs as long as the initial trial vector is not orthogonal to the corresponding eigenvector. By reversing the roles of $\\boldsymbol{K}$ and $M$, convergence to the largest eigenvalue, $\\lambda_{n e q}$, can be achieved. This may be seen from: $$ 0=(K-\\lambda M) \\psi=\\left(M-\\lambda^{-1} K\\right) \\psi $$ Note that when working with the latter form, subspace iteration requires $M$ to be nonsingular. \\textbf{Exercise 1.} Consider the following eigenvalue problem: $$ (K-\\lambda M) \\Psi=0, \\quad \\lambda=\\omega^{2} $$ where $$ \\begin{aligned} M & =\\left[\\begin{array}{cc} m_{1} & 0 \\\\ 0 & m_{2} \\end{array}\\right] \\\\ K & =\\left[\\begin{array}{cc} \\left(k_{1}+k_{2}\\right) & -k_{2} \\\\ -k_{2} & k_{2} \\end{array}\\right] \\\\ \\psi & =\\left\\{\\begin{array}{l} \\psi_{1} \\\\ \\psi_{2} \\end{array}\\right\\} \\end{aligned} $$ Assume $k_{1}=k_{2}=1, m_{1}=3, m_{2}=2$.\\\\ a. Calculate the frequencies (i.e., $\\omega_{1}, \\omega_{2}$ ) and mode shapes (i.e., $\\psi_{1}, \\psi_{2}$ ).\\\\ b. Assuming that the fundamental mode load pattern is given approximately by $$ P=\\left\\{\\begin{array}{c} \\frac{m_{1}}{\\left(k_{1}+k_{2}\\right)} \\\\ \\frac{m_{2}}{k_{2}} \\end{array}\\right\\} $$ use the discrete Rayleigh-Ritz reduction procedure to obtain an estimate of the fundamental frequency and mode shape.\\\\ c. Use the Irons-Guyan procedure to reduce the problem to one degree of freedom. Pick the degree of freedom to be retained according to the criterion presented in Sec. 10.4. Determine the approximate fundamental frequency and mode shape.\\\\ d. Use the subspace iteration procedure to calculate the fundamental frequency and mode shape. Initialize the computations with the load pattern $$ P=\\left\\{\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right\\} $$ Employ two iterations.\n",
      "chunk word length: 344, chunk char length: 3253, chunk = a. $$ \\begin{aligned} M & =\\left[\\begin{array}{ll} 3 & 0 \\\\ 0 & 2 \\end{array}\\right] \\quad K=\\left[\\begin{array}{rr} 2 & -1 \\\\ -1 & 1 \\end{array}\\right] \\\\ 0 & =\\operatorname{det}\\left[\\begin{array}{cc} 2-3 \\lambda & -1 \\\\ -1 & 1-2 \\lambda \\end{array}\\right]=6 \\lambda^{2}-7 \\lambda+1 \\\\ \\lambda_{1,2} & =\\frac{1}{6}, 1 \\end{aligned} $$ $$ \\begin{aligned} & \\omega_{1,2}=\\frac{1}{\\sqrt{6}}, 1=0.40825,1 \\\\ & \\psi_{1}=\\left\\{\\begin{array}{l} \\frac{2}{3} \\\\ 1 \\end{array}\\right\\} \\quad \\psi_{2}=\\left\\{\\begin{array}{r} 1 \\\\ -1 \\end{array}\\right\\} \\end{aligned} $$ b. $$ P=\\left\\{\\begin{array}{l} \\frac{3}{2} \\\\ 2 \\end{array}\\right\\} $$ $$ \\begin{aligned} \\boldsymbol{KR} & =\\boldsymbol{P} \\\\ \\boldsymbol{R} & =\\left\\{\\begin{array}{c} \\frac{7}{2} \\\\ \\frac{11}{2} \\end{array}\\right\\} \\quad \\text { (the } \\frac{1}{2} \\text { factors may be neglected) } \\\\ \\boldsymbol{K}^{*} & =\\boldsymbol{R}^{T} \\boldsymbol{K} \\boldsymbol{R}=\\left\\langle\\begin{array}{ll} 7 & 11 \\end{array}\\right\\rangle\\left[\\begin{array}{rr} 2 & -1 \\\\ -1 & 1 \\end{array}\\right]\\left\\{\\begin{array}{c} 7 \\\\ 11 \\end{array}\\right\\}=65 \\\\ \\textbf{M}^{*} & =\\boldsymbol{R}^{T} M \\boldsymbol{R}=\\left\\langle\\begin{array}{ll} 7 & 11 \\end{array}\\right\\rangle\\left[\\begin{array}{ll} 3 & 0 \\\\ 0 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} 7 \\\\ 11 \\end{array}\\right\\}=389 \\end{aligned} $$ $$ \\left(K^{*}-\\lambda^{*} M^{*}\\right) \\psi^{*}=0 \\Rightarrow \\lambda^{*}=\\frac{65}{389}, \\quad \\psi^{*}=1 $$ $$ \\psi_{1} \\cong R \\psi_{1}^{*}=\\frac{1}{11}\\left\\{\\begin{array}{c} 7 \\\\ 11 \\end{array}\\right\\}=\\left\\{\\begin{array}{c} 0.64 \\\\ 1 \\end{array}\\right\\} ; \\quad \\omega^{*}=\\left(\\frac{65}{389}\\right)^{1 / 2}=0.4087 $$ c. \\[ \\left. \\begin{aligned} \\frac{K_{11}}{M_{11}} &= \\frac{2}{3} \\\\ \\frac{K_{22}}{M_{22}} &= \\frac{1}{2} \\end{aligned} \\right\\} \\quad \\Rightarrow \\text{ retain degree of freedom number 2} \\] Reorder equations into the standard partitioned form. $$ \\begin{aligned} K & =\\left[\\begin{array}{rr} 1 & -1 \\\\ -1 & 2 \\end{array}\\right] \\quad M=\\left[\\begin{array}{ll} 2 & 0 \\\\ 0 & 3 \\end{array}\\right] \\\\ R & =\\left\\{\\begin{array}{c} 1 \\\\ -K_{22}^{-1} K_{21} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} 1 \\\\ \\frac{1}{2} \\end{array}\\right\\} \\\\ K^{*} & =R^{T} K R=\\frac{1}{2} \\\\ M^{*} & =R^{T} M R=\\frac{11}{4} \\end{aligned} $$ $$ \\lambda^{*}=\\frac{2}{11}, \\quad \\omega^{*}=\\left(\\frac{2}{11}\\right)^{1 / 2}=0.4264 \\quad \\psi_{1}^{*}=1 $$ $$ \\psi_{1} \\cong R \\psi_{1}^{*}=\\left\\{\\begin{array}{l} 1 \\\\ \\frac{1}{2} \\end{array}\\right\\}, \\quad \\text { (reorder): } $$ $$ \\Psi_{1}=\\left\\{\\begin{array}{l} \\frac{1}{2} \\\\ 1 \\end{array}\\right\\} $$ d. $P=\\left\\{\\begin{array}{l}0 \\\\ 1\\end{array}\\right\\} ; \\quad \\boldsymbol{K} \\boldsymbol{R}=P ; \\quad \\boldsymbol{R}=\\left\\{\\begin{array}{l}1 \\\\ 2\\end{array}\\right\\}$ $\\left.\\begin{array}{l}K^{*}=2 ; \\quad M^{*}=11 ; \\quad \\lambda^{*}=\\frac{2}{11} \\quad \\text { (same as (c)) } \\\\ \\omega^{*}=0.4264 \\quad \\psi_{1} \\cong\\left\\{\\begin{array}{l}\\frac{1}{2} \\\\ 1\\end{array}\\right\\}\\end{array}\\right\\}$ iteration number 1 \\\\ $\\boldsymbol{P}=\\boldsymbol{M} \\psi_{1}=\\left\\{\\begin{array}{l}\\frac{3}{2} \\\\ 2\\end{array}\\right\\} \\quad$ (same as (b))\\\\ Therefore $$ \\omega *=0.4087 \\quad \\psi_{1} \\cong\\left\\{\\begin{array}{c} 0.64 \\\\ 1 \\end{array}\\right\\} $$\n",
      "chunk word length: 1419, chunk char length: 10023, chunk = \\subsection*{10.6.1 Introduction} The Lanczos algorithm was first proposed in 1950. Lanczos intended his algorithm to be used for computing a few of the extreme eigenvalues and corresponding eigenvectors of a symmetric matrix. However, the algorithm was taken up as a method for reducing a symmetric matrix to tridiagonal form. Lanczos himself observed that round-off error has a significant effect on the algorithm's performance and expensive modifications seemed to be necessary to overcome this defect. By 1955 the Householder method replaced the Lanczos algorithm as a more efficient method for tridiagonalizing a matrix. Most engineering applications require only a few eigenvalues at one end of the spectrum, but for small problems the Householder-QR algorithm can find all the eigenvalues almost as fast as a few. For large problems however it is a waste to tridiagonalize the whole matrix. The Lanczos iteration has the property that well isolated eigenvalues are approximated accurately after a comparatively small number of steps. For example, the largest eigenvalues will often be captured after 20 steps, almost independently of the order of the problem, $n$. Moreover, the number of steps taken by the Lanczos method to compute these eigenvalues will always be less than that for the power method. This property makes the Lanczos algorithm very efficient. However, the eigenvalues most frequently wanted are the smaller ones. The Lanczos algorithm is so powerful that it can compute the smaller eigenvalues of a matrix without any factorization. However, they will not be well approximated until nearly $n$ steps have been taken. Consequently it is necessary, in these cases, to apply the iteration to an inverted form of the matrix. In this respect the Lanczos algorithm is just like the subspace iteration method. See [15] for a comparison of Lanczos and subspace iteration algorithms. In this section we are concerned with the application of the Lanczos algorithm to the generalized symmetric eigenproblem \\begin{equation*} (K-\\lambda M) z=0 \\tag{10.6.1} \\end{equation*} where $\\boldsymbol{K}$ and $\\boldsymbol{M}$ are $\\boldsymbol{n} \\times \\boldsymbol{n}$ real symmetric matrices. We assume for the moment that the eigenvalues of (10.6.1) are real. Later we state the conditions on $K$ and $M$ that must hold to ensure real eigenvalues. We will derive the generalized Lanczos algorithm for the solution of the eigenproblem (10.6.1). The relation between the Lanczos method and vector iteration methods will be established. We then look at the effect of round-off on the algorithm and the resulting loss of orthogonality. We consider two possible modifications to the algorithm that can maintain a desired level of orthogonality among the Lanczos vectors. Finally, we give a detailed description of the algorithm we prefer together with a listing of the computer subprograms. \\subsection*{10.6.2 Spectral Transformation} We turn now to an important misconception concerning the Lanczos algorithm. It is usually thought of as a way of computing eigenpairs, $(\\lambda, z)$, of the standard eigenproblem \\begin{equation*} (A-\\lambda I) z=0 \\tag{10.6.2} \\end{equation*} The Lanczos method is so powerful that one can work directly with $\\boldsymbol{A}$ to evaluate eigenvalues at both ends of the spectrum of (10.6.2) without solving any system of equations. Only products of $\\boldsymbol{A}$ with a sequence of vectors need be computed. This virtue blinded certain users of the Lanczos method to the great advantages to be gained by a shift-and-invert procedure. The matrix $(\\boldsymbol{A}-\\boldsymbol{\\sigma I})^{-1}$ has the same eigenvectors as $\\boldsymbol{A}$. Using $(\\boldsymbol{A}-\\boldsymbol{\\sigma I})^{-1}$ instead of $\\boldsymbol{A}$ will cause eigenvectors corresponding to eigenvalues close to $\\sigma$ to converge much more quickly. Of course there are cases when factoring of $A$ is not possible (e.g., when the matrix is only known implicitly) or not desirable (e.g., when $\\boldsymbol{A}$ has a given sparsity structure that can be destroyed when factored). However, we are more interested in the generalized eigenproblem (10.6.1). For this problem some form of inversion or factoring of a matrix, either explicitly or implicitly, is required. It is interesting to note that when the structure of $M$ is the same as that of $\\boldsymbol{K}$, as in the case of a consistent mass matrix, then the cost of one step of the power iteration method is exactly the same as that for the inverse iteration method. Each requires one triangular factorization. There are two different procedures for transforming the generalized eigenproblem to the standard form:\\\\ i. Factor $\\boldsymbol{M}$ into $\\boldsymbol{M}=\\boldsymbol{C C}^{\\boldsymbol{T}}$. This is the Cholesky factorization of $\\boldsymbol{M}$. Then \\begin{equation*} (K-\\lambda M)=C\\left(C^{-1} K C^{-T}-\\lambda I\\right) C^{T} \\tag{10.6.3} \\end{equation*} and the eigenproblem of $(10.6 .1)$ reduces to the standard form $\\left(A_{1}-\\lambda I\\right) \\hat{\\mathbf{z}}=0$ where $\\boldsymbol{A}_{1}=\\boldsymbol{C}^{-1} \\boldsymbol{K} \\boldsymbol{C}^{-\\boldsymbol{T}}$ and $\\hat{\\boldsymbol{z}}=\\boldsymbol{C}^{\\boldsymbol{T}} \\boldsymbol{z}$. The eigenvalues of the standard problem are the same as those of the generalized problem. Note that if $M$ is positive-semidefinite, then its Cholesky factors are singular and this transformation can not be performed.\\\\ ii. Using a similar procedure, (10.6.1) can be transformed into \\begin{equation*} \\left(A_{2}-\\mu I\\right) \\hat{z}=0 \\tag{10.6.4} \\end{equation*} where $A_{2}=C^{T} \\boldsymbol{K}^{-1} C$ and $\\mu=1 / \\lambda$. Note that in this case both the eigenvalues and the eigenvectors of $A_{2}$ are different from those of (10.6.1). A more general form of the second transformation is to first perform a shift of the origin, $\\left(\\boldsymbol{K}_{\\sigma}-(\\lambda-\\sigma) M\\right) z=0$, where $\\boldsymbol{K}_{\\sigma}=\\boldsymbol{K}-\\sigma \\boldsymbol{M}$, and then perform (ii). This results in a standard eigenproblem with $\\boldsymbol{A}_{\\sigma}=\\boldsymbol{C}^{\\boldsymbol{T}} \\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{C}$. The spectrum of $\\boldsymbol{A}_{\\sigma}$ is related to the original spectrum through \\begin{equation*} \\nu=\\frac{1}{\\lambda-\\sigma} \\tag{10.6.5} \\end{equation*} where $v$ is the eigenvalue of $\\boldsymbol{A}_{\\sigma}[11]$. The second procedure requires two triangular factorizations, one for $M$ and one for $\\boldsymbol{K}$. It is possible to avoid the factorization of $\\boldsymbol{M}$ by working with \\begin{equation*} \\left(K_{\\sigma}^{-1} M-\\nu I\\right) z=0 \\tag{10.6.6} \\end{equation*} Although the matrix of this transformation is not symmetric, it is self-adjoint with respect to the inertial inner product defined by \\begin{equation*} (u, v)_{M}=v^{T} M u \\tag{10.6.7} \\end{equation*} which is shown in the following steps: $$ \\begin{aligned} \\left(K_{\\sigma}^{-1} M u, v\\right)_{M} & =v^{\\top} M K_{\\sigma}^{-1} M u \\\\ & =\\left(u, K_{\\sigma}^{-1} M v\\right)_{M} \\end{aligned} $$ This unsymmetric form is particularly advantageous since it has the same eigenvectors as the original problem [16]. The algorithm that is derived later employs the transformation of equation (10.6.6). \\subsection*{10.6.3 Conditions for Real Eigenvalues} Contrary to common belief, the fact that $K$ and $M$ are symmetric is not a sufficient condition to ensure real eigenvalues for (10.6.1). This can be illustrated using the following simple example. \\subsubsection*{Example 1} Let $K=\\left[\\begin{array}{ll}1 & 1 \\\\ 1 & 0\\end{array}\\right]$ and $M=\\left[\\begin{array}{rr}1 & 0 \\\\ 0 & -1\\end{array}\\right]$. Then the eigenvalues of (10.6.1) with these matrices are $\\frac{1}{2}(1 \\pm i \\sqrt{3})$, where $i^{2}=-1$. The eigenvalues of (10.6.1) are all real if some linear combination of $K$ and $M$ is positive-definite; that is $\\boldsymbol{\\rho K}+ \\boldsymbol{\\tau M}$ is positive-definite for some choice of real $\\rho$ and $\\tau$. This is only a sufficient condition that guarantees real eigenvalues, but there is no easy way of computing $\\rho$ and $\\tau$. \\textbf{Exercise 1.} Prove that $\\rho K+\\tau M$ being positive definite for some choice of $\\rho$ and $\\tau$ is only a sufficient condition for (10.6.1) to have real eigenvalues. Solution Multiply (10.6.1) by $\\rho$ and shift the spectrum by $\\tau$ : $$ \\begin{aligned} 0 & =(\\rho K-\\rho \\lambda M) z \\\\ & =(\\rho K+\\tau M-(\\rho \\lambda+\\tau) M) z \\end{aligned} $$ Because $\\boldsymbol{\\rho K}+\\tau \\boldsymbol{M}$ is positive definite, it possesses a Cholesky decomposition; i.e., $\\boldsymbol{\\rho K}+\\tau \\boldsymbol{M}=\\boldsymbol{C} C^{\\boldsymbol{T}}$. Thus, the above equation can be reduced to standard form: $$ (A-\\mu I) \\hat{z}=0 $$ where $A=C^{-1} M C^{-T}, \\hat{z}=C^{T} z$, and $\\mu=1 /(\\rho \\lambda+\\tau)$. By the symmetry of $A$, the $\\mu^{\\prime}$ s\\\\ are real. Consequently, so are the $\\lambda$ 's. This establishes \"sufficiency\". Now consider the following case. Let $$ K=\\left[\\begin{array}{rrr} 1 & 0 & 0 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & 3 \\end{array}\\right], \\quad \\text { and } \\quad M=\\left[\\begin{array}{rrr} -3 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -1 \\end{array}\\right] $$ Clearly, the eigenvalues of this problem are real. However, there is no linear combination of $K$ and $M$ that is positive definite. This establishes that the above is not a necessary condition. Fortunately, in many problems encountered in structural mechanics, either $\\boldsymbol{K}$ or $M$ or both are positive-definite. \\subsection*{10.6.4 The Rayleigh-Ritz Approximation} Consider a given set of vectors, $X_{m}=\\left[x_{1}, x_{2}, \\cdots, x_{m}\\right]$, with $m \\ll n$. We refer to these as trial vectors. We proceed to obtain an approximation to some of the eigenvectors of (10.6.1) by taking a linear combination of the trial vectors. Let $y=X_{m} s=$ $\\sum_{i=1}^{m} x_{i} s_{i}$ be the desired approximation. The $i$th component of $s$ is the coefficient of $x_{i}$ in this representation of $y$. We denote the\n",
      "chunk word length: 1762, chunk char length: 14379, chunk = corresponding approximation to the eigenvalues of (10.6.1) by $\\theta$. The residual, $r$, associated with the approximating pair $\\{\\theta, \\boldsymbol{y}\\}$ is given by \\begin{equation*} r=K y-\\theta M y \\tag{10.6.8} \\end{equation*} The Rayleigh-Ritz method requires the residual vector be orthogonal to each of the trial vectors; i.e., $$ X_{m}^{T} r=X_{m}^{T} K y-\\theta X_{m}^{T} M y=0 $$ Substituting the representation of $y$ in terms of the trial vectors, in the above orthogonality condition for $r$, we obtain the reduced eigenproblem $$ \\left(K_{m}-\\theta M_{m}\\right) s=0 $$ where $K_{m}=X_{m}^{T} K X_{m}$ and $M_{m}=X_{m}^{T} M X_{m}$. The eigenvector of the reduced eigenproblem, $s^{(k)}$, will determine the approximation, $y^{(k)}$, to the eigenvector of (10.6.1). We refer to $\\boldsymbol{y}^{(k)}$ as the Ritz vector and to its corresponding $\\theta^{(k)}$ as the Ritz value, for each $k=1, \\cdots, m.$\\footnote[4]{Some authors refer to the trial vectors as Ritz vectors. In keeping with more prevalent usage, we prefer to reserve the latter terminology for the $y$ 's. } \\subsection*{10.6.5 Derivation of The Lanczos Algorithm} The Lanczos method can be thought of as a means of constructing an orthogonal set of vectors, known as Lanczos vectors, for use in the Rayleigh-Ritz procedure. The algorithm is closely related to the inverse iteration and power methods for calculating a single eigenpair. Given a pair of matrices $\\boldsymbol{K}_{\\sigma}=\\boldsymbol{K}-\\sigma \\boldsymbol{M}$ and $\\boldsymbol{M}$, and a starting vector $\\boldsymbol{r}$, these basic methods generate a sequence of vectors, $\\left\\{\\boldsymbol{r}, \\boldsymbol{K}_{\\sigma}^{-1} M r\\right.$, $\\left.\\left(\\boldsymbol{K}_{\\sigma}^{-1} M\\right)^{2} \\boldsymbol{r}, \\ldots,\\left(\\boldsymbol{K}_{\\sigma}^{-1} M\\right)^{j} \\boldsymbol{r}\\right\\}$, during $j$ iterations. These vectors are referred to as the Krylov sequence; the sequence converges (as $j \\rightarrow \\infty$ ) to the eigenvector corresponding to the eigenvalue, $\\lambda$, of ( 10.6 .1 ) closest to the shift $\\sigma$. The basic difference between the Lanczos method and the other two is that the information contained in each successive vector of the Krylov sequence is used to obtain the best approximation to the wanted eigenvectors instead of using only the last vector in the sequence. To be more specific, the Lanczos algorithm is equivalent to obtaining the Rayleigh-Ritz approximation with the vectors in the Krylov sequence as the trial vectors. This involves supplementing the Krylov sequence with the GramSchmidt orthogonalization process at each step. The result is a set of $M$-orthonormal vectors (the Lanczos vectors) that is used in the Rayleigh-Ritz procedure to reduce the dimension of the eigenproblem. We show below that orthogonalization is required only with respect to two preceding vectors; a fact recognized by Lanczos. The Rayleigh-Ritz procedure with the $\\boldsymbol{M}$-orthonormal basis of the Krylov subspace leads to a standard eigenproblem with a tridiagonal matrix. To derive the Lanczos algorithm it will be assumed for the moment that the first $j$ Lanczos vectors, $\\left\\{q_{1}, q_{2}, \\cdots, q_{j}\\right\\}$ have been found, and the construction of the $j+1$ vector will be described. The resulting vectors all satisfy the condition $\\boldsymbol{q}_{i}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{q}_{j}=\\boldsymbol{\\delta}_{i j}$, where $\\delta_{i j}$ is the Kronecker delta; that is, the vectors are orthonormal with respect to the mass matrix. To calculate $\\boldsymbol{q}_{j+1}$, we must orthogonalize $\\boldsymbol{v}_{j}=\\left(\\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} \\boldsymbol{M}\\right)^{j} \\boldsymbol{r}$ against the $j$ Lanczos vectors computed so far. From the definition of $v_{j}$ we obtain $\\boldsymbol{v}_{j}=\\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{M} \\boldsymbol{v}_{j-1}$. Now, $\\boldsymbol{v}_{j-1}$ is the vector that is $\\boldsymbol{M}$-orthonormalized against the first $j-1$ Lanczos vectors to obtain $\\boldsymbol{q}_{j}$. Therefore $$ v_{j-1}=\\sum_{i=1}^{j} v_{i} q_{i} $$ where $v_{i}$ is the component of $v_{j-1}$ along $q_{i}$. This result is used to eliminate $v_{j-1}$ in the above recursive relation for $\\boldsymbol{v}_{j}$. We then get $$ \\begin{aligned} \\boldsymbol{v}_{j} & =\\sum_{i=1}^{j} v_{i} \\boldsymbol{K}_{\\sigma}^{-1} M q_{i} \\\\ & =v_{j} \\boldsymbol{K}_{\\sigma}^{-1} M q_{j}+\\sum_{i=1}^{j-1} v_{i} \\boldsymbol{K}_{\\sigma}^{-1} M q_{i} \\end{aligned} $$ Observe that each vector, $\\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} \\mathbf{M} \\boldsymbol{q}_{i}$, in the above summation can be written as a linear combination of the first $i+1$ Lanczos vectors. Therefore the sum can be written as a linear combination of the first $j$ Lanczos vectors. Consequently $$ v_{j}=v_{j} \\boldsymbol{K}_{\\sigma}^{-1} M q_{j}+\\sum_{i=1}^{j} \\bar{v}_{i} q_{i} $$ The $M$-orthogonalization of $v_{j}$ against the preceding $j$ Lanczos vectors will purge the component of $v_{j}$ along each of the $\\boldsymbol{q}_{i}$ 's, and therefore the final result will be unaffected by the sum in the last equation. Therefore the next Lanczos vector, $\\boldsymbol{q}_{j+1}$, will be obtained by first computing a preliminary vector $\\overline{\\boldsymbol{r}}_{\\boldsymbol{j}}$ from the previous vector, $\\boldsymbol{q}_{\\boldsymbol{j}}$, \\begin{equation*} \\bar{r}_{j}=\\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} M q_{j} \\tag{10.6.9} \\end{equation*} and $M$-orthonormalizing it against all the previous Lanczos vectors. Now, in general it may be assumed that this preliminary vector contains components from each of the preceding vectors. Thus \\begin{equation*} \\bar{r}_{j}=r_{j}+\\alpha_{j} q_{j}+\\beta_{j} q_{j-1}+\\gamma_{j} q_{j-2}+\\cdots \\tag{10.6.10} \\end{equation*} where $\\boldsymbol{r}_{\\boldsymbol{j}}$ is the \"pure\" component of $\\bar{r}_{j}$ orthogonal to all previous Lanczos vectors, and $\\alpha_{j}, \\beta_{j}, \\gamma_{j}, \\cdots$ are the amplitudes of the previous vectors contained in $\\bar{j}_{j}$. These amplitude coefficients are evaluated from the orthonormality of the Lanczos vectors. Thus, if both sides of $(10.6 .10)$ are multiplied by $q_{j}^{T} M$, the result is \\begin{equation*} q_{j}^{T} M \\bar{r}_{j}=q_{j}^{T} M r_{j}+\\alpha_{j} q_{j}^{T} M q_{j}+\\beta_{j} q_{j}^{T} M q_{j-1}+\\gamma_{j} q_{j}^{T} M q_{j-2}+\\cdots \\tag{10.6.11} \\end{equation*} Here the first term on the right-hand side vanishes by definition, and all terms beyond the second vanish similarly due to $M$-orthogonality. The normalizing definition applied to the second term then reduces (10.6.11) to an expression for the amplitude of $\\boldsymbol{q}_{j}$ along $\\overline{\\boldsymbol{r}}_{j}$ : \\begin{equation*} \\alpha_{j}=q_{j}^{T} M \\bar{r}_{j} \\tag{10.6.12} \\end{equation*} The amplitude of $\\boldsymbol{q}_{j-1}$ contained in $\\bar{r}_{j}$ may be found similarly by multiplying (10.6.10) by $\\boldsymbol{q}_{j-1}^{T} M$. In this case all terms except the third vanish by orthogonality, and the coefficient of $\\beta_{j}$ is unity, so $\\beta_{j}=\\boldsymbol{q}_{j-1}^{\\boldsymbol{T}} M \\bar{r}_{j}$. But, using (10.6.9) to eliminate $\\bar{r}_{j}$ this gives $\\beta_{j}=\\boldsymbol{q}_{j-1}^{\\boldsymbol{r}} \\boldsymbol{M} \\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} \\boldsymbol{M} \\boldsymbol{q}_{j}$ and applying the transpose of (10.6.9) to the $\\boldsymbol{q}_{j-1}^{\\boldsymbol{T}}$ vector gives \\begin{equation*} \\beta_{j}=\\bar{r}_{j-1}^{r} M q_{j} \\tag{10.6.13} \\end{equation*} Finally, expanding $\\bar{r}_{j-1}$ in terms of its pure component, $r_{j-1}$, and the preceding Lanczos vectors, as in (10.6.10), the transpose of $(10.6 .13)$ becomes \\begin{equation*} \\beta_{j}=q_{j}^{T} M r_{j-1}+\\alpha_{j-1} q_{j}^{T} M q_{j-1}+\\beta_{j-1} q_{j}^{T} M q_{j-2}+\\gamma_{j-1} q_{j}^{T} M q_{j-3}+\\cdots \\tag{10.6.14} \\end{equation*} It is evident that all terms except the first vanish on the right-hand side. Now $\\boldsymbol{q}_{j}$ is the vector obtained by normalizing $\\boldsymbol{r}_{\\boldsymbol{j}-1}$, i.e., \\begin{equation*} q_{j}=\\frac{1}{\\left\\|r_{j-1}\\right\\|_{M}} r_{j-1} \\tag{10.6.15} \\end{equation*} where $\\left\\|r_{j-1}\\right\\|_{M}=\\left(r_{j-1}^{r} M r_{j-1}\\right)^{1 / 2}$. Using this expression for $q_{j}$ in (10.6.14), we obtain $\\beta_{j}$ as $\\beta_{j}=\\left(1 /\\left\\|r_{j-1}\\right\\|_{M}\\right) r_{j-1}^{T} M r_{j-1}$, or \\begin{equation*} \\boldsymbol{\\beta}_{j}^{2}=\\boldsymbol{r}_{j-1}^{T} M \\boldsymbol{r}_{j-1} \\tag{10.6.16} \\end{equation*} This is an alternative to (10.6.13) for evaluating $\\beta_{j}$. In [26] Scott established that computing $\\beta_{j}$ using (10.6.16) is preferable to (10.6.13) and numerical experiments also confirm his results. Using (10.6.16) ensures that the Lanczos vectors are properly normalized even if $q_{j+1}^{T} M q_{j-1}$ is not exactly zero. Continuing in the same way, the amplitude of $\\boldsymbol{q}_{j-2}$ contained in $\\overline{\\boldsymbol{j}}_{j}$ is found to be \\begin{equation*} \\boldsymbol{\\gamma}_{j}=\\boldsymbol{q}_{j-2}^{T} \\boldsymbol{M} \\bar{r}_{j} \\tag{10.6.17} \\end{equation*} Following the procedure used to derive (10.6.14), this leads to \\begin{equation*} \\gamma_{j}=q_{j}^{T} M r_{j-2}+\\alpha_{j-2} q_{j}^{T} M q_{j-2}+\\beta_{j-2} q_{j}^{T} M q_{j-3}+\\gamma_{j-2} q{ }_{j}^{T} M q_{j-4}+\\cdots \\tag{10.6.18} \\end{equation*} But, using the normalizing relationship equivalent to (10.6.15), $r_{j-2}=\\beta_{j-1} q_{j-1}$. Hence, when this is substituted into (10.6.18) all terms on the right-hand side vanish, with the result that $\\gamma_{j}=0$. A corresponding procedure could be used to demonstrate that all further terms in the expansion for $\\bar{r}_{j}$, (10.6.10), vanish; in other words, the orthogonalization procedure used in generating each Lanczos vector need be applied only to the previous two vectors. A summary of the Lanczos algorithm is presented in Table 10.6.1.\\\\ TABLE 10.6.1. The Lanczos Algorithm\\\\ Given an arbitrary vector $r_{0}$ then: \\begin{enumerate} \\item Set\\\\ a. $g_{0}=0$\\\\ b. $\\quad \\beta_{1}=\\left(r_{0}^{T} M r_{0}\\right)^{1 / 2}$\\\\ c. $q_{1}=\\frac{r_{0}}{\\beta_{1}}$\\\\ d. $p_{1}=M q_{1}$ \\item For $j=1,2, \\ldots$, repeat:\\\\ a. $\\overline{\\boldsymbol{r}}_{\\boldsymbol{j}}=\\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{p}_{j}$\\\\ b. $\\quad \\hat{r}_{j}=\\bar{r}_{j}-q_{j-1} \\beta_{j}$\\\\ c. $\\alpha_{j}=q_{j}^{T} M \\hat{r}_{j}=p_{j}^{T} \\hat{r}_{j}$\\\\ d. $r_{j}=\\hat{r}_{j}-q_{j} \\alpha_{j}$\\\\ e. $\\bar{p}_{j}=M r_{j}$\\\\ f. $\\quad \\beta_{j+1}=\\left(r_{j}^{T} M r_{j}\\right)^{1 / 2}=\\left(\\bar{p}_{j}^{T} r_{j}\\right)^{1 / 2}$\\\\ g. if enough vectors, then terminate the loop\\\\ h. $q_{j+1}=\\frac{1}{\\beta_{j+1}} r_{j}$\\\\ i. $\\quad p_{j+1}=\\frac{1}{\\beta_{j+1}} \\bar{p}_{j}$ \\end{enumerate} The above process may be started from a random vector, $r_{0}$, with $\\boldsymbol{q}_{0}=0$ and $\\beta_{1}=\\left(r_{0}^{T} M r_{0}\\right)^{1 / 2}$. At a typical step, $j$, the Lanczos algorithm computes $\\alpha_{j}, \\beta_{j+1}$, and $\\boldsymbol{q}_{j+1}$, in order. In addition to the storage needs of $\\boldsymbol{K}_{\\boldsymbol{\\sigma}}$ and $\\boldsymbol{M}$, the algorithm requires storage for five vectors of length $n$; one for each of the vectors, $q_{j-1}, q_{j}, M r_{j}, p_{j}$, and $r_{j}$. The total cost for one step of the algorithm involves a multiply with $M$, the solution of a system of equations with $\\boldsymbol{K}_{\\sigma}$ as the coefficient matrix, two inner products and four products of a scalar by a vector. \\subsection*{10.6.6 Reduction to Tridiagonal Form} Using the results of the previous section, $(10.6 .10)$ can be rewritten as the three-term relation \\begin{equation*} r_{j}=\\beta_{j+1} q_{j+1}=K_{\\sigma}^{-1} M q_{j}-q_{j} \\alpha_{j}-q_{j-1} \\beta_{j} \\tag{10.6.19} \\end{equation*} where $\\alpha_{j}=\\boldsymbol{q}_{j}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{M} \\boldsymbol{q}_{j}$ and $\\boldsymbol{r}_{j}$ is normalized with respect to the mass matrix to obtain $\\boldsymbol{q}_{j+1}$ with normalizing factor $\\boldsymbol{\\beta}_{j+1}=\\left(\\boldsymbol{r}_{j}^{T} \\boldsymbol{M} r_{j}\\right)^{1 / 2}$. After $m$ Lanczos steps all the quantities obtained from equation (10.6.19) can be arranged in a global matrix form \\[ \\left[ K_{\\sigma}^{-1} M \\right] \\left[ Q_m \\right] - \\left[ Q_m \\right] \\left[ T_m \\right] = \\left[ 0 \\quad 0 \\quad \\cdots \\quad 0 \\quad r_m \\right] = r_m e_m^T \\tag{10.6.20} \\] Here \\( e_m^T = \\langle 0, 0, \\cdots, 0, 1 \\rangle \\), \\( Q_m \\) is an \\( n \\times m \\) matrix with columns \\( q_i, i = 1, 2, \\cdots, m \\), and \\( T_m \\) is a tridiagonal matrix of the form \\[ T_m = \\begin{bmatrix} \\alpha_1 & \\beta_2 & 0 & \\cdots & 0 & 0 \\\\ \\beta_2 & \\alpha_2 & \\beta_3 & \\cdots & 0 & 0 \\\\ 0 & \\beta_3 & \\alpha_3 & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & \\alpha_{m-1} & \\beta_m \\\\ 0 & 0 & 0 & \\cdots & \\beta_m & \\alpha_m \\end{bmatrix} \\tag{10.6.21} \\] The orthogonality property of the Lanczos vectors, $\\boldsymbol{Q}_{m}^{T} \\boldsymbol{M} \\boldsymbol{Q}_{m}=\\boldsymbol{I}_{m}$, where $\\boldsymbol{I}_{m}$ is the $m \\times m$ identity matrix, can be used in (10.6.20) to obtain \\begin{equation*} \\boldsymbol{Q}_{m}^{T} \\boldsymbol{M} \\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} \\boldsymbol{M} \\boldsymbol{Q}_{m}=\\boldsymbol{T}_{m} \\tag{10.6.22} \\end{equation*} Choosing the set of Lanczos vectors, $\\boldsymbol{Q}_{m}$, for the trial vectors, the Rayleigh-Ritz procedure can be used to obtain the best approximation to the eigenvectors of (10.6.6). The approximating Ritz vectors will then be of the form \\begin{equation*} y_{i}^{(m)}=\\boldsymbol{Q}_{m} s_{i}^{(m)}, \\quad i=1, \\ldots, m \\tag{10.6.23} \\end{equation*} When a residual vector, $\\left\\{\\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{i}^{(m)}-\\theta_{i}^{(m)} y_{i}^{(m)}\\right\\}$, associated with the pair $\\left\\{\\theta_{i}^{(m)}, y_{i}^{(m)}\\right\\}$ is $M$-orthogonal to the set of Lanczos vectors, then $\\left\\{\\theta_{i}^{(m)}, y_{j}^{(m)}\\right\\}$ is a Ritz pair. Accordingly $$ \\boldsymbol{Q}_{m}^{T} \\boldsymbol{M}\\left\\{\\boldsymbol{K}_{\\sigma}^{-1} M \\boldsymbol{y}_{i}^{(m)}-\\theta_{i}^{(m)} \\boldsymbol{y}_{i}^{(m)}\\right\\}=0 $$ Using the relation between the Ritz vector, $y_{i}^{(m)}$ and the Lanczos vectors, given in (10.6.23), together with the orthonormality condition of the $q$ 's, and the tridiagonal properties of the Lanczos\n",
      "chunk word length: 109, chunk char length: 882, chunk = vectors, (10.6.22), the above equation reduces to the tridiagonal eigenproblem \\begin{equation*} T_{m} s_{i}^{(m)}-\\theta_{i}^{(m)} s_{i}^{(m)}=0 \\tag{10.6.24} \\end{equation*} Thus $\\left\\{\\theta_{i}^{(m)}, s_{i}^{(m)}\\right\\}$ is an eigenpair of the tridiagonal matrix, $\\boldsymbol{T}_{m}$. As the total number of Lanczos vectors increases, i.e., as we take more Lanczos steps, the size of the tridiagonal matrix increases and the eigenvalues of $T_{m}$ converge to the eigenvalues of the transformed problem (10.6.6), $1 /\\left(\\lambda_{i}-\\sigma\\right)$. When $m=n$, the order of $K$, then $\\theta_{i}^{(n)}=1 /\\left(\\lambda_{i}-\\sigma\\right)$ for all $i$, but we hope to stop long before $m=n$. The steps enumerated in Step 2 of Table 10.6.1 are repeated until the Ritz pairs $\\left\\{\\theta_{i}^{(m)}, y_{i}^{(m)}\\right\\}$ have sufficiently converged to the desired eigenpairs.\n",
      "chunk word length: 1223, chunk char length: 9845, chunk = $$ K=\\frac{1}{80}\\left[\\begin{array}{rrrr} 39 & -9 & 21 & -11 \\\\ -9 & 39 & -11 & 21 \\\\ 21 & -11 & 39 & -9 \\\\ -11 & 21 & -9 & 39 \\end{array}\\right] $$ For this example we assume the mass matrix is the identity. Then the eigenvalue matrix for this problem is $\\boldsymbol{\\Lambda}=\\operatorname{diag}\\left(\\frac{1}{3}, \\frac{1}{4}, \\frac{1}{2}, 1\\right)$. The Lanczos algorithm presented here requires the solution of a linear system of equations with $\\boldsymbol{K}$. We therefore give $\\boldsymbol{K}^{-1}$ explicitly for convenience. Accordingly, $$ K^{-1}=\\frac{1}{2}\\left[\\begin{array}{rrrr} 6 & 0 & -3 & 1 \\\\ 0 & 6 & 1 & -3 \\\\ -3 & 1 & 6 & 0 \\\\ 1 & -3 & 0 & 6 \\end{array}\\right] $$ Choosing a starting vector $r_{0}=\\langle 1,0,0,0\\rangle$, then\\\\ Step 1: $$ \\beta_{1}=\\left\\|r_{0}\\right\\|=1, \\quad q_{1}=\\frac{r_{0}}{\\beta_{1}}=\\left\\{\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right\\}, \\quad K^{-1} q_{1}=\\frac{1}{2}\\left\\{\\begin{array}{r} 6 \\\\ 0 \\\\ -3 \\\\ 1 \\end{array}\\right\\} $$ Sec. 10.6 The Lanczos Algorithm $$ \\alpha_{1}=q_{1}^{T} K^{-1} q_{1}=3, \\quad r_{1}=K^{-1} q_{1}-\\alpha_{1} q_{1}=\\frac{1}{2}\\left\\{\\begin{array}{r} 0 \\\\ 0 \\\\ -3 \\\\ 1 \\end{array}\\right\\}, \\quad T_{1}=[3] $$ Step 2: $$ \\begin{gathered} \\beta_{2}=\\left\\|r_{1}\\right\\|=\\frac{\\sqrt{10}}{2}, \\quad q_{2}=\\frac{r_{1}}{\\beta_{2}}=\\frac{1}{\\sqrt{10}}\\left\\{\\begin{array}{r} 0 \\\\ 0 \\\\ -3 \\\\ 1 \\end{array}\\right\\}, \\quad K^{-1} q_{2}=\\frac{1}{\\sqrt{10}}\\left\\{\\begin{array}{r} 5 \\\\ -3 \\\\ -9 \\\\ 3 \\end{array}\\right\\} \\\\ \\alpha_{2}=q_{2}^{T} K^{-1} q_{2}=3, \\quad r_{2}=K^{-1} q_{2}-\\alpha_{2} q_{2}-\\beta_{2} q_{1}=\\frac{1}{\\sqrt{10}}\\left\\{\\begin{array}{r} 0 \\\\ -3 \\\\ 0 \\\\ 0 \\end{array}\\right\\} \\\\ T_{2}=\\left[\\begin{array}{cc} 3 & \\frac{\\sqrt{10}}{2} \\\\ \\frac{\\sqrt{10}}{2} & 3 \\end{array}\\right] \\end{gathered} $$ Step 3: $$ \\begin{gathered} \\beta_{3}=\\left\\|r_{2}\\right\\|=\\frac{3}{\\sqrt{10}}, \\quad q_{3}=\\frac{r_{2}}{\\beta_{3}}=\\left\\{\\begin{array}{r} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{array}\\right\\}, \\quad K^{-1} q_{3}=\\frac{1}{2}\\left\\{\\begin{array}{r} 0 \\\\ -6 \\\\ -1 \\\\ 3 \\end{array}\\right\\} \\\\ \\alpha_{3}=q_{3}^{T} K^{-1} q_{3}=3, \\quad r_{3}=K^{-1} q_{3}-\\alpha_{3} q_{3}-\\beta_{3} q_{2}=\\frac{1}{5}\\left\\{\\begin{array}{l} 0 \\\\ 0 \\\\ 2 \\\\ 6 \\end{array}\\right\\} \\\\ T_{3}=\\left[\\begin{array}{ccc} 3 & \\frac{\\sqrt{10}}{2} & 0 \\\\ \\frac{\\sqrt{10}}{2} & 3 & \\frac{3}{\\sqrt{10}} \\\\ 0 & \\frac{3}{\\sqrt{10}} & 3 \\end{array}\\right] \\end{gathered} $$ Step 4: $$ \\beta_{4}=\\left\\|r_{3}\\right\\|=\\frac{4}{\\sqrt{10}}, \\quad q_{4}=\\frac{r_{3}}{\\beta_{4}}=\\frac{1}{\\sqrt{10}}\\left\\{\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\\\ 3 \\end{array}\\right\\}, \\quad K^{-1} q_{4}=\\frac{1}{\\sqrt{10}}\\left\\{\\begin{array}{r} 0 \\\\ -4 \\\\ 3 \\\\ 9 \\end{array}\\right\\} $$ $$ \\begin{gathered} \\alpha_{4}=q_{4}^{T} K^{-1} q_{4}=3, \\quad r_{4}=K^{-1} q_{4}-\\alpha_{4} q_{4}-\\beta_{4} q_{3}=\\left\\{\\begin{array}{l} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right\\} \\\\ T_{4}=\\left[\\begin{array}{cccc} 3 & \\frac{\\sqrt{10}}{2} & 0 & 0 \\\\ \\frac{\\sqrt{10}}{2} & 3 & \\frac{3}{\\sqrt{10}} & 0 \\\\ 0 & \\frac{3}{\\sqrt{10}} & 3 & \\frac{4}{\\sqrt{10}} \\\\ 0 & 0 & \\frac{4}{\\sqrt{10}} & 3 \\end{array}\\right] \\end{gathered} $$ The eigenvalues of the tridiagonal matrices converge to the inverses of the eigenvalues of $\\boldsymbol{K}$ in this example. This can be demonstrated by computing the eigenvalues of the tridiagonal at each step as shown in Table 10.6.2 TABLE 10.6.2 Convergence of the Ritz Values for the Small $4 \\times 4$ Example \\begin{center} \\begin{tabular}{c|c} \\hline $j$ & Eigenvalues of $\\boldsymbol{T}_{j}$ \\\\ \\hline 1 & 3.0000 \\\\ 2 & $1.4189,4.5811$ \\\\ 3 & $1.1561,3.0000,4.8439$ \\\\ 4 & $1.0000,2.0000,4.0000,5.0000$ \\\\ \\hline \\end{tabular} \\end{center} In Figure 10.6.1 we plot the Ritz values for a larger example. Notice the early convergence at both ends of the spectrum. \\subsection*{10.6.7 Convergence Criterion for Eigenvalues} The eigenvalues, $\\theta_{i}^{(j)}, i=1, \\ldots, j$, of the tridiagonal $T_{j}$ are the Rayleigh-Ritz approximations to eigenvalues of (10.6.6). As $j$ increases these Ritz values get closer to the eigenvalues they are approximating. Often users wait until the change in the computed quantities is within a specified tolerance; that is \\begin{equation*} \\frac{\\left|\\theta^{(j-1)}-\\theta^{(j)}\\right|}{\\left|\\theta^{(j)}\\right|} \\leq \\text { tol } \\tag{10.6.25} \\end{equation*} Here we demonstrate the weakness of such convergence tests. Suppose one is performing inverse iteration with a specified shift, $\\sigma$, in an interval of interest $\\left[\\sigma_{1}, \\sigma_{2}\\right]$; i.e., $$ \\sigma=\\gamma \\sigma_{1}+(1-\\gamma) \\sigma_{2} $$ Sec. 10.6 The Lanczos Algorithm\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_d72772018091a661a792g-24} Figure 10.6.1 A typical pattern for the progress of the eigenvalues of the tridiagonal matrix, $\\boldsymbol{T}_{\\mathbf{j}}$. The symbol $\\nabla$ indicates the last diagonal entry of \\vspace{0.2 cm}$\\boldsymbol{T}_{\\boldsymbol{j}}$.\\\\ where $\\gamma$ is a positive number less than 1. Further, assume that the problem has eigenvalues, $\\lambda_{a}$ and $\\lambda_{b}$ outside this interval with $\\lambda_{a}<\\sigma_{1}$ and $\\lambda_{b}>\\sigma_{2}$. Then the corresponding eigenvalues of the transformed problem, (10.6.6), are $\\nu_{a}=$ $1 /\\left(\\lambda_{a}-\\sigma\\right)$ and $\\nu_{b}=1 /\\left(\\lambda_{b}-\\sigma\\right)$, with $\\nu_{a}<0$ and $\\nu_{b}>0$. Now, let us perform a step of the inverse iteration method with an unfortunate starting vector, $x^{(1)}=$ $z_{a} \\sin \\psi+z_{b} \\cos \\psi$. Here, $z_{a}$ and $z_{b}$ are eigenvectors corresponding to $\\lambda_{a}$ and $\\lambda_{b}$, respectively. We assume that the eigenvectors are normalized with respect to the mass matrix. Then $\\boldsymbol{x}^{(1)}$ will also be normalized. The Ritz value due to $\\boldsymbol{x}^{(1)}$ is $$ \\begin{aligned} \\theta^{(1)} & =\\boldsymbol{x}^{(1) T} \\boldsymbol{K} \\boldsymbol{x}^{(1)} \\\\ & =\\lambda_{a} \\sin ^{2} \\psi+\\lambda_{b} \\cos ^{2} \\psi \\end{aligned} $$ After one step of inverse iteration the improved vector is $$ x^{(2)}=\\frac{1}{\\left[\\nu_{a}^{2} \\sin ^{2} \\psi+\\nu_{b}^{2} \\cos ^{2} \\psi\\right]^{1 / 2}}\\left(\\nu_{a} z_{a} \\sin \\psi+\\nu_{b} z_{b} \\cos \\psi\\right) $$ and the Ritz value associated with $x^{(2)}$ is $$ \\theta^{(2)}=\\frac{1}{\\left[\\nu_{a}^{2} \\sin ^{2} \\psi+\\nu_{b}^{2} \\cos ^{2} \\psi\\right]^{1 / 2}}\\left(\\nu_{a}^{2} \\lambda_{a} \\sin ^{2} \\psi+\\nu_{b}^{2} \\lambda_{b} \\cos ^{2} \\psi\\right) $$ Now if $\\lambda_{a}=\\sigma-\\tau$ and $\\lambda_{b}=\\sigma+\\tau$, in which case $\\nu_{a}=-1 / \\tau$ and $\\nu_{b}=1 / \\tau$, then $$ \\theta^{(1)}=\\theta^{(2)}=(\\sigma-\\tau) \\sin ^{2} \\psi+(\\sigma+\\tau) \\cos ^{2} \\psi=\\sigma+\\tau \\cos 2 \\psi $$ These $\\theta$ 's, referred to as \"ghost\" eigenvalues in [25], satisfy (10.6.25) and therefore would be accepted as eigenvalues. Further, an appropriate choice for $\\psi$ can result in a Ritz value that is in the interval of interest. This behavior, although rare, has been observed in certain implementations of the subspace iteration method [25] and reflects a serious difficulty. We refer to this as misconvergence in [23] and, as we observed therein, it cannot be detected by a natural criterion such as (10.6.25). A rigorous inexpensive termination criterion can be obtained using the residual error bound given later. Consider a pair $\\{\\theta, y\\}$, which approximates an eigenpair $\\{\\nu, z\\}$, of (10.6.6), where $\\nu$ is the closest eigenvalue of (10.6.6) to $\\theta$. For simplicity we consider only normalized Ritz vectors, $\\|y\\|_{M}=1$ where $\\|y\\|_{M}=\\left(y^{T} M y\\right)^{1 / 2}$. The residual vector associated with this approximation is given by \\begin{equation*} r=K_{\\sigma}^{-1} M y-\\theta y \\tag{10.6.26} \\end{equation*} The norm of the residual vector can then be used to assess the accuracy of $\\theta$ through the inequality \\begin{equation*} |\\nu-\\theta| \\leq\\|r\\|_{M} \\tag{10.6.27} \\end{equation*} For the proof and an in-depth study see [21]. This result allows us to predict the accuracy of any candidate eigenpair of a matrix. We use the residual vector associated with the Ritz pair $\\left\\{\\theta_{i}^{(m)}, y_{i}^{(m)}\\right\\}$ given by (10.6.23) and (10.6.24) to check for their convergence. The tridiagonal property of the Lanczos algorithm, (10.6.20), greatly simplifies the computation of the residual norm. Postmultiplying $(10.6 .20)$ by $s_{i}^{(m)}$ leads to \\begin{equation*} K_{\\sigma}^{-1} M Q_{m} s_{i}^{(m)}-Q_{m} T_{m} s_{i}^{(m)}=r_{m} e_{m}^{T} s_{i}^{(m)} \\tag{10.6.28} \\end{equation*} Since $s_{i}^{(m)}$ is an eigenvector of $T_{m}$ (see ( 10.6 .24$)$ ), we can replace $T_{m} s_{i}^{(m)}$ by $\\theta_{i}^{(m)} s_{i}^{(m)}$. Further, by definition (10.6.23), $\\boldsymbol{Q}_{m} s_{i}^{(m)}$ is simply the Ritz vector $y_{i}^{m}$ and so (10.6.28) will reduce to \\begin{equation*} K_{\\sigma}^{-1} M y_{i}^{(m)}-\\theta_{i}^{(m)} y_{i}^{(m)}=r_{m} e_{m}^{T} s_{i}^{(m)} \\tag{10.6.29} \\end{equation*} Taking norms, \\begin{align*} \\left\\|\\boldsymbol{K}_{\\sigma}^{-1} M_{i}^{(m)}-\\theta_{i}^{(m)} y_{i}^{(m)}\\right\\|_{M} & =\\left\\|r_{m} e_{m}^{T} s_{i}^{(m)}\\right\\|_{M} \\\\ & =\\left\\|r_{m}\\right\\|_{M}\\left|e_{m}^{T} s_{i}^{(m)}\\right| \\\\ & =\\beta_{m+1}\\left|\\zeta_{i}\\right| \\tag{10.6.30} \\end{align*} where $\\zeta_{i}=e_{m}^{\\boldsymbol{T}} s_{i}^{(m)}$ is the bottom element of $s_{i}^{(m)}$, the normalized eigenvector of $T_{m}$. $\\beta_{m+1}$ is a scalar quantity that is computed in the course of the Lanczos process. The bottom element of the eigenvector of $\\boldsymbol{T}_{m}$ can be obtained at little cost once the associated eigenvalue is found. Therefore, the residual norm associated with a Ritz pair can be computed as $\\rho_{m i}=\\beta_{m+1}\\left|\\zeta_{i}\\right|$ and the Ritz value is considered converged once $\\rho_{m i}<$ tol. There is no need to compute $\\boldsymbol{y}_{i}^{(m)}$ until it has converged.\n",
      "chunk word length: 1326, chunk char length: 10201, chunk = In exact arithmetic the simple Lanczos algorithm cannot compute a second copy of a multiple eigenvalue. Suppose $\\lambda$ is an eigenvalue with multiplicity two. Then there is no unique eigenvector associated with $\\lambda$. In fact one can obtain two vectors, $z_{1}$ and $z_{2}$, such that any linear combination of these vectors is also an eigenvector of $\\lambda$. Moreover, it is possible to obtain a linear combination of $z_{1}$ and $z_{2}, \\bar{z}=\\xi_{1} z_{1}+\\xi_{2} z_{2}$, that is orthogonal to $q_{1}$. Then the eigenvector $\\bar{z}$ will be orthogonal to all the subsequent Lanczos vectors. Therefore $\\bar{z}$ will never be found. Fortunately, this argument only holds in exact arithmetic. In finite precision, roundoff comes to the rescue. If $\\boldsymbol{q}_{1}$ is perfectly orthogonal to an eigenvector, $\\boldsymbol{z}$, then because of roundoff error, $q_{2}$ will have a small component along $\\bar{z}$. This component, although tiny, will eventually grow such that $\\bar{z}$ can be represented by a linear combination of the computed Lanczos vectors. However, the second copy of $\\lambda$ will converge some steps after the first has converged. \\subsection*{10.6.8 Loss of Orthogonality} In a previous section we derived the equations governing the Lanczos algorithm. These relations are only satisfied by quantities obtained in exact arithmetic. In finite precision, however, each computation introduces a small error and therefore each computed quantity will differ from its exact counterpart. Our objective here is to describe the effect of roundoff on the Lanczos process. For this purpose we need to introduce an important quantity that measures the accuracy of the arithmetic. Let $\\epsilon$ be the smallest number in the computer such that $1+\\epsilon>1$. It is known as the unit roundoff error. Although the tridiagonal relation, (10.6.20), is preserved to within roundoff, the $\\boldsymbol{M}$-orthogonality property of the Lanczos vectors completely breaks down after a certain number of steps, depending on $\\epsilon$ and the distribution of the eigenvalues of (10.6.6). The Lanczos vectors, which are orthogonal in exact arithmetic, not only lose their orthogonality, but may even become linearly dependent. Initially it was believed that loss of orthogonality is due to cancellations that occur each time $r_{j}$ is evaluated using (10.6.19). This step is simply $M$-orthogonalization of $\\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} M q_{j}$ against $q_{j}$ and $q_{j-1}$. Comparing the final vector resulting from this computation to the starting vector, one can obtain a measure of the cancellation that occurs in this step; i.e., the ratio \\begin{equation*} \\chi_{j}^{2}=\\frac{\\left\\|r_{j}\\right\\|_{M}^{2}}{\\left\\|K_{\\sigma}^{-1} M q_{j}\\right\\|_{M}^{2}}=\\frac{\\beta_{j+1}^{2}}{\\beta_{j}^{2}+\\alpha_{j}^{2}+\\beta_{j+1}^{2}} \\tag{10.6.31} \\end{equation*} indicates how much cancellation has occurred. $\\chi_{j}$ is the sine of the angle between the vector $\\boldsymbol{K}_{\\sigma}^{-1} M q_{j}$ and the plane containing the vectors $q_{j}$ and $q_{j-1}$. When $\\chi_{j}$ is zero it indicates that in the $j$ th Lanczos step we are orthogonalizing a vector that is already in this plane against $\\boldsymbol{q}_{\\boldsymbol{j}}$ and $\\boldsymbol{q}_{j-1}$, and therefore complete cancellation occurs. In practice $\\chi_{j}$ rarely drops below $\\frac{1}{10}$. When $\\chi_{j}$ drops to $\\frac{1}{100}$, for instance, it indicates that $\\boldsymbol{K}_{\\sigma}^{-1} M q_{j}$ is nearly parallel to this plane, and therefore at the end of this Lanczos step the computed $r_{j}$ may not be orthogonal to the plane containing $q_{j}$ and $q_{j-1}$ to working accuracy. In such cases $\\boldsymbol{r}_{j}$ should be orthogonalized against $\\boldsymbol{q}_{j}$ and $\\boldsymbol{q}_{j-1}$ a second time. For a long time it was believed that loss of orthogonality was solely due to this cancellation. Indeed if this were the case, then we would have no option other than a complete reorthogonalization at each step. However, as we soon shall see, a completely different mechanism is at work. Suppose for the moment the algorithm was executed in exact arithmetic for $j$ steps, except that at some step $k<j$ a small error was introduced into the computation of $q_{k}$. The first $k-1$ Lanczos vectors will be perfectly $M$-orthonormal, but they will not be orthogonal to all the vectors computed after the $k$ th step. The error introduced at step $k$ will be amplified in the subsequent steps to such an extent that linear independency may also be lost. Hence, the loss of orthogonality can be viewed as the subsequent amplification of the error introduced after each computation. To analyze the way in which orthogonality deteriorates, we let $\\boldsymbol{Q}_{m}$ denote the computed Lanczos vectors and define the following matrix: \\begin{equation*} \\boldsymbol{H}_{m}=\\boldsymbol{Q}_{m}^{T} M \\boldsymbol{Q}_{m} \\tag{10.6.32} \\end{equation*} where the $\\boldsymbol{i}, \\boldsymbol{j}$-component of $\\boldsymbol{H}_{\\boldsymbol{m}}$ is $\\boldsymbol{\\eta}_{i, j}=\\boldsymbol{q}_{\\boldsymbol{i}}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{q}_{j}$. In exact arithmetic $\\boldsymbol{H}_{\\boldsymbol{m}}$ is the identity matrix. The off-diagonals of $\\boldsymbol{H}_{\\boldsymbol{m}}$ will depend on $\\boldsymbol{\\epsilon}$, the unit roundoff error. Further, (10.6.19) will be satisfied by the computed quantities only to within roundoff error. Now, multiply (10.6.19) by $\\boldsymbol{q}_{\\boldsymbol{i}}^{\\boldsymbol{T}} M$ and use the above definition to get the approximate relation \\begin{equation*} q_{i}^{T} M K_{\\sigma}^{-1} M q_{j} \\cong \\alpha_{j} \\eta_{i, j}+\\beta_{j} \\eta_{i, j-1}+\\beta_{j+1} \\eta_{i, j+1} \\tag{10.6.33} \\end{equation*} A similar equation can be obtained when the above procedure is repeated for the $i$ th Lanczos step \\begin{equation*} \\boldsymbol{q}_{j}^{T} M \\boldsymbol{K}_{\\sigma}^{-1} M q_{i} \\cong \\alpha_{i} \\eta_{j, i}+\\beta_{i} \\eta_{j, i-1}+\\beta_{i+1} \\eta_{j, i+1} \\tag{10.6.34} \\end{equation*} By symmetry of $M K_{\\sigma}^{-1} \\boldsymbol{M}$, the terms on the left-hand side of (10.6.33) and (10.6.34) are equal and therefore can be eliminated by subtraction, resulting in the relation \\begin{equation*} \\beta_{j+1} \\eta_{j+1, i} \\cong \\beta_{i+1} \\eta_{j, i+1}+\\left(\\alpha_{i}-\\alpha_{j}\\right) \\eta_{j, i}+\\beta_{i} \\eta_{j, i-1}-\\beta_{j} \\eta_{j-1, i} \\tag{10.6.35} \\end{equation*} This recursion holds for $j \\geq 2$ and $1 \\leq i \\leq j-1$ and starts by assuming that the diagonal of $\\boldsymbol{H}_{\\boldsymbol{m}}$ is the identity, $\\eta_{i, j}=1$ for all $j \\geq 1$, and the first off-diagonal of $\\boldsymbol{H}_{m}$ is at roundoff level, $\\eta_{j, j-1}=\\epsilon$ for $j \\geq 2$. The above relation is due to Simon [27]. It provides a means of estimating the elements of a column of $\\boldsymbol{H}_{m}$ from the elements of $\\boldsymbol{T}_{\\boldsymbol{m}}$ and the elements in the previous columns of $\\boldsymbol{H}_{\\boldsymbol{m}}$. This recursion can be restated in the vector form \\begin{equation*} \\beta_{j+1} h_{j+1} \\cong T_{j-1} h_{j}-\\alpha_{j} h_{j}-\\beta_{j} h_{j-1} \\tag{10.6.36} \\end{equation*} where $\\boldsymbol{h}_{\\boldsymbol{j}-1}, \\boldsymbol{h}_{\\boldsymbol{j}}$ and $\\boldsymbol{h}_{\\boldsymbol{j}+1}$ are vectors of length $\\boldsymbol{j}-1$ containing the top $\\boldsymbol{j}-1$ elements of the $(j-1)$ st, $j$ th, and $(j+1)$ st columns of $\\left(H_{m}-I_{m}\\right)$. Here, the bottom element of $\\boldsymbol{h}_{\\boldsymbol{j}-1}$ is zero. Then, all the terms in $\\boldsymbol{h}_{\\boldsymbol{j}}$ depend on $\\epsilon$. Taking norms we can show \\begin{align*} \\beta_{j+1}\\left\\|h_{j+1}\\right\\| & \\leq\\left(\\left\\|T_{j-1}\\right\\|+\\left|\\alpha_{j}\\right|\\right)\\left\\|h_{j}\\right\\|+\\beta_{j}\\left\\|h_{j-1}\\right\\| \\\\ & \\leq 2\\left\\|T_{j}\\right\\| \\max \\left(\\left\\|h_{j}\\right\\|,\\left\\|h_{j-1}\\right\\|\\right) \\tag{10.6.37} \\end{align*} This result shows that the level of nonorthogonality can grow by at most a factor of $2\\left\\|T_{j}\\right\\| / \\beta_{j+1}$ after each step. A drop in the value of $\\beta_{j+1}$ can result in a sudden loss of orthogonality but this rarely occurs. An alternative characterization of the pattern in which orthogonality is lost was presented by Paige [18-20]. Instead of examining the vector $\\boldsymbol{h}_{j+1}=\\boldsymbol{Q}_{j}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{q}_{j+1}$, he looked at a linear combination of the components in this vector. To be more specific, he examined the inner product between each Ritz vector $y_{i}^{(0)}$ given by (10.6.23) and $q_{j+1}$. That is \\begin{align*} \\boldsymbol{y}_{j}^{(i)} T_{M} \\boldsymbol{q}_{j+1} & =s_{i}^{(j)} \\boldsymbol{Q}_{j}^{T} M \\boldsymbol{q}_{j+1} \\\\ & =s_{i}^{(j) T} h_{j+1} \\tag{10.6.38} \\end{align*} In exact arithmetic this value is zero. However, in his work Paige showed that \\begin{equation*} y_{i}^{(j)} T_{M q_{j+1}}=\\frac{\\gamma_{ji} \\epsilon\\left\\|T_{j}\\right\\|}{\\beta_{j+1}\\left|\\zeta_{i}\\right|} \\tag{10.6.39} \\end{equation*} Recall that $\\zeta_{i}$ is the bottom element of $s_{i}{ }^{(j)}$, the $i$th eigenvector of $T_{j}$. $\\gamma_{j i}$ is a scalar quantity usually close to unity. We omit the derivation of this result and refer the interested reader to [21]. Note that Paige's result also shows that a sudden drop in $\\beta_{j+1}$ can result in a severe loss of orthogonality. Moreover, recall the quantity in the denominator is also a measure of the convergence of the Ritz value $\\theta_{i}^{()}$(see (10.6.30)). The only way the left side of (10.6.39) can rise up to values like 0.1 is for $\\rho_{j i}\\left(=\\beta_{j+1}\\left|\\zeta_{i}\\right|\\right)$ to drop down to $10 \\gamma_{j i} \\epsilon\\left\\|T_{j}\\right\\|$, so \\textbf{Loss of orthogonality $\\Rightarrow$ convergence of a Ritz value}\\\\ When only a single Ritz value converges at step $j$, then $q_{j+1}$ loses orthogonality by tilting toward the converged Ritz vector, which in turn is a linear combination of the previous Lanczos vectors. When more than one Ritz vector converges simultaneously, the picture is more complicated. In this case $\\boldsymbol{q}_{j+1}$ tilts toward a linear combination of these vectors.\n",
      "chunk word length: 1313, chunk char length: 9349, chunk = In theory, if two successive Lianczos vectors, $\\boldsymbol{q}_{j-1}$ and $\\boldsymbol{q}_{j}$, are orthogonal to an eigenvector, $z$, then all the subs a pent Lanczos vectors will also be orthogonal to $z$. In practice, however, a converged Ritz vector, $y_{i}$, will not remain orthogonal to all the subsequent Lanczos vectors. This is a consequence of the same mechanism by which\\\\ multiple eigenvalues are found. Roundoff errors will add to each Lanczos vector a small component of $y_{i}$, which will grow eventually to such a magnitude that orthogonality to $y_{i}$ is lost. This phenomenon can also be observed in the inverse iteration method; i.e., orthogonalizing the starting vector against the first eigenvector does not guarantee convergence of the iteration vectors to the next eigenvector. The state of orthogonality between a converged Ritz vector, $y_{i}$; and the current Lanczos vector, $q_{j}$, can be measured by the component of $\\boldsymbol{y}_{i}$ along $q_{j}$. We define $\\tau_{j}=y_{i}^{T} M q_{j}$. Then multiplying (10.6.19) by $\\boldsymbol{y}_{i}^{T} \\boldsymbol{M}$ and considering the effect of roundoff on (10.6.19) yields $$ \\beta_{j+1} y_{i}^{T} M q_{j+1} \\cong y_{i}^{T} M K_{\\sigma}^{-1} M q_{j}-\\alpha_{j} y_{i}^{T} M q_{j}-\\beta_{j} y_{i}^{T} M q_{j-1} $$ Using the fact that $y_{i}$ is a converged Ritz value, i.e., $K_{\\sigma}^{-1} M y_{i}=\\theta_{i} y_{i}$, together with the definition of $\\tau_{j}$ we obtain \\begin{equation*} \\tau_{j+1} \\cong \\frac{\\left(\\theta_{i}-\\alpha_{j}\\right) \\tau_{j}-\\beta_{j} \\tau_{j-1}}{\\beta_{j+1}} \\tag{10.6.40} \\end{equation*} This recurrence can be updated for each converged Ritz value, $\\boldsymbol{\\theta}_{\\boldsymbol{i}}$. The magnitude of $\\tau_{j}$ can be used as an indicator for loss of orthogonality against a converged Ritz vector. The growth of the components of the Lanczos vectors along a converged Ritz vector is referred to as the return of a banished Ritz vector. \\subsection*{10.6.9 Restoring Orthogonality} In this section we look at a number of preventive measures that we can adopt to maintain a certain level of orthogonality. Lanczos [14] was aware of the effects of round-off on the algorithm when he presented his work. He proposed that the newly computed Lanczos vector, $q_{j+1}$, be explicitly orthogonalized against all the preceding vectors at the end of each step $j$. We will refer to this technique as the \"full reorthogonalization\" method. This scheme is adopted in [12, 29]. With this procedure the Lanczos vectors will meet the stringent requirement \\begin{equation*} \\left|q_{i}^{T} M q_{j}\\right|<n \\epsilon, \\quad i \\neq j \\tag{10.6.41} \\end{equation*} Although this scheme increases the overall cost of an eigenvalue computation, for short Lanczos runs (when the number of Lanczos steps is less than the half-bandwidth of $K$ ) the increase in cost is small compared to the cost of solving a system of equations with $\\boldsymbol{K}_{\\sigma}$. For longer runs the cost of a full reorthogonalization step will begin to dominate the cost of a Lanczos step, although vector computers will delay this effect. The orthogonality condition of (10.6.41) can be replaced by a more relaxed condition, \\begin{equation*} \\left|q_{i}^{T} M q_{j}\\right|<\\sqrt{n \\epsilon}, \\quad i \\neq j \\tag{10.6.42} \\end{equation*} We refer to this as the semiorthogonality condition and we refer to procedures that adopt the weaker condition as selective orthogonalization methods. Imposing the\\\\ more relaxed condition can result in considerable reduction in the number of operations in the reorthogonalization step and semiorthogonality is sufficient to make $\\boldsymbol{T}_{j}$ exact to working precision. We consider two different reorthogonalization schemes that adopt the more relaxed condition of (10.6.42).\\\\ \\textbf{A. Orthogonalization against Ritz vectors:} This procedure is a consequence of the result of Paige [18-20]. As soon as a Ritz value converges, its corresponding Ritz vector is computed and the component of this vector along $q_{j+1}$ is purged. Further, for each converged Ritz value, the three-term recurrence for $\\tau_{j}$ is updated and whenever $\\tau_{j}$ becomes greater than $\\sqrt{\\epsilon}$ in absolute value, it signals that the component of the corresponding eigenvector has grown too much. So the new Lanczos vector is orthogonalized against this known eigenvector [22, 25].\\\\ \\textbf{B. Orthogonalization against previous Lanczos vectors:} This scheme can be based on either of the techniques given in [13,27]. In [27] the vector $\\boldsymbol{h}_{j+1}$ is updated using (10.6.36) and the magnitudes of its elements are monitored. Whenever the $i$ th element of $\\boldsymbol{h}_{j+1}$ is greater than $\\sqrt{\\boldsymbol{\\epsilon}}$ then semiorthogonality is lost between $\\boldsymbol{q}_{j+1}$ and $\\boldsymbol{q}_{i}$. At this step the appropriate Lanczos vectors are brought in from secondary storage and their components along $q_{j+1}$ are removed. This scheme is also adopted in [17]. Both schemes indicate loss of orthogonality at about the same step. The first method performs orthogonalization against fewer vectors and therefore costs less. However, alone it has some shortcomings. The result in (10.6.39) gives an accurate picture at the early stages of a Lanczos run when one or two Ritz values converge. When for a number of steps, a few Ritz values converge at the same time, then a gradual loss of semiorthogonality may occur. This appears most often in a Lanczos run with spectral transformation. For this reason we do not use scheme $A$ on its own. However, for short Lanczos runs where only a very few (less than 10) eigenpairs are wanted, scheme $A$ is very effective. We advocate a combination of the above two schemes. Whenever possible we orthogonalize against a Ritz vector (scheme A) because of lower costs. We update $\\boldsymbol{h}_{\\boldsymbol{j} \\boldsymbol{1}}$ using (10.6.36) and monitor its elements. If any of the elements of $\\boldsymbol{h}_{j+1}$ is greater than $\\sqrt{\\epsilon}$, then semiorthogonality may have been lost. This is the only procedure we use to determine loss of semiorthogonality, which can occur in two possible ways:\\\\ i. Convergence of a Ritz value.\\\\ ii. Growth of components of computed eigenvectors along the Lanczos vector. We take different actions for each of these. If the observed orthogonality loss is due to (i), then we perform a step of scheme B. The Lanczos vectors are brought in from secondary storage and the newly computed Lanczos vector, $\\boldsymbol{r}_{\\boldsymbol{j}}$, is orthogonalized against them. Further, to minimize data transfer from secondary storage, we also compute some of the converged Ritz vectors at the same time. Therefore the computation of newly converged Ritz vectors is delayed until the Lanczos vectors are brought back for reorthogonalization. We should note that when (i) occurs, both methods $A$ and $B$ would require recalling the Lanczos vectors from secondary storage\\\\ but for different reasons: the first for computing Ritz vectors and the second for reorthogonalization. We perform both. Our method would clearly require more operations than scheme A but has the following two advantages:\\\\ a. An eigenvector is computed in a few steps after the convergence of its eigenvalue and therefore has more than half correct digits.\\\\ b. The gradual loss of orthogonality just mentioned will not occur with the combined procedure. However, if loss of orthogonality is due to (ii), then we can use scheme A, i.e., orthogonalize against a computed Ritz vector. For this reason, the $\\tau$ recurrence, (10.6.40), is also monitored to establish which Ritz vector, $\\boldsymbol{y}_{k}$, has contaminated $\\boldsymbol{q}_{j+1}$. The orthogonalization of $\\boldsymbol{q}_{j+1}$ against $\\boldsymbol{y}_{k}$ alters the state of orthogonality among the Lanczos vectors. The modified $\\bar{q}_{j+1}$ is given by $$ \\bar{q}_{j+1}=q_{j+1}-\\xi_{k} y_{k} $$ where $\\boldsymbol{\\xi}_{k}=\\boldsymbol{y}_{k}^{T} \\boldsymbol{M} \\boldsymbol{q}_{j+1}$. Let $\\boldsymbol{h}_{j+1}=\\boldsymbol{Q}_{j}^{T} \\boldsymbol{M} \\bar{q}_{j+1}$. Then \\begin{align*} \\boldsymbol{h}_{j+1} & =\\boldsymbol{Q}_{j}^{T} \\boldsymbol{M}\\left(\\boldsymbol{q}_{j+1}-\\xi_{k} \\boldsymbol{y}_{k}\\right) \\\\ & =\\boldsymbol{h}_{j+1}-\\xi_{k} s_{k} \\tag{10.6.43} \\end{align*} An approximation to $\\xi_{k}$ can be obtained via $$ \\xi_{k}=y_{k}^{T} M q_{j+1}=s_{k}^{T} \\boldsymbol{Q}_{j}^{T} M q_{j+1}=s_{k}^{T} h_{j+1} $$ If the approximate result for $\\xi_{k}$ is used then (10.6.43) reduces to the orthogonalization of $\\boldsymbol{h}_{j+1}$ against $s_{k}$. This simple step is used to modify $\\boldsymbol{h}_{j+1}$ to reflect the changes caused by orthogonalizing $q_{j+1}$ against a Ritz vector. When it becomes necessary to restore semiorthogonality, the computed vector $q_{j+1}$ must be orthogonalized against some linear combination of the previous Lanczos vectors. $\\boldsymbol{q}_{j}$ and $\\boldsymbol{q}_{j-1}$ remain unchanged at the end of this step. But, at the next step, $\\boldsymbol{q}_{j}$ appears again in the computation of $\\boldsymbol{q}_{j+2}$. If no action is taken, then $\\boldsymbol{q}_{j+2}$ will be contaminated by $q_{j}$ and the reorthogonalization efforts of the previous step would be wasted. Therefore a second reorthogonalization step must be\n",
      "chunk word length: 86, chunk char length: 525, chunk = performed. If the Lanczos vectors or the converged Ritz vectors reside in secondary storage, they must be retrieved in two successive steps. Alternatively, at the same time the orthogonality of $q_{j+1}$ is being restored, we can also perform similar modifications on $q_{j}$. Then at the end of the next step, no reorthogonalization of $q_{j+2}$ will be necessary. The number of operations for this scheme is the same as that of the scheme above, but vectors are retrieved only once and therefore the I/O overhead is halved.\n",
      "chunk word length: 29, chunk char length: 234, chunk = \\begin{enumerate} \\item K. J. Bathe, Finite Element Procedures in Engineering Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1982. \\item B. Noble, Applied Linear Algebra. Englewood Cliffs, N.J.: Prentice-Hall, 1969. \\end{enumerate}\n",
      "chunk word length: 21, chunk char length: 191, chunk = \\begin{enumerate} \\setcounter{enumi}{2} \\item E. L. Wilson, \"The Static Condensation Algorithm,\" International Journal for Numerical Methods in Engineering, 8 (1974), 199-203. \\end{enumerate}\n",
      "chunk word length: 73, chunk char length: 543, chunk = \\begin{enumerate} \\setcounter{enumi}{3} \\item B. M. Irons, \"Eigenvalue Economisers in Vibration Problems,\" Journal of the Royal Aeronautical Society, 67 (1963), 526. \\item B. M. Irons, \"Structural Eigenvalue Problems: Elimination of Unwanted Variables,\" AIAA Journal, 3 (1965), 961. \\item R. J. Guyan, \"Reduction of Stiffness and Mass Matrices,\" AIAA Journal, 3 (1965), 380. \\item R. D. Henshell and J. H. Ong, \"Automatic Masters for Eigenvalue Economization,\" Earthquake Engineering and Structural Dynamics, 3 (1975), 375-383. \\end{enumerate}\n",
      "chunk word length: 40, chunk char length: 333, chunk = \\begin{enumerate} \\setcounter{enumi}{7} \\item K. J. Bathe, Finite Element Procedures in Engineering Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1982. \\item E. L. Wilson, \"Numerical Methods for Dynamic Analysis,\" International Symposium on Numerical Methods in Offshore Engineering. Swansea, January 11-15, 1977. \\end{enumerate}\n",
      "chunk word length: 458, chunk char length: 3245, chunk = \\begin{enumerate} \\setcounter{enumi}{9} \\item J. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart, LINPACK Users' Guide, SIAM, Philadelphia, 1979. \\item T. Ericsson and A. Ruhe, \"The Spectral Transformation Lanczos Method for the Numerical Solution of Large Sparse Generalized Symmetric Eigenvalue Problems,\" Mathematics of Computation, 35 (1980), 1251-1268. \\item G. Golub, R. Underwood, and J. H. Wilkinson, \"The Lanczos Algorithm for the Symmetric $A x=\\lambda B x$ Problem,\" Tech. Rep. STAN-CS-72-720, Computer Science Department, Stanford University, 1972. \\item J. Grcar, \"Analyses of the Lanczos Algorithm and of the Approximation Problem in Richardson's Method,\" Ph.D. Thesis, University of Illinois at Urbana-Champaign, 1981. \\item C. Lanczos, \"An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators,\" Journal of Research of the National Bureau of Standards, 45 (1950), 255-281. \\item B. Nour-Omid, B. N. Parlett, and R. L. Taylor, \"Lanczos versus Subspace Iteration for Solution of Eigenvalue Problems,\" International Journal for Numerical Methods in Engineering, 19 (1983), 859-871. \\item B. Nour-Omid and B. N. Parlett, \"How to Implement the Spectral Transformation,\" Tech. Rep. PAM-224, Center for Pure and Applied Mathematics, University of California, Berkeley, 1984. \\item B. Nour-Omid and R. W. Clough, \"Dynamic Analysis of Structures Using Lanczos Coordinates,\" Earthquake Engineering and Structural Dynamics, 12 (1984), 565-577. \\item C. C. Paige, \"Computational Variants of the Lanczos Method for the Eigenproblem,\" Journal of the Institute for Mathematics and its Applications, 10 (1972), 373-381. \\item C. C. Paige, \"Error Analysis of the Lanczos Algorithm for Tridiagonalizing a Symmetric Matrix,\", Journal of the Institute for Mathematics and its Applications, 18 (1976), $341-349$. \\item C. C. Paige, \"Accuracy and Effectiveness of the Lanczos Algorithm for Symmetric Eigenproblems,\" Linear Algebra and its Applications, 34 (1980), 235-258. \\item B. N. Parlett, The Symmetric Eigenvalue Problem. Englewood Cliffs, N.J.: Prentice-Hall, 1980. \\item B. N. Parlett and D. Scott, \"The Lanczos Algorithm with Selective Orthogonalization,\" Mathematics of Computation, 33, no. 145 (1979), 217-238. \\item B. N. Parlett, H. D. Simon, and L. M. Stringer, \"On Estimating the Largest Eigenvalue with the Lanczos Algorithm,\" Mathematics of Computation, 38, no. 157 (1982), 153-165. \\item B. N. Parlett and B. Nour-Omid, \"The Use of Refined Error Bounds When Updating Eigenvalues of Tridiagonals,\" Linear Algebra and its Applications, 68 (1985), 179-219. \\item R. L. Taylor, private communication, 1978. \\item D. S. Scott, \"Analysis of the Symmetric Lanczos Process,\" Tech. Rep. ERL-M78/40, Electronics Research Laboratory, University of California, Berkeley, 1978. \\item H. D. Simon, \"The Lanczos Algorithm with Partial Reorthogonalization,\" Mathematics of Computation, 42, no. 165 (1984), 115-142. \\item H. D. Wilkinson, The Algebraic Eigenvalue Problem. Oxford: Clarendon, 1965. \\item E. L. Wilson, M. Yuan, and J. M. Dickens, \"Dynamic Analysis by Direct Superposition of Ritz Vectors,\" Earthquake Engineering and Structural Dynamics, 10 (1982), 813-821. \\end{enumerate}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|| 20/20 [01:53<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are generated\n",
      "Questions are embedded\n",
      "saved ../data/hughes_latex_Q_then_A_use_context/hughes_ch10_Qs_n40_by_sections_tpc1536.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "For generating questions, we want larger chunks with a bit of overlap.\n",
    "The following values are just for this demo, so please adjust them as needed.\n",
    "\n",
    "I only ran Chapter One.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chapter = chpt_for_quest_answ\n",
    "latex_file_path = f'../data/FEM_Hughes_LaTeX_Textbook/chapter{chapter}.tex'\n",
    "\n",
    "max_questions = 40                             # max number of questions per chunk\n",
    "\n",
    "tokens_per_chunk = 1536                       \n",
    "token_overlap = int(0.2 * tokens_per_chunk)   # 10% overlap\n",
    "environment_sensitive = True                  # If True, equations won't be split between chunks, which may result in chunks larger than the specified tokens_per_chunk\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def embed_all_q(questions):\n",
    "    all_questions = []\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            all_questions.append(sub_item['question'])\n",
    "    # using api\n",
    "    embeddings = get_embeddings(client, all_questions, model = embedding_model) \n",
    "    # add them to data:\n",
    "    k = 0\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            sub_item['embedding'] = embeddings[k]\n",
    "            k +=1\n",
    "    print('Questions are embedded')\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_file_name = f\"{main_dir}/hughes_ch{chapter}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_file_name = f\"{main_dir}/hughes_ch{chapter}_Qs_n{max_questions}_by_sections_tpc{tokens_per_chunk}.json\"  \n",
    "    token_overlap = 0 \n",
    "\n",
    "if not os.path.exists(questions_file_name):\n",
    "    question_chunks = process_latex_files(latex_file_path, tokens_per_chunk, token_overlap, environment_sensitive, chunk_by_section=chunk_by_section)\n",
    "    \n",
    "    if production_mode == False:\n",
    "        question_chunks = question_chunks[0:7] # for testing small batch\n",
    "    \n",
    "    for question in question_chunks:\n",
    "        print(f\"chunk word length: {len(question.split(\" \"))}, chunk char length: {len(question)}, chunk = {question}\")\n",
    "\n",
    "    questions = {}  # main data\n",
    "\n",
    "    # we should save generation info we used\n",
    "    questions['info'] = {\n",
    "        'tokens_per_chunk': tokens_per_chunk,\n",
    "        'token_overlap': token_overlap,\n",
    "        'environment_sensitive': environment_sensitive,\n",
    "        'max_questions': max_questions,\n",
    "        'embedding_model': embedding_model,\n",
    "        'llm_model_questions': llm_model_questions,\n",
    "        'llm_model_answers': llm_model_answers\n",
    "    }\n",
    "\n",
    "    ## step 1: generate questions\n",
    "    questions['data'] = []\n",
    "    for i in tqdm(range(len(question_chunks)), desc=\"Generating Questions\"):\n",
    "        # q_for_chunk = gen_questions(client, question_chunks[i], max_questions, model=llm_model_questions)\n",
    "        q_for_chunk = gen_questions_s(client, question_chunks[i], max_questions, model=llm_model_questions)   # Using the new function\n",
    "        questions['data'].append({'chunk': question_chunks[i],'questions': q_for_chunk})\n",
    "    print('Questions are generated')\n",
    "\n",
    "    ## step 2: embedding all questions at once\n",
    "    questions = embed_all_q(questions)\n",
    "    \n",
    "\n",
    "    with open(questions_file_name, 'w') as json_file:\n",
    "        json.dump(questions, json_file, indent=4)\n",
    "    print('saved', questions_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_file_name, 'r') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_file_name)\n",
    "\n",
    "if questions['info']['embedding_model'] != embedding_model:\n",
    "    print(\"embedding model mismatch. re-embedding questions\")\n",
    "    questions = embed_all_q(questions)\n",
    "    questions['info']['embedding_model'] = embedding_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Retrieval and Generating Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k context added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions: 100%|| 20/20 [18:15<00:00, 54.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are answered\n",
      "saved ../data/hughes_latex_Q_then_A_use_context/hughes_ch10_QAs_n40_topk10_by_sections.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "Since we answer each question separately, this process is slow.\n",
    "We might want to consider using the batch API for this.\n",
    "\"\"\"\n",
    "\n",
    "top_k = 10   # number of retrieved closest contexts         \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_answers_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}_topk{top_k}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_answers_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"   \n",
    "\n",
    "if not os.path.exists(questions_answers_file_name):\n",
    "\n",
    "    questions_answers = questions.copy()\n",
    "\n",
    "    # step 1) finding top_k context from the book embedding and adding them to each question\n",
    "    for item in questions_answers['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            ind = fixed_knn_retrieval(sub_item['embedding'], embedding_space, top_k)\n",
    "            context = ''\n",
    "            for i, chunk in enumerate(chunks[ind]):\n",
    "                context += f'\\n\\n Additional context {i}: {chunk}' \n",
    "            sub_item['context'] = context\n",
    "    print('top_k context added')\n",
    "\n",
    "    # step 2) generating answers (slow)  (should we try batch API?)\n",
    "    for item in tqdm(questions_answers['data'], desc=\"Answering Questions\"):\n",
    "        question_chunk = item['chunk']\n",
    "        for sub_item in item['questions']:\n",
    "            question = sub_item['question']\n",
    "            context = question_chunk + sub_item['context']\n",
    "            sub_item['answer'] = gen_answer(client, question, context, model = llm_model_answers)\n",
    "    print('Questions are answered')\n",
    "    \n",
    "    with open(questions_answers_file_name, 'w') as json_file:\n",
    "        json.dump(questions_answers, json_file, indent=4)\n",
    "    print('saved', questions_answers_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_answers_file_name, 'r') as json_file:\n",
    "        questions_answers = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_answers_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_chunk</th>\n",
       "      <th>context</th>\n",
       "      <th>coverage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\maketitle</td>\n",
       "      <td>\\n\\n Additional context 0: \\maketitle\\n\\n Addi...</td>\n",
       "      <td>30</td>\n",
       "      <td>What is the significance of the title in a res...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...</td>\n",
       "      <td>\\n\\n Additional context 0: The Galerkin method...</td>\n",
       "      <td>90</td>\n",
       "      <td>What is the generalized eigenproblem in the co...</td>\n",
       "      <td>The generalized eigenproblem in the context of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{equation*} \\...</td>\n",
       "      <td>85</td>\n",
       "      <td>Explain the significance of the eigenvalues an...</td>\n",
       "      <td>The eigenvalues and eigenvectors in the genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...</td>\n",
       "      <td>\\n\\n Additional context 0: The shape functions...</td>\n",
       "      <td>80</td>\n",
       "      <td>Describe the conditions under which the eigenv...</td>\n",
       "      <td>The eigenvalues of the generalized eigenproble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...</td>\n",
       "      <td>\\n\\n Additional context 0: \\subsection*{10.1 T...</td>\n",
       "      <td>95</td>\n",
       "      <td>Why are the lower modes of the generalized eig...</td>\n",
       "      <td>The lower modes of the generalized eigenproble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>\\begin{enumerate} \\setcounter{enumi}{9} \\item ...</td>\n",
       "      <td>\\n\\n Additional context 0: amplitude of $\\bold...</td>\n",
       "      <td>75</td>\n",
       "      <td>Describe the process and benefits of using the...</td>\n",
       "      <td>The spectral transformation in the Lanczos met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>\\begin{enumerate} \\setcounter{enumi}{9} \\item ...</td>\n",
       "      <td>\\n\\n Additional context 0: amplitude of $\\bold...</td>\n",
       "      <td>60</td>\n",
       "      <td>How does the dynamic analysis of structures be...</td>\n",
       "      <td>The dynamic analysis of structures benefits fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>\\begin{enumerate} \\setcounter{enumi}{9} \\item ...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>65</td>\n",
       "      <td>What are the challenges and solutions associat...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>\\begin{enumerate} \\setcounter{enumi}{9} \\item ...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>50</td>\n",
       "      <td>Explain the significance of the LINPACK Users'...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>\\begin{enumerate} \\setcounter{enumi}{9} \\item ...</td>\n",
       "      <td>\\n\\n Additional context 0: amplitude of $\\bold...</td>\n",
       "      <td>70</td>\n",
       "      <td>What are the advantages of using the Lanczos a...</td>\n",
       "      <td>Answer: The Lanczos algorithm is advantageous ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_chunk  \\\n",
       "0                                           \\maketitle   \n",
       "1    \\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...   \n",
       "2    \\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...   \n",
       "3    \\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...   \n",
       "4    \\subsection*{10.1 THE GENERALIZED EIGENPROBLEM...   \n",
       "..                                                 ...   \n",
       "223  \\begin{enumerate} \\setcounter{enumi}{9} \\item ...   \n",
       "224  \\begin{enumerate} \\setcounter{enumi}{9} \\item ...   \n",
       "225  \\begin{enumerate} \\setcounter{enumi}{9} \\item ...   \n",
       "226  \\begin{enumerate} \\setcounter{enumi}{9} \\item ...   \n",
       "227  \\begin{enumerate} \\setcounter{enumi}{9} \\item ...   \n",
       "\n",
       "                                               context  coverage  \\\n",
       "0    \\n\\n Additional context 0: \\maketitle\\n\\n Addi...        30   \n",
       "1    \\n\\n Additional context 0: The Galerkin method...        90   \n",
       "2    \\n\\n Additional context 0: \\begin{equation*} \\...        85   \n",
       "3    \\n\\n Additional context 0: The shape functions...        80   \n",
       "4    \\n\\n Additional context 0: \\subsection*{10.1 T...        95   \n",
       "..                                                 ...       ...   \n",
       "223  \\n\\n Additional context 0: amplitude of $\\bold...        75   \n",
       "224  \\n\\n Additional context 0: amplitude of $\\bold...        60   \n",
       "225  \\n\\n Additional context 0: \\begin{enumerate} \\...        65   \n",
       "226  \\n\\n Additional context 0: \\begin{enumerate} \\...        50   \n",
       "227  \\n\\n Additional context 0: amplitude of $\\bold...        70   \n",
       "\n",
       "                                              question  \\\n",
       "0    What is the significance of the title in a res...   \n",
       "1    What is the generalized eigenproblem in the co...   \n",
       "2    Explain the significance of the eigenvalues an...   \n",
       "3    Describe the conditions under which the eigenv...   \n",
       "4    Why are the lower modes of the generalized eig...   \n",
       "..                                                 ...   \n",
       "223  Describe the process and benefits of using the...   \n",
       "224  How does the dynamic analysis of structures be...   \n",
       "225  What are the challenges and solutions associat...   \n",
       "226  Explain the significance of the LINPACK Users'...   \n",
       "227  What are the advantages of using the Lanczos a...   \n",
       "\n",
       "                                                answer  \n",
       "0                             Answer: NOT ENOUGH INFO.  \n",
       "1    The generalized eigenproblem in the context of...  \n",
       "2    The eigenvalues and eigenvectors in the genera...  \n",
       "3    The eigenvalues of the generalized eigenproble...  \n",
       "4    The lower modes of the generalized eigenproble...  \n",
       "..                                                 ...  \n",
       "223  The spectral transformation in the Lanczos met...  \n",
       "224  The dynamic analysis of structures benefits fr...  \n",
       "225                           Answer: NOT ENOUGH INFO.  \n",
       "226                           Answer: NOT ENOUGH INFO.  \n",
       "227  Answer: The Lanczos algorithm is advantageous ...  \n",
       "\n",
       "[228 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I think it's better to work with JSON/DataFrame in the code, but for reviewing QAs, CSV is easier to work with\n",
    "\"\"\"\n",
    "\n",
    "csv_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}.csv\"   \n",
    "# ----------------------------------\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in questions_answers['data']:\n",
    "    question_chunk = item['chunk']\n",
    "    for sub_item in item['questions']:\n",
    "        new_item = {}\n",
    "        new_item['question_chunk'] = question_chunk\n",
    "        for k,v in sub_item.items():\n",
    "            if k == 'embedding':\n",
    "                continue\n",
    "            new_item[k] = v\n",
    "        data.append(new_item)\n",
    "\n",
    "# data[0]\n",
    "df = pd.DataFrame(data)[['question_chunk','context','coverage','question','answer']]\n",
    "df.to_csv(csv_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=160):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "Derive the reduced system form (10.1.7) from the standard form (10.1.8) using the transformations provided.\n",
      "A:\n",
      "To derive the reduced system form (10.1.7) from the standard form (10.1.8), we need to use the transformations provided in the context. Let's go through the\n",
      "steps:  1. **Start with the Standard Form (10.1.8):**     $$    \\left(\\bar{K}^{*}-\\lambda^{*} \\boldsymbol{I}\\right) \\bar{\\Psi}^{*}=\\mathbf{0}    $$  2. **Use\n",
      "the Transformation for $\\bar{K}^{*}$ (10.1.9):**     $$    \\overline{\\mathbf{K}}^{*}=\\overline{\\boldsymbol{U}}^{-T} \\mathbf{K}^{*} \\bar{U}^{-1}    $$\n",
      "Substitute this into the standard form:     $$    \\left(\\overline{\\boldsymbol{U}}^{-T} \\mathbf{K}^{*} \\bar{U}^{-1} - \\lambda^{*} \\boldsymbol{I}\\right)\n",
      "\\bar{\\Psi}^{*} = \\mathbf{0}    $$  3. **Use the Transformation for $\\bar{\\Psi}^{*}$ (10.1.10):**     $$\n",
      "\\overline{\\boldsymbol{\\Psi}}^{*}=\\overline{\\boldsymbol{U}} \\boldsymbol{\\Psi}^{*}    $$     Substitute this into the equation:     $$\n",
      "\\left(\\overline{\\boldsymbol{U}}^{-T} \\mathbf{K}^{*} \\bar{U}^{-1} - \\lambda^{*} \\boldsymbol{I}\\right) \\overline{\\boldsymbol{U}} \\boldsymbol{\\Psi}^{*} =\n",
      "\\mathbf{0}    $$  4. **Simplify the Equation:**     Multiply through by $\\overline{\\boldsymbol{U}}^{T}$ to eliminate $\\overline{\\boldsymbol{U}}^{-T}$:     $$\n",
      "\\left(\\mathbf{K}^{*} \\bar{U}^{-1} - \\lambda^{*} \\overline{\\boldsymbol{U}}^{T} \\boldsymbol{I}\\right) \\boldsymbol{\\Psi}^{*} = \\mathbf{0}    $$     Since\n",
      "$\\overline{\\boldsymbol{U}}^{T} \\boldsymbol{I} = \\overline{\\boldsymbol{U}}^{T}$, we have:     $$    \\left(\\mathbf{K}^{*} - \\lambda^{*} \\boldsymbol{M}^{*}\\right)\n",
      "\\boldsymbol{\\Psi}^{*} = \\mathbf{0}    $$     Here, $\\boldsymbol{M}^{*} = \\overline{\\boldsymbol{U}}^{T} \\overline{\\boldsymbol{U}}$, which is consistent with the\n",
      "transformation of the mass matrix using the Cholesky factorization (10.1.11).  5. **Conclusion:**     The reduced system form (10.1.7) is derived as:     $$\n",
      "\\left(\\boldsymbol{K}^{*}-\\lambda^{*} \\boldsymbol{M}^{*}\\right) \\Psi^{*}=0    $$     This completes the derivation using the transformations provided in the\n",
      "context.\n",
      "\n",
      "Chunk used for Q generation:\n",
      "\\subsection*{10.1 THE GENERALIZED EIGENPROBLEM} In this chapter we are concerned with algorithms for solving the so-called generalized eigenproblem arising from\n",
      "finite element discretizations. Recall that this takes the form:\\footnote{Problems of this type are motivated and formulated in Chapter 7. To simplify\n",
      "subsequent writing we omit the superscript $h$ on $\\lambda$.} \\begin{equation*} (K-\\lambda M) \\psi=0 \\tag{10.1.1} \\end{equation*} where $K$ is symmetric and\n",
      "positive-semidefinite, $M$ is symmetric and positive-definite, and both matrices possess typical band/profile structure. For convenience we recall some basic\n",
      "properties of the solution of (10.1.1). We assume as always that the dimensions of $K$ and $M$ are $n_{e q} \\times n_{e q}$. There exist $n_{e q}$ eigenvalues\n",
      "and corresponding eigenvectors\\footnote{The eigenvectors may be nonunique.} that satisfy (10.1.1). That is, \\begin{equation*} \\left(K-\\lambda_{l} M\\right)\n",
      "\\psi_{l}=0, \\quad \\text { (no sum) } \\tag{10.1.2} \\end{equation*} where $l=1,2, \\ldots, n_{e q}$ denotes the mode number. Furthermore, \\begin{equation*} 0 \\leq\n",
      "\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n_{\\text {eq }}} \\tag{10.1.3} \\end{equation*} and \\[ \\begin{array}{lll} \\Psi_{k}^{T} M \\Psi_{l}=\\delta_{k\n",
      "l} & \\text { (M orthonormality) } \\\\ \\Psi_{k}^{T} K \\Psi_{l}=\\lambda_{l} \\delta_{k l} & \\text { (no sum) } & \\text { (K orthogonality) } \\tag{10.1.5}\n",
      "\\end{array} \\] The eigenvectors $\\left\\{\\psi_{l}\\right\\}_{l=1}^{n_{e q}}$ constitute a basis for $\\mathbb{R}^{n_{e e}}$. If, additionally, $\\boldsymbol{K}$ is\n",
      "positivedefinite, as is often the case, then (10.1.3) may be strengthened as follows: \\begin{equation*} 0<\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq\n",
      "\\lambda_{n_{\\text {eq }}} \\tag{10.1.6} \\end{equation*} In practice, we are typically interested only in the lower modes. These are the most important from a\n",
      "physical standpoint. For example, the lowest mode is the most important in a structural stability (i.e., \"buckling\") calculation and the lower frequencies and\n",
      "corresponding mode shapes are usually the most important in considerations of dynamic response. Additionally, the higher modes of finite element discretizations\n",
      "are not accurate renditions of physical behavior but rather spurious artifacts of the discretization process. Consequently, they are of no engineering interest.\n",
      "For these reasons, we are interested in economical computational algorithms for extracting $\\left\\{\\lambda_{l}, \\Psi_{l}\\right\\}, 1 \\leq l \\leq n_{\\text {modes\n",
      "}}$, where $n_{\\text {modes }} \\ll n_{e q}$ is the number of desired eigenpairs. In practice $n_{\\text {eq }}$ may be very large, and thus solution of the\n",
      "eigenvalue problem, even for only a few eigenpairs, may entail an extensive and costly calculation. Most procedures used for solving the large-scale generalized\n",
      "eigenproblem, (10.1.1), involve a reduced system of the form \\begin{equation*} \\left(\\boldsymbol{K}^{*}-\\lambda^{*} \\boldsymbol{M}^{*}\\right) \\Psi^{*}=0\n",
      "\\tag{10.1.7} \\end{equation*} where $K^{*}$ and $M^{*}$ are small, full, symmetric matrices. Algorithms, such as the generalized Jacobi method [1], are available\n",
      "for directly solving (10.1.7). Systems such as (10.1.7) may also be solved by first transforming to standard form: \\begin{equation*}\n",
      "\\left(\\bar{K}^{*}-\\lambda^{*} \\boldsymbol{I}\\right) \\bar{\\Psi}^{*}=\\mathbf{0} \\tag{10.1.8} \\end{equation*} where \\begin{align*} &\n",
      "\\overline{\\mathbf{K}}^{*}=\\overline{\\boldsymbol{U}}^{-T} \\mathbf{K}^{*} \\bar{U}^{-1} \\tag{10.1.9}\\\\ & \\overline{\\boldsymbol{\\Psi}}^{*}=\\overline{\\boldsymbol{U}}\n",
      "\\boldsymbol{\\Psi}^{*} \\tag{10.1.10} \\end{align*} and $\\boldsymbol{U}$ is the upper-triangular Cholesky factor of $M^{*}$, i.e., \\begin{equation*}\n",
      "\\boldsymbol{M}^{*}=\\overline{\\boldsymbol{U}}^{T} \\boldsymbol{\\overline{U}} \\tag{10.1.11} \\end{equation*} (The Cholesky factor is related to the Crout factor by\n",
      "$\\boldsymbol{\\overline{U}}=D^{1 / 2} \\boldsymbol{U}$, where $\\boldsymbol{M}^{*}=$ $\\boldsymbol{U}^{\\boldsymbol{T}} \\boldsymbol{D} \\boldsymbol{U},\n",
      "\\boldsymbol{U}$ is upper-triangular with ones on the diagonal and $\\boldsymbol{D}$ is a diagonal matrix of positive numbers; see Chapter 11 for further\n",
      "information on the Crout factorization.) Observe that the eigenvalues of the standard form, (10.1.8), are identical to the generalized form, (10.1.7). However,\n",
      "the eigenvectors need to be transformed as indicated by (10.1.10) \\textbf{Exercise 1.} Derive (10.1.7) from (10.1.8) through (10.1.11). There are many classical\n",
      "and widely available procedures for solving (10.1.8). For example, the Jacobi, Givens, and Householder-QR methods may be mentioned in this regard (e.g., see\n",
      "Bathe [1] and Noble [2]). For general matrices, it is currently felt that the most efficient strategy for solving the generalized eigenproblem, (10.1.7), is to\n",
      "first transform to standard form, (10.1.8), and then use the Householder-QR algorithm. However, under certain circumstances, such as when the subspace iteration\n",
      "procedure is employed (see Sec. 10.5), for example, direct use of the generalized Jacobi method proves very effective. It is very difficult to make sweeping\n",
      "statements about efficiency because the type of computer (e.g., sequential, vector, or parallel) strongly influences the performance of algorithms.\n",
      "\\textbf{Exercise 2.} Consider the undamped equation of motion, \\begin{equation*} M \\Ddot{d}+K d=F \\tag{10.1.12} \\end{equation*} subject to zero initial\n",
      "displacement and velocity. Expand the solution in terms of the eigenvectors of the associated eigenproblem. Diagonalize the system and exactly solve the\n",
      "individual modal equations. Show that \\begin{equation*} d(t)=\\sum_{l=1}^{n_{e q}}\\left\\{\\frac{1}{\\omega_{l}} \\int_{0}^{t} F_{(l)}(\\tau) \\sin \\omega_{l}(t-\\tau)\n",
      "d \\tau \\Psi_{l}\\right\\} \\tag{10.1.13} \\end{equation*} The $1 / \\omega_{l}$ factor in the expansion illustrates the diminishing influence of the higher modes.\n",
      "This analysis reveals why low mode response is viewed as \"most important\" and therefore, why, in practical calculations the summation in (10.1.13) is truncated\n",
      "at $n_{\\text {modes }} \\ll n_{\\text {eq }}$. \\textbf{Exercise 3.} Obtain an exact solution for the static problem, \\begin{equation*} \\boldsymbol{K}\n",
      "\\boldsymbol{d}=\\boldsymbol{F} \\tag{10.1.14} \\end{equation*} by way of an eigenvector expansion. Discuss the relative importance of low and high modes for this\n",
      "case. \\textbf{Exercise 4.} Generalize Exercise 2 to account for Rayleigh damping and non-zero initial conditions. Discuss the influence of low and high modes.\n",
      "\\subsection*{10.2 STATIC CONDENSATION} In practice one often encounters eigenvalue problems which can be written in the following partitioned form: \\[\n",
      "\\left(\\left[\\begin{array}{ll} K_{11} & K_{12} \\tag{10.2.1}\\\\ K_{21} & K_{22} \\end{array}\\right]-\\lambda\\left[\\begin{array}{cc} M_{11} & 0 \\\\ 0 & 0\n",
      "\\end{array}\\right]\\right)\\left\\{\\begin{array}{l} \\Psi_{1} \\\\ \\Psi_{2} \\end{array}\\right\\}=0 \\] where $M_{11}$ is symmetric and positive-definite. That is, many\n",
      "degrees of freedom are \"massless.\" Problems of this type arise naturally when a relatively light structure is used to support heavy nonstructural masses which\n",
      "can be \"lumped\" at a few degrees of freedom. In such situations the structural mass is often insignificant and may be neglected, resulting in a system like\n",
      "(10.2.1). As it stands, (10.2.1) is ill-posed in the sense that the zero-diagonal masses give rise to infinite eigenvalues. The mass matrix can be \"regularized\"\n",
      "by the addition of small positive nonzero diagonal masses, in which case we return to the format of the generalized eigenvalue problem originally considered,\n",
      "or, alternatively, the zero-mass degrees of freedom can be eliminated by static condensation. To arrive at the statically condensed form, we expand (10.2.1): \\[\n",
      "\\begin{array}{r} K_{11} \\Psi_{1}+K_{12} \\Psi_{2}-\\lambda M_{11} \\Psi_{1}=0 \\\\ K_{21} \\Psi_{1}+K_{22} \\Psi_{2}=0 \\tag{10.2.3} \\end{array} \\] The next step is to\n",
      "solve (10.2.3) for $\\Psi_{2}$ and then substitute in (10.2.2), which results in \\begin{equation*} \\left(\\boldsymbol{K}_{11}^{*}-\\lambda M_{11}\\right) \\Psi_{1}=0\n",
      "\\tag{10.2.4} \\end{equation*} where \\begin{equation*} \\boldsymbol{K}_{11}^{*}=\\boldsymbol{K}_{11}-K_{12} K_{22}^{-1} K_{21} \\quad \\text { (statically condensed\n",
      "stiffness) } \\tag{10.2.5} \\end{equation*} The advantage of transforming to statically condensed form is that the problem size is reduced. However, $K_{11}^{*}$\n",
      "tends to be full. Thus, unless the size of the nonzero-mass matrix is rather small, the reduction to statically condensed form may be uneconomical because the\n",
      "profile structure of $\\boldsymbol{K}$ is lost. Note that to calculate the statically condensed stiffness (10.2.5) efficiently, $\\boldsymbol{K}_{22}$ is never\n",
      "actually inverted. The following steps may be used in practice: \\begin{equation*} K_{22}=\\boldsymbol{U}^{T} D \\boldsymbol{U} \\quad \\text { (Crout factorization)\n",
      "} \\tag{10.2.6} \\end{equation*} \\begin{align*} \\boldsymbol{U} D^{1 / 2} \\boldsymbol{Z} & =\\boldsymbol{K}_{21} \\quad \\text { (solve for } \\boldsymbol{Z} \\text { )\n",
      "} \\tag{10.2.7}\\\\ \\boldsymbol{K}_{12} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21} & =\\boldsymbol{Z}^{T} \\boldsymbol{Z} \\tag{10.2.8} \\end{align*} Equation\n",
      "(10.2.7) amounts to solution of an equation system with upper triangular coefficient array and multiple right-hand sides (i.e., the columns of\n",
      "$\\boldsymbol{K}_{21}$ ). For further computational considerations regarding static condensation, see [3]. \\subsection*{10.3 DISCRETE RAYLEIGH-RITZ REDUCTION} In\n",
      "the discrete Rayleigh-Ritz approach, static load patterns, $\\boldsymbol{P}$, are selected and corresponding displacement vectors, $\\boldsymbol{R}$, are\n",
      "calculated from:\\\\ where $n_{l p}$ refers to the number of load patterns. The displacements $R$, referred to as the trial vectors, are used to form the reduced\n",
      "eigenproblem, i.e., (10.1.7) by defining \\begin{align*} K^{*} & =\\boldsymbol{R}^{T} \\boldsymbol{K} \\boldsymbol{R} \\tag{10.3.2}\\\\ M^{*} & =\\boldsymbol{R}^{T}\n",
      "\\boldsymbol{M R} \\tag{10.3.3} \\end{align*} The load patterns, $\\boldsymbol{P}$, are usually subject to the following criteria:\\\\ i. The columns of\n",
      "$\\boldsymbol{P}$ should be linearly independent.\\\\ ii. The columns of $\\textbf{P}$ should be selected to arouse the low modes by activating the heaviest masses\n",
      "and most flexible areas of the model.\n",
      "\n",
      "Retrieved context:\n",
      "\n",
      "\n",
      " 0: In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \\subsection*{Forward reduction} \\begin{itemize} \\item Divide\n",
      "row 1 by $K_{11}$. \\item Subtract $K_{21} \\times$ row 1 from row 2. \\item Subtract $K_{31} \\times$ row 1 from row 3. \\item Subtract $K_{n 1} \\times$ row 1 from\n",
      "row $n$. \\end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \\left[\\begin{array}{llll|l} 1 &\n",
      "\\boldsymbol{K}_{12}^{\\prime} & \\boldsymbol{K}_{3}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & \\boldsymbol{K}_{22}^{\\prime\n",
      "\\prime} & \\boldsymbol{K}_{23}^{\\prime \\prime} & \\boldsymbol{K}_{24}^{\\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime} \\\\ 0 & \\boldsymbol{K}_{32}^{\\prime} &\n",
      "\\boldsymbol{K}_{33}^{\\prime 3} & \\boldsymbol{K}_{34}^{\\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime} \\\\ \\mathbf{0} & \\boldsymbol{K}_{42}^{\\prime} &\n",
      "\\boldsymbol{K}_{43}^{3} & \\boldsymbol{K}_{44}^{\\prime \\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime} \\end{array}\\right] $$ Note that if $\\boldsymbol{K}_{\\mathbf{A\n",
      "1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \\begin{itemize} \\item Divide row 2 by $K_{22}^{\\prime \\prime}$. \\item\n",
      "Subtract $K_{32}^{\\prime \\prime} \\times$ row 2 from row 3. \\item Subtract $K_{42}^{n} \\times$ row 2 from row 4. \\item Subtract $K_{n 2}^{\\prime \\prime} \\times$\n",
      "row 2 from row $n$. \\end{itemize} The result for the example will look like\\\\ $\\left[\\begin{array}{cccc|c}1 & \\boldsymbol{K}_{12}^{\\prime} &\n",
      "\\boldsymbol{K}_{13}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & 1 & \\boldsymbol{K}_{23}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{24}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{33}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{34}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{43}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{44}^{\\prime \\prime\\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime \\prime} \\\\ & & & & \\end{array}\\right]$ Note that only the submatrix enclosed in\n",
      "dashed lines is affected in this procedure.\\\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \\subsection*{Back\n",
      "substitution} \\begin{itemize} \\item Subtract $K_{n-1, n}^{\\prime} \\times$ row $n$ from row $n-1$. \\item Subtract $K_{n-2, n}^{\\prime} \\times$ row $n$ from row\n",
      "$n-2$.\\\\ \\vdots \\item Subtract $K_{1, n}^{\\prime} \\times$ row $n$ from row 1 \\end{itemize} After these steps the augmented matrix, for this example, will look\n",
      "like $$ \\left[\\begin{array}{cccc|c} 1 & \\bar{K}_{12}^{\\prime} & \\bar{K}_{3}^{\\prime} & 0 & F_{1}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 1 & K_{23}^{\\prime} & 0 &\n",
      "F_{2}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 0 & 1 & 0 & d_{3} \\\\ 0 & 0 & 0 & 1 & d_{4} \\end{array}\\right] $$ Note that the submatrix enclosed in dashed lines is\n",
      "unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\\prime}$ is\n",
      "altered. Now clear the second-to-last column in the coefficient matrix: \\begin{itemize} \\item Subtract $K_{n-2, n-1}^{\\prime} \\times$ row $n-1$ from row $n-2$.\n",
      "\\item Subtract $K_{n-3, n-1}^{\\prime} \\times$ row $n-1$ from row $n-3$.\\\\ \\vdots \\item Subtract $K_{1 . n-1}^{\\prime} \\times$ row $n-1$ from row 1.\n",
      "\\end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\\boldsymbol{F}$ ). Repeat as above until\n",
      "columns $\\boldsymbol{n}-2, n-3, \\ldots, 2$ are cleared. The result is (1.11.2). \\subsection*{Remarks} \\begin{enumerate} \\item In passing we note that the above\n",
      "procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss\n",
      "elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the\n",
      "procedure. However, it is felt that the given scheme is the clearest for hand calculations. \\item The numerical example with which we close this section\n",
      "illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain\n",
      "zero throughout the calculations). The reader is urged to perform the calculations. \\end{enumerate}\n",
      "\n",
      " 1: \\begin{enumerate} \\item If $n_{l p}$ is small, $K^{*}$ and $M^{*}$ will be full, but small. \\item Equation (10.3.1) amounts to solution of a multiple right-\n",
      "hand-side system with profile coefficient matrix. \\item The discrete Rayleigh-Ritz procedure is a general formalism for obtaining the reduced system. However,\n",
      "the guidelines for selecting $\\boldsymbol{P}$ are somewhat vague and thus a more systematic strategy is required for practical use. \\item The discrete Rayleigh-\n",
      "Ritz reduction is often referred to as a \"projection method.\" \\item The eigenvector approximations are defined by $\\boldsymbol{\\Psi \\cong R \\Psi^*}$. \\item By\n",
      "virtue of the fact that the calculation of trial vectors from load patterns requires solution of (10.3.1), $\\boldsymbol{K}$ must be nonsingular. This precludes\n",
      "application to cases in which there are zero eigenvalues (e.g., structures which possess rigid body modes). A simple reformulation of the original problem\n",
      "involving a positive shifting of the eigenvalues allows us to handle this case: Note that by adding and subtracting $\\boldsymbol{\\alpha M} \\boldsymbol{\\Psi}$,\n",
      "where $\\boldsymbol{\\alpha}$ is a positive number, we produce an eigenproblem \\end{enumerate} \\begin{align*} (\\boldsymbol{\\overline{K}}-\\bar{\\lambda}\n",
      "\\boldsymbol{M}) \\Psi & =0 \\tag{10.3.4}\\\\ \\boldsymbol{\\overline{K}} & =\\boldsymbol{K}+\\alpha \\boldsymbol{M} \\tag{10.3.5}\\\\ \\bar{\\lambda} & =\\lambda+\\alpha\n",
      "\\tag{10.3.6} \\end{align*} in which $\\overline{\\boldsymbol{K}}$ is positive-definite, and the eigenvectors are unchanged. The eigenvalues are related by\n",
      "(10.3.6). This stratagem may also be employed if some of the $\\lambda$ 's are negative. Suppose an estimate of the smallest eigenvalue, $\\lambda_{1}$, is\n",
      "available. Then select $\\alpha>0$ such that $-\\alpha<\\lambda_{1} \\leq 0$. The $\\bar{\\lambda}$ 's are then all positive and solution may proceed as usual.\\\\ 7.\n",
      "Shifting is a particular example of a general class of spectral transformations. Let \\begin{equation*} S=\\left[\\Psi_{1}, \\Psi_{2}, \\ldots, \\Psi_{n_{e q}}\\right]\n",
      "\\tag{10.3.7} \\end{equation*} and \\begin{equation*} \\mathbf{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{n_{\\text {eq }}}\\right)\n",
      "\\tag{10.3.8} \\end{equation*} Then it is easily verified that \\begin{equation*} \\boldsymbol{K}=\\boldsymbol{S} \\boldsymbol{\\Lambda}\n",
      "\\boldsymbol{S}^{\\boldsymbol{T}} \\tag{10.3.9} \\end{equation*} and \\begin{equation*} \\boldsymbol{M=S S^{T}} \\tag{10.3.10} \\end{equation*} Let $f=f(\\lambda)$\n",
      "represent a scalar-valued function of $\\lambda$. Define a matrix-valued function $\\boldsymbol{f}=\\boldsymbol{f}(\\boldsymbol{K})$ by way of \\begin{equation*}\n",
      "f(K)=S \\operatorname{diag}\\left(f\\left(\\lambda_{1}\\right), f\\left(\\lambda_{2}\\right), \\ldots, f\\left(\\lambda_{n_{\\text {eq }}}\\right)\\right) S^{T} \\tag{10.3.11}\n",
      "\\end{equation*} Then it is readily established that solutions of the eigenproblem \\begin{equation*} (f(K)-\\bar{\\lambda} M) \\phi=0 \\tag{10.3.12} \\end{equation*}\n",
      "are related to solutions of \\begin{equation*} (\\boldsymbol{K}-\\lambda M) \\boldsymbol{\\Psi}=\\mathbf{0} \\tag{10.3.13} \\end{equation*} by \\begin{align*}\n",
      "\\bar{\\lambda} & =f(\\lambda) \\tag{10.3.14}\\\\ \\phi & =\\psi \\tag{10.3.15} \\end{align*} \\textbf{Exercise 1.} Consider the partitioned eigenproblem defined by\n",
      "(10.2.1). Show that the statically condensed eigenproblem, (10.2.4) and (10.2.5), can be obtained from the discrete Rayleigh-Ritz approach by selecting the\n",
      "trial vectors to be \\[ R=\\left[\\begin{array}{ccc} \\quad \\quad \\quad \\boldsymbol{I} & \\tag{10.3.16}\\\\ \\hdashline-\\boldsymbol{K}_{22}^{1} & \\boldsymbol{K}_{21}\n",
      "\\end{array}\\right] \\] \\subsection*{10.4 IRONS-GUYAN REDUCTION} The Irons-Guyan reduction [4-6] fits into the discrete Rayleigh-Ritz framework described in the\n",
      "previous section. To understand the method it is helpful to rewrite the generalized eigenproblem in partitioned form: \\[ \\left(\\left[\\begin{array}{ll} K_{11} &\n",
      "K_{12} \\tag{10.4.1}\\\\ K_{21} & K_{22} \\end{array}\\right]-\\lambda\\left[\\begin{array}{ll} M_{11} & M_{12} \\\\ M_{21} & M_{22}\n",
      "\\end{array}\\right]\\right)\\left\\{\\begin{array}{l} \\psi_{1} \\\\ \\psi_{2} \\end{array}\\right\\}=0 \\] The degrees of freedom in $\\psi_{1}$ are to be retained in the\n",
      "reduced eigenproblem, whereas those in $\\psi_{2}$ are to be eliminated. For these purposes $R$ is defined by (10.3.16), which leads to the definitions of the\n",
      "reduced arrays (verify!): \\begin{align*} & \\boldsymbol{K}^{*}=\\boldsymbol{R}^{T} \\boldsymbol{K} \\boldsymbol{R}=\\boldsymbol{K}_{11}-\\boldsymbol{K}_{12}\n",
      "\\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21} \\tag{10.4.2}\\\\ & \\boldsymbol{M}^{*}=\\boldsymbol{R}^{\\boldsymbol{T}} \\boldsymbol{M}\n",
      "\\boldsymbol{R}=\\boldsymbol{M}_{11}-\\boldsymbol{M}_{12} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21}-\\boldsymbol{K}_{12}\n",
      "\\boldsymbol{K}_{22}^{-1}\\left(\\boldsymbol{M}_{21}-\\boldsymbol{M}_{22} \\boldsymbol{K}_{22}^{-1} \\boldsymbol{K}_{21}\\right) \\tag{10.4.3} \\end{align*} Note that\n",
      "(10.4.2) is the statically condensed stiffness and, if $M_{22}=0$ and $M_{12}=$ $\\boldsymbol{M}_{21}^{T}=0$, then $\\boldsymbol{M}^{*}=\\boldsymbol{M}_{11}$ as in\n",
      "the statically condensed problem of Sec. 10.2. The Irons-Guyan reduction may be thought of as invoking a \"static relation,\" namely \\begin{equation*}\n",
      "\\psi_{2}=-K_{22}^{-1} K_{21} \\psi_{1} \\tag{10.4.4} \\end{equation*} to eliminate the unwanted variables. Sometimes $\\psi_{1}$ and $\\psi_{2}$ are referred to as\n",
      "the \"master\" and \"slave\" degrees of freedom, respectively. In practice, the following guidelines are suggested:\\\\ i. Determine the number of eigenvalues and\n",
      "eigenvectors required, say $\\boldsymbol{n}_{\\text {modes }}$.\\\\ ii. Retain a multiple of $n_{\\text {modes }}$ degrees of freedom (e.g., $5 \\times n_{\\text\n",
      "{modes }}$ degrees of freedom).\\\\ iii. The degrees of freedom to be retained are identified by calculating the ratios of diagonal elements in $M$ and $K$,\n",
      "namely, $K_{p p} / M_{p p} 1 \\leq p \\leq n_{e q}$. The degrees of freedom with the smallest ratios are retained (i.e., they are the degrees of freedom in\n",
      "$\\psi_{1} ;$ [7]).\n",
      "\n",
      " 2: a. $$ \\begin{aligned} M & =\\left[\\begin{array}{ll} 3 & 0 \\\\ 0 & 2 \\end{array}\\right] \\quad K=\\left[\\begin{array}{rr} 2 & -1 \\\\ -1 & 1 \\end{array}\\right] \\\\\n",
      "0 & =\\operatorname{det}\\left[\\begin{array}{cc} 2-3 \\lambda & -1 \\\\ -1 & 1-2 \\lambda \\end{array}\\right]=6 \\lambda^{2}-7 \\lambda+1 \\\\ \\lambda_{1,2} &\n",
      "=\\frac{1}{6}, 1 \\end{aligned} $$ $$ \\begin{aligned} & \\omega_{1,2}=\\frac{1}{\\sqrt{6}}, 1=0.40825,1 \\\\ & \\psi_{1}=\\left\\{\\begin{array}{l} \\frac{2}{3} \\\\ 1\n",
      "\\end{array}\\right\\} \\quad \\psi_{2}=\\left\\{\\begin{array}{r} 1 \\\\ -1 \\end{array}\\right\\} \\end{aligned} $$ b. $$ P=\\left\\{\\begin{array}{l} \\frac{3}{2} \\\\ 2\n",
      "\\end{array}\\right\\} $$ $$ \\begin{aligned} \\boldsymbol{KR} & =\\boldsymbol{P} \\\\ \\boldsymbol{R} & =\\left\\{\\begin{array}{c} \\frac{7}{2} \\\\ \\frac{11}{2}\n",
      "\\end{array}\\right\\} \\quad \\text { (the } \\frac{1}{2} \\text { factors may be neglected) } \\\\ \\boldsymbol{K}^{*} & =\\boldsymbol{R}^{T} \\boldsymbol{K}\n",
      "\\boldsymbol{R}=\\left\\langle\\begin{array}{ll} 7 & 11 \\end{array}\\right\\rangle\\left[\\begin{array}{rr} 2 & -1 \\\\ -1 & 1 \\end{array}\\right]\\left\\{\\begin{array}{c} 7\n",
      "\\\\ 11 \\end{array}\\right\\}=65 \\\\ \\textbf{M}^{*} & =\\boldsymbol{R}^{T} M \\boldsymbol{R}=\\left\\langle\\begin{array}{ll} 7 & 11\n",
      "\\end{array}\\right\\rangle\\left[\\begin{array}{ll} 3 & 0 \\\\ 0 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} 7 \\\\ 11 \\end{array}\\right\\}=389 \\end{aligned} $$ $$\n",
      "\\left(K^{*}-\\lambda^{*} M^{*}\\right) \\psi^{*}=0 \\Rightarrow \\lambda^{*}=\\frac{65}{389}, \\quad \\psi^{*}=1 $$ $$ \\psi_{1} \\cong R\n",
      "\\psi_{1}^{*}=\\frac{1}{11}\\left\\{\\begin{array}{c} 7 \\\\ 11 \\end{array}\\right\\}=\\left\\{\\begin{array}{c} 0.64 \\\\ 1 \\end{array}\\right\\} ; \\quad\n",
      "\\omega^{*}=\\left(\\frac{65}{389}\\right)^{1 / 2}=0.4087 $$ c. \\[ \\left. \\begin{aligned} \\frac{K_{11}}{M_{11}} &= \\frac{2}{3} \\\\ \\frac{K_{22}}{M_{22}} &=\n",
      "\\frac{1}{2} \\end{aligned} \\right\\} \\quad \\Rightarrow \\text{ retain degree of freedom number 2} \\] Reorder equations into the standard partitioned form. $$\n",
      "\\begin{aligned} K & =\\left[\\begin{array}{rr} 1 & -1 \\\\ -1 & 2 \\end{array}\\right] \\quad M=\\left[\\begin{array}{ll} 2 & 0 \\\\ 0 & 3 \\end{array}\\right] \\\\ R &\n",
      "=\\left\\{\\begin{array}{c} 1 \\\\ -K_{22}^{-1} K_{21} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} 1 \\\\ \\frac{1}{2} \\end{array}\\right\\} \\\\ K^{*} & =R^{T} K\n",
      "R=\\frac{1}{2} \\\\ M^{*} & =R^{T} M R=\\frac{11}{4} \\end{aligned} $$ $$ \\lambda^{*}=\\frac{2}{11}, \\quad \\omega^{*}=\\left(\\frac{2}{11}\\right)^{1 / 2}=0.4264 \\quad\n",
      "\\psi_{1}^{*}=1 $$ $$ \\psi_{1} \\cong R \\psi_{1}^{*}=\\left\\{\\begin{array}{l} 1 \\\\ \\frac{1}{2} \\end{array}\\right\\}, \\quad \\text { (reorder): } $$ $$\n",
      "\\Psi_{1}=\\left\\{\\begin{array}{l} \\frac{1}{2} \\\\ 1 \\end{array}\\right\\} $$ d. $P=\\left\\{\\begin{array}{l}0 \\\\ 1\\end{array}\\right\\} ; \\quad \\boldsymbol{K}\n",
      "\\boldsymbol{R}=P ; \\quad \\boldsymbol{R}=\\left\\{\\begin{array}{l}1 \\\\ 2\\end{array}\\right\\}$ $\\left.\\begin{array}{l}K^{*}=2 ; \\quad M^{*}=11 ; \\quad\n",
      "\\lambda^{*}=\\frac{2}{11} \\quad \\text { (same as (c)) } \\\\ \\omega^{*}=0.4264 \\quad \\psi_{1} \\cong\\left\\{\\begin{array}{l}\\frac{1}{2} \\\\\n",
      "1\\end{array}\\right\\}\\end{array}\\right\\}$ iteration number 1 \\\\ $\\boldsymbol{P}=\\boldsymbol{M} \\psi_{1}=\\left\\{\\begin{array}{l}\\frac{3}{2} \\\\\n",
      "2\\end{array}\\right\\} \\quad$ (same as (b))\\\\ Therefore $$ \\omega *=0.4087 \\quad \\psi_{1} \\cong\\left\\{\\begin{array}{c} 0.64 \\\\ 1 \\end{array}\\right\\} $$\n",
      "\n",
      " 3: Another possibility is to eliminate $\\boldsymbol{v}_{n+1}$ from (8.1.3) via (8.1.8). Thus in place of (8.1.9) we have \\begin{equation*} \\frac{1}{\\alpha\n",
      "\\Delta t}(M+\\alpha \\Delta t K) d_{n+1}=F_{n+1}+\\frac{1}{\\alpha \\Delta t} M \\widetilde{d}_{n+1} \\tag{8.1.11} \\end{equation*} In this implementation, (8.1.11) is\n",
      "used to determine $\\boldsymbol{d}_{n+1}$ and then $\\boldsymbol{v}_{n+1}$ may be determined from (8.1.8), i.e., \\begin{equation*}\n",
      "v_{n+1}=\\frac{d_{n+1}-\\widetilde{d}_{n+1}}{\\alpha \\Delta t} \\tag{8.1.12} \\end{equation*} The advantage of this implementation occurs when $M$ is diagonal. In\n",
      "this case the calculation of the right-hand side of (8.1.11) may be performed much more economically than the right-hand side of (8.1.9). The equation-solving\n",
      "burden is of course the same for (8.1.9) and (8.1.11).\n",
      "\n",
      " 4: Upon encountering representations such as (7.1.17) and (7.1.18) for the first time, we may wonder if the approximation is valid only when the exact solution\n",
      "is \"separable\" in $\\boldsymbol{x}$ and $\\boldsymbol{t}$ (e.g., see [1] for a discussion of the separation-of-variables\\\\ technique). This is not the case. Even\n",
      "nonseparable exact solutions may be approximated arbitrarily closely by representations like (7.1.17) and (7.1.18). To develop the matrix equations, we\n",
      "substitute (7.1.17) and (7.1.18) into (7.1.15) and (7.1.16) and proceed along the same lines as in Secs. 2.4 and 2.5. The end result is the following matrix\n",
      "problem: Given $F:] 0, T\\left[\\rightarrow \\mathbb{R}^{n_{e q}}\\right.$, find $d:[0, T] \\rightarrow \\mathbb{R}^{n_{e q}}$ such that \\begin{align*} M \\dot{d}+K d\n",
      "& =F \\quad t \\in] 0, T[ \\tag{7.1.19}\\\\ d(0) & =d_{0} \\tag{7.1.20} \\end{align*} where \\begin{align*} M & =A_{e=1}^{n_{e l}}\\left(m^{e}\\right) \\tag{7.1.21}\\\\ m &\n",
      "=\\left[m_{a b}^{e}\\right] \\tag{7.1.22}\\\\ m_{a b}^{e} & =\\int_{\\boldsymbol{\\Omega}^{e}} N_{a} \\rho c N_{b} d \\Omega \\tag{7.1.23} \\end{align*} \\begin{align*}\n",
      "\\boldsymbol{K} & =\\boldsymbol{A}_{e=1}^{n_{e l}}\\left(k^{e}\\right) \\tag{7.1.24}\\\\ k^{e} & =\\left[k_{a b}^{e}\\right] \\tag{7.1.25}\\\\ k_{a b}^{e} &\n",
      "=\\int_{\\boldsymbol{\\Omega}^{e}} \\boldsymbol{B}_{a}^{T} D \\boldsymbol{B}_{b} d \\Omega \\tag{7.1.26} \\end{align*} $(M)$ \\begin{equation*} F(t)=F_{\\text {nodal\n",
      "}}(t)+\\mathbf{A}_{e=1}^{n_{el}}\\left(f^{e}(t)\\right) \\tag{7.1.27} \\end{equation*} \\begin{align*} & f^{e}=\\left\\{f_{a}^{e}\\right\\} \\tag{7.1.28}\\\\ &\n",
      "f_{a}^{e}=\\int_{\\Omega^{e}} N_{a} f d \\Omega+\\int_{\\Gamma_{k}^{e}} N_{a} h d \\Gamma-\\sum_{b=1}^{n_{e}}\\left(k_{a b}^{e} q_{b}^{e}+m_{a b}^{e}\n",
      "\\dot{g}_{b}^{e}\\right) \\tag{7.1.29} \\end{align*} \\begin{align*} & d_{0}=M^{-1} \\hat{A}_{e=1}^{n_{e}}\\left(\\hat{d}^{e}\\right) \\tag{7.1.30}\\\\ &\n",
      "\\hat{d}^{e}=\\left\\{\\hat{d}_{a}^{e}\\right\\} \\tag{7.1.31}\\\\ & \\hat{d}_{a}^{e}=\\int_{\\Omega^{e}} N_{a} \\rho c u_{0} d \\Omega-\\sum_{b=1}^{n_{en}} m_{a b}^{e}\n",
      "g_{b}^{e}(0) \\tag{7.1.32} \\end{align*}\n",
      "\n",
      " 5: \\begin{enumerate} \\item If we compare with the steady formulation of Chapter 2 , we see that the only new matrix which appears is $M$, the capacity matrix.\n",
      "That $M$ is symmetric and positive-definite follows directly from its definition. \\item Equation (7.1.19) is a coupled system of ordinary differential\n",
      "equations. Thus to solve the initial-value problem, i.e., (7.1.19) and (7.1.20), we need to introduce algorithms for solving systems of ordinary differential\n",
      "equations. This subject is taken up in subsequent chapters. \\item Observe in (7.1.29) that the element capacity matrices come into play in accounting for the\n",
      "effect of specified boundary temperatures. \\item It is common in practice to simplify the specification of initial conditions. That is, rather than employ\n",
      "(7.1.30) through (7.1.32), which emanate from (7.1.16), we directly specify $\\boldsymbol{d}_{0}$ such that the given nodal values are interpolated [i.e.,\n",
      "$d_{04}=$ $\\left.u_{0}\\left(x_{A}\\right), A \\in \\eta-\\eta_{a}\\right]$ \\item The element capacity matrix $m^{e}$ turns out to be virtually identical to the\n",
      "element \"mass\" matrix, which will be introduced in the next section. Consequently, we shall postpone more detailed consideration of the structure of $m^{e}$\n",
      "until later. \\item Recall that $g_{b}^{e}(t)=0$ if degree of freedom $b$ of element $e$ is not specified. The same rule applies to $\\dot{g}_{b}^{e}$.\n",
      "\\end{enumerate} Exercise 2. The details of arriving at (7.1.19) through (7.1.32) are straightforward given familiarity with the analogous developments in\n",
      "Chapter 2. If the results are not \"obvious,\" the reader should fill in all details in step-by-step fashion.\n",
      "\n",
      " 6: Replace $\\xi$ and $\\eta$ by $r$ and $s$, respectively; set $t=1-r-s$ throughout.\n",
      "\n",
      " 7: The calculation of the reduced mass, (10.4.3), is costly! \\subsection*{10.5 SUBSPACE ITERATION} A disadvantage of reduction techniques such as the Irons-\n",
      "Guyan procedure is that there is no guarantee that the eigenvalues and eigenvectors of the reduced problem,\\\\ namely, $\\lambda_l^*$ and $R \\psi_l^*$, will be\n",
      "good approximations of those of the original problem, $\\lambda_{l}$ and $\\psi_{l}$, respectively. Consequently, methods have been developed in which a reduced\n",
      "problem is used along with an iterative strategy to obtain exactly the lower modes of the generalized eigenproblem. This is the underlying idea of the subspace\n",
      "iteration, or block power, method, which is widely used for large-scale finite element calculations. Roughly speaking, the procedure is as follows: Load\n",
      "patterns are selected and trial vectors are calculated. The trial vectors are used to form a reduced problem, which is solved. New load patterns are calculated\n",
      "from the \"inertial\" forces engendered by the eigenvectors of the reduced problem, namely, \\begin{equation*} \\underbrace{\\boldsymbol{P}}_{n_{e q} \\times n_{lp}}=\n",
      "\\boldsymbol{MR}\\left[\\psi_{1}^{*}, \\psi_{2}^{*}, \\ldots, \\psi_{n_{lp}}^{*}\\right] \\tag{10.5.1} \\end{equation*} With these load patterns the process is repeated\n",
      "until convergence is achieved. The calculations are summarized in the flowchart contained in Table 10.5.1. Extensive discussion and further details are\n",
      "presented in [8].\n",
      "\n",
      " 8: amplitude of $\\boldsymbol{q}_{j-2}$ contained in $\\overline{\\boldsymbol{j}}_{j}$ is found to be \\begin{equation*}\n",
      "\\boldsymbol{\\gamma}_{j}=\\boldsymbol{q}_{j-2}^{T} \\boldsymbol{M} \\bar{r}_{j} \\tag{10.6.17} \\end{equation*} Following the procedure used to derive (10.6.14), this\n",
      "leads to \\begin{equation*} \\gamma_{j}=q_{j}^{T} M r_{j-2}+\\alpha_{j-2} q_{j}^{T} M q_{j-2}+\\beta_{j-2} q_{j}^{T} M q_{j-3}+\\gamma_{j-2} q{ }_{j}^{T} M\n",
      "q_{j-4}+\\cdots \\tag{10.6.18} \\end{equation*} But, using the normalizing relationship equivalent to (10.6.15), $r_{j-2}=\\beta_{j-1} q_{j-1}$. Hence, when this is\n",
      "substituted into (10.6.18) all terms on the right-hand side vanish, with the result that $\\gamma_{j}=0$. A corresponding procedure could be used to demonstrate\n",
      "that all further terms in the expansion for $\\bar{r}_{j}$, (10.6.10), vanish; in other words, the orthogonalization procedure used in generating each Lanczos\n",
      "vector need be applied only to the previous two vectors. A summary of the Lanczos algorithm is presented in Table 10.6.1.\\\\ TABLE 10.6.1. The Lanczos\n",
      "Algorithm\\\\ Given an arbitrary vector $r_{0}$ then: \\begin{enumerate} \\item Set\\\\ a. $g_{0}=0$\\\\ b. $\\quad \\beta_{1}=\\left(r_{0}^{T} M r_{0}\\right)^{1 / 2}$\\\\\n",
      "c. $q_{1}=\\frac{r_{0}}{\\beta_{1}}$\\\\ d. $p_{1}=M q_{1}$ \\item For $j=1,2, \\ldots$, repeat:\\\\ a.\n",
      "$\\overline{\\boldsymbol{r}}_{\\boldsymbol{j}}=\\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{p}_{j}$\\\\ b. $\\quad \\hat{r}_{j}=\\bar{r}_{j}-q_{j-1} \\beta_{j}$\\\\ c.\n",
      "$\\alpha_{j}=q_{j}^{T} M \\hat{r}_{j}=p_{j}^{T} \\hat{r}_{j}$\\\\ d. $r_{j}=\\hat{r}_{j}-q_{j} \\alpha_{j}$\\\\ e. $\\bar{p}_{j}=M r_{j}$\\\\ f. $\\quad\n",
      "\\beta_{j+1}=\\left(r_{j}^{T} M r_{j}\\right)^{1 / 2}=\\left(\\bar{p}_{j}^{T} r_{j}\\right)^{1 / 2}$\\\\ g. if enough vectors, then terminate the loop\\\\ h.\n",
      "$q_{j+1}=\\frac{1}{\\beta_{j+1}} r_{j}$\\\\ i. $\\quad p_{j+1}=\\frac{1}{\\beta_{j+1}} \\bar{p}_{j}$ \\end{enumerate} The above process may be started from a random\n",
      "vector, $r_{0}$, with $\\boldsymbol{q}_{0}=0$ and $\\beta_{1}=\\left(r_{0}^{T} M r_{0}\\right)^{1 / 2}$. At a typical step, $j$, the Lanczos algorithm computes\n",
      "$\\alpha_{j}, \\beta_{j+1}$, and $\\boldsymbol{q}_{j+1}$, in order. In addition to the storage needs of $\\boldsymbol{K}_{\\boldsymbol{\\sigma}}$ and\n",
      "$\\boldsymbol{M}$, the algorithm requires storage for five vectors of length $n$; one for each of the vectors, $q_{j-1}, q_{j}, M r_{j}, p_{j}$, and $r_{j}$. The\n",
      "total cost for one step of the algorithm involves a multiply with $M$, the solution of a system of equations with $\\boldsymbol{K}_{\\sigma}$ as the coefficient\n",
      "matrix, two inner products and four products of a scalar by a vector. \\subsection*{10.6.6 Reduction to Tridiagonal Form} Using the results of the previous\n",
      "section, $(10.6 .10)$ can be rewritten as the three-term relation \\begin{equation*} r_{j}=\\beta_{j+1} q_{j+1}=K_{\\sigma}^{-1} M q_{j}-q_{j} \\alpha_{j}-q_{j-1}\n",
      "\\beta_{j} \\tag{10.6.19} \\end{equation*} where $\\alpha_{j}=\\boldsymbol{q}_{j}^{\\boldsymbol{T}} \\boldsymbol{M} \\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{M}\n",
      "\\boldsymbol{q}_{j}$ and $\\boldsymbol{r}_{j}$ is normalized with respect to the mass matrix to obtain $\\boldsymbol{q}_{j+1}$ with normalizing factor\n",
      "$\\boldsymbol{\\beta}_{j+1}=\\left(\\boldsymbol{r}_{j}^{T} \\boldsymbol{M} r_{j}\\right)^{1 / 2}$. After $m$ Lanczos steps all the quantities obtained from equation\n",
      "(10.6.19) can be arranged in a global matrix form \\[ \\left[ K_{\\sigma}^{-1} M \\right] \\left[ Q_m \\right] - \\left[ Q_m \\right] \\left[ T_m \\right] = \\left[ 0\n",
      "\\quad 0 \\quad \\cdots \\quad 0 \\quad r_m \\right] = r_m e_m^T \\tag{10.6.20} \\] Here \\( e_m^T = \\langle 0, 0, \\cdots, 0, 1 \\rangle \\), \\( Q_m \\) is an \\( n \\times m\n",
      "\\) matrix with columns \\( q_i, i = 1, 2, \\cdots, m \\), and \\( T_m \\) is a tridiagonal matrix of the form \\[ T_m = \\begin{bmatrix} \\alpha_1 & \\beta_2 & 0 &\n",
      "\\cdots & 0 & 0 \\\\ \\beta_2 & \\alpha_2 & \\beta_3 & \\cdots & 0 & 0 \\\\ 0 & \\beta_3 & \\alpha_3 & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots &\n",
      "\\vdots \\\\ 0 & 0 & 0 & \\cdots & \\alpha_{m-1} & \\beta_m \\\\ 0 & 0 & 0 & \\cdots & \\beta_m & \\alpha_m \\end{bmatrix} \\tag{10.6.21} \\] The orthogonality property of\n",
      "the Lanczos vectors, $\\boldsymbol{Q}_{m}^{T} \\boldsymbol{M} \\boldsymbol{Q}_{m}=\\boldsymbol{I}_{m}$, where $\\boldsymbol{I}_{m}$ is the $m \\times m$ identity\n",
      "matrix, can be used in (10.6.20) to obtain \\begin{equation*} \\boldsymbol{Q}_{m}^{T} \\boldsymbol{M} \\boldsymbol{K}_{\\boldsymbol{\\sigma}}^{-1} \\boldsymbol{M}\n",
      "\\boldsymbol{Q}_{m}=\\boldsymbol{T}_{m} \\tag{10.6.22} \\end{equation*} Choosing the set of Lanczos vectors, $\\boldsymbol{Q}_{m}$, for the trial vectors, the\n",
      "Rayleigh-Ritz procedure can be used to obtain the best approximation to the eigenvectors of (10.6.6). The approximating Ritz vectors will then be of the form\n",
      "\\begin{equation*} y_{i}^{(m)}=\\boldsymbol{Q}_{m} s_{i}^{(m)}, \\quad i=1, \\ldots, m \\tag{10.6.23} \\end{equation*} When a residual vector,\n",
      "$\\left\\{\\boldsymbol{K}_{\\sigma}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{i}^{(m)}-\\theta_{i}^{(m)} y_{i}^{(m)}\\right\\}$, associated with the pair\n",
      "$\\left\\{\\theta_{i}^{(m)}, y_{i}^{(m)}\\right\\}$ is $M$-orthogonal to the set of Lanczos vectors, then $\\left\\{\\theta_{i}^{(m)}, y_{j}^{(m)}\\right\\}$ is a Ritz\n",
      "pair. Accordingly $$ \\boldsymbol{Q}_{m}^{T} \\boldsymbol{M}\\left\\{\\boldsymbol{K}_{\\sigma}^{-1} M \\boldsymbol{y}_{i}^{(m)}-\\theta_{i}^{(m)}\n",
      "\\boldsymbol{y}_{i}^{(m)}\\right\\}=0 $$ Using the relation between the Ritz vector, $y_{i}^{(m)}$ and the Lanczos vectors, given in (10.6.23), together with the\n",
      "orthonormality condition of the $q$ 's, and the tridiagonal properties of the Lanczos vectors, (10.6.22), the above equation reduces to the tridiagonal\n",
      "eigenproblem \\begin{equation*} T_{m} s_{i}^{(m)}-\\theta_{i}^{(m)} s_{i}^{(m)}=0 \\tag{10.6.24} \\end{equation*} Thus $\\left\\{\\theta_{i}^{(m)},\n",
      "s_{i}^{(m)}\\right\\}$ is an eigenpair of the tridiagonal matrix, $\\boldsymbol{T}_{m}$. As the total number of Lanczos vectors increases, i.e., as we take more\n",
      "Lanczos steps, the size of the tridiagonal matrix increases and the eigenvalues of $T_{m}$ converge to the eigenvalues of the transformed problem (10.6.6), $1\n",
      "/\\left(\\lambda_{i}-\\sigma\\right)$. When $m=n$, the order of $K$, then $\\theta_{i}^{(n)}=1 /\\left(\\lambda_{i}-\\sigma\\right)$ for all $i$, but we hope to stop\n",
      "long before $m=n$. The steps enumerated in Step 2 of Table 10.6.1 are repeated until the Ritz pairs $\\left\\{\\theta_{i}^{(m)}, y_{i}^{(m)}\\right\\}$ have\n",
      "sufficiently converged to the desired eigenpairs.\n",
      "\n",
      " 9: Replace $\\xi, \\eta$, and $\\zeta$ by $r, s$, and $t$, respectively; set $u=1-r-s-t$ throughout.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9  # try different QAs\n",
    "\n",
    "print('Q:')\n",
    "print_wrapped(df.iloc[i,:]['question'])\n",
    "print('A:')\n",
    "print_wrapped(df.iloc[i,:]['answer'])\n",
    "print('\\nChunk used for Q generation:')\n",
    "print_wrapped(df.iloc[i,:]['question_chunk'])\n",
    "print('\\nRetrieved context:')\n",
    "for item in df.iloc[i,:]['context'].split('Additional context'):\n",
    "    print_wrapped(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

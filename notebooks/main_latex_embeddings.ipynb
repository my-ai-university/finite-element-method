{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading functions from the scripts\n",
    "\"\"\"\n",
    "Mostafa:\n",
    "I used the new structured output for question generation.\n",
    "It's a beta version, but it works on my end (10/23/2024).\n",
    "https://platform.openai.com/docs/guides/structured-outputs/structured-outputs\n",
    "\n",
    "For answer generation, I had some issues, so I used the standard API.\"\n",
    "\n",
    "Please upgrade before running this notebook: pip install --upgrade openai\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Get parent directory of the notebook \n",
    "sys.path.append(parent_dir)  #  to the Python path\n",
    "\n",
    "from scripts.chunking import process_latex_files\n",
    "from scripts.embedding import get_embeddings, fixed_knn_retrieval\n",
    "from scripts.prompts import gen_questions, gen_questions_s, gen_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting API and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "production_mode = True\n",
    "chunk_by_section = True\n",
    "chpt_for_quest_answ = \"2021-fall\"\n",
    "author_for_quest_answ = \"students\"            # hughes, garikipati, or students  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I suggest using 'gpt-4o' for production runs, but it is more expensive.\n",
    "For embeddings, I recommend 'text-embedding-3-large.' We only need to run it once, but it also costs more.\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "# https://openai.com/index/new-embedding-models-and-api-updates/\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "\"\"\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "if production_mode == False:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "elif production_mode == True:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "\n",
    "embedding_model = f\"text-embedding-3-{embedding_size}\"  # NOTE: this must be the same for all embeddings. \n",
    "author_for_quest_answ = author_for_quest_answ.lower()\n",
    "\n",
    "# Setting path for root data folder\n",
    "main_dir = f'../data/{author_for_quest_answ}_latex_Q_then_A_use_context'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Context Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding space filename: ../data/students_latex_Q_then_A_use_context/students_latex_embedding_space_by_sections_tpc4096_large.json\n",
      "loaded\n",
      "Space size: (221, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I used fixed size chunks (512) with a 25% overlap.\n",
    "Make sure environment_sensitive is set to False for fixed size.\n",
    "\n",
    "We should embed all chapters to generate the embedding space. For the demo, I only included two chapters.\n",
    "please update the paths in latex_file_paths.\n",
    "\"\"\"\n",
    "\n",
    "# add all book chapters paths\n",
    "\n",
    "if author_for_quest_answ.lower() == \"hughes\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "    ]\n",
    "\n",
    "elif author_for_quest_answ == \"garikipati\" or \"students\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter101.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter102.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter103.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter104.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter105.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter106.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter107.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter108.tex',\n",
    "    ]\n",
    "\n",
    "tokens_per_chunk = 4096                         # was 512\n",
    "token_overlap = int(0.25 * tokens_per_chunk)    # 25% overlap\n",
    "environment_sensitive = False                   # If False, equations can split between two chunks, but chunk lengths remain fixed.\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_tpc{tokens_per_chunk}_o{token_overlap}_{embedding_size}.json'\n",
    "elif chunk_by_section == True:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_by_sections_tpc{tokens_per_chunk}_{embedding_size}.json'\n",
    "    token_overlap = 0\n",
    "print(f\"embedding space filename: {embedding_space_file_name}\")\n",
    "\n",
    "space = {}\n",
    "if not os.path.exists(embedding_space_file_name):\n",
    "    \n",
    "    chunks = process_latex_files(latex_file_paths, \n",
    "                                 tokens_per_chunk, \n",
    "                                 token_overlap, \n",
    "                                 environment_sensitive, \n",
    "                                 chunk_by_section = chunk_by_section)\n",
    "    \n",
    "    chunk_length = []\n",
    "    char_length = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        print(f\"chunk word length: {len(chunk.split(\" \"))}, chunk char length: {len(chunk)}, chunk = {chunk}\")\n",
    "        chunk_length.append(len(chunk.split(\" \")))\n",
    "        char_length.append(len(chunk))\n",
    "    print(f\"max chunk length in words = {np.max(chunk_length)}\")\n",
    "    print(f\"max chunk length in char = {np.max(char_length)}\")\n",
    "    #print(f\"chunk lengths = {chunk_length}\")\n",
    "\n",
    "    # using api\n",
    "    embedding_space = get_embeddings(client, chunks, model=embedding_model)\n",
    "    \n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'w') as json_file:\n",
    "        json.dump({'embedding_model': embedding_model, 'chunks': chunks, 'embedding_space': embedding_space}, json_file)\n",
    "\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'r') as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "\n",
    "    chunks = loaded_data['chunks']\n",
    "    embedding_space = np.array(loaded_data['embedding_space'])\n",
    "    print(\"loaded\")\n",
    "\n",
    "chunks = np.array(chunks)\n",
    "embedding_space = np.array(embedding_space)\n",
    "print(\"Space size:\", embedding_space.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Questions and Their Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|██████████| 1/1 [00:00<00:00, 6269.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are generated\n",
      "Questions are embedded\n",
      "saved ../data/students_latex_Q_then_A_use_context/students_Qs_n40_tpc1536_o307.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "For generating questions, we want larger chunks with a bit of overlap.\n",
    "The following values are just for this demo, so please adjust them as needed.\n",
    "\n",
    "I only ran Chapter One.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "if author_for_quest_answ == \"hughes\":\n",
    "    latex_file_path = f'../data/FEM_Hughes_LaTeX_Textbook/chapter{chpt_for_quest_answ}.tex'\n",
    "elif author_for_quest_answ == \"garikipati\":\n",
    "    latex_file_path = f'../data/FEM_Garikipati_lectures/chapter{chpt_for_quest_answ}.tex'\n",
    "\n",
    "max_questions = 40                             # max number of questions per chunk\n",
    "\n",
    "tokens_per_chunk = 1536                       \n",
    "token_overlap = int(0.2 * tokens_per_chunk)   # 10% overlap\n",
    "environment_sensitive = True                  # If True, equations won't be split between chunks, which may result in chunks larger than the specified tokens_per_chunk\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def embed_all_q(questions):\n",
    "    all_questions = []\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            all_questions.append(sub_item['question'])\n",
    "    # using api\n",
    "    embeddings = get_embeddings(client, all_questions, model = embedding_model) \n",
    "    # add them to data:\n",
    "    k = 0\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            sub_item['embedding'] = embeddings[k]\n",
    "            k +=1\n",
    "    print('Questions are embedded')\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if author_for_quest_answ == \"hughes\" or author_for_quest_answ == \"garikipati\":\n",
    "    if chunk_by_section == False:\n",
    "        questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "    elif chunk_by_section == True:\n",
    "        questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_Qs_n{max_questions}_by_sections_tpc{tokens_per_chunk}.json\"  \n",
    "        token_overlap = 0 \n",
    "elif author_for_quest_answ == \"students\":\n",
    "    questions_file_name = f\"{main_dir}/{author_for_quest_answ}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"\n",
    "    token_overlap = 0\n",
    "\n",
    "if not os.path.exists(questions_file_name):\n",
    "    questions = {}  # main data\n",
    "\n",
    "    # we should save generation info we used\n",
    "    questions['info'] = {\n",
    "        'tokens_per_chunk': tokens_per_chunk,\n",
    "        'token_overlap': token_overlap,\n",
    "        'environment_sensitive': environment_sensitive,\n",
    "        'max_questions': max_questions,\n",
    "        'embedding_model': embedding_model,\n",
    "        'llm_model_questions': llm_model_questions,\n",
    "        'llm_model_answers': llm_model_answers\n",
    "    }\n",
    "\n",
    "    if author_for_quest_answ == \"garikipati\" or author_for_quest_answ == \"hughes\":\n",
    "        question_chunks = process_latex_files(latex_file_path, tokens_per_chunk, token_overlap, environment_sensitive, chunk_by_section=chunk_by_section)\n",
    "    \n",
    "        if production_mode == False:\n",
    "            question_chunks = question_chunks[0:1] # for testing small batch\n",
    "        \n",
    "        for question in question_chunks:\n",
    "            print(f\"chunk word length: {len(question.split(\" \"))}, chunk char length: {len(question)}, chunk = {question}\")\n",
    "\n",
    "    \n",
    "    elif author_for_quest_answ == 'students':\n",
    "        question_path = f'../data/FEM_Student_questions/{chpt_for_quest_answ}.csv'\n",
    "        df_students = pd.read_csv(question_path, index_col=False)\n",
    "        if production_mode == False:\n",
    "            df_students = df_students.iloc[0:1]\n",
    "        question_chunks = df_students['Directory'].to_list()\n",
    "        question_chunks = [f\"{chpt_for_quest_answ}-{str(i)}\" for i in question_chunks]\n",
    "        q_for_chunk = df_students['Questions'].to_list()\n",
    "        q_for_chunk = [literal_eval(i) for i in q_for_chunk]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"author for question/answer not supported\")\n",
    "\n",
    "    ## step 1: generate questions\n",
    "    questions['data'] = []\n",
    "    for i in tqdm(range(len(question_chunks)), desc=\"Generating Questions\"):\n",
    "        # q_for_chunk = gen_questions(client, question_chunks[i], max_questions, model=llm_model_questions)\n",
    "        # return list of dictionaries w/ 'question' and 'coverage' keys\n",
    "        if author_for_quest_answ == \"garikipati\" or author_for_quest_answ == \"hughes\":\n",
    "            q_for_chunk = gen_questions_s(client, question_chunks[i], max_questions, model=llm_model_questions)   # Using the new function\n",
    "            questions['data'].append({'chunk': question_chunks[i],'questions': q_for_chunk})\n",
    "        elif author_for_quest_answ == \"students\":\n",
    "            questions['data'].append({'chunk': (question_chunks[i]),'questions': q_for_chunk[i]})\n",
    "    print('Questions are generated')\n",
    "\n",
    "    ## step 2: embedding all questions at once\n",
    "    questions = embed_all_q(questions)\n",
    "    \n",
    "\n",
    "    with open(questions_file_name, 'w') as json_file:\n",
    "        json.dump(questions, json_file, indent=4)\n",
    "    print('saved', questions_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_file_name, 'r') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_file_name)\n",
    "\n",
    "if questions['info']['embedding_model'] != embedding_model:\n",
    "    print(\"embedding model mismatch. re-embedding questions\")\n",
    "    questions = embed_all_q(questions)\n",
    "    questions['info']['embedding_model'] = embedding_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Retrieval and Generating Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k context added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions: 100%|██████████| 1/1 [00:43<00:00, 43.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are answered\n",
      "saved ../data/students_latex_Q_then_A_use_context/students_2021-fall_QAs_n40_topk10_by_sections.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "Since we answer each question separately, this process is slow.\n",
    "We might want to consider using the batch API for this.\n",
    "\"\"\"\n",
    "\n",
    "top_k = 10   # number of retrieved closest contexts         \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if author_for_quest_answ == \"hughes\" or author_for_quest_answ == \"garikipati\":\n",
    "    if chunk_by_section == False:\n",
    "        questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "    elif chunk_by_section == True:\n",
    "        questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"   \n",
    "elif author_for_quest_answ == \"students\":\n",
    "    questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"\n",
    "\n",
    "if not os.path.exists(questions_answers_file_name):\n",
    "\n",
    "    questions_answers = questions.copy()\n",
    "\n",
    "    # step 1) finding top_k context from the book embedding and adding them to each question\n",
    "    for item in questions_answers['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            ind = fixed_knn_retrieval(sub_item['embedding'], embedding_space, top_k)\n",
    "            context = ''\n",
    "            for i, chunk in enumerate(chunks[ind]):\n",
    "                context += f'\\n\\n Additional context {i}: {chunk}' \n",
    "            sub_item['context'] = context\n",
    "    print('top_k context added')\n",
    "\n",
    "    # step 2) generating answers (slow)  (should we try batch API?)\n",
    "    for item in tqdm(questions_answers['data'], desc=\"Answering Questions\"):\n",
    "        question_chunk = item['chunk']\n",
    "        for sub_item in item['questions']:\n",
    "            question = sub_item['question']\n",
    "            context = question_chunk + sub_item['context']\n",
    "            sub_item['answer'] = gen_answer(client, question, context, model = llm_model_answers)\n",
    "    print('Questions are answered')\n",
    "    \n",
    "    with open(questions_answers_file_name, 'w') as json_file:\n",
    "        json.dump(questions_answers, json_file, indent=4)\n",
    "    print('saved', questions_answers_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_answers_file_name, 'r') as json_file:\n",
    "        questions_answers = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_answers_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I think it's better to work with JSON/DataFrame in the code, but for reviewing QAs, CSV is easier to work with\n",
    "\"\"\"\n",
    "\n",
    "csv_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}.csv\"   \n",
    "# ----------------------------------\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in questions_answers['data']:\n",
    "    question_chunk = item['chunk']\n",
    "    for sub_item in item['questions']:\n",
    "        new_item = {}\n",
    "        new_item['question_chunk'] = question_chunk\n",
    "        for k,v in sub_item.items():\n",
    "            if k == 'embedding':\n",
    "                continue\n",
    "            new_item[k] = v\n",
    "        data.append(new_item)\n",
    "\n",
    "# data[0]\n",
    "df = pd.DataFrame(data)[['question_chunk','context','coverage','question','answer']]\n",
    "df.to_csv(csv_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=160):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9  # try different QAs\n",
    "\n",
    "print('Q:')\n",
    "print_wrapped(df.iloc[i,:]['question'])\n",
    "print('A:')\n",
    "print_wrapped(df.iloc[i,:]['answer'])\n",
    "print('\\nChunk used for Q generation:')\n",
    "print_wrapped(df.iloc[i,:]['question_chunk'])\n",
    "print('\\nRetrieved context:')\n",
    "for item in df.iloc[i,:]['context'].split('Additional context'):\n",
    "    print_wrapped(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

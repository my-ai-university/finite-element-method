{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/Documents/repos/comp-phys-transformer/scripts/prompts.py:186: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/home/ben/Documents/repos/comp-phys-transformer/scripts/prompts.py:186: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/home/ben/Documents/repos/comp-phys-transformer/scripts/prompts.py:186: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/home/ben/Documents/repos/comp-phys-transformer/scripts/prompts.py:186: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "/home/ben/Documents/repos/comp-phys-transformer/scripts/prompts.py:186: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading functions from the scripts\n",
    "\"\"\"\n",
    "Mostafa:\n",
    "I used the new structured output for question generation.\n",
    "It's a beta version, but it works on my end (10/23/2024).\n",
    "https://platform.openai.com/docs/guides/structured-outputs/structured-outputs\n",
    "\n",
    "For answer generation, I had some issues, so I used the standard API.\"\n",
    "\n",
    "Please upgrade before running this notebook: pip install --upgrade openai\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Get parent directory of the notebook \n",
    "sys.path.append(parent_dir)  #  to the Python path\n",
    "\n",
    "from scripts.chunking import process_latex_files\n",
    "from scripts.embedding import get_embeddings, fixed_knn_retrieval\n",
    "from scripts.prompts import gen_questions, gen_questions_s, gen_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting API and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I suggest using 'gpt-4o' for production runs, but it is more expensive.\n",
    "For embeddings, I recommend 'text-embedding-3-large.' We only need to run it once, but it also costs more.\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "# https://openai.com/index/new-embedding-models-and-api-updates/\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "\"\"\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "production_mode = False\n",
    "chunk_by_section = True\n",
    "chpt_for_quest_answ = 1\n",
    "\n",
    "if production_mode == False:\n",
    "    llm_model = \"gpt-4o-mini\"\n",
    "    embedding_model = \"text-embedding-3-small\"    # NOTE: this must be the same for all embeddings. \n",
    "elif production_mode == True:\n",
    "    llm_model = \"gpt-4o\"\n",
    "    embedding_model = \"text-embedding-3-large\"    # NOTE: this must be the same for all embeddings. \n",
    "\n",
    "\n",
    "# Setting path for root data folder\n",
    "main_dir = '../data/hughes_latex_Q_then_A_use_context'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Context Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\maketitle', 'The main constituents of a finite element method for the solution of a boundary-value problem are\\\\\\\\ i. The variational or weak statement of the problem; and\\\\\\\\ ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple example.\\\\\\\\ Suppose we want to solve the following differential equation for $u$ : \\\\begin{equation*} u_{, x x}+f=0 \\\\tag{1.1.1} \\\\end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \\\\begin{equation*} f: [0,1] \\\\to \\\\mathbb{R} \\\\tag{1.1.2} \\\\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \\\\leq x \\\\leq 1$ ) and $\\\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\\\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\\\in[0,1], f(x) \\\\in \\\\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\\\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\\\textbf{\\\\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\\\[ \\\\boldsymbol{\\\\Omega}=] 0,1[ \\\\quad \\\\text { (open) } \\\\tag{1.1.3} \\\\] \\\\[ \\\\overline{\\\\boldsymbol{\\\\Omega}}=[0,1] \\\\quad \\\\text { (closed) } \\\\tag{1.1.4} \\\\] See Fig. 1.1.1. \\\\begin{figure}[h] \\\\centering \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\\\vspace{0.5em} \\\\textbf{Figure 1.1.1} \\\\end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work.', 'A boundary-value problem for (1.1.1) involves imposing \\\\textbf{\\\\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \\\\begin{align} u(1) &= g \\\\tag{1.2.1} \\\\\\\\ -u_{, x}(0) &= h \\\\tag{1.2.2} \\\\end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \\\\textbf{\\\\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \\\\[ \\\\text{(S)} \\\\quad \\\\left\\\\{ \\\\parbox{0.8\\\\textwidth}{ \\\\text{Given } $f:\\\\overline{\\\\boldsymbol{\\\\Omega}} \\\\to \\\\mathbb{R}$ \\\\text{ and constants } $g$ \\\\text{ and } $h$, \\\\text{ find } $u:\\\\overline{\\\\boldsymbol{\\\\Omega}} \\\\to \\\\mathbb{R}$, \\\\text{ such that:} \\\\begin{align*} u_{,xx} + f &= 0 \\\\quad \\\\text{on } \\\\Omega \\\\\\\\ u(1) &= g \\\\\\\\ -u_{,x}(0) &= h \\\\end{align*} } \\\\right. \\\\] When we write $u_{, x x}+f=0$ on $\\\\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \\\\in \\\\Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \\\\begin{equation*} u(x)=g+(1-x) h+\\\\int_{x}^{1}\\\\left\\\\{\\\\int_{0}^{y} f(z) d z\\\\right\\\\} d y \\\\tag{1.2.3} \\\\end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section.', \"To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \\\\begin{equation*} \\\\int_{0}^{1}(u_{,x})^{2} d x<\\\\infty \\\\tag{1.3.1} \\\\end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\\\\boldsymbol{u} \\\\in \\\\boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \\\\in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\\\\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \\\\begin{equation*} \\\\mathcal{S}=\\\\left\\\\{u \\\\mid u \\\\in H^{1}, u(1)=g\\\\right\\\\} \\\\quad \\\\text { (trial solutions) } \\\\tag{1.3.2} \\\\end{equation*} The fact that $\\\\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\\\\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \\\\textbf{\\\\textit{weighting functions}}, or \\\\textbf{\\\\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\\\\mathcal{U}$ and defined by\\\\\\\\ \\\\begin{equation*} \\\\mathcal{U} = \\\\{w \\\\mid w \\\\in H^{1}, w(1)=0\\\\} \\\\quad \\\\text{(weighting functions)} \\\\tag{1.3.3} \\\\end{equation*} It simplifies matters somewhat to continue to think of $f: \\\\Omega \\\\rightarrow \\\\mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \\\\[ (\\\\mathcal{W}) \\\\quad \\\\left\\\\{ \\\\parbox{0.8\\\\textwidth}{ \\\\text{Given } $f,$q and $h$,\\\\text{ as before. Find } $u \\\\in \\\\mathfrak{f}$, \\\\text{ such that for all} \\\\mbox{$w \\\\in \\\\mathcal{U}$} \\\\begin{align*} \\\\int_{0}^{1} w_{, x} u_{, x} d x=\\\\int_{0}^{1} w f d x+w(0) h \\\\tag{1.3.4} \\\\end{align*} } \\\\right. \\\\] Formulations of this type are often called \\\\textbf{\\\\textit{virtual work}}, or \\\\textbf{\\\\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \\\\textbf{\\\\textit{virtual displacements}}. Equation (1.3.4) is called the \\\\textbf{\\\\textit{variational equation}}, or (especially in mechanics) the \\\\textbf{\\\\textit{equation of virtual work.}} The solution of $(W)$ is called the \\\\textbf{\\\\textit{weak}}, or \\\\textbf{\\\\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider.\", 'Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. \"Proofs\" of this kind are sometimes euphemistically referred to as \"formal proofs.\" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall \"prove\" the following. \\\\subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\\\\\\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \\\\subsection*{Formal Proof} \\\\bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \\\\begin{equation*} 0=-\\\\int_{0}^{1} w(u_{, x x}+f) d x \\\\tag{1.4.1} \\\\end{equation*} for any $w \\\\in \\\\mathcal{U}$. Integrating (1.4.1) by parts results in \\\\begin{equation*} 0=\\\\int_{0}^{1} w_{, x} u_{, x} d x-\\\\int_{0}^{1} w f d x-\\\\left.w u_{, x}\\\\right|_{0} ^{1} \\\\tag{1.4.2} \\\\end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \\\\begin{equation*} \\\\int_{0}^{1} w_{, x} u_{, x} d x=\\\\int_{0}^{1} w f d x+w(0) h \\\\tag{1.4.3} \\\\end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\\\\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \\\\in \\\\mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\\\\\\\ \\\\\\\\ b. Now $u$ is assumed to be a weak solution. Thus $u \\\\in \\\\mathcal{S}$; consequently $u(1)=g$, and $$ \\\\int_{0}^{1} w_{, x} u_{, x} d x=\\\\int_{0}^{1} w f d x+w(0) h $$ for all $w \\\\in \\\\mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \\\\begin{equation*} 0=\\\\int_{0}^{1} w\\\\left(u_{, x x}+f\\\\right) d x+w(0)\\\\left[u_{, x}(0)+h\\\\right]. \\\\tag{1.4.4} \\\\end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\\\\\\\ i. $u_{, x x}+f=0$ on $\\\\Omega$; and\\\\\\\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\\\\boldsymbol{w}$ in (1.4.4) by \\\\begin{equation*} w=\\\\phi\\\\left(u_{, x x}+f\\\\right) \\\\tag{1.4.5} \\\\end{equation*} where $\\\\phi$ is smooth; $\\\\phi(x)>0$ for all $x \\\\in \\\\Omega=] 0,1[$; and $\\\\phi(0)=\\\\phi(1)=0$. For example, we can take $\\\\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \\\\in \\\\mathcal{U}$, so (1.4.5) defines a\\\\\\\\ \\\\begin{figure}[h] \\\\centering \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \\\\vspace{0.5em} \\\\textbf{Figure 1.4.1} \\\\end{figure} \\\\footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\\\\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\\\\\\\ \\\\begin{equation*} 0=\\\\int_{0}^{1} \\\\phi \\\\underbrace{\\\\left(u_{, x x}+f\\\\right)^{2}}_{\\\\geq 0} d x+0 \\\\tag{1.4.6} \\\\end{equation*} Since $\\\\phi>0$ on $\\\\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\\\\\\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \\\\begin{equation*} 0=w(0)\\\\left[u_{, x}(0)+h\\\\right] \\\\tag{1.4.7} \\\\end{equation*} That $w \\\\in \\\\mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \\\\neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition.', '\\\\begin{enumerate} \\\\item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \\\\textbf{\\\\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \\\\textbf{\\\\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \\\\item The method used to prove part (b) of the proposition goes under the name of the \\\\textbf{\\\\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \\\\end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\\\\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\\\\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \\\\begin{align*} a(w, u) & =\\\\int_{0}^{1} w_{, x} u_{, x} d x \\\\tag{1.4.8}\\\\\\\\ (w, f) & =\\\\int_{0}^{1} w f d x \\\\tag{1.4.9} \\\\end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \\\\begin{equation*} a(w, u)=(w, f)+w(0) h \\\\tag{1.4.10} \\\\end{equation*} Here, $a(\\\\cdot, \\\\cdot)$ and $(\\\\cdot, \\\\cdot)$ are examples of \\\\textbf{\\\\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \\\\begin{align*} a(u, v) & =a(v, u) \\\\tag{1.4.11}\\\\\\\\ (u, v) & =(v, u) \\\\tag{1.4.12} \\\\end{align*} Bilinearity means linearity in each \"slot\"; for example, \\\\begin{align*} a\\\\left(c_{1} u+c_{2} v, w\\\\right) & =c_{1} a(u, w)+c_{2} a(v, w) \\\\tag{1.4.13}\\\\\\\\ \\\\left(c_{1} u+c_{2} v, w\\\\right) & =c_{1}(u, w)+c_{2}(v, w) \\\\tag{1.4.14} \\\\end{align*} \\\\\\\\ Exercise 1. Use the definitions of $a(\\\\cdot, \\\\cdot)$ and $(\\\\cdot, \\\\cdot)$ to verify the properties of symmetry and bilinearity.\\\\\\\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability.', 'We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\\\\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\\\\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\\\\mathcal{U}^{h}$ with a \\\\textit{\\\\textbf{mesh}}, or \\\\textit{\\\\textbf{discretization}}, of the domain $\\\\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\\\\mathcal{U}^{h}$ as being subsets of $f$ and $\\\\mathcal{U}$, respectively. This is written as \\\\begin{align*} f^{h} \\\\subset f & \\\\text { (i.e., if } \\\\left.u^{h} \\\\in f^{h}, \\\\text { then } u^{h} \\\\in f\\\\right) \\\\tag{1.5.1}\\\\\\\\ \\\\mathcal{U}^{h} \\\\subset \\\\mathcal{U} & \\\\text { (i.e., if } w^{h} \\\\in \\\\mathcal{U}^{h}, \\\\text { then } w^{h} \\\\in\\\\mathcal{U}) \\\\tag{1.5.2} \\\\end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \\\\in f^{h}$ and $w^{h} \\\\in \\\\mathcal{U}^{h}$, then \\\\begin{align*} & u^{h}(1)=q \\\\tag{1.5.3}\\\\\\\\ & w^{h}(1)=0 \\\\tag{1.5.4} \\\\end{align*} The collections, $f, \\\\mathcal{U}, d^{h}$, and $\\\\mathcal{U}^{h}$, are often referred to as \\\\textit{\\\\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\\\\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\\\\mathcal{U}$. Both $\\\\mathcal{U}$ and $\\\\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \\\\notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$.', 'Assume the collection $\\\\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \\\\in \\\\mathcal{U}^{h}$, we construct a function $u^{h} \\\\in f^{h}$ by \\\\begin{equation*} u^{h}=v^{h}+g^{h} \\\\tag{1.5.5} \\\\end{equation*} where $g^{\\\\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \\\\begin{equation*} g^{h}(1)=g \\\\tag{1.5.6} \\\\end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \\\\begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \\\\tag{1.5.7}\\\\\\\\ & =0+g \\\\end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\\\\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \\\\in \\\\mathcal{W}^{h}$ and $u^{h} \\\\in \\\\delta^{h}$ \\\\begin{equation*} a\\\\left(w^{h}, u^{h}\\\\right)=\\\\left(w^{h}, A\\\\right)+w^{h}(0) h \\\\tag{1.5.8} \\\\end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\\\\boldsymbol{h}}$. \\\\footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology \"variational crimes\" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many \"variational crimes\" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\\\\cdot, \\\\cdot)$ enables us to write \\\\begin{equation*} a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, f\\\\right)+w^{h}(0) h-a\\\\left(w^{h}, g^{h}\\\\right) \\\\tag{1.5.9} \\\\end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\\\\\\\ \\\\[ \\\\text{(G)} \\\\quad \\\\left\\\\{ \\\\parbox{0.8\\\\textwidth}{ \\\\text { Given } $f, q$, \\\\text { and } $h$, \\\\text {, as before, find } $u^{h}=v^{h}+q^{h}$ \\\\text {, where } $v^{h} \\\\in \\\\mathcal{U}^{h}$ \\\\text { such that for all } $w^{h} \\\\in \\\\mathcal{U}^{h}$ \\\\\\\\ \\\\begin{align*} a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, f\\\\right)+w^{h}(0) h-a\\\\left(w^{h}, g^{h}\\\\right) \\\\end{align*} } \\\\right. \\\\] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\\\\mathcal{U}^\\\\text{h}$. To make matters more specific, $g^{h}$ and $\\\\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \\\\textit{\\\\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\\\\mathcal{U^\\\\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \\\\textit{\\\\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \\\\textit{\\\\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4].', \"The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\\\\mathcal{U}^{h}$. Let $\\\\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \\\\bar{\\\\Omega} \\\\rightarrow \\\\mathbb{R}$, where $A=1,2, \\\\ldots, n$. By this we mean that if $w^{h} \\\\in \\\\mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \\\\ldots, n$, such that \\\\begin{align*} w^{h} & =\\\\sum_{A=1}^{n} c_{A} N_{A} \\\\\\\\ & =c_{1} N_{1}+c_{2} N_{2}+\\\\cdots+c_{n} N_{n} \\\\tag{1.6.1} \\\\end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \\\\begin{equation*} N_{A}(1)=0, \\\\quad A=1,2, \\\\ldots, n \\\\tag{1.6.2} \\\\end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\\\\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \\\\bar{\\\\Omega} \\\\rightarrow \\\\mathbb{R}$, which has the property \\\\begin{equation*} N_{n+1}(1)=1 \\\\tag{1.6.3} \\\\end{equation*} (Note $N_{n+1} \\\\notin \\\\mathcal{U}^{h}$. ) Then $g^{h}$ is given by \\\\begin{equation*} g^{h}=g N_{n+1} \\\\tag{1.6.4} \\\\end{equation*} and thus \\\\begin{equation*} g^{h}(1)=g \\\\tag{1.6.5} \\\\end{equation*} With these definitions, a typical $u^{h} \\\\in f^{h}$ may be written as \\\\begin{align*} u^{h} & =v^{h}+g^{h} \\\\\\\\ & =\\\\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \\\\tag{1.6.6} \\\\end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\\\\\\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \\\\begin{align*} a\\\\left(\\\\sum_{A=1}^{n} c_{A} N_{A}, \\\\sum_{B=1}^{n} d_{B} N_{B}\\\\right)=\\\\left(\\\\sum_{A=1}^{n} c_{A} N_{A}, f\\\\right)+ & {\\\\left[\\\\sum_{A=1}^{n} c_{A} N_{A}(0)\\\\right] h } \\\\\\\\ & -a\\\\left(\\\\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\\\\right) \\\\tag{1.6.7} \\\\end{align*} By using the bilinearity of $a(\\\\cdot, \\\\cdot)$ and $(\\\\cdot, \\\\cdot),(1.6 .7)$ becomes \\\\begin{equation*} 0=\\\\sum_{A=1}^{n} c_{A} G_{A} \\\\tag{1.6.8} \\\\end{equation*} where \\\\begin{equation*} G_{A}=\\\\sum_{B=1}^{n} a\\\\left(N_{A}, N_{B}\\\\right) d_{B}-\\\\left(N_{A}, f\\\\right)-N_{A}(0) h+a\\\\left(N_{A}, N_{n+1}\\\\right) q \\\\tag{1.6.9} \\\\end{equation*} Now the Galerkin equation is to hold for all $w^{h} \\\\in \\\\mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \\\\ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \\\\ldots, n$, must be identically zero, i.e., from (1.6.9) \\\\begin{equation*} \\\\sum_{B=1}^{n} a\\\\left(N_{A}, N_{B}\\\\right) d_{B}=\\\\left(N_{A}, f\\\\right)+N_{A}(0) h-a\\\\left(N_{A}, N_{n+1}\\\\right) g \\\\tag{1.6.10} \\\\end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \\\\begin{align*} K_{A B} & =a\\\\left(N_{A}, N_{B}\\\\right) \\\\tag{1.6.11}\\\\\\\\ F_{A} & =\\\\left(N_{A}, f\\\\right)+N_{A}(0) h-a\\\\left(N_{A}, N_{n+1}\\\\right) g \\\\tag{1.6.12} \\\\end{align*} Then (1.6.10) becomes \\\\begin{equation*} \\\\sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \\\\quad A=1,2, \\\\ldots, n \\\\tag{1.6.13} \\\\end{equation*} Further simplicity is gained by adopting a matrix notation. Let \\\\begin{align*} & \\\\boldsymbol{K}=\\\\left[K_{A B}\\\\right]=\\\\left[\\\\begin{array}{cccc} K_{11} & \\\\boldsymbol{K}_{12} & \\\\cdots & K_{1 n} \\\\\\\\ \\\\boldsymbol{K}_{21} & K_{22} & \\\\cdots & K_{2 n} \\\\\\\\ \\\\vdots & \\\\vdots & & \\\\vdots \\\\\\\\ K_{n 1} & K_{n 2} & \\\\cdots & K_{n n} \\\\end{array}\\\\right] \\\\tag{1.6.14}\\\\\\\\ & \\\\boldsymbol{F}=\\\\left\\\\{F_{A}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} F_{1} \\\\\\\\ F_{2} \\\\\\\\ \\\\vdots \\\\\\\\ F_{n} \\\\end{array}\\\\right\\\\} \\\\tag{1.6.15} \\\\end{align*} and \\\\[ d=\\\\left\\\\{d_{B}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} d_{1} \\\\tag{1.6.16}\\\\\\\\ d_{2} \\\\\\\\ \\\\vdots \\\\\\\\ d_{n} \\\\end{array}\\\\right\\\\} \\\\] Now (1.6.13) may be written as \\\\begin{equation*} \\\\boldsymbol{K} \\\\boldsymbol{d}=\\\\boldsymbol{F} \\\\tag{1.6.17} \\\\end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \\\\begin{aligned} \\\\boldsymbol{K} & =\\\\text { stiffness matrix } \\\\\\\\ \\\\boldsymbol{F} & =\\\\text { force vector } \\\\\\\\ \\\\boldsymbol{d} & =\\\\text { displacement vector } \\\\end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\\\\\\\ (M) $\\\\left\\\\{\\\\begin{array}{c}\\\\text { Given the coefficient matrix } \\\\boldsymbol{K} \\\\text { and vector } \\\\boldsymbol{F}, \\\\text { find } \\\\boldsymbol{d} \\\\text { such that } \\\\\\\\ \\\\boldsymbol{K} \\\\boldsymbol{d}=\\\\boldsymbol{F}\\\\end{array}\\\\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \\\\boldsymbol{F}$ (assuming the inverse of $\\\\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \\\\in \\\\bar{\\\\Omega}$ by employing (1.6.6), viz., \\\\begin{equation*} u^{h}(x)=\\\\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \\\\tag{1.6.18} \\\\end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$.\", \"\\\\begin{enumerate} \\\\item The matrix $K$ is symmetric. This follows from the symmetry of $a(\\\\cdot, \\\\cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \\\\end{enumerate} \\\\begin{align*} K_{A B} & =a\\\\left(N_{A}, N_{B}\\\\right) \\\\\\\\ & =a\\\\left(N_{B}, N_{A}\\\\right) \\\\\\\\ & =K_{B A} \\\\tag{1.6.19} \\\\end{align*} In matrix notation \\\\begin{equation*} K=K^{\\\\boldsymbol{T}} \\\\tag{1.6.20} \\\\end{equation*} where the superscript $\\\\boldsymbol{T}$ denotes transpose. The symmetry of $\\\\boldsymbol{K}$ has important computational consequences.\\\\\\\\ \\\\\\\\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \\\\begin{equation*} (S) \\\\Leftrightarrow(W) \\\\approx(G) \\\\Leftrightarrow(M) \\\\tag{1.6.21} \\\\end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\\\\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\\\\\\\ 3. It is sometimes convenient to write \\\\begin{equation*} u^{h}(x)=\\\\sum_{A=1}^{n+1} N_{A}(x) d_{A} \\\\tag{1.6.22} \\\\end{equation*} where $d_{n+1}=g$.\", 'In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \\\\subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \\\\begin{align*} K & =\\\\left[K_{11}\\\\right]=K_{11} \\\\tag{1.7.1}\\\\\\\\ F & =\\\\left\\\\{F_{1}\\\\right\\\\}=F_{1} \\\\tag{1.7.2}\\\\\\\\ d & =\\\\left\\\\{d_{1}\\\\right\\\\}=d_{1} \\\\tag{1.7.3}\\\\\\\\ K_{11} & =a\\\\left(N_{1}, N_{1}\\\\right)=\\\\int_{0}^{1} \\\\underbrace{N_{1,x}}_{-1} \\\\underbrace{N_{1, x}}_{-1} d x=1 \\\\tag{1.7.4}\\\\\\\\ F_{1} & =\\\\left(N_{1}, f\\\\right)+N_{1}(0) h-a\\\\left(N_{1}, N_{2}\\\\right) q \\\\\\\\ & =\\\\int_{0}^{1}(1-x) f(x) d x+h-\\\\int_{0}^{1} \\\\underbrace{N_{1, x}}_{-1} \\\\underbrace{N_{2, x}}_{+1} d x g \\\\\\\\ & =\\\\int_{0}^{1}(1-x) f(x) d x+h+q \\\\tag{1.7.5}\\\\\\\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \\\\tag{1.7.6} \\\\end{align*} Consequently \\\\begin{equation*} u^{h}(x)=[\\\\underbrace{\\\\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \\\\tag{1.7.7} \\\\end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\\\\\\\ i. Let $f=0$. Then \\\\begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \\\\tag{1.7.8} \\\\end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \\\\neq 0$ ).\\\\\\\\ \\\\\\\\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \\\\begin{equation*} u_{\\\\text {part }}(x)=\\\\frac{p\\\\left(1-x^{2}\\\\right)}{2} \\\\tag{1.7.9} \\\\end{equation*} and \\\\begin{equation*} u_{\\\\text {part }}^{h}(x)=\\\\frac{p(1-x)}{2} \\\\tag{1.7.10} \\\\end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\\\\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\\\\text {part, } x}^{h}$ is exact at $x=\\\\frac{1}{2}$. (It should be clear that it is impossible for $u_{\\\\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\\\\\\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \\\\begin{equation*} u_{\\\\text {part }}(x)=\\\\frac{q\\\\left(1-x^{3}\\\\right)}{6} \\\\tag{1.7.11} \\\\end{equation*} and \\\\begin{equation*} u_{\\\\text {pata }}^{h}(x)=\\\\frac{q(1-x)}{6} \\\\tag{1.7.12} \\\\end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\\\\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \\\\sqrt{3}$, at which $u_{\\\\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\\\\\\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\\\\\\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\\\\\\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii).', \"In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \\\\begin{align*} & N_{1}(x)=\\\\left\\\\{\\\\begin{array}{cc} 1-2 x & 0 \\\\leq x \\\\leq \\\\frac{1}{2} \\\\\\\\ 0 & \\\\frac{1}{2} \\\\leq x \\\\leq 1 \\\\end{array}\\\\right. \\\\tag{1.7.6}\\\\\\\\ & N_{2}(x)=\\\\left\\\\{\\\\begin{array}{cc} 2 x & 0 \\\\leq x \\\\leq \\\\frac{1}{2} \\\\\\\\ 2(1-x) & \\\\frac{1}{2} \\\\leq x \\\\leq 1 \\\\end{array}\\\\right. \\\\tag{1.7.7}\\\\\\\\ & N_{3}(x)=\\\\left\\\\{\\\\begin{array}{cc} 0 & 0 \\\\leq x \\\\leq \\\\frac{1}{2} \\\\\\\\ 2 x-1 & \\\\frac{1}{2} \\\\leq x \\\\leq 1 \\\\end{array}\\\\right. \\\\tag{1.7.8} \\\\end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \\\\in \\\\mathcal{U}^{h}$ and $u^{h} \\\\in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\\\\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \\\\begin{align*} & K=\\\\left[\\\\begin{array}{ll} K_{11} & K_{12} \\\\\\\\ K_{21} & K_{22} \\\\end{array}\\\\right] \\\\tag{1.7.9}\\\\\\\\ & F=\\\\left\\\\{\\\\begin{array}{l} F_{1} \\\\\\\\ F_{2} \\\\end{array}\\\\right\\\\} \\\\tag{1.7.10}\\\\\\\\ & d=\\\\left\\\\{\\\\begin{array}{l} d_{1} \\\\\\\\ d_{2} \\\\end{array}\\\\right\\\\} \\\\tag{1.7.11}\\\\\\\\ & K_{A B}=a\\\\left(N_{A}, N_{B}\\\\right)=\\\\int_{0}^{1} N_{A, x} N_{B, x} d x=\\\\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\\\\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \\\\tag{1.7.12}\\\\\\\\ & K_{11}=2, \\\\quad K_{12}=K_{21}=-2, \\\\quad K_{22}=4 \\\\tag{1.7.13}\\\\\\\\ & K=2\\\\left[\\\\begin{array}{rr} 1 & -1 \\\\\\\\ -1 & 2 \\\\end{array}\\\\right] \\\\tag{1.7.14}\\\\\\\\ & F_{A}=\\\\left(N_{A}, f\\\\right)+N_{A}(0) h-a\\\\left(N_{A}, N_{3}\\\\right) g \\\\\\\\ &=\\\\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\\\\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \\\\tag{1.7.15} \\\\end{align*} \\\\begin{center} \\\\includegraphics[max width=\\\\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \\\\end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \\\\begin{align*} & F_{1}=\\\\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \\\\tag{1.7.16}\\\\\\\\ & F_{2}=2 \\\\int_{0}^{1 / 2} x f(x) d x+2 \\\\int_{1 / 2}^{1}(1-x) f(x) d x+2 g \\\\tag{1.7.17} \\\\end{align*} Note that due to the shape functions' discontinuities in slope at $x=\\\\frac{1}{2}$, it is convenient to express integrals over the subintervals $\\\\left[0, \\\\frac{1}{2}\\\\right]$ and $\\\\left[\\\\frac{1}{2}, 1\\\\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\\\\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\\\\\\\ i. $f=0$. \\\\begin{align*} F & =\\\\left\\\\{\\\\begin{array}{c} h \\\\\\\\ 2 g \\\\end{array}\\\\right\\\\} \\\\tag{1.7.18}\\\\\\\\ d & =K^{-1} F \\\\end{align*} \\\\begin{align*} & =\\\\left[\\\\begin{array}{ll} 1 & \\\\frac{1}{2} \\\\\\\\ \\\\frac{1}{2} & \\\\frac{1}{2} \\\\end{array}\\\\right]\\\\left\\\\{\\\\begin{array}{l} h \\\\\\\\ 2 g \\\\end{array}\\\\right\\\\} \\\\\\\\ & =\\\\left\\\\{\\\\begin{array}{l} g+h \\\\\\\\ g+\\\\frac{h}{2} \\\\end{array}\\\\right\\\\} \\\\tag{1.7.19} \\\\end{align*} This results in \\\\begin{align*} u^{h} & =(g+h) N_{1}+\\\\left(g+\\\\frac{h}{2}\\\\right) N_{2}+g N_{3} \\\\\\\\ & =g\\\\left(N_{1}+N_{2}+N_{3}\\\\right)+h\\\\left(N_{1}+\\\\frac{N_{2}}{2}\\\\right) \\\\tag{1.7.20}\\\\\\\\ u^{h}(x) & =g+h(1-x) \\\\tag{1.7.21} \\\\end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\\\\\\\ ii. $f(x)=p=$ constant. \\\\begin{align*} & F_{1}=\\\\frac{p}{4}+h \\\\tag{1.7.22}\\\\\\\\ & F_{2}=\\\\frac{p}{2}+2 g \\\\tag{1.7.23}\\\\\\\\ & d=\\\\left[\\\\begin{array}{ll} 1 & \\\\frac{1}{2} \\\\\\\\ \\\\frac{1}{2} & \\\\frac{1}{2} \\\\end{array}\\\\right]\\\\left\\\\{\\\\begin{array}{l} \\\\frac{p}{4}+h \\\\\\\\ \\\\frac{p}{2}+2 g \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} \\\\frac{p}{2}+g+h \\\\\\\\ \\\\frac{3 p}{8}+g+\\\\frac{h}{2} \\\\end{array}\\\\right\\\\} \\\\tag{1.7.24} \\\\end{align*} The solution takes the form \\\\begin{align*} & u^{h}(x)=g+h(1-x)+u_{\\\\text {part }}^{h}(x) \\\\tag{1.7.25}\\\\\\\\ & u_{\\\\text {part }}^{h}=\\\\frac{p}{2} N_{1}+\\\\frac{3 p}{8} N_{2} \\\\tag{1.7.26} \\\\end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \\\\frac{1}{2}$ and 1 , and derivatives coincide at $x=\\\\frac{1}{4}$ and $\\\\frac{3}{4}$.\\\\\\\\ iii. $f(x)=q x, q=$ constant. \\\\begin{align*} & F_{1}=\\\\frac{q}{24}+h \\\\tag{1.7.27}\\\\\\\\ & F_{2}=\\\\frac{q}{4}+2 g \\\\tag{1.7.28} \\\\end{align*} \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \\\\[ d=\\\\left\\\\{\\\\begin{array}{l} \\\\frac{q}{6}+g+h \\\\tag{1.7.29}\\\\\\\\ \\\\frac{7 q}{48}+g+\\\\frac{h}{2} \\\\end{array}\\\\right\\\\} \\\\] Again $u^{h}$ may be expressed in the form (1.7.25), where \\\\begin{equation*} u_{\\\\text {part }}^{h}=\\\\frac{q}{6} N_{1}+\\\\frac{7 q}{48} N_{2} \\\\tag{1.7.30} \\\\end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \\\\frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\\\\\\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\\\\\\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\\\\\\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\\\\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details.\", 'The examples of the preceding section employed definitions of $\\\\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\\\\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\\\\left[x_{A}, x_{A+1}\\\\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \\\\ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ \\'s are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\\\\max h_{A}, A=1,2, \\\\ldots, n$ ). The smaller $h$, the more \"refined\" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \\\\leq A \\\\leq n$ ) \\\\[ N_{A}(x)=\\\\left\\\\{\\\\begin{array}{cl} \\\\frac{\\\\left(x-x_{A-1}\\\\right)}{h_{A-1}}, & x_{A-1} \\\\leq x \\\\leq x_{A} \\\\tag{1.8.1}\\\\\\\\ \\\\frac{\\\\left(x_{A+1}-x\\\\right)}{h_{A}}, & x_{A} \\\\leq x \\\\leq x_{A+1} \\\\\\\\ 0, & \\\\text { elsewhere } \\\\end{array}\\\\right. \\\\] whereas for the boundary nodes we have \\\\begin{align*} & N_{1}(x)=\\\\frac{x_{2}-x}{h_{1}}, \\\\quad x_{1} \\\\leq x \\\\leq x_{2} \\\\tag{1.8.2}\\\\\\\\ & N_{n+1}(x)=\\\\frac{x-x_{n}}{h_{n}}, \\\\quad x_{n} \\\\leq x \\\\leq x_{n+1} \\\\tag{1.8.3} \\\\end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as \"hat,\" \"chapeau,\" and \"roof\" functions. Note that $N_{A}\\\\left(x_{B}\\\\right)=\\\\delta_{A B}$, where\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\\\\\\\ $\\\\delta_{A B}$ is the Kronecker delta (i.e., $\\\\delta_{A B}=1$ if $A=B$, whereas $\\\\delta_{A B}=0$ if $A \\\\neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \\\\in \\\\mathcal{U}^{h}$ has the form $\\\\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\\\\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\\\\boldsymbol{n}$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\\\\boldsymbol{h}} \\\\in \\\\mathcal{U}^{\\\\boldsymbol{k}}$.\\\\\\\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\\\\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \\\\begin{equation*} \\\\int_{0}^{1} w_{, x} u_{, x} d x=\\\\int_{0}^{1} w f d x+w(0) h \\\\tag{1.8.4} \\\\end{equation*} where $w \\\\in \\\\mathcal{U}$ and $u \\\\in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \\\\ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \\\\begin{align*} 0= & \\\\sum_{A=1}^{n} \\\\int_{x_{A}}^{x_{A}+1} w\\\\left(u_{, x x}+f\\\\right) d x+w(0)\\\\left[u_{, x}\\\\left(0^{+}\\\\right)+h\\\\right] \\\\\\\\ & +\\\\sum_{A=2}^{n} w\\\\left(x_{A}\\\\right)\\\\left[u_{, x}\\\\left(x_{A}^{+}\\\\right)-u_{, x}\\\\left(x_{A}^{-}\\\\right)\\\\right] \\\\tag{1.8.5} \\\\end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\\\\\\\ i. $u_{, x x}(x)+f(x)=0$, where $\\\\left.x \\\\in\\\\right] x_{A}, x_{A+1}[$ and $A=1,2, \\\\ldots, n$,\\\\\\\\ ii. $-u_{, x}\\\\left(0^{+}\\\\right)=h$; and\\\\\\\\ iii. $u_{, x}\\\\left(x_{A}^{-}\\\\right)=u_{, x}\\\\left(x_{A}^{+}\\\\right)$, where $A=2,3, \\\\ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained.', \"The shape functions $N_{A}, A=1,2, \\\\ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \\\\begin{equation*} K_{A B}=\\\\int_{0}^{1} \\\\underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \\\\tag{1.9.1} \\\\end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\\\\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\\\\boldsymbol{K}$.\\\\\\\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\\\\boldsymbol{n} \\\\times \\\\boldsymbol{n}$ matrix $\\\\boldsymbol{A}$ is said to be positive definite if\\\\\\\\ i. $c^{T} A c \\\\geq 0$ for all $n$-vectors $c$; and\\\\\\\\ ii. $c^{\\\\boldsymbol{T}} \\\\boldsymbol{A c}=0$ implies $\\\\boldsymbol{c}=0$. \\\\subsection*{Remarks} \\\\begin{enumerate} \\\\item A symmetric positive-definite matrix posesses a unique inverse. \\\\item The eigenvalues of a positive-definite matrix are real and positive. \\\\end{enumerate} Theorem. The $\\\\boldsymbol{n} \\\\times n$ matrix $K$ defined by (1.6.11) is positive definite.\\\\\\\\ Proof\\\\\\\\ i. Let $c_{A}, A=1,2, \\\\ldots, n$, be the components of $c$ (i.e., $c=\\\\left\\\\{c_{A}\\\\right\\\\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\\\\mathcal{U}^{h}, w^{h}=\\\\sum_{A=1}^{n} c_{A} N_{A}$, where\\\\\\\\ the $N_{A}$ 's are the basis functions for $\\\\mathcal{U}^{h}$. Then $$ \\\\begin{array}{rlrl} c^{T} K c & =\\\\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\\\\\\\ & =\\\\sum_{A, B=1}^{n} c_{A} a\\\\left(N_{A}, N_{B}\\\\right) c_{B} & & \\\\text { (definition of } \\\\left.K_{A B}\\\\right) \\\\\\\\ & =a\\\\left(\\\\sum_{A=1}^{n} c_{A} N_{A}, \\\\sum_{B=1}^{n} c_{B} N_{B}\\\\right) & & \\\\text { (bilinearity of } a(\\\\cdot, \\\\cdot)) \\\\\\\\ & =a\\\\left(w^{h}, w^{h}\\\\right) & & \\\\text { (definition of } \\\\left.w^{h}\\\\right) \\\\\\\\ & =\\\\int_{0}^{1} \\\\underbrace{\\\\left(w_{0}^{h} x\\\\right.}_{\\\\geq 0})^{2} d x & & \\\\text { (by (1.4.8) } \\\\\\\\ & \\\\geq 0 & \\\\end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \\\\int_{0}^{1}\\\\left(w_{, x}^{h}\\\\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \\\\in \\\\mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \\\\in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \\\\ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\\\\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\\\\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\\\\\\\ i. Symmetric\\\\\\\\ ii. Banded\\\\\\\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\\\\boldsymbol{K} \\\\boldsymbol{d}=\\\\boldsymbol{F}$ may be performed.\", \"In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\\\\boldsymbol{u}^{\\\\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\\\\delta_{y}(x)=\\\\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \\\\begin{align*} \\\\left(w, \\\\delta_{y}\\\\right) & =\\\\int_{0}^{1} w(x) \\\\delta(x-y) d x \\\\tag{1.10.1}\\\\\\\\ & =w(y) \\\\end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\\\\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\\\\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \\\\begin{align*} g_{, x x}+\\\\delta_{y} & =0 \\\\quad \\\\text { on } \\\\Omega \\\\tag{1.10.2}\\\\\\\\ g(1) & =0 \\\\tag{1.10.3}\\\\\\\\ g_{, x}(0) & =0 \\\\tag{1.10.4} \\\\end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\\\\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\\\\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\\\\delta_{y}$ is the Heaviside, or unit step, function: \\\\[ H_{y}(x)=H(x-y)= \\\\begin{cases}0, & x<y \\\\tag{1.10.5}\\\\\\\\ 1, & x>y\\\\end{cases} \\\\] The integral of $\\\\dot{H}_{y}$ is the Macaulay bracket: \\\\[ \\\\langle x-y\\\\rangle=\\\\left\\\\{\\\\begin{array}{cl} 0, & x \\\\leq y \\\\tag{1.10.6}\\\\\\\\ x-y, & x>y \\\\end{array}\\\\right. \\\\] The preceding functions are depicted in Fig. 1.10.1.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \\\\begin{equation*} g_{, x}+H_{y}=c_{1} \\\\tag{1.10.7} \\\\end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \\\\begin{equation*} g(x)+\\\\langle x-y\\\\rangle=c_{1} x+c_{2} \\\\tag{1.10.8} \\\\end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \\\\begin{equation*} g(x)=(1-y)-\\\\langle x-y\\\\rangle \\\\tag{1.10.9} \\\\end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \\\\in \\\\mathcal{U}^{h}$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\\\\\\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\\\\delta_{y}$, and $g$ and $h$ by 0 , viz., \\\\begin{equation*} a(w, g)=\\\\left(w, \\\\delta_{y}\\\\right)=w(y) \\\\tag{1.10.10} \\\\end{equation*} Equation (1.10.10) holds for all continuous $w \\\\in \\\\mathcal{U}$. The square-integrability of derivatives of functions $w \\\\in \\\\mathcal{U}$ actually implies the continuity of all $w \\\\in \\\\mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\\\\left(x_{A}\\\\right)=u\\\\left(x_{A}\\\\right), A=1,2, \\\\ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\\\\left(u-u^{h}, w^{h}\\\\right)=0$ for all $w^{h} \\\\in \\\\mathcal{U}^{h}$.\\\\\\\\ Proof. We have observed previously that $\\\\mathcal{U}^{h} \\\\in \\\\mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \\\\begin{equation*} a\\\\left(w^{h}, u\\\\right)=\\\\left(w^{h}, f\\\\right)+w^{h}(0) h \\\\tag{1.10.11} \\\\end{equation*} Equation (1.10.11) holds for all $w^{h} \\\\in \\\\mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\\\\\\\ from (1.10.11) and using the bilinearity and symmetry of $a(\\\\cdot, \\\\cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\\\\left(u-u^{h}, g\\\\right)$, where $g$ is the Green's function.\\\\\\\\ Proof $$ \\\\begin{aligned} u(y)-u^{h}(y) & =\\\\left(u-u^{h}, \\\\delta_{y}\\\\right) & & \\\\text { (definition of } \\\\left.\\\\delta_{y}\\\\right) \\\\\\\\ & =a\\\\left(u-u^{h}, g\\\\right) & & \\\\text { (by (1.10.10)) } \\\\end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\\\\mathcal{U}$.\\\\\\\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \\\\in \\\\mathcal{U}^{h}$. Let us take this to be the case. Then $$ \\\\begin{aligned} u\\\\left(x_{A}\\\\right)-u^{h}\\\\left(x_{A}\\\\right) & =a\\\\left(u-u^{h}, g\\\\right) & & \\\\text { (Lemma 2) } \\\\\\\\ & =0 & & \\\\text { (Lemma 1) } \\\\end{aligned} $$ The theorem is valid for $A=1,2, \\\\ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed.\", \"In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \\\\subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \\\\rightarrow \\\\mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \\\\begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \\\\frac{1}{2}(z-y)^{2} f_{,xx}(y) \\\\\\\\ & + \\\\frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \\\\cdots + \\\\tag{1.10.12} \\\\\\\\ & + \\\\frac{1}{k!}(z-y)^{k} f_{,\\\\underbrace{x \\\\dots x}_{k \\\\text{ times}}}(c) \\\\end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \\\\subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \\\\geq 1$ (i.e., $f$ is continuously differentiable): \\\\begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \\\\tag{1.10.13} \\\\end{equation*} Consider a typical subinterval $\\\\left[x_{A}, x_{A+1}\\\\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\\\\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \\\\begin{equation*} \\\\left.u_{, x}^{h}(x)=\\\\frac{u^{h}\\\\left(x_{A+1}\\\\right)-u^{h}\\\\left(x_{A}\\\\right)}{h_{A}}, \\\\quad x \\\\in\\\\right] x_{A}, x_{A+1}[ \\\\tag{1.10.14} \\\\end{equation*} \\\\begin{center} \\\\includegraphics[max width=\\\\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \\\\end{center} Figare 1.10.3\\\\\\\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \\\\in] x_{A}, x_{A+1}[$ such that \\\\begin{equation*} \\\\frac{u\\\\left(x_{A+1}\\\\right)-u\\\\left(x_{A}\\\\right)}{h_{A}}=u_{, x}(c) \\\\tag{1.10.15} \\\\end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\\\\left(x_{A}\\\\right)=u^{h}\\\\left(x_{A}\\\\right)$ and $u\\\\left(x_{A+1}\\\\right)=u^{h}\\\\left(x_{A+1}\\\\right)$, we may rewrite (1.10.15) as \\\\begin{equation*} \\\\frac{u^{h}\\\\left(x_{A+1}\\\\right)-u^{h}\\\\left(x_{A}\\\\right)}{h_{A}}=u_{, x}(c) \\\\tag{1.10.16} \\\\end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \\\\subsection*{Remarks} \\\\begin{enumerate} \\\\item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \\\\item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \\\\end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\\\\alpha) \\\\stackrel{\\\\operatorname{def} .}{=} u_{, x}^{h}(\\\\alpha)-u_{, x}(\\\\alpha)=\\\\frac{u^{h}\\\\left(x_{A+1}\\\\right)-u^{h}\\\\left(x_{A}\\\\right)}{h_{A}}-u_{, x}(\\\\alpha) $$ the error in the derivative at $\\\\alpha \\\\in\\\\left[x_{A}, x_{A+1}\\\\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\\\\boldsymbol{u}$ is three times continuously differentiable. Then \\\\begin{align*} e_{, x}(\\\\alpha)= & \\\\left(\\\\frac{x_{A+1}+x_{A}}{2}-\\\\alpha\\\\right) u_{, x x}(\\\\alpha) \\\\\\\\ & +\\\\frac{1}{3!h_{A}}\\\\left[\\\\left(x_{A+1}-\\\\alpha\\\\right)^{3} u_{, x x x}\\\\left(c_{1}\\\\right)-\\\\left(x_{A}-\\\\alpha\\\\right)^{3} u_{, x x x}\\\\left(c_{2}\\\\right)\\\\right] \\\\tag{1.10.17} \\\\end{align*} where $c_{1}$ and $c_{2}$ are in $\\\\left[x_{A}, x_{A+1}\\\\right]$.\\\\\\\\ Proof. Expand $u\\\\left(x_{A+1}\\\\right)$ and $u\\\\left(x_{A}\\\\right)$ in finite Taylor expansions about $\\\\alpha \\\\in\\\\left[x_{A}, x_{A+1}\\\\right]$, viz., $$ \\\\begin{aligned} u\\\\left(x_{A+1}\\\\right)= & u(\\\\alpha)+\\\\left(x_{A+1}-\\\\alpha\\\\right) u_{, x}(\\\\alpha)+\\\\frac{1}{2}\\\\left(x_{A+1}-\\\\alpha\\\\right)^{2} u_{, x x}(\\\\alpha) \\\\\\\\ & +\\\\frac{1}{3!}\\\\left(x_{A+1}-\\\\alpha\\\\right)^{3} u_{, x x x}\\\\left(c_{1}\\\\right), \\\\quad c_{1} \\\\in\\\\left[\\\\alpha, x_{A+1}\\\\right] \\\\\\\\ u\\\\left(x_{A}\\\\right)= & u(\\\\alpha)+\\\\left(x_{A}-\\\\alpha\\\\right) u_{, x}(\\\\alpha)+\\\\frac{1}{2}\\\\left(x_{A}-\\\\alpha\\\\right)^{2} u_{, x x}(\\\\alpha) \\\\\\\\ & +\\\\frac{1}{3!}\\\\left(x_{A}-\\\\alpha\\\\right)^{3} u_{, x x x}\\\\left(c_{2}\\\\right), \\\\quad c_{2} \\\\in\\\\left[x_{A}, \\\\alpha\\\\right] \\\\end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \\\\begin{aligned} \\\\frac{u\\\\left(x_{A+1}\\\\right)-u\\\\left(x_{A}\\\\right)}{h_{A}}= & u_{, x}(\\\\alpha)+\\\\left(\\\\frac{x_{A+1}+x_{A}}{2}-\\\\alpha\\\\right) u_{, x x}(\\\\alpha) \\\\\\\\ & +\\\\frac{1}{3!h_{A}}\\\\left[\\\\left(x_{A+1}-\\\\alpha\\\\right)^{3} u_{, x x x}\\\\left(c_{1}\\\\right)-\\\\left(x_{A}-\\\\alpha\\\\right)^{3} u_{1, x x x}\\\\left(c_{2}\\\\right)\\\\right] \\\\end{aligned} $$ Replacing $u\\\\left(x_{A+1}\\\\right)$ by $u^{h}\\\\left(x_{A+1}\\\\right)$ and $u\\\\left(x_{A}\\\\right)$ by $u^{h}\\\\left(x_{A}\\\\right)$ in the left-hand side and rearranging terms completes the proof.\", 'To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\\\\left(h_{\\\\mathrm{A}}^{2}\\\\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\\\\left(h_{A}\\\\right)$. The exponent of $h_{\\\\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\\\\alpha \\\\rightarrow x_{A}$. Then $$ e_{, x}\\\\left(x_{A}\\\\right)=\\\\frac{h_{A}}{2} u_{, x x}\\\\left(x_{A}\\\\right)+\\\\frac{h_{A}^{2}}{3!} u_{, x x x}\\\\left(c_{1}\\\\right)=O\\\\left(h_{A}\\\\right) $$ As $\\\\boldsymbol{h}_{A} \\\\rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\\\\alpha \\\\in\\\\left[x_{A}, x_{A+1}\\\\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\\\\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \\\\equiv\\\\left(x_{A}+x_{A+1}\\\\right) / 2$ (i.e., the midpoint). Then $$ \\\\begin{aligned} e_{, x}\\\\left(x_{A+1 / 2}\\\\right) & =\\\\frac{h_{A}^{2}}{24} u_{, x x x}(c), \\\\quad c \\\\in\\\\left[x_{A}, x_{A+1}\\\\right] \\\\\\\\ & =O\\\\left(h_{A}^{2}\\\\right) \\\\end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\\\\left(x_{A+1 / 2}\\\\right)=\\\\frac{h_{A}^{2}}{48}\\\\left[u_{, x x x}\\\\left(c_{1}\\\\right)+u_{, x x x}\\\\left(c_{2}\\\\right)\\\\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\\\\frac{1}{2}\\\\left[u_{, x x x}\\\\left(c_{1}\\\\right)+u_{, x x x}\\\\left(c_{2}\\\\right)\\\\right] $$ Combining these facts completes the proof.', '\\\\begin{enumerate} \\\\item From the corollary we see that the derivatives are second-order accurate at the midpoints. \\\\end{enumerate} \\\\footnotetext{${ }^{3} \\\\mathrm{~A}$ function $f(x)$ is said to be $O\\\\left(x^{k}\\\\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \\\\rightarrow$ a constant as $x \\\\rightarrow 0$. For example, $f(x)=x^{k}$ is $O\\\\left(x^{k}\\\\right)$, as is $f(x)=\\\\sum_{j=k}^{k+1} x^{j}, l \\\\geq 0$. But neither is $O\\\\left(x^{k+1}\\\\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\\\\\\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear \"elements\" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \\\\ldots, n$ ). Consider the standard finite difference \"stencil\" for $u_{, x x}+\\\\phi=0$ at a typical internal node, namely, \\\\begin{equation*} \\\\frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \\\\tag{1.10.18} \\\\end{equation*} Assuming \\\\& varies in piecewise linear fashion and so can be expanded as \\\\begin{equation*} f=\\\\sum_{A=1}^{n+1} f_{A} N_{A} \\\\tag{1.10.19} \\\\end{equation*} where the $f_{A}$ \\'s are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\\\\\\\ i. It is exact at the nodes.\\\\\\\\ ii. There exists at least one point in each element at which the derivative is exact.\\\\\\\\ iii. The derivative is second-order accurate at the midpoints of the elements.', \"It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\\\\\\\ $\\\\boldsymbol{K} \\\\boldsymbol{d}=\\\\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \\\\subsection*{Gauss Elimination} \\\\begin{itemize} \\\\item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \\\\item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \\\\item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \\\\item Solve the $n$-th equation for $d_{n}$. \\\\end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \\\\left[\\\\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\\\\\\\ K_{21} & K_{22} & K_{23} & K_{24} \\\\\\\\ K_{31} & K_{32} & K_{33} & K_{34} \\\\\\\\ K_{41} & K_{42} & K_{43} & K_{44} \\\\end{array}\\\\right]\\\\left\\\\{\\\\begin{array}{l} d_{1} \\\\\\\\ d_{2} \\\\\\\\ d_{3} \\\\\\\\ d_{4} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} F_{1} \\\\\\\\ F_{2} \\\\\\\\ F_{3} \\\\\\\\ F_{4} \\\\end{array}\\\\right\\\\} $$ The augmented matrix corresponding to this system is \\\\[ \\\\left[ \\\\begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\\\\\\\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\\\\\\\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\\\\\\\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\\\\\\\ \\\\multicolumn{4}{c|}{\\\\underbrace{\\\\phantom{K_{11}\\\\, K_{12}\\\\, K_{13}\\\\, K_{14}}}_{K}} & \\\\underbrace{\\\\phantom{F_1}}_{F} \\\\end{array} \\\\right] \\\\] After the forward reduction, the augmented matrix becomes\\\\\\\\ \\\\[ \\\\left[ \\\\begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\\\\\\\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\\\\\\\ 0 & 0 & 1 & K'_{34} & F'_3 \\\\\\\\ 0 & 0 & 0 & 1 & d_4 \\\\\\\\ \\\\multicolumn{4}{c|}{\\\\underbrace{\\\\phantom{K_{11}\\\\, K_{12}\\\\, K_{13}\\\\, K_{14}}}_{U}} & \\\\underbrace{\\\\phantom{F_1}}_{F'} \\\\end{array} \\\\tag{1.11.1} \\\\right] \\\\] corresponding to the upper triangular system $\\\\boldsymbol{U} \\\\boldsymbol{d}=\\\\boldsymbol{F}^{\\\\prime} \\\\cdot{ }^{4}$ It is a simply verified fact that if $\\\\boldsymbol{K}$ is banded, then $\\\\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \\\\begin{itemize} \\\\item Eliminate $d_{n}$ from equations $n-1, n-2, \\\\ldots, 1$.\\\\\\\\ \\\\footnotetext{${ }^{4} \\\\text{Primes will be used to denote intermediate quantities throughout this section}.$} \\\\item Eliminate $d_{n-1}$ from equations $n-2, n-3, \\\\ldots, 1$. \\\\item Eliminate $d_{2}$ from the first equation. \\\\end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\\\\\\\ \\\\[ \\\\left[ \\\\begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\\\\\\\ 0 & 1 & 0 & 0 & d_2 \\\\\\\\ 0 & 0 & 1 & 0 & d_3 \\\\\\\\ 0 & 0 & 0 & 1 & d_4 \\\\\\\\ \\\\multicolumn{4}{c|}{\\\\underbrace{\\\\phantom{K_{11}\\\\, K_{12}\\\\, K_{13}\\\\, K_{14}}}_{I}} & \\\\underbrace{\\\\phantom{F_1}}_{d} \\\\end{array} \\\\tag{1.11.2} \\\\right] \\\\] corresponding to the identity $1 \\\\boldsymbol{d}=\\\\boldsymbol{d}$. The solution winds up in the last column.\", 'In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \\\\subsection*{Forward reduction} \\\\begin{itemize} \\\\item Divide row 1 by $K_{11}$. \\\\item Subtract $K_{21} \\\\times$ row 1 from row 2. \\\\item Subtract $K_{31} \\\\times$ row 1 from row 3. \\\\item Subtract $K_{n 1} \\\\times$ row 1 from row $n$. \\\\end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \\\\left[\\\\begin{array}{llll|l} 1 & \\\\boldsymbol{K}_{12}^{\\\\prime} & \\\\boldsymbol{K}_{3}^{\\\\prime} & \\\\boldsymbol{K}_{14}^{\\\\prime} & \\\\boldsymbol{F}_{1}^{\\\\prime} \\\\\\\\ 0 & \\\\boldsymbol{K}_{22}^{\\\\prime \\\\prime} & \\\\boldsymbol{K}_{23}^{\\\\prime \\\\prime} & \\\\boldsymbol{K}_{24}^{\\\\prime \\\\prime} & \\\\boldsymbol{F}_{2}^{\\\\prime \\\\prime} \\\\\\\\ 0 & \\\\boldsymbol{K}_{32}^{\\\\prime} & \\\\boldsymbol{K}_{33}^{\\\\prime 3} & \\\\boldsymbol{K}_{34}^{\\\\prime} & \\\\boldsymbol{F}_{3}^{\\\\prime \\\\prime} \\\\\\\\ \\\\mathbf{0} & \\\\boldsymbol{K}_{42}^{\\\\prime} & \\\\boldsymbol{K}_{43}^{3} & \\\\boldsymbol{K}_{44}^{\\\\prime \\\\prime} & \\\\boldsymbol{F}_{4}^{\\\\prime \\\\prime} \\\\end{array}\\\\right] $$ Note that if $\\\\boldsymbol{K}_{\\\\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \\\\begin{itemize} \\\\item Divide row 2 by $K_{22}^{\\\\prime \\\\prime}$. \\\\item Subtract $K_{32}^{\\\\prime \\\\prime} \\\\times$ row 2 from row 3. \\\\item Subtract $K_{42}^{n} \\\\times$ row 2 from row 4. \\\\item Subtract $K_{n 2}^{\\\\prime \\\\prime} \\\\times$ row 2 from row $n$. \\\\end{itemize} The result for the example will look like\\\\\\\\ $\\\\left[\\\\begin{array}{cccc|c}1 & \\\\boldsymbol{K}_{12}^{\\\\prime} & \\\\boldsymbol{K}_{13}^{\\\\prime} & \\\\boldsymbol{K}_{14}^{\\\\prime} & \\\\boldsymbol{F}_{1}^{\\\\prime} \\\\\\\\ 0 & 1 & \\\\boldsymbol{K}_{23}^{\\\\prime \\\\prime \\\\prime} & \\\\boldsymbol{K}_{24}^{\\\\prime \\\\prime \\\\prime} & \\\\boldsymbol{F}_{2}^{\\\\prime \\\\prime \\\\prime} \\\\\\\\ \\\\mathbf{0} & \\\\mathbf{0} & \\\\boldsymbol{K}_{33}^{\\\\prime \\\\prime \\\\prime} & \\\\boldsymbol{K}_{34}^{\\\\prime \\\\prime \\\\prime} & \\\\boldsymbol{F}_{3}^{\\\\prime \\\\prime \\\\prime} \\\\\\\\ \\\\mathbf{0} & \\\\mathbf{0} & \\\\boldsymbol{K}_{43}^{\\\\prime \\\\prime \\\\prime} & \\\\boldsymbol{K}_{44}^{\\\\prime \\\\prime\\\\prime} & \\\\boldsymbol{F}_{4}^{\\\\prime \\\\prime \\\\prime} \\\\\\\\ & & & & \\\\end{array}\\\\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\\\\\\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \\\\subsection*{Back substitution} \\\\begin{itemize} \\\\item Subtract $K_{n-1, n}^{\\\\prime} \\\\times$ row $n$ from row $n-1$. \\\\item Subtract $K_{n-2, n}^{\\\\prime} \\\\times$ row $n$ from row $n-2$.\\\\\\\\ \\\\vdots \\\\item Subtract $K_{1, n}^{\\\\prime} \\\\times$ row $n$ from row 1 \\\\end{itemize} After these steps the augmented matrix, for this example, will look like $$ \\\\left[\\\\begin{array}{cccc|c} 1 & \\\\bar{K}_{12}^{\\\\prime} & \\\\bar{K}_{3}^{\\\\prime} & 0 & F_{1}^{\\\\prime \\\\prime \\\\prime \\\\prime} \\\\\\\\ 0 & 1 & K_{23}^{\\\\prime} & 0 & F_{2}^{\\\\prime \\\\prime \\\\prime \\\\prime} \\\\\\\\ 0 & 0 & 1 & 0 & d_{3} \\\\\\\\ 0 & 0 & 0 & 1 & d_{4} \\\\end{array}\\\\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\\\\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \\\\begin{itemize} \\\\item Subtract $K_{n-2, n-1}^{\\\\prime} \\\\times$ row $n-1$ from row $n-2$. \\\\item Subtract $K_{n-3, n-1}^{\\\\prime} \\\\times$ row $n-1$ from row $n-3$.\\\\\\\\ \\\\vdots \\\\item Subtract $K_{1 . n-1}^{\\\\prime} \\\\times$ row $n-1$ from row 1. \\\\end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\\\\boldsymbol{F}$ ). Repeat as above until columns $\\\\boldsymbol{n}-2, n-3, \\\\ldots, 2$ are cleared. The result is (1.11.2). \\\\subsection*{Remarks} \\\\begin{enumerate} \\\\item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \\\\item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \\\\end{enumerate}', '$$ \\\\left[\\\\begin{array}{rrrr} 1 & -1 & 0 & 0 \\\\\\\\ -1 & 2 & -1 & 0 \\\\\\\\ 0 & -1 & 2 & -1 \\\\\\\\ 0 & 0 & -1 & 2 \\\\end{array}\\\\right]\\\\left\\\\{\\\\begin{array}{l} d_{1} \\\\\\\\ d_{2} \\\\\\\\ d_{3} \\\\\\\\ d_{4} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} 1 \\\\\\\\ 0 \\\\\\\\ 0 \\\\\\\\ 0 \\\\end{array}\\\\right\\\\} $$ \\\\subsection*{Augmented matrix} $$ \\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ -1 & 2 & -1 & 0 & 0 \\\\\\\\ 0 & -1 & 2 & -1 & 0 \\\\\\\\ 0 & 0 & -1 & 2 & 0 \\\\end{array}\\\\right] $$ Forward reduction $$ \\\\begin{aligned} & {\\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & -1 & 0 & 1 \\\\\\\\ 0 & -1 & 2 & -1 & 0 \\\\\\\\ 0 & 0 & -1 & 2 & 0 \\\\end{array}\\\\right]} \\\\\\\\ & {\\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & -1 & 0 & 1 \\\\\\\\ 0 & 0 & 1 & -1 & 1 \\\\\\\\ 0 & 0 & -1 & 2 & 0 \\\\end{array}\\\\right]} \\\\\\\\ & {\\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & -1 & 0 & 1 \\\\\\\\ 0 & 0 & 1 & -1 & 1 \\\\\\\\ 0 & 0 & 0 & 1 & 1 \\\\end{array}\\\\right]} \\\\end{aligned} $$ \\\\subsection*{Back substitution} $$ \\\\begin{aligned} & {\\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & -1 & 0 & 1 \\\\\\\\ 0 & 0 & 1 & 0 & 2 \\\\\\\\ 0 & 0 & 0 & 1 & 1 \\\\end{array}\\\\right]} \\\\\\\\ & {\\\\left[\\\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\\\\\ 0 & 1 & 0 & 0 & 3 \\\\\\\\ 0 & 0 & 1 & 0 & 2 \\\\\\\\ 0 & 0 & 0 & 1 & 1 \\\\end{array}\\\\right]} \\\\\\\\ & {\\\\left[\\\\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\\\\\\\ 0 & 1 & 0 & 0 & 3 \\\\\\\\ 0 & 0 & 1 & 0 & 2 \\\\\\\\ 0 & 0 & 0 & 1 & 1 \\\\end{array}\\\\right]} \\\\\\\\ \\\\begin{array}{l} \\\\left\\\\{ \\\\begin{array}{l} d_{1} \\\\\\\\ d_{2} \\\\\\\\ d_{3} \\\\\\\\ d_{4} \\\\end{array} \\\\right\\\\} = \\\\left\\\\{ \\\\begin{array}{l} 4 \\\\\\\\ 3 \\\\\\\\ 2 \\\\\\\\ 1 \\\\end{array} \\\\right\\\\} \\\\end{array} \\\\end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \\\\begin{aligned} u_{, x x}(x)+f(x) & =0 \\\\quad x \\\\in] 0,1[ \\\\\\\\ u(1) & =g \\\\\\\\ -u_{, x}(0) & =h \\\\end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\\\\\\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\\\\left(h=\\\\right.$ mesh parameter $\\\\left.=\\\\frac{1}{4}\\\\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\\\\left(h=1\\\\right.$ and $h=\\\\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\\\\boldsymbol{K} \\\\boldsymbol{d}=\\\\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\\\\\\\ b. Let $r e_{, x}=\\\\left|u_{, x}^{h}-u_{. x}\\\\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\\\\\\\ c. Employing the data for $h=1, \\\\frac{1}{2}$, and $\\\\frac{1}{4}$, plot $\\\\ln r e_{, x}$ versus $\\\\ln h$.\\\\\\\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\\\\\\\ i. What is the significance of the slope of the graph in part (c)?\\\\\\\\ ii. What is the significance of the $y$-intercept?', 'So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and \"local support\" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \\\\begin{center} \\\\begin{tabular}{lll} $(g 1)$ & Domain: & $\\\\left[x_{A}, x_{A+1}\\\\right]$ \\\\\\\\ $(g 2)$ & Nodes: & $\\\\left\\\\{x_{A}, x_{A+1}\\\\right\\\\}$ \\\\\\\\ $(g 3)$ & Degrees of freedom: & $\\\\left\\\\{d_{A}, d_{A+1}\\\\right\\\\}$ \\\\\\\\ $(g 4)$ & Shape functions: & $\\\\left\\\\{N_{A}, N_{A+1}\\\\right\\\\}$ \\\\\\\\ $(g 5)$ & Interpolation function: & \\\\\\\\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \\\\in\\\\left[x_{A}, x_{A+1}\\\\right]$ & \\\\\\\\ \\\\end{tabular} \\\\end{center} (Recall $d_{A}=u^{h}\\\\left(x_{A}\\\\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\\\\\\\ (l1) Domain: $\\\\left[\\\\xi_{1}, \\\\xi_{2}\\\\right]$ \\\\footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\\\\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\\\\left\\\\{\\\\xi_{1}, \\\\xi_{2}\\\\right\\\\}$\\\\\\\\ (l3) Degrees of freedom: $\\\\left\\\\{d_{1}, d_{2}\\\\right\\\\}$\\\\\\\\ (I4) Shape functions: $\\\\left\\\\{N_{1}, N_{2}\\\\right\\\\}$\\\\\\\\ (15) Interpolation function: $$ u^{h}(\\\\xi)=N_{1}(\\\\xi) d_{1}+N_{2}(\\\\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\\\\\\\ We shall relate the domains of the global and local descriptions by an \"affine\" transformation $\\\\xi:\\\\left[x_{A}, x_{A+1}\\\\right] \\\\rightarrow\\\\left[\\\\xi_{1}, \\\\xi_{2}\\\\right]$, such that $\\\\xi\\\\left(x_{A}\\\\right)=\\\\xi_{1}$ and $\\\\xi\\\\left(x_{A+1}\\\\right)=\\\\xi_{2}$. It is standard practice to take $\\\\xi_{1}=-1$ and $\\\\xi_{2}=+1$. Thus $\\\\xi$ may be represented by the expression \\\\begin{equation*} \\\\xi(x)=c_{1}+c_{2} x \\\\tag{1.12.1} \\\\end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \\\\[ \\\\left.\\\\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \\\\tag{1.12.2}\\\\\\\\ 1 & =c_{1}+x_{A+1} c_{2} \\\\end{array}\\\\right\\\\} \\\\] Solving this system yields \\\\begin{equation*} \\\\xi(x)=\\\\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \\\\tag{1.12.3} \\\\end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\\\\xi$ is obtained by solving for $x$ : \\\\begin{equation*} x(\\\\xi)=\\\\frac{h_{A} \\\\xi+x_{A}+x_{A+1}}{2} \\\\tag{1.12.4} \\\\end{equation*} In (1.12.1), $\\\\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\\\\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \\\\ldots$ pertain to the local numbering system. The subscripts $A, B, C, \\\\ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\\\\xi)=N_{A}\\\\left(x^{e}(\\\\xi)\\\\right)$, where $x^{e}:\\\\left[\\\\xi_{1}, \\\\xi_{2}\\\\right] \\\\rightarrow$ $\\\\left[x_{1}^{e}, x_{2}^{e}\\\\right]=\\\\left[x_{A}, x_{A+1}\\\\right]$, etc.). In terms of $\\\\xi$, the shape functions in the local description take on a standard form \\\\begin{equation*} N_{a}(\\\\xi)=\\\\frac{1}{2}\\\\left(1+\\\\xi_{a} \\\\xi\\\\right), \\\\quad a=1,2 \\\\tag{1.12.5} \\\\end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \\\\begin{equation*} x^{e}(\\\\xi)=\\\\sum_{a=1}^{2} N_{a}(\\\\xi) x_{a}^{e} \\\\tag{1.12.6} \\\\end{equation*} This has the same form as the interpolation function (cf. 15).\\\\\\\\ For future reference, we note the following results: \\\\begin{gather*} N_{a, \\\\xi}=\\\\frac{\\\\xi_{a}}{2}=\\\\frac{(-1)^{a}}{2} \\\\tag{1.12.7}\\\\\\\\ x_{, \\\\xi}^{e}=\\\\frac{h^{e}}{2} \\\\tag{1.12.8} \\\\end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \\\\begin{equation*} \\\\xi_{, x}^{e}=\\\\left(x_{, \\\\xi}^{e}\\\\right)^{-1}=\\\\frac{2}{h^{e}} \\\\tag{1.12.9} \\\\end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element.', 'To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \\\\leq e \\\\leq n_{e l}$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\\\\\\\ Now recall the definitions of the (global) stiffness matrix and force vector\\\\\\\\ \\\\\\\\ \\\\[ K = \\\\underbrace{\\\\left[ K_{AB} \\\\right]}_{n \\\\times n}, \\\\quad F = \\\\underbrace{\\\\left\\\\{ F_A \\\\right\\\\}}_{n \\\\times 1} \\\\tag{1.13.1} \\\\] where \\\\begin{gather*} K_{A B}=a\\\\left(N_{A}, N_{B}\\\\right)=\\\\int_{0}^{1} N_{A, x} N_{B, x} d x \\\\tag{1.13.2}\\\\\\\\ F_{A}=\\\\left(N_{A}, f\\\\right)+\\\\delta_{A 1} h-a\\\\left(N_{A}, N_{n+1}\\\\right) g \\\\\\\\ =\\\\int_{0}^{1} N_{A} f d x+\\\\delta_{A 1} h-\\\\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \\\\tag{1.13.3} \\\\end{gather*} ( $\\\\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\\\\left(x_{1}\\\\right)=\\\\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \\\\[ \\\\begin{array}{ll} \\\\boldsymbol{K}=\\\\sum_{e=1}^{n_{e l}} \\\\boldsymbol{K}^{e}, & \\\\boldsymbol{K}^{e}=\\\\left[K_{A B}^{e}\\\\right] \\\\\\\\ \\\\boldsymbol{F}=\\\\sum_{e=1}^{n_{e l}} \\\\boldsymbol{F}^{e}, & \\\\boldsymbol{F}^{e}=\\\\left\\\\{F_{\\\\hat{A}}^{e}\\\\right\\\\} \\\\tag{1.13.5} \\\\end{array} \\\\] where \\\\begin{align*} K_{A B}^{e} & =a\\\\left(N_{A}, N_{B}\\\\right)^{e}=\\\\int_{\\\\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \\\\tag{1.13.6}\\\\\\\\ F_{A}^{e} & =\\\\left(N_{A}, f\\\\right)^{e}+\\\\delta_{e 1} \\\\delta_{A 1} h-a\\\\left(N_{A}, N_{n+1}\\\\right)^{e} g \\\\\\\\ & =\\\\int_{\\\\Omega^{e}} N_{A} f d x+\\\\delta_{e 1} \\\\delta_{A 1} h-\\\\int_{\\\\Omega^{e}} N_{A, x} N_{n+1, x} d x g \\\\tag{1.13.7} \\\\end{align*} and $\\\\Omega^{e}=\\\\left[x_{1}^{e}, x_{2}^{e}\\\\right]$, the domain of the eth element.\\\\\\\\ The important observation to make is that $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ \\'s, we have that \\\\begin{equation*} K_{A B}^{e}=0, \\\\quad \\\\text { if } A \\\\neq e \\\\text { or } e+1 \\\\text { or } B \\\\neq e \\\\text { or } e+1 \\\\tag{1.13.8} \\\\end{equation*} and \\\\begin{equation*} F_{A}^{e}=0, \\\\quad \\\\text { if } A \\\\neq e \\\\text { or } e+1 \\\\tag{1.13.9} \\\\end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \\\\begin{align*} & k^{e}=\\\\underbrace{\\\\left[k_{a b}^{e}\\\\right]}_{2 \\\\times 2}, \\\\quad f^{e}=\\\\underbrace{\\\\left\\\\{f_{a}^{e}\\\\right\\\\}}_{2 \\\\times 1} \\\\tag{1.13.10}\\\\\\\\ & k_{a b}^{e}=a\\\\left(N_{a}, N_{b}\\\\right)^{e}=\\\\int_{\\\\Omega^{e}} N_{a, x} N_{b, x} d x \\\\tag{1.13.11}\\\\\\\\ & f_{a}^{e}=\\\\int_{\\\\Omega^{e}} N_{a} f d x+\\\\left\\\\{\\\\begin{array}{cl} \\\\delta_{a 1} h & e=1 \\\\\\\\ 0 & e=2,3, \\\\ldots, n_{e l}-1 \\\\\\\\ -k_{a 2 }^{e} g & e=n_{e l} \\\\end{array}\\\\right. \\\\tag{1.13.12} \\\\end{align*} \\\\begin{center} \\\\includegraphics[max width=\\\\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \\\\end{center} Here $\\\\boldsymbol{k}^{e}$ and $\\\\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\\\\boldsymbol{K}^{e}$ and $\\\\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ \"go\" in $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section.', 'In a finite element computer program, it is the task of a \"finite element subroutine\" to produce $k^{e}$ and $f^{e}, e=1,2, \\\\ldots, n_{e l}$, from given data and to provide an \"assembly subroutine\" enough information so that the terms in $\\\\boldsymbol{k}^{e}$ and $\\\\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\\\\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \\\\[ A=\\\\mathrm{LM}(a, e)=\\\\left\\\\{\\\\begin{array}{cc} e & \\\\text { if } a=1 \\\\tag{1.14.1}\\\\\\\\ e+1 & \\\\text { if } a=2 \\\\end{array}\\\\right. \\\\] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\\\\mathrm{LM}\\\\left(2, n_{e l}\\\\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \\\\leq e \\\\leq n_{e l}$ \\\\begin{center} \\\\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow[b]{3}{*}{\\\\( \\\\begin{aligned} & \\\\text { Local } \\\\\\\\ & \\\\text { node } \\\\\\\\ & \\\\text { number } \\\\end{aligned} \\\\)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\\\\\\\ \\\\hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\\\\\\\ \\\\hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\\\\\\\ \\\\hline \\\\end{tabular} \\\\end{center} Figure 1.14.1 LM array for example problem.\\\\\\\\ As an example, assume we want to add the eth elemental contributions, where $1 \\\\leq e \\\\leq n_{e l-1}$, to the partially assembled $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \\\\begin{align*} K_{e e} & \\\\leftarrow K_{e e}+k_{11}^{e} \\\\tag{1.14.2}\\\\\\\\ K_{e, e+1} & \\\\leftarrow K_{e, e+1}+k_{12}^{e} \\\\tag{1.14.3}\\\\\\\\ K_{e+1, e} & \\\\leftarrow K_{e+1, e}+k_{21}^{e} \\\\tag{1.14.4}\\\\\\\\ K_{e+1, e+1} & \\\\leftarrow K_{e+1, e+1}+k_{22}^{e} \\\\tag{1.14.5} \\\\end{align*} \\\\footnote{${ }^{6}$ Due to symmetry $k_{21}^{\\\\prime}$ would not actually be assembled in practice.} \\\\begin{align*} F_{e} & \\\\leftarrow F_{e}+f_{i}^{e} \\\\tag{1.14.6}\\\\\\\\ F_{e+1} & \\\\leftarrow F_{e+1}+f_{2}^{e} \\\\tag{1.14.7} \\\\end{align*} where the arrow $(\\\\leftarrow)$ is read \"is replaced by.\"\\\\\\\\ For element $n_{e l}$ we have only that \\\\begin{align*} K_{n n} & \\\\leftarrow K_{m n}+k_{11}^{n_{el}} \\\\tag{1.14.8}\\\\\\\\ F_{n} & \\\\leftarrow F_{n}+f_{1}^{n_{t}} \\\\tag{1.14.9} \\\\end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$; see Fig. 1.14.2.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\\\\mathbf{A}$, the assembly operator, vis., \\\\begin{equation*} \\\\boldsymbol{K}=\\\\boldsymbol{A}_{e=1}^{n_{el}}\\\\left(k^{e}\\\\right), \\\\quad \\\\boldsymbol{F}=A_{e=1}^{n_{el}}\\\\left(f^{e}\\\\right) \\\\tag{1.14.10} \\\\end{equation*}', 'The explicit computation of $\\\\boldsymbol{k}^{\\\\boldsymbol{e}}$ and $\\\\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \\\\subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\\\\left[x_{1}, x_{2}\\\\right] \\\\rightarrow \\\\mathbb{R}$ be an integrable function and let $x:\\\\left[\\\\xi_{1}, \\\\xi_{2}\\\\right] \\\\rightarrow\\\\left[x_{1}, x_{2}\\\\right]$ be continuously differentiable, with $x\\\\left(\\\\xi_{1}\\\\right)=x_{1}$ and $x\\\\left(\\\\xi_{2}\\\\right)=x_{2}$. Then \\\\begin{equation*} \\\\int_{x_{1}}^{x_{2}} f(x) d x=\\\\int_{\\\\xi_{1}}^{\\\\xi_{2}} f(x(\\\\xi)) x_{, \\\\xi}(\\\\xi) d \\\\xi \\\\tag{1.15.1} \\\\end{equation*}', 'Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \\\\begin{equation*} \\\\frac{\\\\partial}{\\\\partial \\\\xi} f(x(\\\\xi))=f_{, x}(x(\\\\xi)) x_{, \\\\xi}(\\\\xi) \\\\tag{1.15.2} \\\\end{equation*} Proofs of these results may be found in [11].\\\\\\\\ The computation of $k^{e}$ proceeds as follows: $$ \\\\begin{aligned} k_{a b}^{e} & =\\\\int_{\\\\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \\\\quad \\\\text { (by definition) } \\\\\\\\ & =\\\\int_{-1}^{+1} N_{a, x}(x(\\\\xi)) N_{b, x}(x(\\\\xi)) x_{, \\\\xi}(\\\\xi) d \\\\xi \\\\end{aligned} $$ (Change of variables, where $x(\\\\xi)$ is defined by (1.12.6)) $$ =\\\\int_{-1}^{+1} N_{a, \\\\xi}(\\\\xi) N_{b, \\\\xi}(\\\\xi)\\\\left(x_{, \\\\xi}(\\\\xi)\\\\right)^{-1} d \\\\xi $$ (Chain rule; $\\\\left.N_{a, \\\\xi}(\\\\xi)=(\\\\partial / \\\\partial \\\\xi) N_{a}(x(\\\\xi))=N_{a, x}(x(\\\\xi)) x_{, \\\\xi}(\\\\xi)\\\\right)$ $$ =(-1)^{a+b} / h^{e} \\\\quad(\\\\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \\\\[ k^{e}=\\\\frac{1}{h^{e}}\\\\left[\\\\begin{array}{rr} 1 & -1 \\\\tag{1.15.3}\\\\\\\\ -1 & 1 \\\\end{array}\\\\right] \\\\] Observe that $N_{a, \\\\xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\\\\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\\\\boldsymbol{x}_{, \\\\xi}$ and $\\\\boldsymbol{\\\\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\\\\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \\\\begin{equation*} f^{h}=\\\\sum_{a=1}^{2} f_{a} N_{a} \\\\tag{1.15.4} \\\\end{equation*} where $f_{a}=f\\\\left(x\\\\left(\\\\xi_{a}\\\\right)\\\\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear \"loading\" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \\\\begin{align*} \\\\int_{\\\\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\\\\int_{-1}^{+1} N_{a}(x(\\\\xi)) f^{h}(x(\\\\xi)) x_{, \\\\xi}(\\\\xi) d \\\\xi \\\\quad \\\\text { (change of variables) } \\\\\\\\ & =\\\\frac{h^{e}}{2} \\\\sum_{b=1}^{2} \\\\int_{-1}^{+1} N_{a}(\\\\xi) N_{b}(\\\\xi) d \\\\xi f_{b} \\\\quad \\\\text { (by (1.12.8)) } \\\\tag{1.15.5} \\\\end{align*} \\\\begin{center} \\\\includegraphics[max width=\\\\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \\\\end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\\\\left(\\\\int_{-1}^{+1} N_{a} N_{b} d \\\\xi=\\\\left(1+\\\\delta_{a b}\\\\right) / 3\\\\right)$ yields \\\\[ \\\\begin{array}{rlr} \\\\mathfrak{f}^{e} & =\\\\frac{h^{e}}{6}\\\\left[\\\\begin{array}{cc} 2 & 1 \\\\\\\\ 1 & 2 \\\\end{array}\\\\right]\\\\left\\\\{\\\\begin{array}{l} f_{1} \\\\\\\\ f_{2} \\\\end{array}\\\\right\\\\} & \\\\begin{array}{c} \\\\text { (+ boundary terms } \\\\\\\\ \\\\text { cf. (1.13.12)) } \\\\end{array} \\\\\\\\ & =\\\\frac{h^{e}}{6}\\\\left\\\\{\\\\begin{array}{l} 2 f_{1} + f_{2} \\\\\\\\ f_{1} + 2 f_{2} \\\\end{array}\\\\right\\\\} & \\\\text { (+ boundary terms) } \\\\tag{1.15.6} \\\\end{array} \\\\] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\\\\left(h^{2}\\\\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\\\\left(h^{2}\\\\right)$ errors in $u^{\\\\boldsymbol{h}}$ and $u_{, x}^{\\\\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \\\\subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\\\\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \\\\left.u_{,xx}-\\\\lambda u+f=0 \\\\quad \\\\text { on } \\\\Omega=\\\\right] 0,1[ $$ where $\\\\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \\\\int_{\\\\Omega}\\\\left(w_{\\\\cdot x} u_{, x}+w \\\\lambda u\\\\right) d x=\\\\int_{\\\\Omega} w f d x+w(0) h $$ where $u \\\\in \\\\mathfrak{f}, w \\\\in \\\\mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \\\\lambda u)=(w, f)+w(0) h $$ i. Let $\\\\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \\\\begin{array}{r} a\\\\left(w^{h}, v^{h}\\\\right)+\\\\square= \\\\\\\\ \\\\left(w^{h}, f\\\\right)+w^{h}(0) h-a\\\\left(w^{h}, gS^{h}\\\\right) \\\\\\\\ -\\\\square \\\\end{array} $$ ii. Define $K_{A B}=a\\\\left(N_{A}, N_{B}\\\\right)+\\\\square$\\\\\\\\ and $$ k_{a b}^{e}=a\\\\left(N_{a}, N_{b}\\\\right)^{e}+\\\\square $$ iii. Determine $\\\\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\\\\left[k_{a b}^{e}\\\\right]=[\\\\square] $$ iv. Show that $\\\\boldsymbol{K}$ is symmetric.\\\\\\\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\\\\\\\ vi. The Green\\'s function for this problem satisfies $$ g_{. x x}-\\\\lambda g+\\\\delta_{y}=0 $$ and can be written as $$ g(x)= \\\\begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \\\\leq x \\\\leq y \\\\\\\\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \\\\leq x \\\\leq 1\\\\end{cases} $$ where $p=\\\\lambda^{1 / 2}$ and the $c$ \\'s are determined from the following four boundary and continuity conditions: $$ \\\\begin{aligned} g(1) & =0 \\\\\\\\ g_{, x}(0) & =0 \\\\\\\\ g\\\\left(y^{+}\\\\right) & =g\\\\left(y^{-}\\\\right) \\\\\\\\ g_{, x}\\\\left(y^{+}\\\\right) & =g_{. x}\\\\left(y^{-}\\\\right)-1 \\\\end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\\\\\\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \\\\quad x \\\\in \\\\Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ \\'s are determined from $$ d_{a}^{e}=u^{h}\\\\left(x_{a}^{e}\\\\right), \\\\quad a=1,2 $$ What is the attribute which this choice of functions attains?', \"This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\\\\Omega=] 0,1[, \\\\bar{\\\\Omega}=[0,1]$ ).\\\\\\\\ \\\\[ \\\\text{(S)} \\\\quad \\\\left\\\\{ \\\\begin{minipage}{0.8\\\\textwidth} \\\\text {Given } $f: \\\\Omega \\\\rightarrow \\\\mathbb{R}$ \\\\text { and constants } $M$ \\\\text { and } $f$, \\\\text { find } $u: \\\\bar{\\\\Omega} \\\\rightarrow \\\\mathbb{R}$ \\\\text { such that:} \\\\begin{align*} E I u_{,xxxx} = f \\\\quad \\\\text {on } \\\\Omega \\\\quad \\\\text {(transverse equilibrium)}\\\\\\\\ u(1) &= 0 & \\\\text{(zero transverse displacement)} \\\\\\\\ u_{x}(1) &= 0 & \\\\text{(zero slope)} \\\\\\\\ E I u_{,xx}(0) &= M & \\\\text{(prescribed moment)} \\\\\\\\ E I u_{,xxx}(0) &= Q & \\\\text{(prescribed shear)} \\\\end{align*} \\\\end{minipage} \\\\right. \\\\] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\\\\\\\ The setup is shown in Fig. 1.16.1.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\\\\\\\ Let $\\\\mathfrak{f}=\\\\mathcal{U}=\\\\left\\\\{w \\\\mid w \\\\in H^{2}(\\\\Omega), w(1)=w_{x}(1)=0\\\\right\\\\}^{7}$. Then a corresponding weak form of the problem is:\\\\\\\\ \\\\[ \\\\text{(W)} \\\\quad \\\\left\\\\{ \\\\begin{minipage}{0.8\\\\textwidth} \\\\text{Given} $f, M$, \\\\text{, and} $Q$, find $u \\\\in \\\\mathfrak{f}$ \\\\text{such that for all} $w \\\\in \\\\mathcal{U}$\\\\\\\\ \\\\begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \\\\end{align*} \\\\end{minipage} \\\\right. \\\\] where $$ \\\\begin{aligned} a(w, u) & =\\\\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\\\\\\\ (w, f) & =\\\\int_{0}^{1} w f d x \\\\end{aligned} $$ \\\\footnotetext{${ }^{7} w \\\\in H^{2}(\\\\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\\\\left.f_{0}^{1}\\\\left(w_{, x x}\\\\right)^{2} d x<\\\\infty\\\\right)$. }The collection of functions, $\\\\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \\\\in \\\\mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\\\\mathfrak{d}^{h}=\\\\mathcal{U}^{h}$ be a finite-dimensional approximation of $\\\\mathfrak{f}$. In particular, we assume $w^{h} \\\\in \\\\mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\\\\\\\ (G) $\\\\left\\\\{\\\\begin{array}{c}\\\\text { Given } f, M, \\\\text { and } Q, \\\\text { find } u^{h} \\\\in \\\\delta^{h} \\\\text { such that for all } w^{h} \\\\in \\\\mathcal{U}^{h} \\\\\\\\ a\\\\left(w^{h}, u^{h}\\\\right)=\\\\left(w^{h}, f\\\\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\\\\end{array}\\\\right.$\\\\\\\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\\\\\\\ b. Assume $0=x_{1}<x_{2}<\\\\cdots<x_{n+1}=1$ and $\\\\mathcal{U}^{h}=\\\\left\\\\{w^{h} \\\\mid w^{h} \\\\in C^{1}(\\\\bar{\\\\Omega})\\\\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\\\\left[x_{A}, x_{A+1}\\\\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\\\\left.\\\\left.1, x, x^{2}, x^{3}\\\\right)\\\\right\\\\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \\\\in \\\\mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\\\\mathrm{A}}$ and $x_{\\\\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\\\\left(x_{1}\\\\right)+N_{3}(x) w^{h}\\\\left(x_{2}\\\\right)+N_{2}(x) w_{. x}^{h}\\\\left(x_{1}\\\\right)+N_{4}(x) w_{, x}^{h}\\\\left(x_{2}\\\\right) $$ where $$ \\\\begin{aligned} & N_{1}(x)=\\\\frac{-\\\\left(x-x_{2}\\\\right)^{2}\\\\left[-h+2\\\\left(x_{1}-x\\\\right)\\\\right]}{h^{3}} \\\\\\\\ & N_{2}(x)=\\\\frac{\\\\left(x-x_{1}\\\\right)^{\\\\prime}\\\\left(x-x_{2}\\\\right)^{2}}{h^{2}} \\\\\\\\ & N_{3}(x)=\\\\frac{\\\\left(x-x_{1}\\\\right)^{2}\\\\left[h+2\\\\left(x_{2}-x\\\\right)\\\\right]}{h^{3}} \\\\\\\\ & N_{4}(x)=\\\\frac{\\\\left(x-x_{1}\\\\right)^{2}\\\\left(x-x_{2}\\\\right)}{h^{2}} \\\\end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\\\\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \\\\begin{aligned} w^{h}\\\\left(x_{1}\\\\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\\\\\\\ w^{h}\\\\left(x_{2}\\\\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\\\\\\\ w_{,x}^{h}\\\\left(x_{1}\\\\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\\\\\\\ w_{,x}^{h}\\\\left(x_{2}\\\\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \\\\end{aligned} $$ \\\\footnote{${ }^{8}$ The notation $w^{k} \\\\in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\\\\\\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\\\\xi$-element coordinate system introduced in Sec. 1.12 (recall $\\\\left.\\\\xi=\\\\left(2 x-x_{A}-x_{A+1}\\\\right) / h_{A}\\\\right)$, the location of the Barlow curvature points may be expressed as $\\\\xi= \\\\pm 1 / \\\\sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\\\\\\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\\\\left(h^{3}\\\\right)$ ).\\\\\\\\ e. If the segment of the beam $\\\\left[x_{A}, x_{A+1}\\\\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\\\\\\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\\\\\\\ g. Prove that $$ \\\\begin{gathered} u^{h}\\\\left(x_{A}\\\\right)=u\\\\left(x_{A}\\\\right) \\\\\\\\ u_{, x}^{h}\\\\left(x_{A}\\\\right)=u_{, x}\\\\left(x_{A}\\\\right) \\\\end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\\\\delta_{x}\\\\left(x-x_{A}\\\\right)$, which is the generalized derivative of the delta function.\\\\\\\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\\\\\\\ i. Why do we require that the functions in $\\\\mathcal{U}^{h}$ have continuous first derivatives?\\\\\\\\ j. Calculate the $4 \\\\times 4$ element stiffness matrix, $$ k_{p q}^{e}=\\\\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \\\\quad 1 \\\\leq p, q \\\\leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\\\\mathrm{e}}$.\\\\\\\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \\\\in \\\\mathcal{U}$ and $u \\\\in \\\\mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \\\\begin{aligned} 0= & \\\\sum_{A=1}^{n} \\\\int_{x_{A}}^{x_{A}+1} w\\\\left(E I u_{, x x x x}-f\\\\right) d x \\\\\\\\ & -w_{, x}(0)\\\\left(E I u_{, x x}\\\\left(0^{+}\\\\right)-M\\\\right) \\\\\\\\ & +w(0)\\\\left(E I u_{, x x x}\\\\left(0^{+}\\\\right)-Q\\\\right) \\\\\\\\ & -\\\\sum_{A=2}^{n} w_{, x}\\\\left(x_{A}\\\\right) E I\\\\left(u_{, x x}\\\\left(x_{A}^{+}\\\\right)-u_{, x x}\\\\left(x_{A}^{-}\\\\right)\\\\right) \\\\\\\\ & +\\\\sum_{A=2}^{n} w\\\\left(x_{A}\\\\right) E I\\\\left(u_{, x x x}\\\\left(x_{A}^{+}\\\\right)-u_{, x x x}\\\\left(x_{A}^{-}\\\\right)\\\\right) \\\\end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\\\\\\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\\\\left.x \\\\in\\\\right] x_{A}, x_{A+1}[$ and $A=1,2, \\\\ldots, n$\\\\\\\\ ii. $E I u_{, x x}\\\\left(0^{+}\\\\right)=M$\\\\\\\\ iii. EI $u_{, x x x}\\\\left(0^{+}\\\\right)=Q$\\\\\\\\ iv. $E I u_{, x x}\\\\left(x_{A}^{+}\\\\right)=E I u_{, x x}\\\\left(x_{A}^{-}\\\\right)$, where $A=2,3, \\\\ldots, n$\\\\\\\\ v. $E I u_{, x x x}\\\\left(x_{A}^{+}\\\\right)=E I u_{, x x x}\\\\left(x_{A}^{-}\\\\right)$, where $A=2,3, \\\\ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v).\", '\\\\subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \\\\Omega \\\\rightarrow \\\\mathbb{R}$ (recall $\\\\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\\\\Omega)$, if its derivatives of order $j$, where $0 \\\\leq j \\\\leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\\\\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \\\\ldots, \\\\infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \\\\in \\\\Omega$ ). \\\\subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\\\\infty}$. \\\\subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\\\\Omega$, as are all its derivatives; hence it is $C^{\\\\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \\\\in \\\\Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \\\\geq 0$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \\\\subsection*{Example 3} The function \\\\[ f(x)= \\\\begin{cases}x, & x \\\\leq \\\\frac{1}{2} \\\\tag{1.I.1}\\\\\\\\ 1 / 2, & x>\\\\frac{1}{2}\\\\end{cases} \\\\] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \\\\geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \\\\[ f(x)=\\\\left\\\\{\\\\begin{array}{cc} \\\\frac{x^{2}}{2}, & x \\\\leq \\\\frac{1}{2} \\\\tag{1.I.2}\\\\\\\\ \\\\frac{\\\\left(x-\\\\frac{1}{4}\\\\right)}{2}, & x>\\\\frac{1}{2} \\\\end{array}\\\\right. \\\\] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a \"smooth\" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\\\\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\\\\overline{\\\\boldsymbol{\\\\Omega}}=[0,1]$, instead of $\\\\boldsymbol{\\\\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\\\\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\\\\Omega)$ and $C_{b}^{k}(\\\\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally \"rough.\" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a \"generalized derivative,\" as was used in solving the Green\\'s function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \\\\begin{equation*} H^{k}=H^{k}(\\\\Omega)=\\\\left\\\\{w \\\\mid w \\\\in L_{2} ; w_{, x} \\\\in L_{2} ; \\\\ldots ; \\\\underbrace{w_{x \\\\ldots x}}_{\\\\text{k times}} \\\\in L_{2}\\\\right\\\\} \\\\tag{1.I.3} \\\\end{equation*} where \\\\begin{equation*} L_{2}=L_{2}(\\\\Omega)=\\\\left\\\\{w \\\\mid \\\\int_{0}^{1} w^{2} d x<\\\\infty\\\\right\\\\} \\\\tag{1.I.4} \\\\end{equation*} In words, the Sobolev space of degree $k$, denoted by $\\\\boldsymbol{H}^{\\\\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \\\\subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev\\'s theorem, which states that, in one dimension, $H^{k+1} \\\\subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev\\'s theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev\\'s theorem and thus possess bounded, continuous, classical derivatives. Certain \"singularities\" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in \"functional analysis.\" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein.', '\\\\begin{enumerate} \\\\item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{1} \\\\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \\\\item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \\\\item B. A. Finlayson and L. E. Scriven, \"The Method of Weighted Residuals-A Review,\" Applied Mechanics Reviews, 19, (1966), 735-738. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{4} \\\\item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \\\\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \\\\item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \\\\item J. Barlow, \"Optimal Stress Locations in Finite Element Models,\" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \\\\item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{9} \\\\item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, \"Stiffness and deflection analysis of complex structures,\" Journal of Aeronautical Sciences, 23 (1956), 805-823. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{10} \\\\item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \\\\item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{12} \\\\item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \\\\item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \\\\item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \\\\end{enumerate} \\\\maketitle', '\\\\subsection*{2.1 INTRODUCTORY REMARKS} It makes no sense to attempt to \"solve\" a boundary-value problem without a precise knowledge of what the problem is. The truth of this statement seems self-evident. Unfortunately, attempts are often made to solve vaguely defined problems, creating considerable confusion and, sometimes, totally erroneous results. In this chapter we present precise statements of multidimensional boundary-value problems in classical linear heat conduction and elastostatics. The presentation is similar in many respects to that for the one-dimensional model problem of Chapter 1. In particular, we discuss strong and weak forms, their equivalence, corresponding Galerkin formulations, the definitions of element arrays, and pertinent data processing concepts. In multidimensions, the data processing ideas necessarily become more involved. The reader is urged to study them carefully as they are necessary in order to understand the computer implementation of finite element techniques. \\\\subsection*{2.2 PRELIMINARIES} Let $n_{s d}(=2$ or 3 ) denote the number of space dimensions of the problem under consideration. Let $\\\\Omega \\\\subset \\\\mathbb{R}^{n_{sd}}$ be an open set\\\\footnote{For our purposes, it is sufficient to think of an open set as one without its boundary. }. We shall employ the following alternative representations for $\\\\boldsymbol{x}$ and $\\\\boldsymbol{n}$ : with piecewise smooth boundary $\\\\Gamma$. A general point in $\\\\mathbb{R}^{n_{sd}}$ is denoted by $\\\\boldsymbol{x}$. We will identify the point $\\\\boldsymbol{x}$ with its position vector emanating from the origin of ${R}^{n_{sd}}$. The unit outward normal vector to $\\\\Gamma$ is denoted by $\\\\boldsymbol{n}$. \\\\begin{align*} & \\\\left(n_{s d}=2\\\\right): \\\\quad x=\\\\left\\\\{x_{i}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} x_{1} \\\\\\\\ x_{2} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} x \\\\\\\\ y \\\\end{array}\\\\right\\\\} \\\\quad n=\\\\left\\\\{n_{i}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} n_{1} \\\\\\\\ n_{2} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} n_{x} \\\\\\\\ n_{y} \\\\end{array}\\\\right\\\\} \\\\tag{2.2.1}\\\\\\\\ & \\\\left(n_{s d}=3\\\\right): \\\\quad x=\\\\left\\\\{x_{i}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} x_{1} \\\\\\\\ x_{2} \\\\\\\\ x_{3} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} x \\\\\\\\ y \\\\\\\\ z \\\\end{array}\\\\right\\\\} \\\\quad n=\\\\left\\\\{n_{i}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} n_{1} \\\\\\\\ n_{2} \\\\\\\\ n_{3} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} n_{x} \\\\\\\\ n_{y} \\\\\\\\ n_{z} \\\\end{array}\\\\right\\\\} \\\\tag{2.2.2} \\\\end{align*} where $x_{i}$ and $n_{i}, 1 \\\\leq i \\\\leq n_{s d}$, are the Cartesian components of $\\\\boldsymbol{x}$ and $\\\\boldsymbol{n}$, respectively; see Figure 2.2.1. Unless otherwise specified we shall work in terms of Cartesian components of vectors and tensors.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-02} Figure 2.2.1\\\\\\\\ We assume that $\\\\Gamma$ admits the decomposition \\\\begin{equation*} \\\\Gamma=\\\\overline{\\\\Gamma_{g} \\\\cup \\\\Gamma_{h}} \\\\tag{2.2.3} \\\\end{equation*} where \\\\begin{equation*} \\\\Gamma_{g} \\\\cap \\\\Gamma_{h}=\\\\varnothing \\\\tag{2.2.4} \\\\end{equation*} and $\\\\Gamma_{g}$ and $\\\\Gamma_{h}$ are open sets in $\\\\Gamma$. The notations are defined as follows: $\\\\cup$ is the set union symbol. Thus $\\\\Gamma_{g} \\\\cup \\\\Gamma_{h}$ means the set of all points $x$ contained in either $\\\\Gamma_{g}$ or $\\\\Gamma_{h}$. Also, $\\\\cap$ is the set intersection symbol. Thus $\\\\Gamma_{g} \\\\cap \\\\Gamma_{h}$ means the set of all points contained in both $\\\\Gamma_{g}$ and $\\\\Gamma_{h}$. The empty set is denoted by $\\\\varnothing$. Thus (2.2.4) means that there is no point $x$ contained in both $\\\\Gamma_{g}$ and $\\\\Gamma_{h}$ (i.e., $\\\\Gamma_{g}$ and $\\\\Gamma_{h}$ do not intersect or overlap). A bar above a set means set closure, i.e., the union of the set with its boundary. Thus \\\\begin{equation*} \\\\bar{\\\\Omega}={\\\\Omega} \\\\cup \\\\Gamma \\\\tag{2.2.5} \\\\end{equation*} To understand the meaning of $\\\\overline{\\\\Gamma_{g} \\\\cup \\\\Gamma_{h}}$, we must define the boundaries of $\\\\Gamma_{g}$ and $\\\\Gamma_{h}$ in $\\\\Gamma$. We shall do this with the aid of an example.', 'Let $\\\\Omega=\\\\left\\\\{x \\\\in \\\\mathbb{R}^{2} \\\\mid x^{2}+y^{2}<1\\\\right.$, i.e., the interior of the unit disc $\\\\}$. The boundary of $\\\\Omega$ is $\\\\Gamma=\\\\left\\\\{x \\\\in R^{2} \\\\mid x^{2}+y^{2}=1\\\\right.$, i.e., the unit circle $\\\\}$. Let \\\\begin{equation*} \\\\Gamma_{g}=\\\\Gamma \\\\cap\\\\left\\\\{x \\\\in \\\\mathbb{R}^{2} \\\\mid y>0\\\\right\\\\} \\\\tag{2.2.6} \\\\end{equation*} The \"boundary of $\\\\Gamma_{g}$ \" consists of the endpoints of the upper semicircle, i.e., $\\\\left\\\\{x \\\\in R^{2} \\\\mid x=-1, y=0\\\\right.$, and $\\\\left.x=+1, y=0\\\\right\\\\}$. Thus \\\\begin{equation*} \\\\overline{\\\\Gamma_{g}}=\\\\Gamma \\\\cap\\\\left\\\\{x \\\\in R^{2} \\\\mid y \\\\geq 0\\\\right\\\\} \\\\tag{2.2.7} \\\\end{equation*} Similarly, let \\\\begin{equation*} \\\\Gamma_{h}=\\\\Gamma \\\\cap\\\\left\\\\{x \\\\in \\\\mathbb{R}^{2} \\\\mid y<0\\\\right\\\\} \\\\tag{2.2.8} \\\\end{equation*} Thus \\\\begin{equation*} \\\\overline{\\\\Gamma_{h}}=\\\\Gamma \\\\cap\\\\left\\\\{x \\\\in R^{2} \\\\mid y \\\\leq 0\\\\right\\\\} \\\\tag{2.2.9} \\\\end{equation*} Clearly \\\\begin{equation*} \\\\overline{\\\\Gamma_{g} \\\\cup \\\\Gamma_{h}}=\\\\overline{\\\\Gamma_{g}} \\\\cup \\\\overline{\\\\Gamma_{h}}=\\\\bar{\\\\Gamma} \\\\tag{2.2.10} \\\\end{equation*} These sets are depicted in Fig. 2.2.2.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-03} Figure 2.2.2 We shall assume throughout that $\\\\Gamma_{g} \\\\neq \\\\varnothing$ but allow for the case $\\\\Gamma_{h}=\\\\varnothing$.\\\\\\\\ Let the indices $i, j, k, l$, run over the values $1, \\\\ldots, n_{s d}$. Differentiation is denoted by a comma (e.g., $u_{, i}=u_{, x_{i}}=\\\\partial u / \\\\partial x_{i}$ ) and repeated indices imply summation (e.g., in $\\\\mathbb{R}^{3}, u_{, i i}=u_{, 11}+u_{, 22}+u_{, 33}=\\\\partial^{2} u / \\\\partial x^{2}+\\\\partial^{2} u / \\\\partial y^{2}+\\\\partial^{2} u / \\\\partial z^{2}$ ). The summation convention only applies to the indices $i, j, k$, and $l$ and only to two repeated indices. If there are three, or more, repeated indices in an expression, then\\\\\\\\ the summation convention is not in effect. (This is in keeping with the usual convention.) Divergence theorem. Let $f: \\\\bar{\\\\Omega} \\\\rightarrow \\\\mathbb{R}$ be $C^{1}$. Then \\\\begin{equation*} \\\\int_{\\\\boldsymbol{\\\\Omega}} f_{,i} d \\\\boldsymbol{\\\\Omega}=\\\\int_{\\\\boldsymbol{\\\\Gamma}} f \\\\boldsymbol{n_{i}} d \\\\boldsymbol{\\\\Gamma} \\\\tag{2.2.11} \\\\end{equation*} The proof may be found in [1].\\\\\\\\ \\\\textbf{Integration by parts.} Let $f$ be as above and also let $g: \\\\overline{\\\\Omega} \\\\rightarrow \\\\mathbb{R}$ be $C^{\\\\mathbf{1}}$. Then \\\\begin{equation*} \\\\int_{\\\\mathbf{\\\\Omega}} f_{,i} g d \\\\mathbf{\\\\Omega}=-\\\\int_{\\\\mathbf{\\\\Omega}} f g_{,i} d \\\\mathbf{\\\\Omega}+\\\\int_{\\\\mathbf{\\\\Gamma}} f g \\\\mathbf{n_{i}} d \\\\mathbf{\\\\Gamma} \\\\tag{2.2.12} \\\\end{equation*} \\\\textit{Proof}. We integrate the identity (i.e., \"product rules of differentiation,\" see [1]) $$ (f g)_{,i}=f_{,i} g+f g_{,i} $$ to get $$ \\\\int_{\\\\mathbf{\\\\Omega}}(f g)_{,i} d \\\\mathbf{\\\\Omega}=\\\\int_{\\\\mathbf{\\\\Omega}} f_{,i} g d \\\\mathbf{\\\\Omega}+\\\\int_{\\\\mathbf{\\\\Omega}} f g_{,i} d \\\\mathbf{\\\\Omega} $$ and then use the divergence theorem to convert the left-hand side into a boundary integral. \\\\subsection*{2.3 CLASSICAL LINEAR HEAT CONDUCTION: STRONG AND WEAK FORMS; EQUIVALENCE} Let $q_{i}$ denote (Cartesian components of) the heat flux vector, let $u$ be the temperature, and let $\\\\ell$ be the heat supply per unit volume. Assume the heat flux vector is defined in terms of the temperature gradient by the generalized Fourier law\\\\footnote{The generalized fourier law is a constitutive equation, or equation of state, which reflects the heat conduction properties of the body (i.e., $\\\\Omega$ ) under consideration. }: \\\\begin{equation*} q_{i}=-\\\\kappa_{i j} u_{, j}, \\\\quad \\\\kappa_{i j}=\\\\kappa_{j i} \\\\quad \\\\text { (symmetry) } \\\\tag{2.3.1} \\\\end{equation*} where the conductivities, $\\\\kappa_{i j}$ \\'s, are given functions of $x$. (If the $\\\\kappa_{i j}$ \\'s are constant throughout $\\\\Omega$, the body is said to be homogeneous.) The conductivity matrix, $\\\\boldsymbol{\\\\kappa}=\\\\left[\\\\kappa_{i j}\\\\right]$, is assumed positive definite (see the definition in Sec. 1.9). The most common situation in practice is the isotropic case in which $\\\\kappa_{i j}(x)=\\\\kappa(x) \\\\delta_{i j}$, where $\\\\delta_{i j}$ is the Kronecker delta. A formal statement\\\\footnote{By a formal statement, we mean one in which we do not precisely delineate the spaces to which the functions involved belong.} of the strong form of the boundary-value problem is as follows: \\\\[ (S) \\\\begin{cases} \\\\begin{alignedat}{3} &\\\\text{Given } \\\\ell : \\\\Omega \\\\to \\\\mathbb{R},\\\\ g : \\\\Gamma_g \\\\to \\\\mathbb{R},\\\\ \\\\text{and } h : \\\\Gamma_h \\\\to \\\\mathbb{R},\\\\ \\\\text{find } u : \\\\overline{\\\\Omega} \\\\to \\\\mathbb{R}\\\\ \\\\text{such that} & & \\\\\\\\ & q_{i,i} = \\\\ell \\\\quad \\\\quad \\\\text{in } \\\\Omega \\\\quad \\\\text{(heat equation)} \\\\quad \\\\text{(2.3.2)} \\\\\\\\ & u = g \\\\quad \\\\quad \\\\text{on } \\\\Gamma_g \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\text{(2.3.3)}\\\\footnotemark \\\\\\\\ & - q_i n_i = h \\\\quad \\\\quad \\\\text{on } \\\\Gamma_h \\\\quad \\\\quad \\\\quad \\\\text{(2.3.4)} \\\\\\\\ & \\\\text{where $q_i$ is defined by (2.3.1).} \\\\end{alignedat} \\\\end{cases} \\\\] \\\\footnotetext{The statement $u=g$ on $\\\\Gamma_{g}$ means $u(x)=g(x)$ for all $x \\\\in \\\\Gamma_{g}$, and so on.} The functions $g$ and $h$ are the prescribed boundary temperature and heat flux, respectively. This problem possesses a unique solution for appropriate restrictions on the given data. In more mathematical terminology, (2.3.2) is a generalized Poisson equation, (2.3.3) is a Dirichlet boundary condition, and (2.3.4) is a Neumann boundary condition. We shall now construct a weak formulation of the boundary-value problem analogous to that for the one-dimensional problem of Chapter 1. In particular, (2.3.3) and (2.3.4) will be treated as essential and natural boundary conditions, respectively. As before, let $\\\\delta$ denote the trial solution space and $\\\\underline{\\\\mathcal{V}}$ the variation space. This time $\\\\delta$ and $\\\\mathcal{V}$ consist of real-valued functions defined on $\\\\bar{\\\\Omega}$ satisfying certain smoothness requirements, such that all members of $\\\\delta$ satisfy (2.3.3), whereas if $w \\\\in \\\\mathcal{V}$, then \\\\begin{equation*} w=0 \\\\quad \\\\text { on } \\\\Gamma_{g} \\\\tag{2.3.5} \\\\end{equation*} The weak formulation of the problem goes as follows: \\\\[ (W) \\\\begin{cases} \\\\begin{alignedat}{3} & \\\\text{Given } \\\\ell : \\\\Omega \\\\to \\\\mathbb{R}, g : \\\\Gamma_g \\\\to \\\\mathbb{R}, \\\\text{and } h : \\\\Gamma_h \\\\to \\\\mathbb{R}, \\\\text{find } u \\\\in \\\\delta \\\\text{ such that for all } w \\\\in \\\\mathcal{V}, \\\\\\\\ & - \\\\int_{\\\\Omega} w_{,i} q_i \\\\, d\\\\Omega = \\\\int_{\\\\Omega} w \\\\ell \\\\, d\\\\Omega + \\\\int_{\\\\Gamma_h} w h \\\\, d\\\\Gamma \\\\quad \\\\quad \\\\quad \\\\text{(2.3.6)} \\\\\\\\ & \\\\text{where \\\\(q_i\\\\) is defined by (2.3.1).} \\\\end{alignedat} \\\\end{cases} \\\\] \\\\textbf{Theorem.} Assume all functions involved are smooth enough to justify the manipulations. Then a solution of $(S)$ is a solution of $(W)$ and vice versa. \\\\textbf{Proof. 1.} Assume $u$ is the solution of ($S$). By virtue of (2.3.3), $u \\\\in \\\\delta$. Pick any $w \\\\in \\\\mathcal{V}$ and proceed as follows: \\\\[ \\\\begin{aligned} & 0 = \\\\int_{\\\\Omega} w \\\\underbrace{(q_{i,i} - \\\\ell)}_{0} \\\\, d\\\\Omega \\\\quad \\\\quad \\\\quad \\\\text{(heat equation, i.e., (2.3.2))}\\\\\\\\ & =-\\\\int_{\\\\Omega} w_{,i} q_{i} d \\\\Omega+\\\\int_{\\\\Gamma} w q_{i} n_{i} d \\\\Gamma-\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega \\\\quad \\\\text { (integration by parts) } \\\\\\\\ & =-\\\\int_{\\\\Omega} w_{,i} q_{i} d \\\\Omega-\\\\int_{\\\\Gamma_{h}} w h d \\\\Gamma-\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega \\\\quad \\\\begin{array}{l} \\\\left(w=0 \\\\text { on } \\\\Gamma_{g},\\\\right. \\\\text { and heat flux } \\\\\\\\ \\\\text { boundary condition, i.e. (2.3.4)) } \\\\end{array} \\\\end{aligned} \\\\] Therefore (2.3.6) is satisfied and so $u$ is a solution of ($W$).\\\\\\\\ 2. Assume $u$ is the solution of (W). Then $u=g$ on $\\\\Gamma_{g}$, and for all $w \\\\in \\\\mathcal{V}$ \\\\begin{align*} 0 & =\\\\int_{\\\\Omega} w_{,i} q_{i} d \\\\Omega+\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega+\\\\int_{\\\\Gamma_{h}} w h d \\\\Gamma \\\\text{(by (2.3.6))}\\\\\\\\ & =\\\\int_{\\\\Omega} w\\\\left(-q_{i, i}+\\\\ell) d \\\\Omega+\\\\int_{\\\\Gamma_{h}} w\\\\left(n_{i} q_{i}+h\\\\right) d \\\\Gamma\\\\right. \\\\tag{2.3.7} \\\\end{align*} Let $$ \\\\begin{aligned} & \\\\alpha=-q_{i, i}+\\\\ell \\\\\\\\ & \\\\beta=q_{i} n_{i}+h \\\\end{aligned} $$ To show (2.3.2) and (2.3.4) are satisfied and thus complete the proof, we must prove that $$ \\\\begin{array}{ll} \\\\alpha=0 & \\\\text { on } \\\\Omega \\\\\\\\ \\\\beta=0 & \\\\text { on } \\\\Gamma_{h} \\\\end{array} $$ First pick $w=\\\\alpha \\\\phi$ where\\\\\\\\ i. $\\\\phi>0$ on $\\\\Omega$;\\\\\\\\ ii. $\\\\phi=0$ on $\\\\Gamma$; and\\\\\\\\ iii. $\\\\phi$ is smooth.\\\\\\\\ (These conditions insure that $w \\\\in \\\\mathcal{V}$.) With this choice for $w,(2.3 .7)$ becomes $$ 0=\\\\int_{\\\\Omega} \\\\alpha^{2} \\\\phi d \\\\Omega $$ which implies $\\\\alpha=0$ on $\\\\Omega$.\\\\\\\\ Now pick $w=\\\\beta \\\\psi$, where\\\\\\\\ i\\'. $\\\\psi>0$ on $\\\\Gamma_{h}$;\\\\\\\\ ii\\'. $\\\\psi=0$ on $\\\\Gamma ;$ and\\\\\\\\ iii\\'. $\\\\psi$ is smooth.\\\\\\\\ (These conditions insure that $w \\\\in \\\\mathcal{V}$.) With this choice for $w,(2.3 .7)$ becomes\\\\\\\\ (making use of $\\\\boldsymbol{\\\\alpha}=0$ ): $$ 0=\\\\int_{\\\\Gamma_{h}} \\\\beta^{2} \\\\psi d \\\\Gamma $$ from which it follows that $\\\\beta=0$ on $\\\\Gamma_{h}$. Thus $u$ is a solution of $(S)$.\\\\\\\\ It is convenient to introduce an abstract version of (2.3.6). Let \\\\begin{gather*} a(w, u)=\\\\int_{\\\\Omega} w_{,i} \\\\kappa_{i j} u_{, j} d \\\\Omega \\\\tag{2.3.8}\\\\\\\\ (w, \\\\ell)=\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega \\\\tag{2.3.9}\\\\\\\\ (w, h)_{\\\\Gamma}=\\\\int_{\\\\Gamma_{h}} w h d \\\\Gamma \\\\tag{2.3.10} \\\\end{gather*} Then (2.3.6) may be written as \\\\begin{equation*} a(w, u)=(w, \\\\ell)+(w, h)_{\\\\Gamma} \\\\tag{2.3.11} \\\\end{equation*} \\\\subsection*{Exercise 1.} Verify that $a(\\\\cdot, \\\\cdot),(\\\\cdot, \\\\cdot)$ and $(\\\\cdot, \\\\cdot)_{r}$, as just defined, are symmetric bilinear forms. (Note that the symmetry of $a(\\\\cdot, \\\\cdot)$ follows from the symmetry of the conductivities.) In manipulating terms in theories involving vector and tensor quantities, the indicial notation used is very explicit and convenient. However, when we come to the Galerkin formulation analogous to (2.3.6), additional indices necessarily appear. The situation becomes very complicated due to the greater number of indices involved and due to the ranges of the various indices being different. When we come to elasticity theory, the situation is even worse as the corresponding terms have an even greater number of indices. For these reasons it is useful at this point to adopt an index-free notation for (2.3.6). Aside from stemming the proliferation of indices, we shall find later on that this formulation is conducive to the computer implementation of the element arrays, especially in more complicated situations such as elasticity. In introducing our index-free notation we shall assume for definiteness that $n_{s d}=2$. Let $\\\\nabla$ denote the gradient operator; thus \\\\begin{align*} & \\\\nabla u=\\\\left\\\\{u_{, i}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} u_{, 1} \\\\\\\\ u_{, 2} \\\\end{array}\\\\right\\\\} \\\\tag{2.3.12}\\\\\\\\ & \\\\nabla w=\\\\left\\\\{w_{,}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{l} w_{, 1} \\\\\\\\ w_{, 2} \\\\end{array}\\\\right\\\\} \\\\tag{2.3.13} \\\\end{align*} In the case of two space dimensions, the conductivity matrix may be written as \\\\[ \\\\kappa=\\\\left[\\\\kappa_{i j}\\\\right]=\\\\left[\\\\begin{array}{ll} \\\\kappa_{11} & \\\\kappa_{12} \\\\tag{2.3.14}\\\\\\\\ \\\\kappa_{21} & \\\\kappa_{22} \\\\end{array}\\\\right] \\\\quad \\\\text { (symmetric) } \\\\] In the isotropic case, (2.3.14) simplifies to \\\\[ \\\\kappa=\\\\kappa\\\\left[\\\\delta_{i j}\\\\right]=\\\\kappa\\\\left[\\\\begin{array}{ll} 1 & 0 \\\\tag{2.3.15}\\\\\\\\ 0 & 1 \\\\end{array}\\\\right] \\\\] In terms of the above expressions, the integrand of (2.3.8) may be written in indexfree fashion: \\\\begin{equation*} w_{, i} \\\\kappa_{i j} u_{, j}=(\\\\nabla w)^{T} \\\\kappa(\\\\nabla u) \\\\tag{2.3.16} \\\\end{equation*} Thus in place of (2.3.8) we may write \\\\begin{equation*} a(w, u)=\\\\int_{\\\\Omega}(\\\\nabla w)^{T} \\\\kappa(\\\\nabla u) d \\\\Omega \\\\tag{2.3.17} \\\\end{equation*} \\\\subsection*{Exercise 2.} Verify (2.3.16) for the cases $n_{s d}=2$ and 3. \\\\subsection*{2.4 HEAT CONDUCTION: GALERKIN FORMULATION; SYMMETRY AND POSITIVE-DEFINITENESS OF K} Let $\\\\delta^{h}$ and $\\\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\\\delta$ and $\\\\mathcal{V}$, respectively. We assume all members of $\\\\mathcal{V}^{h}$ vanish, or vanish approximately, on $\\\\Gamma_{g}$ and that each member of $\\\\delta^{h}$ admits the representation \\\\begin{equation*} u^{h}=v^{h}+g^{h} \\\\tag{2.4.1} \\\\end{equation*} where $v^{h} \\\\in \\\\mathcal{V}^{h}$ and $g^{h}$ results in satisfaction, or at least approximate satisfaction, of the boundary condition $u=g$ on $\\\\Gamma_{g}$. The Galerkin formulation is given as follows:\\\\\\\\ (G) $\\\\left\\\\{\\\\begin{array}{c}\\\\text { Given } \\\\ell, g \\\\text {, and } h\\\\left[\\\\text { as in (W)], find } u^{h}=v^{h}+y^{h} \\\\in \\\\delta^{h} \\\\text { such that for all }\\\\right. \\\\\\\\ w^{h} \\\\in \\\\mathcal{V}^{h}(\\\\mathrm{cf} . \\\\text { Sec. 1.5): } \\\\\\\\ a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, \\\\ell \\\\right)+\\\\left(w^{h}, h\\\\right)_{\\\\Gamma}-a\\\\left(w^{h}, g^{h}\\\\right) \\\\\\\\ \\\\text { (2.4.2) }\\\\end{array}\\\\right.$ We now view our domain as \"discretized\" into element domains $\\\\Omega^{\\\\text {e }}$, $1 \\\\leq e \\\\leq n_{e l}$. In two dimensions the element domains might be simply triangles and quadrilaterals; see Fig. 2.4.1. Nodal points may exist anywhere on the domain but most frequently appear at the element vertices and interelement boundaries and less often in the interiors.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-09} Figure 2.4.1\\\\\\\\ In Sec. 1.9 the global nodal ordering and ordering of equations in the matrix system coincided. In multidimensional applications this would prove to be an inconvenient restriction with regard to data preparation. In what follows, a more flexible scheme is described. Let $\\\\boldsymbol{\\\\eta}=\\\\left\\\\{1,2, \\\\ldots, n_{\\\\text {np}}\\\\right\\\\}$, the set of global node numbers where $n_{\\\\text {np}}$ is the number of nodal points. By the terminology $g$-node we shall mean a node, $A$, at which it is prescribed that $\\\\boldsymbol{u}^{\\\\boldsymbol{h}}=\\\\boldsymbol{g}$. Let $\\\\eta_{g} \\\\subset \\\\eta$ be the set of \" $g$-nodes.\" The complement of $\\\\eta_{g}$ in $\\\\eta$, denoted by $\\\\eta-\\\\eta_{g}$, is the set of nodes at which $\\\\boldsymbol{u}^{\\\\boldsymbol{h}}$ is to be determined. The number of nodes in $\\\\boldsymbol{\\\\eta}-\\\\boldsymbol{\\\\eta}_{\\\\boldsymbol{g}}$ equals $n_{\\\\text {eq }}$, the number of equations. A typical member of $\\\\mathcal{V}^{h}$ is assumed to have the form \\\\begin{equation*} w^{h}(x)=\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g}} N_{A}(x) c_{A} \\\\tag{2.4.3} \\\\end{equation*} where $N_{A}$ is the shape function associated with node number $A$ and $c_{A}$ is a constant. We assume throughout that $w^{h}=0$ if and only if $c_{A}=0$ for each $A \\\\in \\\\eta-\\\\eta_{g}$. Likewise \\\\begin{equation*} v^{h}(x)=\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g}} N_{A}(x) d_{A} \\\\tag{2.4.4} \\\\end{equation*} where $d_{A}$ is the unknown at node $A$ (i.e., temperature) and \\\\begin{equation*} g^{h}(x)=\\\\sum_{A \\\\in \\\\eta_{g}} N_{A}(x) g_{A}, \\\\quad g_{A}=g\\\\left(x_{A}\\\\right) \\\\tag{2.4.5} \\\\end{equation*} From (2.4.5), we see that $g^{h}$ has been defined to be the nodal interpolate of $g$ by way of the shape functions. \\\\footnote{This is not the only possibility, nor the best from the standpoint of accuracy. However, in practice it is generally the most convenient. } Consequently, $g^{h}$ will be, generally, only an approximation of $g$. See Fig. 2.4.2. Additional sources of error are (1) the use of approximations $\\\\ell^{h}$ and $h^{h}$ in place of $\\\\ell$ and $h$, respectively; and (2) domain approximations in which the element boundaries do not exactly coincide with $\\\\Gamma$. Analyses of these approximations are presented in Strang and Fix [2].\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-10} Figure 2.4.2 Piecewise linear approximation of boundary data (schematic).\\\\\\\\ Substituting (2.4.3)-(2.4.5) into (2.4.2) and arguing as in Sec. 1.6, results in \\\\[ \\\\begin{array}{r} \\\\sum_{B \\\\in \\\\eta-\\\\eta_{g}} a\\\\left(N_{A}, N_{B}\\\\right) d_{B}=\\\\left(N_{A}, \\\\ell\\\\right)+\\\\left(N_{A}, h\\\\right)_{\\\\Gamma}-\\\\sum_{B \\\\in \\\\eta_{g}} a\\\\left(N_{A}, N_{B}\\\\right) g_{B} \\\\tag{2.4.6}\\\\\\\\ A \\\\in \\\\eta-\\\\eta_{g} \\\\end{array} \\\\] To define the global stiffness matrix and force vector, we need to first specify the global ordering of equations. For this purpose we introduce the ID array, sometimes called the destination array, which assigns to node $A$ the corresponding global equation number, viz., \\\\[ \\\\text{ID}(A) = \\\\begin{cases} \\\\overbrace{P}^{\\\\text{Global equation number}} & \\\\text{if } A \\\\in \\\\eta - \\\\eta_{g}, \\\\\\\\ 0 & \\\\text{if } A \\\\in \\\\eta_{g} \\\\end{cases} \\\\] where $1 \\\\leq P \\\\leq n_{e q}$. The dimension of ID is $n_{n p}$. As may be seen from (2.4.7), nodes at which $g$ is prescribed are assigned \"equation number\" zero. An example of the setup of ID and other important data-processing arrays is presented in Sec. 2.6. The matrix equivalent of (2.4.6) is given as follows: \\\\begin{gather*} K d=F \\\\tag{2.4.8}\\\\\\\\ \\\\boldsymbol{K}=\\\\left[K_{P Q}\\\\right], \\\\quad d=\\\\left\\\\{d_{Q}\\\\right\\\\}, \\\\quad F=\\\\left\\\\{F_{P}\\\\right\\\\}, \\\\quad 1 \\\\leq P, Q \\\\leq n_{e q} \\\\tag{2.4.9}\\\\\\\\ K_{P Q}=a\\\\left(N_{A}, N_{B}\\\\right), \\\\quad P=\\\\operatorname{ID}(A), \\\\quad Q=\\\\operatorname{ID}(B) \\\\tag{2.4.10}\\\\\\\\ F_{P}=\\\\left(N_{A}, \\\\ell\\\\right)+\\\\left(N_{A}, h\\\\right)_{\\\\Gamma}-\\\\sum_{B \\\\in \\\\eta_{g}} a\\\\left(N_{A}, N_{B}\\\\right) g_{B} \\\\tag{2.4.11} \\\\end{gather*} The main properties of $\\\\boldsymbol{K}$ are established in the following theorem. \\\\\\\\ \\\\\\\\ \\\\textbf{Theorem} \\\\begin{enumerate} \\\\item $K$ is symmetric. \\\\item $K$ is positive definite. \\\\end{enumerate} \\\\textbf{Proof} \\\\begin{enumerate} \\\\item The symmetry of $\\\\boldsymbol{K}$ follows directly from the symmetry of $a(\\\\cdot, \\\\cdot)$, viz., \\\\end{enumerate} $$ \\\\begin{aligned} K_{P Q} & =a\\\\left(N_{A}, N_{B}\\\\right) & & \\\\text { (by definition) } \\\\\\\\ & =a\\\\left(N_{B}, N_{A}\\\\right) & & \\\\text { (symmetry of } a(\\\\cdot, \\\\cdot)) \\\\\\\\ & =K_{Q P} & & \\\\text { (by definition) } \\\\end{aligned} $$ \\\\begin{enumerate} \\\\setcounter{enumi}{1} \\\\item (Recall that we must show (i) $c^{T} K c \\\\geq 0$ and (ii) $c^{T} K c=0$ implies $c=0$.) \\\\end{enumerate} To each $n_{e q}$-vector $c=\\\\left\\\\{c_{p}\\\\right\\\\}$, we may associate a member $w^{h} \\\\in \\\\mathcal{V}^{h}$ by the expression $\\\\boldsymbol{w}^{h}=\\\\Sigma_{A \\\\in \\\\eta-\\\\eta_{g}} N_{A} \\\\bar{c}_{A}$, where $\\\\bar{c}_{A}=c_{P}, P=\\\\operatorname{ID}(A)$.\\\\\\\\ i. $$ \\\\begin{array}{rlrl} \\\\boldsymbol{c}^{T} K c & =\\\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{PQ} c_{Q} & \\\\\\\\ & =\\\\sum_{A, B \\\\in \\\\eta - \\\\eta_g} \\\\bar{c}_{A} a\\\\left(N_{A}, N_{B}\\\\right) \\\\bar{c}_{B} & \\\\\\\\ & =a\\\\left(\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g}} N_{A} \\\\bar{c}_{A}, \\\\sum_{B \\\\in \\\\eta-\\\\eta_{g}} N_{B} \\\\bar{c}_{B}\\\\right) & & \\\\text { (bilinearity of } a(\\\\cdot, \\\\cdot)) \\\\\\\\ & =a\\\\left(w^{h}, w^{h}\\\\right) & & \\\\text { (definition of } \\\\left.w^{h}\\\\right) \\\\\\\\ & =\\\\int_{\\\\Omega} \\\\underbrace{w_{,i}^{h} k_{ij} w_{,j}^{h}}_{\\\\geq 0} d \\\\Omega & & \\\\text { (positive-definiteness of conductivities) } \\\\\\\\ & \\\\geq 0 & & \\\\end{array} $$ ii. Assume $\\\\boldsymbol{c}^{\\\\boldsymbol{T}} \\\\boldsymbol{K c}=\\\\mathbf{0}$. By the proof of part (i), $$ \\\\int_{\\\\Omega} \\\\underbrace{w_{, i}^{h} \\\\kappa_{i j} w_{, j}^{h}}_{\\\\geq 0} d \\\\Omega=0 $$ and thus it follows that $$ w_{, i}^{h} \\\\kappa_{i j} w_{, j}^{h}=0 $$ By the positive-definiteness hypothesis on the conductivities, this requires $w_{, i}^{h}=0$ and so $w^{h}$ must be constant. However $w^{h}=0$ on $\\\\Gamma_{g}$ (which is not empty) and so $w^{h}$ must be zero throughout $\\\\Omega$. By the definition of', '$\\\\boldsymbol{w}^{\\\\boldsymbol{h}}$, it follows that each $c_{P}=0$; that is $c=0$, which was to be proved. \\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\\\\\ \\\\textbf{Remarks} \\\\begin{enumerate} \\\\item Observe that it is the positive-definiteness hypothesis on the constitutive coefficients (i.e., $\\\\kappa_{ij}$ \\'s) and the boundary condition incorporated in the definition of $\\\\mathcal{V}^{\\\\boldsymbol{h}}$ which together result in the positive-definiteness of $\\\\boldsymbol{K}$ and thus ensure its invertibility. \\\\item The explicit structure of the shape functions, which will be delineated in Chapter 3, will also result in $\\\\boldsymbol{K}$ being banded. \\\\end{enumerate} \\\\subsection*{Exercise 1.} (This exercise is a multidimensional analog of the one contained in Sec. 1.8.) Let $$ \\\\Gamma_{\\\\text{int }}=\\\\left(\\\\bigcup_{e=1}^{n_{el}} \\\\Gamma^{e}\\\\right)-\\\\Gamma \\\\quad \\\\text { (interior element boundaries) } $$ One side of $\\\\Gamma_{\\\\text{int}}$ is (arbitrarily) designated to be the \" + side\" and the other is the \" - side.\" Let $\\\\boldsymbol{n}^{+}$and $\\\\boldsymbol{n}^{-}$be unit normals to $\\\\Gamma_{\\\\text {int }}$ which point in the minus and plus directions, respectively. Clearly $n^{+}=-n^{-}$. Let $q_{i}^{+}$and $q_{i}^{-}$denote the values of $q_{i}$ obtained by approaching $x \\\\in \\\\Gamma_{\\\\mathrm{int}}$ from + and - sides, respectively. The \"jump\" in $q_{n}=q_{i} n_{i}$ at $x$ is defined to be $$ \\\\begin{aligned} {\\\\left[q_{n}\\\\right] } & =\\\\left(q_{i}^{+}-q_{i}^{-}\\\\right) n_{i}^{+} \\\\\\\\ & =q_{i}^{+} n_{i}^{+}+q_{i}^{-} n_{i}^{-} \\\\end{aligned} $$ As may be easily verified, the jump is invariant with respect to reversing the + and - designations. Consider the weak formulation (i.e., (2.3.6)) and assume $w$ and $u$ are smooth on the element interiors but may experience discontinuities in gradient across element boundaries. (Functions of this type contain the standard $C^{0}$ finite element interpolations; see Chapter 3.) Show that $$ 0=\\\\sum_{e=1}^{n_{e l}} \\\\int_{\\\\Omega^{e}} w\\\\left(q_{i, i}-\\\\ell\\\\right) d \\\\Omega-\\\\int_{\\\\Gamma_{h}} w\\\\left(q_{n}+h\\\\right) d \\\\Gamma-\\\\int_{\\\\Gamma_{int}} w\\\\left[q_{n}\\\\right] d \\\\Gamma $$ from which the Euler-Lagrange conditions may be readily deduced:\\\\\\\\ i. $q_{i, i}= \\\\ell$ in $\\\\bigcup_{e=1}^{n_{el}} \\\\Omega^{e}$\\\\\\\\ ii. $-q_{n}=h$ on $\\\\Gamma_{h}$\\\\\\\\ iii. $\\\\left[q_{n}\\\\right]=0$ on $\\\\Gamma_{\\\\text {int }}$ As may be seen, (i) is the heat equation on the element interiors and (iii) is a continuity condition across element boundaries on the heat flux. Contrast the present results with those obtained assuming $w$ and $u$ are globally smooth. The Galerkin finite element formulation obtains an approximate solution to (i) through (iii). \\\\subsection*{2.5 HEAT CONDUCTION: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As before, we can break up the global arrays into sums of elemental contributions: \\\\[ \\\\begin{array}{ll} \\\\boldsymbol{K}=\\\\sum_{e=1}^{n_{el}} \\\\boldsymbol{K}^{e}, & \\\\boldsymbol{K}^{e}=\\\\left[K_{P Q}^{e}\\\\right] \\\\\\\\ \\\\boldsymbol{F}=\\\\sum_{e=1}^{n_{el}} \\\\boldsymbol{F}^{e}, & \\\\boldsymbol{F}^{e}=\\\\left\\\\{F_{P}^{e}\\\\right\\\\} \\\\tag{2.5.2} \\\\end{array} \\\\] where \\\\begin{align*} & K_{P Q}^{e}=a\\\\left(N_{A}, N_{B}\\\\right)^{e}=\\\\int_{\\\\Omega^{e}}\\\\left(\\\\nabla N_{A}\\\\right)^{T} \\\\boldsymbol{k}\\\\left(\\\\nabla N_{B}\\\\right) d \\\\Omega \\\\tag{2.5.3}\\\\\\\\ & F_{P}^{e}=\\\\left(N_{A}, \\\\ell\\\\right)^{e}+\\\\left(N_{A}, h\\\\right)_{\\\\Gamma}^e-\\\\sum_{B \\\\in \\\\eta_{g}} a\\\\left(N_{A}, N_{B}\\\\right)^{e} g_{B} \\\\\\\\ & =\\\\int_{\\\\Omega^{e}} N_{A} \\\\ell d \\\\Omega+\\\\int_{\\\\Gamma_{h}^{e}} N_{A} h d \\\\Gamma-\\\\sum_{B \\\\in \\\\eta_{g}} a\\\\left(N_{A}, N_{B}\\\\right)^{e} g_{B} \\\\tag{2.5.4}\\\\\\\\ & \\\\Gamma_{h}^{e}=\\\\Gamma_{h} \\\\cap \\\\Gamma^{e}, \\\\quad P=\\\\operatorname{ID}(A), \\\\quad Q=\\\\operatorname{ID}(B) \\\\tag{2.5.5} \\\\end{align*} See Fig. 2.5.1 for an illustration of $\\\\Gamma_{h}^{e}$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-13} The element stiffness, $\\\\boldsymbol{k}^{e}$, and element force vector, $\\\\boldsymbol{f}^{\\\\boldsymbol{e}}$, may be deduced from these equations: \\\\begin{align*} & k^{e}=\\\\left[k_{a b}^{e}\\\\right], \\\\quad f^{e}=\\\\left\\\\{f_{a}^{e}\\\\right\\\\}, \\\\quad 1 \\\\leq a, b \\\\leq n_{e n} \\\\tag{2.5.6}\\\\\\\\ & k_{a b}^{e}=a\\\\left(N_{a}, N_{b}\\\\right)^{e}=\\\\int_{\\\\Omega^{e}}\\\\left(\\\\nabla N_{a}\\\\right)^{T} \\\\kappa\\\\left(\\\\nabla N_{b}\\\\right) d \\\\Omega \\\\tag{2.5.7}\\\\\\\\ & f_{a}^{e}=\\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} N_{a} \\\\ell d \\\\Omega+\\\\int_{\\\\Gamma_{h}^{e}} N_{a} h d \\\\Gamma-\\\\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \\\\tag{2.5.8} \\\\end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\\\\left(x_{b}^{e}\\\\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\\\\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all\\' $x \\\\in \\\\bar{\\\\Omega}^{\\\\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$ may be formed from the element arrays $\\\\boldsymbol{k}^{\\\\boldsymbol{e}}$ and $\\\\boldsymbol{f}^{\\\\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \\\\begin{equation*} k^{e}=\\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} B^{T} D B d \\\\Omega \\\\tag{2.5.9} \\\\end{equation*} where, in the present case, \\\\begin{align*} & \\\\underbrace{\\\\boldsymbol{D}}_{\\\\boldsymbol{n}_{s d} \\\\times \\\\boldsymbol{n}_{s d}}=\\\\boldsymbol{\\\\kappa} \\\\tag{2.5.10}\\\\\\\\ & \\\\underbrace{\\\\boldsymbol{B}}_{\\\\boldsymbol{n}_{s d} \\\\times \\\\boldsymbol{n}_{en}}=\\\\left[\\\\boldsymbol{B}_{1}, \\\\boldsymbol{B}_{2}, \\\\ldots, \\\\boldsymbol{B}_{n_{e n}}\\\\right] \\\\tag{2.5.11}\\\\\\\\ & \\\\underbrace{B_{a}}_{n_{s d} \\\\times 1}=\\\\nabla N_{a} \\\\tag{2.5.12} \\\\end{align*} The component version of $(2.5 .9)$ is \\\\begin{equation*} k_{a b}^{e}=\\\\int_{\\\\Omega^{e}} B_{a}^{T} D B_{b} d \\\\Omega \\\\tag{2.5.13} \\\\end{equation*} \\\\subsection*{Exercise 1.} Let\\\\\\\\ \\\\[ \\\\underbrace{d^{e}}_{n_{e n} \\\\times 1}=\\\\left\\\\{d_{a}^{e}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} d_{1}^{e} \\\\tag{2.5.14}\\\\\\\\ d_{2}^{e} \\\\\\\\ \\\\vdots \\\\\\\\ d_{n_{e n}^{e}}^{e} \\\\end{array}\\\\right\\\\} \\\\] where \\\\begin{equation*} d_{a}^{e}=u^{h}\\\\left(x_{a}^{e}\\\\right) \\\\tag{7} \\\\end{equation*} $d^{e}$ is called the element temperature vector. Show that the heat flux vector at point $\\\\boldsymbol{x} \\\\in \\\\boldsymbol{\\\\Omega}^{\\\\boldsymbol{e}}$ can be calculated from the formula \\\\begin{equation*} q(x)=-D(x) B(x) d^{e}=-D(x) \\\\sum_{a=1}^{n_{en}} B_{a} d_{a}^{e} \\\\tag{2.5.16} \\\\end{equation*} \\\\subsection*{Exercise 2.} Consider the strong statement of the boundary-value problem in classical linear heat conduction in which the $h$-type boundary condition (i.e., eq. (2.3.4)) is replaced by the following expression: \\\\begin{equation*} \\\\lambda u-q_{i} n_{i}=h \\\\quad \\\\text { on } \\\\Gamma_{h} \\\\tag{2.5.17}\\\\footnotemark \\\\end{equation*} \\\\footnotetext{The $\\\\boldsymbol{g}$-boundary conditions are accounted for in this definition.} where $\\\\lambda \\\\geq 0$ is a given function of $x \\\\in \\\\Gamma_{h}$. Generalize the weak formulation to include (2.5.17) as a natural boundary condition. Obtain an expression for the additional contribution to $k_{a b}^{e}$ (cf. (2.5.13)) arising from (2.5.17). Show that $\\\\boldsymbol{K}$ is positive-definite. The boundary condition (2.5.17) is equivalent to what is often called Newton\\'s law of heat transfer; $\\\\lambda$ is called the coefficient of heat transfer. This boundary condition applies to the case in which the heat flux is proportional to the difference of the surface temperatures of the body and surrounding medium, the latter formally represented by $h / \\\\lambda$ in (2.5.17). \\\\subsection*{2.6 HEAT CONDUCTION: DATA PROCESSING ARRAYS; \\\\\\\\ ID, IEN, AND LM} The element nodal data is stored in the array IEN, the element nodes array, which relates local node numbers to global node numbers, viz.,\\\\\\\\ \\\\[ \\\\text{IEN}(\\\\underbrace{a}_{\\\\text{Local node number}}, \\\\underbrace{e}_{\\\\text{Element number}}) = \\\\underbrace{A}_{\\\\text{Global node number}} \\\\] The relationship between global node numbers and global equation numbers as well as nodal boundary condition information is stored in the ID array (see 2.4.7). In practice, the IEN and ID arrays are set up from input data. The LM array, which was described in the context of the one-dimensional model problem in Sec. 1.14, may then be constructed from the relation\\\\\\\\ \\\\begin{equation*} \\\\operatorname{LM}(a, e)=\\\\operatorname{ID}(\\\\operatorname{IEN}(a, e)) \\\\tag{2.6.2} \\\\end{equation*} Because of the previous relationship, we often think of LM as the element \"localization\" of ID. Strictly speaking the LM array is redundant. However, it is generally convenient in computing to set up LM once and for all, rather than make use of (2.6.2) repeatedly. Example 1 illustrates the structure of the ID, IEN, and LM arrays.', 'Consider the mesh of four-node, rectangular elements shown in Fig. 2.6.1. We assume that the local node numbering begins at the lower left-hand node of each element and proceeds in counterclockwise fashion. This is illustrated in Fig. 2.6.1 for element 2, which is typical. We also assume that essential boundary conditions (i.e., \" $g$-type\") are specified at nodes $1,4,7$, and 10 . Thus there will only be eight equations in the global system $\\\\boldsymbol{K} d=\\\\boldsymbol{F}$. We adopt the usual convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. The ID, IEN, and LM arrays are given in Fig. 2.6.2. The reader is urged to verify the details.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-16} Figure 2.6.1 Mesh of four-node, rectangular elements; global and local node numbers, element numbers, and essential boundary condition nodes. ID array:\\\\\\\\ \\\\begin{center} \\\\overbrace{ \\\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\\\hline 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\\\\\\\ \\\\hline $0 *$ & 1 & 2 & 0 & 3 & 4 & 0 & 5 & 6 & 0 & 7 & 8 \\\\\\\\ \\\\hline \\\\end{tabular}}^{\\\\text{Global node numbers ($\\\\boldsymbol{A}$)} } \\\\end{center}$\\\\quad\\\\left(n_{n p}=12\\\\right)$ \\\\vfill \\\\textbf{IEN array:} \\\\[ \\\\begin{array}{c|c c c c c c} & \\\\multicolumn{6}{c}{\\\\text{Element numbers} \\\\ (e)} \\\\\\\\ \\\\text{Local node numbers} \\\\ (a) & 1 & 2 & 3 & 4 & 5 & 6 \\\\\\\\ \\\\hline 1 & 1 & 2 & 4 & 5 & 7 & 8 \\\\\\\\ 2 & 2 & 3 & 5 & 6 & 8 & 9 \\\\\\\\ 3 & 5 & 6 & 8 & 9 & 11 & 12 \\\\\\\\ 4 & 4 & 5 & 7 & 8 & 10 & 11 \\\\\\\\ \\\\end{array} \\\\] \\\\[ A = \\\\text{IEN}(a, e) \\\\] \\\\[ (n_{en} = 4) \\\\quad (n_{el} = 6) \\\\] \\\\vspace{1cm} \\\\textbf{LM array:} \\\\[ \\\\begin{array}{c|c c c c c c} & \\\\multicolumn{6}{c}{\\\\text{Element numbers} \\\\ (e)} \\\\\\\\ \\\\text{Local node numbers} \\\\ (a) & 1 & 2 & 3 & 4 & 5 & 6 \\\\\\\\ \\\\hline 1 & 0^* & 1 & 0 & 3 & 0 & 5 \\\\\\\\ 2 & 1 & 2 & 3 & 4 & 5 & 6 \\\\\\\\ 3 & 3 & 4 & 5 & 6 & 7 & 8 \\\\\\\\ 4 & 0 & 3 & 0 & 5 & 0 & 7 \\\\\\\\ \\\\end{array} \\\\] \\\\[ P = \\\\text{LM}(a, e) = \\\\text{ID}(\\\\text{IEN}(a, e)) \\\\] \\\\[ (n_{en} = 4) \\\\quad (n_{el} = 6) \\\\] \\\\footnotetext{\"Temperature boundary conditions (\"$g$-type\") denoted by zeros. } Figure 2.6.2. DD, IEN, and LM arrays for the mesh of Fig. 2.6.1. In terms of the IEN and LM arrays, a precise definition of the $g_{a}^{e}$ \\'s may be given (see (2.5.8)): \\\\[ g_{a}^{e}=\\\\left\\\\{\\\\begin{array}{ll} 0 & \\\\text { if } \\\\operatorname{LM}(a, e) \\\\neq 0 \\\\tag{2.6.3}\\\\\\\\ g_{A} & \\\\text { if } \\\\operatorname{LM}(a, e)=0, \\\\end{array} \\\\text { where } A=\\\\operatorname{IEN}(a, e)\\\\right. \\\\] This definition may be easily programmed.\\\\\\\\ In our final example of this section we shall illustrate the assembly procedure for a typical element subjected to essential boundary conditions.', \"Consider a typical, four-noded element $e$. Assume values of the LM array, for this element, are given as follows: \\\\[ \\\\left.\\\\begin{array}{l} \\\\mathrm{LM}(1, e)=5 \\\\\\\\ \\\\mathrm{LM}(2, e)=0 \\\\tag{2.6.4}\\\\\\\\ \\\\mathrm{LM}(3, e)=0 \\\\\\\\ \\\\mathrm{LM}(4, e)=9 \\\\end{array}\\\\right\\\\} \\\\] We deduce from (2.6.4) that the contributions to the global arrays are given as follows:\\\\footnote{Due to symmetry, $k_{41}^{e}$ is not actually assembled in practice.} \\\\[ \\\\left.\\\\begin{array}{c} K_{55} \\\\leftarrow K_{55}+k_{11}^{e} \\\\\\\\ K_{59} \\\\leftarrow K_{99}+k_{14}^{e} \\\\tag{2.6.5}\\\\\\\\ K_{95} \\\\leftarrow K_{95}+k_{41}^{e} \\\\\\\\ K_{99} \\\\leftarrow K_{99}+k_{44}^{e} \\\\end{array}\\\\right\\\\} \\\\] \\\\[ \\\\left.\\\\begin{array}{c} F_{5} \\\\leftarrow F_{5}+f_{1}^{e} \\\\\\\\ \\\\tag{2.6.6}\\\\\\\\ F_{9} \\\\leftarrow F_{9}+f_{4}^{e} \\\\\\\\ \\\\end{array}\\\\right\\\\} \\\\] Note that all terms in the second and third rows and columns of $\\\\boldsymbol{k}^{\\\\boldsymbol{e}}$ do not contribute to $K$. However, they may contribute to $F$ via $f_{1}^{f}$ and $f_{4}^{f}$, since, by (2.5.8) we have \\\\begin{align*} & f_{1}^{e}=\\\\cdots-k_{12}^{e} g_{2}^{e}-k_{13}^{e} g_{3}^{e} \\\\tag{2.6.7}\\\\\\\\ & f_{4}^{e}=\\\\cdots-k_{42}^{e} g_{2}^{e}-k_{43}^{e} g_{3}^{e} \\\\tag{2.6.8} \\\\end{align*} in which, for clarity, we have omitted the first two terms of the right-hand side of (2.5.8).\\\\\\\\ Special subroutines are easily programmed to carry out the operations indicated in (2.6.5)-(2.6.8). It is instructive to visualize the contributions of the \\\\textit{e}th element to the global stiffness and force. These contributions are depicted in Fig. 2.6.3. We note that all necessary element assembly information is provided by the LM array. Sec. 2.7 Classical Linear Elastostatics: Strong and Weak Forms\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-19(1)} Figure 2.6.3 Contributions of heat conduction element in Example 2 to global arrays. \\\\subsection*{Exercise 1.} Consider the accompanying mesh. Set up the ID, IEN, and LM arrays.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-19} \\\\subsection*{2.7 CLASSICAL LINEAR ELASTOSTATICS: \\\\\\\\ STRONG AND WEAK FORMS; EQUIVALENCE} Classical elastostatics is a rich subject in its own right. See [3-7] for background and references to the literature. These references range from the very physical to the very\\\\\\\\ mathematical. The most physical book is the one by Timoshenko and Goodier. In ascending order of mathematical content are Sokolnikoff, Gurtin, Duvaut-Lions, and Fichera. The reader is reminded that indices $i, j, k$, and $l$ take on values $1, \\\\ldots, n_{\\\\text {sd }}$, where $n_{s d}$ is the number of spatial dimensions, and the summation convention applies to repeated indices $i, j, k$, and $l$ only. Let $\\\\sigma_{i j}$ denote (Cartesian components of) the (Cauchy) stress tensor, let $u_{i}$ denote the displacement vector, and let $\\\\ell_{i}$ be the prescribed body force per unit volume. The (infinitesimal) strain tensor, $\\\\epsilon_{i j}$, is defined to be the symmetric part of the displacement gradients, viz., \\\\begin{equation*} \\\\epsilon_{i j}=u_{(i, j)} \\\\stackrel{\\\\text { def. }}{=} \\\\frac{u_{i, j}+u_{j, i}}{2} \\\\quad \\\\text { (strain-displacement equations) } \\\\tag{2.7.1} \\\\end{equation*} The stress tensor is defined in terms of the strain tensor by the generalized Hooke's law: \\\\footnote{This is another constitutive equation, which reflects the elastic properties of the body under consideration.} \\\\begin{equation*} \\\\sigma_{i j}=c_{i j k l} \\\\epsilon_{k l} \\\\tag{2.7.2} \\\\end{equation*} where the $c_{i j k l}$ 's, the elastic coefficients, are given functions of $x$. (If the $c_{i j k l}$ 's are constants throughout, the body is called homogeneous.) The elastic coefficients are assumed to satisfy the following properties: Symmetry \\\\begin{equation*} c_{i j k l}=c_{k l i j} \\\\quad \\\\text { (major symmetry) } \\\\tag{2.7.3} \\\\end{equation*} \\\\[ \\\\left. \\\\begin{aligned} c_{ijkl} &= c_{jikl} \\\\\\\\ c_{ijkl} &= c_{ijlk} \\\\end{aligned} \\\\right\\\\} \\\\quad \\\\quad \\\\text{(minor symmetries)} \\\\]\", '\\\\begin{align*} & c_{i j k l}(x) \\\\psi_{i j} \\\\psi_{k l} \\\\geq 0 \\\\tag{2.7.5}\\\\\\\\ & c_{i j k l}(x) \\\\psi_{i j} \\\\psi_{k l}=0 \\\\text { implies } \\\\psi_{i j}=0 \\\\tag{2.7.6} \\\\end{align*} for all $\\\\boldsymbol{x} \\\\in \\\\overline{\\\\boldsymbol{\\\\Omega}}$ and all $\\\\psi_{i j}=\\\\psi_{j i}$.\\\\\\\\ Note. The positive-definiteness condition is in terms of symmetric arrays, $\\\\psi_{i j}$.\\\\\\\\ We shall see in Sec. 2.8 that a consequence of the major symmetry (2.7.3) is that $K$ is symmetric. The first minor symmetry implies the symmetry of the stress tensor (i.e., $\\\\sigma_{i j}=\\\\sigma_{j i}$ ).\\\\footnote{From a fundamental continuum mechanics standpoint, the symmetry of the Cauchy stress tensor emanates from the balance of angular momentum. } The positive-definiteness condition, when combined with appropriate boundary conditions on the displacement, leads to the positive definiteness of $\\\\boldsymbol{K}$. In the present theory, the unknown is a vector (i.e., the displacement vector). Consequently, a generalization of the boundary conditions considered previously is necessitated. We shall assume that $\\\\Gamma$ admits decompositions \\\\[ \\\\left.\\\\begin{array}{l} \\\\Gamma=\\\\overline{\\\\Gamma_{g_{i}} \\\\cup \\\\Gamma_{h_{i}}} \\\\tag{2.7.7}\\\\\\\\ \\\\varnothing=\\\\Gamma_{g_{i}} \\\\cap \\\\Gamma_{h_{i}} \\\\end{array}\\\\right\\\\} \\\\quad i=1, \\\\ldots, n_{s d} \\\\] For example, in two dimensions the situation might appear as in Fig. 2.7.1. As can be seen there can be a different decomposition for each $i=1,2, \\\\ldots, n_{s d}$. \\\\includegraphics[max width=\\\\textwidth, center]{images/chapter2_2.7.1.png} A formal statement of the strong form of the boundary-value problem goes as follows:\\\\\\\\ \\\\[ \\\\text{(S)} \\\\begin{cases} \\\\text{Given } \\\\ell_i : \\\\Omega \\\\to \\\\mathbb{R}, \\\\, g_i : \\\\Gamma_{g_i} \\\\to \\\\mathbb{R}, \\\\, \\\\text{and } h_i : \\\\Gamma_{h_i} \\\\to \\\\mathbb{R}, \\\\, \\\\text{find } u_i : \\\\overline{\\\\Omega} \\\\to \\\\mathbb{R} \\\\text{ such that}\\\\\\\\ \\\\sigma_{ij,j} + \\\\ell_i = 0 \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\text{in } \\\\Omega \\\\quad \\\\text{(equilibrium equations)} \\\\quad (2.7.8) \\\\\\\\ u_i = g_i \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\text{on } \\\\Gamma_{g_i} \\\\quad (2.7.9) \\\\\\\\ \\\\sigma_{ij} n_j = h_i \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\quad \\\\text{on } \\\\Gamma_{h_i} \\\\quad (2.7.10) \\\\\\\\ \\\\text{where } \\\\sigma_{ij} \\\\text{ is defined in terms of } u_i \\\\text{ by (2.7.1) and (2.7.2)}. \\\\end{cases} \\\\]', '\\\\begin{enumerate} \\\\item The functions $g_{i}$ and $h_{i}$ are called the prescribed boundary displacements and tractions, respectively. \\\\item (S) is sometimes referred to as the mixed boundary-value problem of linear elastostatics. Under appropriate hypotheses on the data, $(S)$ possesses a unique solution (see Fichera [4]). \\\\item The additional complexity of the present theory, when compared with heat conduction, is that the unknown (i.e., $u=\\\\left\\\\{u_{i}\\\\right\\\\}$ ) is vector-valued rather than scalarvalued. \\\\item In practice, it is important to be able to deal with somewhat more complicated boundary-condition specification. In order not to encumber the present exposition, this generalization will be considered later on in the form of an exercise (see Exercise 5 in Sec. 2.12). \\\\end{enumerate} Let $\\\\delta_{i}$ denote the trial solution space and $\\\\mathcal{V}_{i}$ the variation space. Each member $u_{i} \\\\in \\\\delta_{i}$ satisfies $u_{i}=g_{i}$ on $\\\\Gamma_{g_i}$, whereas each $w_{i} \\\\in \\\\mathcal{V}_{i}$ satisfies $w_{i}=0$ on $\\\\Gamma_{g_{i}}$. Equation (2.7.10) will be a natural boundary condition. The weak formulation goes as follows:\\\\\\\\ Given $\\\\ell_{i}: \\\\Omega \\\\rightarrow \\\\mathbb{R}, g_{i}: \\\\Gamma_{g_{i}} \\\\rightarrow \\\\mathbb{R}$ and $h_{i}: \\\\Gamma_{h_{i}} \\\\rightarrow \\\\mathbb{R}$, find $u_{i} \\\\in \\\\delta_{i}$ such that for all $w_{i} \\\\in \\\\mathcal{V}_{i}$ \\\\begin{equation*} \\\\int_{\\\\Omega} w_{(i, j)} \\\\sigma_{i j} d \\\\Omega=\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega+\\\\sum_{i=1}^{n_{sd}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right) \\\\tag{2.7.11} \\\\end{equation*} where $\\\\sigma_{i j}$ is defined in terms of $u_{i}$ by (2.7.1) and (2.7.2).', '\\\\begin{enumerate} \\\\setcounter{enumi}{4} \\\\item In the solid mechanics literature, $(W)$ is sometimes referred to as the principle of virtual work, or principle of virtual displacements, $w_{i}$ being the virtual displacements. \\\\item The existence and uniqueness of weak solutions is discussed in Duvaut-Lions [3]. \\\\end{enumerate} Note. The boundary integral in (2.7.11) takes on the explicit form: \\\\begin{equation*} \\\\sum_{i=1}^{n_{s d}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right)=\\\\int_{\\\\Gamma_{h_{1}}} w_{1} h_{1} d \\\\Gamma+\\\\cdots+\\\\int_{\\\\Gamma_{h_{n_{s d}}}} w_{n_{s d}} h_{n_{s d}} d \\\\Gamma \\\\tag{2.7.12} \\\\end{equation*} Theorem. Assume all functions are smooth enough to justify the manipulations. Then a solution of $(S)$ is a solution of $(W)$, and vice versa.', '\\\\begin{enumerate} \\\\setcounter{enumi}{6} \\\\item The proof of the equivalence theorem requires some preliminary results, which we shall establish in the following lemmas. \\\\end{enumerate} Lemma 1. (Euclidean decomposition of a second-rank tensor.) Let $s_{i j}$ denote a general nonsymmetric, second-rank tensor. Then $s_{i j}=s_{(ij)}+s_{[ij]}$, where $s_{(ij)}$ is symmetric (i.e., $s_{(i j)}=s_{(j i)}$ ) and $s_{[i j]}$ is skew-symmetric (i.e., $s_{[i j}=-s_{[ji]}$ ). Proof. Define \\\\begin{equation*} s_{(ij)}=\\\\frac{s_{i j}+s_{j i}}{2} \\\\tag{2.7.13} \\\\end{equation*} \\\\begin{equation*} s_{[ij]}=\\\\frac{s_{i j}-s_{j i}}{2} \\\\tag{2.7.14} \\\\end{equation*} It is easily verified that $s_{(i j)}$ is symmetric and $s_{[i j]}$ is skew-symmetric.', '\\\\begin{enumerate} \\\\setcounter{enumi}{7} \\\\item $s_{(ij)}$ and $s_{[i j}$ are called the symmetric and skew-symmetric parts of $s_{i j}$, respectively. \\\\end{enumerate} Lemma 2. Let $s_{i j}$ be a nonsymmetric tensor and let $t_{i j}$ be a symmetric tensor. Then \\\\begin{equation*} s_{i j} t_{i j}=s_{(i j)} t_{i j} \\\\tag{2.7.15} \\\\end{equation*} Proof. By Lemma 1, $s_{i j}=s_{(i j)}+s_{[i j]}$. Since $$ s_{i j} t_{i j}=s_{(i j)} t_{i j}+s_{[i j]} t_{i j} $$ the lemma will be established if we can show that $s_{[i j]} t_{i j}=0$. We proceed as follows: $$ \\\\begin{aligned} s_{[ij]} t_{i j} & =-s_{[ji]} t_{i j} & & \\\\text { (skew-symmetry of } \\\\left.s_{[i j]}\\\\right) \\\\\\\\ & =-s_{[j i]} t_{j i} & & \\\\text { (symmetry of } \\\\left.t_{i j}\\\\right) \\\\\\\\ & =-s_{[i j]} t_{i j} & & \\\\text { (redefinition of dummy indices) } \\\\end{aligned} $$ from which the result follows.\\\\\\\\ Proof of Theorem \\\\begin{enumerate} \\\\item Let $u_{i}$ be a solution of (S). Thus $u_{i} \\\\in \\\\delta_{1}$. Multiply (2.7.8) by $w_{i} \\\\in \\\\mathcal{V}_{i}$ and integrate over $\\\\Omega$, viz., \\\\end{enumerate} $$ \\\\begin{aligned} & 0=\\\\int_{\\\\Omega} w_{i}\\\\left(\\\\sigma_{i j, j}+\\\\ell_{i}\\\\right) d \\\\Omega=-\\\\int_{\\\\Omega} w_{i, j} \\\\sigma_{i j} d \\\\Omega+\\\\int_{\\\\Gamma} w_{i} \\\\sigma_{i j} n_{j} d \\\\Gamma+\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega \\\\\\\\ & \\\\text { (integration by parts) } \\\\\\\\ & =-\\\\int_{\\\\Omega} w_{(i,j)} \\\\sigma_{i j} d \\\\Omega+\\\\sum_{i=1}^{n_{sd}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right)+\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega \\\\quad \\\\begin{array}{l} \\\\text { (symmetry of } \\\\sigma_{i j}, \\\\\\\\ \\\\text { Lemma 2, } w_{i}=0 \\\\end{array} \\\\\\\\ & \\\\text { on } \\\\Gamma_{g_i} \\\\text {, and (2.7.10)) } \\\\end{aligned} $$ Therefore, $u_{i}$ is a solution of (W).\\\\\\\\ 2. Assume $u_{i}$ is a solution of (W). Since $u_{i} \\\\in \\\\delta_{i}, u_{i}=g_{i}$ on $\\\\Gamma_{g_{i}}$. From (2.7.11) $$ 0=-\\\\int_{\\\\Omega} \\\\underbrace{w_{(i,j)} \\\\sigma_{i j}}_{\\\\left(=w_{i, j} \\\\sigma_{i j}\\\\right. \\\\text { by Lemma 2) }} d \\\\Omega+\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega+\\\\sum_{i=1}^{n_{sd}} \\\\int_{\\\\Gamma_{k_{i}}} w_{i} h_{i} d \\\\Gamma $$ \\\\[ =\\\\int_{\\\\Omega} w_{i}\\\\left(\\\\sigma_{ij, j}+\\\\ell_{i}\\\\right) d \\\\Omega-\\\\sum_{i=1}^{n_{s d}} \\\\int_{\\\\Gamma_{h_{i}}} w_{i}\\\\left(\\\\sigma_{i j} n_{j}-h_{i}\\\\right) d \\\\Gamma \\\\quad \\\\begin{align*} & \\\\text { (integration by parts, } \\\\tag{2.7.16}\\\\\\\\ & \\\\left.w_{i}=0 \\\\text { on } \\\\Gamma_{g_{i}}\\\\right) \\\\end{align*} \\\\] Let $$ \\\\begin{aligned} & \\\\alpha_{i}=\\\\sigma_{i j, j}+\\\\ell_{i} \\\\\\\\ & \\\\beta_{i}=\\\\sigma_{i j} n_{j}-h_{i} \\\\end{aligned} $$ Thus to complete the proof we must show that $$ \\\\begin{array}{ll} \\\\alpha_{i}=0 & \\\\text { on } \\\\Omega \\\\\\\\ \\\\beta_{i}=0 & \\\\text { on } \\\\Gamma_{h_{i}} \\\\end{array} $$ These conditions can be proved by the techniques used in Sec. 2.3. Let $w_{i}=\\\\alpha_{i} \\\\phi$, where\\\\\\\\ i. $\\\\phi>0$ on $\\\\Omega$;\\\\\\\\ ii. $\\\\phi=0$ on $\\\\Gamma$; and\\\\\\\\ iii. $\\\\phi$ is smooth.\\\\\\\\ (These conditions insure that $w_{i} \\\\in \\\\mathcal{V}_{i}$.) Substituting this $w_{i}$ into (2.7.16) yields $$ 0=\\\\int_{\\\\Omega} \\\\underbrace{\\\\alpha_{i} \\\\alpha_{i}}_{\\\\geq 0} \\\\underbrace{\\\\phi}_{>0} d \\\\Omega $$ which implies $\\\\alpha_{i}=0$ on $\\\\Omega$.\\\\\\\\ Now take $w_{i}=\\\\delta_{i 1} \\\\beta_{1} \\\\psi$, where\\\\\\\\ i\\'. $\\\\psi>0$ on $\\\\Gamma_{h_{1}}$;\\\\\\\\ ii\\'. $\\\\psi=0$ on $\\\\Gamma_{g_{1}}$; and\\\\\\\\ iii\\'. $\\\\psi$ is smooth.\\\\\\\\ (Again, $w_{i} \\\\in \\\\mathcal{V}_{1 .}$ ) Substituting this $w_{i}$ into (2.7.16) and making use of $\\\\alpha_{i}=0$ results in $$ 0=\\\\int_{\\\\Gamma_{h_{1}}} \\\\beta_{1}^{2} \\\\psi d \\\\Gamma $$ from which it follows that $\\\\beta_{1}=0$ on $\\\\Gamma_{h_{1}}$.\\\\\\\\ We may proceed analogously to show $\\\\beta_{2}=0$, and so on. Consequently, $u_{i}$ is a solution of $(\\\\boldsymbol{S})$. The abstract notation for the present case is \\\\begin{align*} a(w, u) & =\\\\int_{\\\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\\\Omega \\\\tag{2.7.17}\\\\\\\\ (w, \\\\ell) & =\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega \\\\tag{2.7.18}\\\\\\\\ (w, h)_{\\\\Gamma} & =\\\\sum_{i=1}^{n_{s d}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right) \\\\tag{2.7.19} \\\\end{align*} \\\\subsection*{Exercise 1.} Verify that $a(\\\\cdot, \\\\cdot),(\\\\cdot, \\\\cdot)$ and $(\\\\cdot, \\\\cdot)_{\\\\Gamma}$, as just defined, are symmetric, bilinear forms (cf. Sec. 1.4). Let $\\\\delta=\\\\left\\\\{u \\\\mid u_{i} \\\\in \\\\delta_{i}\\\\right\\\\}$ and let $\\\\mathcal{V}=\\\\left\\\\{w \\\\mid w_{i} \\\\in \\\\mathcal{V}_{i}\\\\right\\\\}$. Then the weak form can be concisely written in terms of (2.7.17-2.7.19) as follows: Given $\\\\boldsymbol{\\\\ell}, \\\\boldsymbol{g}$, and $\\\\boldsymbol{h}$ (in which the components are defined in (W)), find $\\\\boldsymbol{u} \\\\in \\\\mathcal{\\\\delta}$ such that for all $w \\\\in \\\\mathcal{V}$\\\\\\\\ (W) \\\\begin{equation*} a(w, u)=(w, \\\\ell)+(w, h)_{\\\\Gamma} \\\\tag{2.7.20} \\\\end{equation*} As discussed in Sec. 2.3, it is desirable to construct index-free counterparts of the expressions on the right-hand sides of (2.7.17)-(2.7.19). For concreteness we shall assume that $n_{s d}=2$; thus $1 \\\\leq i, j, k, l \\\\leq 2$. Let\\\\footnote{According to our previous notational conventions, $\\\\epsilon=\\\\left[\\\\epsilon_{i j}\\\\right]$, the matrix of strain components. However, we will have no need for this matrix, and consequently we reserve - for the \"strain vector\" defined in (2.7.21). A similar notational conflict occurs with respect to the \"stress vector,\" $\\\\sigma$, defined in Exercise 4. Note that factors of one-half have been eliminated from the shearing components (i.e., last components) of (2.7.21) and (2.7.22). (Compare (2.7.21) and (2.7.22) with (2.7.1).) This will considerably simplify subsequent writing. } \\\\begin{gather*} \\\\epsilon(u)=\\\\left\\\\{\\\\epsilon_{I}(u)\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} u_{1,1} \\\\\\\\ u_{2,2} \\\\\\\\ u_{1,2}+u_{2,1} \\\\end{array}\\\\right\\\\} \\\\tag{2.7.21}\\\\\\\\ \\\\epsilon(w)=\\\\left\\\\{\\\\epsilon_{I}(w)\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} w_{1,1} \\\\\\\\ w_{2,2} \\\\\\\\ w_{1,2}+w_{2,1} \\\\end{array}\\\\right\\\\} \\\\tag{2.7.22}\\\\\\\\ D=\\\\left[D_{I J}\\\\right]=\\\\left[\\\\begin{array}{lll} D_{11} & D_{12} & D_{13} \\\\\\\\ & D_{22} & D_{23} \\\\\\\\ \\\\text { symmetric } & D_{33} \\\\end{array}\\\\right] \\\\tag{2.7.23} \\\\end{gather*} where\\\\\\\\ \\\\begin{equation*} D_{I J}=c_{i j k l} \\\\tag{2.7.24} \\\\end{equation*} in which the indices are related by the following table: \\\\begin{table}[ht] \\\\centering \\\\begin{tabular}{|c|c|c|} \\\\hline \\\\diagbox[width=2em]{\\\\textit{J}}{\\\\textit{I}} & \\\\diagbox[width=2em]{\\\\textit{k}}{\\\\textit{i}} & \\\\diagbox[width=2em]{\\\\textit{l}}{\\\\textit{j}} \\\\\\\\ \\\\hline 1 & 1 & 1 \\\\\\\\ 3 & 1 & 2 \\\\\\\\ 3 & 2 & 1 \\\\\\\\ 2 & 2 & 2 \\\\\\\\ \\\\hline \\\\end{tabular} \\\\caption{TABLE 2.7.1} \\\\end{table} As should be clear, we have \"collapsed\" pairs of indices $(i, j$, and $k, l)$ into single indices ($I$ and $J$, respectively) taking account of the symmetries of $c_{i j k l}, u_{(k, l)}$ and $w_{(i, j)}$. Observe that the indices $I$ and $J$ take on values 1, 2, and 3. In $n_{s d}$ dimensions, the $I$ and $J$ indices will take on values $1,2, \\\\ldots, n_{s d}\\\\left(n_{s d}+1\\\\right) / 2$. It can be shown that \\\\begin{equation*} w_{(i, j)} c_{i j k l} u_{(k, l)}=\\\\epsilon(w)^{T} D \\\\epsilon(u) \\\\tag{2.7.25} \\\\end{equation*} and so \\\\begin{equation*} a(w, u)=\\\\int_{\\\\Omega} \\\\epsilon(w)^{T} D \\\\epsilon(u) d \\\\Omega \\\\tag{2.7.26} \\\\end{equation*} \\\\subsection*{Exercise 2.} Verify (2.7.25) for $n_{s d}=2$.\\\\\\\\ \\\\subsection*{Exercise 3.} Construct the analog of Table 2.7.1 for the case $n_{s d}=$ 3. For definiteness of the ordering, take \\\\[ \\\\epsilon(u)=\\\\left\\\\{\\\\begin{array}{c} u_{1,1} \\\\tag{2.7.27}\\\\\\\\ u_{2,2} \\\\\\\\ u_{3,3} \\\\\\\\ u_{2,3}+u_{3,2} \\\\\\\\ u_{1,3}+u_{3,1} \\\\\\\\ u_{1,2}+u_{2,1} \\\\end{array}\\\\right\\\\} \\\\] \\\\subsection*{Exercise 4.} Show that \\\\begin{equation*} \\\\sigma=\\\\boldsymbol{D} \\\\epsilon(u) \\\\tag{2.7.28} \\\\end{equation*} where \\\\[ \\\\begin{array}{ll} \\\\sigma=\\\\left\\\\{\\\\begin{array}{ll} \\\\sigma_{11} \\\\\\\\ \\\\sigma_{22} \\\\\\\\ \\\\sigma_{12} \\\\end{array}\\\\right\\\\}, & n_{s d}=2 \\\\\\\\ \\\\sigma=\\\\left\\\\{\\\\begin{array}{l} \\\\sigma_{11} \\\\\\\\ \\\\sigma_{22} \\\\\\\\ \\\\sigma_{33} \\\\\\\\ \\\\sigma_{23} \\\\\\\\ \\\\sigma_{13} \\\\\\\\ \\\\sigma_{12} \\\\end{array}\\\\right\\\\}, & n_{s d}=3 \\\\tag{2.7.30} \\\\end{array} \\\\] \\\\subsection*{Exercise 5.} If the body in question is isotropic, then \\\\begin{equation*} c_{i j k l}(x)=\\\\mu(x)\\\\left(\\\\delta_{i k} \\\\delta_{j l}+\\\\delta_{i l} \\\\delta_{j k}\\\\right)+\\\\lambda(x) \\\\delta_{i j} \\\\delta_{k l} \\\\tag{2.7.31} \\\\end{equation*} where $\\\\lambda$ and $\\\\mu$ are the Lam parameters; $\\\\mu$ is often referred to as the shear modulus and denoted by G. The relationships of $\\\\lambda$ and $\\\\mu$ to $E$, Young\\'s modulus, and $\\\\nu$, Poisson\\'s ratio, are given by \\\\begin{align*} & \\\\lambda=\\\\frac{\\\\nu E}{(1+\\\\nu)(1-2 \\\\nu)} \\\\tag{2.7.32}\\\\\\\\ & \\\\mu=\\\\frac{E}{2(1+\\\\nu)} \\\\tag{2.7.33} \\\\end{align*} (See Sokolnikoff [6], p. 71, for further relations with other equivalent moduli.) If (2.7.31) is not satisfied, the body is said to be anisotropic. Using (2.7.31) set up $D$ for $n_{s d}=2$ and $n_{s d}=3$. Hint: The answer for $n_{s d}=2$ is \\\\[ D=\\\\left[\\\\begin{array}{ccc} \\\\lambda+2 \\\\mu & \\\\lambda & 0 \\\\tag{2.7.34}\\\\\\\\ & \\\\lambda+2 \\\\mu & 0 \\\\\\\\ \\\\text { Symmetric } & \\\\mu \\\\end{array}\\\\right] \\\\] This matrix manifests the plane strain hypothesis. (See Sokolnikoff [6] for elaboration.)', '\\\\begin{enumerate} \\\\setcounter{enumi}{8} \\\\item The case of isotropic plane stress may be determined from (2.7.34) by replacing ${\\\\lambda}$ by $\\\\bar{\\\\lambda}$, where \\\\end{enumerate} \\\\begin{equation*} \\\\bar{\\\\lambda}=\\\\frac{2 \\\\lambda \\\\mu}{\\\\lambda+2 \\\\mu} \\\\tag{2.7.35} \\\\end{equation*} (See Sokolnikoff [6] or Timoshenko and Goodier [7] for elaboration on the physical ideas.) \\\\subsection*{Exercise 6.} (See Exercise 1, Sec. 2.4 for background and an analogous result.)\\\\\\\\ Let the \"jump\" in $\\\\sigma_{i n}=\\\\sigma_{i j} n_{j}$ be denoted by [$\\\\sigma_{i n}$]. Consider the weak formulation (i.e., (2.7.11)) and assume $w_{i}$ and $u_{i}$ are smooth on element interiors, but experience gradient discontinuities across element boundaries. Show that $$ 0=\\\\sum_{e=1}^{n_{e l}} \\\\int_{\\\\Omega^{e}} w_{i}\\\\left(\\\\sigma_{ij, j}+\\\\ell_{i}\\\\right) d \\\\Omega-\\\\sum_{i=1}^{n_{sd}} \\\\int_{\\\\Gamma_{h_{i}}} w_{i}\\\\left(\\\\sigma_{i n}-h_{i}\\\\right) d \\\\Gamma-\\\\int_{\\\\Gamma_{int}} w_{i}\\\\left[\\\\sigma_{i n}\\\\right] d \\\\Gamma $$ from which the Euler-Lagrange conditions may be read: $$ \\\\begin{aligned} & \\\\text { i. } \\\\sigma_{i j, j}+\\\\ell_{i}=0 \\\\text { in } \\\\bigcup_{e=1}^{n_{el}} \\\\Omega^{e} \\\\\\\\ & \\\\text { ii. } \\\\sigma_{i n}=h_{i} \\\\text { on } \\\\Gamma_{h_{i}} \\\\\\\\ & \\\\text { iii. }\\\\left[\\\\sigma_{i n}\\\\right]=0 \\\\text { on } \\\\Gamma_{\\\\text {int }} \\\\end{aligned} $$ Here (i) is the equilibrium equation on element interiors, and (iii) is a traction continuity condition across element boundaries. Compare these results with those obtained assuming $w_{i}$ and $u_{i}$ are globally smooth. \\\\subsection*{2.8 ELASTOSTATICS: GALERKIN FORMULATION, SYMMETRY, AND POSITIVE-DEFINITENESS OF K} Let $\\\\delta^{h}$ and $\\\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\\\delta$ and $\\\\mathcal{V}$, respectively. We assume members $w^{h} \\\\in \\\\mathcal{V}^{h}$ result in satisfaction, or approximate satisfaction, of the boundary condition $w_{i}=0$ on $\\\\Gamma_{g_i}$, and members of $\\\\delta^{h}$ admit the decomposition \\\\begin{equation*} u^{h}=v^{h}+g^{h} \\\\tag{2.8.1} \\\\end{equation*} where $\\\\boldsymbol{g}^{h} \\\\in \\\\mathcal{V}^{h}$ and $\\\\boldsymbol{g}^{h}$ results in satisfaction, or approximate satisfaction, of the boundary condition $u_{i}=g_{i}$ on $\\\\Gamma_{g_i}$. The Galerkin formulation of our problem is given as follows:\\\\\\\\ (G) $\\\\left\\\\{\\\\begin{array}{c}\\\\text { Given } \\\\ell, g, \\\\text { and } h \\\\text { (as in }(W)) \\\\text {, find } u^{h}=v^{h}+{g}^{h} \\\\in \\\\delta^{h} \\\\text { such that for all } w^{h} \\\\in \\\\mathbb{V}^{h} \\\\\\\\ a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, \\\\ell\\\\right)+\\\\left(w^{h}, h\\\\right)_{\\\\Gamma}-a\\\\left(w^{h},g^{h}\\\\right) \\\\quad \\\\text { (2.8.2) }\\\\end{array}\\\\right.$ To define the global stiffness matrix and force vector for elasticity, it is necessary to introduce the ID array. This entails a generalization of the definition given in Sec. 2.6, since in the present case there will be more than 1 degree of freedom per node. For elasticity there are $n_{s d}$ degrees of freedom per node, but in order to include in our\\\\\\\\ definition cases such as heat conduction, we shall take the fully general situation in which it is assumed that there are $n_{\\\\text {dof}}$ degrees of freedom per node.\\\\footnote{In general, this is taken to mean the maximum number of degrees of freedom per node in the global model. It is possible in practice to have elements with fewer degrees of freedom per node contributing to the model.} In this case\\\\\\\\ \\\\[ \\\\text{ID}(\\\\underbrace{i}_{\\\\text{Degree of freedom number}}, \\\\underbrace{A}_{\\\\text{Global node number}}) = \\\\left\\\\{ \\\\begin{aligned} &\\\\underbrace{P}_{\\\\text{Global equation number}} &\\\\quad \\\\text{if } A \\\\in \\\\eta - \\\\eta_{g_i} \\\\\\\\ &0 &\\\\quad \\\\text{if } A \\\\in \\\\eta_{g_i} \\\\end{aligned} \\\\right. \\\\] where $1 \\\\leq i \\\\leq n_{\\\\text {dof }}$. Thus ID has dimensions $n_{\\\\mathrm{dof}} \\\\times n_{n p}$. If $n_{\\\\mathrm{dof}}=1$, we reduce to the case considered previously in Sec. 2.6 (i.e., $\\\\operatorname{ID}(i, A)=\\\\operatorname{ID}(A)$ ). Recall that $\\\\boldsymbol{\\\\eta}=\\\\left\\\\{1,2, \\\\ldots, n_{n p}\\\\right\\\\}$ denotes the set of global node numbers. Let $\\\\boldsymbol{\\\\eta}_{\\\\boldsymbol{g}_{i}} \\\\subset \\\\boldsymbol{\\\\eta}$ be the set of nodes at which $u_{i}^{h}=\\\\boldsymbol{g}_{i}$ and let $\\\\boldsymbol{\\\\eta}-\\\\boldsymbol{\\\\eta}_{\\\\boldsymbol{g}_{i}}$ be the complement of $\\\\eta_{g i}$. For each node in $\\\\eta-\\\\eta_{g i}$, the nodal value of $u_{i}^{h}$ is to be determined. The explicit representations of $v_{i}^{h}$ and $g_{i}^{h}$, in terms of the shape functions and nodal values are \\\\begin{align*} & v_{i}^{h}=\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g_{i}}} N_{A} d_{\\\\underbrace{i}_{\\\\text{Degree of freedom number}} \\\\underbrace{A}_{\\\\text{Global node number}}} \\\\quad \\\\text { (no sum on } i \\\\text { ) } \\\\tag{2.8.4}\\\\\\\\ & g_{i}^{h}=\\\\sum_{A \\\\in \\\\eta_{g_{i}}} N_{A} g_{iA} \\\\quad \\\\text { (no sum on } i \\\\text { ) } \\\\end{align*} Let $e_{i}$ denote the $i$th Euclidean basis vector for $\\\\mathbb{R}^{n_{sd}} ; \\\\boldsymbol{e}_{i}$ has a 1 in slot $i$ and zeros elsewhere. For example \\\\[ \\\\left(n_{s d}=2\\\\right) \\\\quad e_{1}=\\\\left\\\\{\\\\begin{array}{l} 1 \\\\tag{2.8.6}\\\\\\\\ 0 \\\\end{array}\\\\right\\\\}, \\\\quad e_{2}=\\\\left\\\\{\\\\begin{array}{l} 0 \\\\\\\\ 1 \\\\end{array}\\\\right\\\\} \\\\] \\\\[ \\\\left(n_{s d}=3\\\\right) \\\\quad e_{1}=\\\\left\\\\{\\\\begin{array}{l} 1 \\\\tag{2.8.7}\\\\\\\\ 0 \\\\\\\\ 0 \\\\end{array}\\\\right\\\\}, \\\\quad e_{2}=\\\\left\\\\{\\\\begin{array}{l} 0 \\\\\\\\ 1 \\\\\\\\ 0 \\\\end{array}\\\\right\\\\} \\\\quad e_{3}=\\\\left\\\\{\\\\begin{array}{l} 0 \\\\\\\\ 0 \\\\\\\\ 1 \\\\end{array}\\\\right\\\\} \\\\] The vector versions of (2.8.4) and (2.8.5) may be defined with the aid of $e_{i}$, viz., \\\\begin{align*} & v^{h}=v_{i}^{h} e_{i} \\\\tag{2.8.8}\\\\\\\\ & g^{h}=g_{i}^{h} e_{i} \\\\tag{2.8.9} \\\\end{align*} Likewise, a typical member $w^{h} \\\\in \\\\mathcal{V}^{h}$ has the representation \\\\begin{equation*} w^{h}=w_{i}^{h} e_{i}, \\\\quad w_{i}^{h}=\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g_{i}}} N_{A} c_{i A} \\\\quad \\\\text {(no sum on i)}\\\\tag{2.8.10} \\\\end{equation*} Substituting (2.8.4), (2.8.5), and (2.8.8)-(2.8.10) into (2.8.2) and arguing along the lines of Sec. 1.6 results in (verify!) \\\\begin{align*} & \\\\sum_{j=1}^{n_{\\\\text {dof }}}\\\\left(\\\\sum_{B \\\\in \\\\eta-\\\\eta_{g_j}} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) d_{j B}\\\\right)=\\\\left(N_{A} e_{i}, \\\\ell\\\\right)+\\\\left(N_{A} e_{i}, h\\\\right)_{\\\\Gamma} \\\\\\\\ & \\\\quad-\\\\sum_{j=1}^{n_{dof}}\\\\left(\\\\sum_{B \\\\in \\\\eta_{g_j}} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) g_{j B}\\\\right), \\\\quad A \\\\in \\\\eta-\\\\eta_{g_i}, \\\\quad 1 \\\\leq i \\\\leq n_{s d} \\\\tag{2.8.11\\\\footnotemark} \\\\end{align*} \\\\footnotetext{For correct interpretation of the meaning of these equations, the sum on $j$ should be taken first. For example, in two dimensions \\\\[ \\\\begin{aligned} \\\\sum_{j=1}^{2}\\\\left(\\\\sum_{B \\\\in \\\\eta-\\\\eta_{g_j}} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) d_{jB}\\\\right)= & \\\\sum_{B \\\\in \\\\eta-\\\\eta_{g_1}} a\\\\left(N_{A} e_{i}, N_{B} e_{1}\\\\right) d_{1 B} \\\\\\\\ & +\\\\sum_{B \\\\in \\\\eta-\\\\eta_{g_2}} a\\\\left(N_{A} e_{i}, N_{B} e_{2}\\\\right) d_{2 B} \\\\end{aligned} \\\\] and \\\\[ \\\\begin{aligned} \\\\sum_{j=1}^{2}\\\\left(\\\\sum_{B \\\\in n_{g_j}}\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) g_{jB}\\\\right)= & \\\\sum_{B \\\\in \\\\eta_{g_1}} a\\\\left(N_{A} e_{i}, N_{B} e_{1}\\\\right) g_{1 B} \\\\\\\\ & +\\\\sum_{B \\\\in n_{g_2}} a\\\\left(N_{A} e_{i}, N_{B} e_{2}\\\\right) g_{2 B} \\\\end{aligned} \\\\] } This is equivalent to the matrix equation\\\\\\\\ \\\\begin{align*} & K d=\\\\boldsymbol{F} \\\\tag{2.8.12}\\\\\\\\ & \\\\text { where } \\\\\\\\ & K=\\\\left[K_{P Q}\\\\right] \\\\tag{2.8.13}\\\\\\\\ & d=\\\\left\\\\{d_{Q}\\\\right\\\\} \\\\tag{2.8.14}\\\\\\\\ & F=\\\\left\\\\{F_{P}\\\\right\\\\} \\\\tag{2.8.15}\\\\\\\\ & K_{P Q}=a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) \\\\tag{2.8.16}\\\\\\\\ & F_{P}=\\\\left(N_{A} e_{i}, \\\\ell\\\\right)+\\\\left(N_{A} e_{i}, h\\\\right)_{\\\\Gamma}-\\\\sum_{j=1}^{n_{dof}}\\\\left(\\\\sum_{B \\\\in \\\\eta_{g_j}} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) g_{jB}\\\\right) \\\\tag{2.8.17}\\\\\\\\ & \\\\text { in which } \\\\\\\\ & P=\\\\operatorname{ID}(i, A), \\\\quad Q=\\\\operatorname{ID}(j, B) \\\\end{align*} Equation (2.8.16) may be written in more explicit form by using (2.7.26) and noting that (see (2.7.21) and (2.7.22)): \\\\begin{equation*} \\\\epsilon\\\\left(N_{A} e_{i}\\\\right)=B_{A} e_{i} \\\\tag{2.8.19} \\\\end{equation*} where \\\\begin{align*} & \\\\left(n_{s d}=2\\\\right) \\\\quad B_{A}=\\\\left[\\\\begin{array}{cc} N_{A, 1} & 0 \\\\\\\\ 0 & N_{A, 2} \\\\\\\\ N_{A, 2} & N_{A, 1} \\\\end{array}\\\\right] \\\\tag{2.8.20}\\\\\\\\ & \\\\left(n_{s d}=3\\\\right) \\\\quad B_{A}=\\\\left[\\\\begin{array}{ccc} N_{A, 1} & 0 & 0 \\\\\\\\ 0 & N_{A, 2} & 0 \\\\\\\\ 0 & 0 & N_{A, 3} \\\\\\\\ 0 & N_{A, 3} & N_{A, 2} \\\\\\\\ N_{A, 3} & 0 & N_{A, 1} \\\\\\\\ N_{A, 2} & N_{A, 1} & 0 \\\\end{array}\\\\right] \\\\tag{2.8.21} \\\\end{align*} \\\\subsection*{Exercise 1.} Verify (2.8.19)-(2.8.21). With these, (2.8.16) becomes\\\\\\\\ \\\\[ K_{PQ} = e_i^T \\\\int_\\\\Omega B_A^T D B_B \\\\, d\\\\Omega \\\\, e_j \\\\] \\\\begin{itemize} \\\\item $PQ$: Global equation numbers \\\\item $i, j$: Degree of freedom numbers \\\\item $A, B$: Global node numbers \\\\end{itemize} and the indices are related by (2.8.18). Equation (2.8.17) is also amenable to explication. Note that, by (2.7.18) \\\\begin{equation*} \\\\left(N_{A} e_{i}, \\\\ell\\\\right)=\\\\int_{\\\\Omega} N_{A} \\\\ell_{i} d \\\\Omega \\\\tag{2.8.23} \\\\end{equation*} and likewise by (2.7.19) \\\\begin{equation*} \\\\left(N_{A} e_{i}, h\\\\right)_{\\\\Gamma}=\\\\int_{\\\\Gamma_{h_{i}}} N_{A} h_{i} d \\\\Gamma \\\\quad \\\\text { (no sum) } \\\\tag{2.8.24} \\\\end{equation*} Thus (2.8.17) may be written as \\\\begin{equation*} F_{P}=\\\\int_{\\\\Omega} N_{A} \\\\ell_{i} d \\\\Omega+\\\\int_{\\\\Gamma_{h_i}} N_{A} h_{i} d \\\\Gamma-\\\\sum_{j=1}^{n_{dof}}\\\\left(\\\\sum_{B \\\\in \\\\eta_{g_j}} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) g_{j B}\\\\right) \\\\tag{2.8.25} \\\\end{equation*} Now that we have defined $\\\\boldsymbol{K}$, we can establish its fundamental properties. We shall need the following preliminary results. Let $n_{s d}=2$ or 3 and let $w: \\\\Omega \\\\rightarrow \\\\mathbb{R}^{n_{s d}}$. If $w_{(i, j)}=0$ (\"zero strains\"), then $\\\\boldsymbol{w}$ admits the representations: \\\\begin{align*} & \\\\left(n_{s d}=2\\\\right) \\\\quad w(x)=\\\\overbrace{c}^{\\\\text{Translation}}+\\\\overbrace{c_3\\\\left(x_{1} e_{2}-x_{2} e_{1}\\\\right)}^{\\\\text{Rotation}} \\\\tag{2.8.26}\\\\\\\\ & \\\\left(n_{s d}=3\\\\right) \\\\quad w(x)=\\\\underbrace{c_{1}}_{\\\\text{Translation}}+\\\\underbrace{c_{2} \\\\times x}_{\\\\text{Rotation}}\\\\tag{2.8.27} \\\\end{align*} where \\\\[ c=\\\\left\\\\{\\\\begin{array}{l} c_{1} \\\\tag{2.8.28}\\\\\\\\ c_{2} \\\\end{array}\\\\right\\\\} \\\\quad c_{1}=\\\\left\\\\{\\\\begin{array}{l} c_{11} \\\\\\\\ c_{12} \\\\\\\\ c_{13} \\\\end{array}\\\\right\\\\} \\\\quad c_{2}=\\\\left\\\\{\\\\begin{array}{l} c_{21} \\\\\\\\ c_{22} \\\\\\\\ c_{23} \\\\end{array}\\\\right\\\\} \\\\] and $c_{3}$ are constants; and $\\\\times$ denotes the vector cross product. Equations (2.8.26) and (2.8.27) define infinitesimal rigid-body motions.', '$} We assume that the homogeneous boundary conditions incorporated into the definition of $\\\\mathcal{V}^{h}$ preclude nontrivial infinitesimal rigid-body motions. In other words, we assume that if $w^{h} \\\\in \\\\mathcal{V}^{h}$ is a rigid-body motion, then $w^{h}$ is identically zero.', '\\\\begin{enumerate} \\\\item $K$ is symmetric. \\\\item If Assumption $R$ holds, then $K$ is also positive definite. \\\\end{enumerate} Proof of $1$. Symmetry. We may note that symmetry of $\\\\boldsymbol{K}$ follows from (2.8.16) and the symmetry of $a(\\\\cdot, \\\\cdot)$. However, we shall provide an alternative proof in terms of (2.8.22).\\\\\\\\ $$ \\\\begin{aligned} K_{P Q} & =e_{i}^{T} \\\\int_{\\\\Omega} B_{A}^{T} D B_{B} d \\\\Omega e_{j} \\\\\\\\ & =e_{j}^{T} \\\\int_{\\\\Omega} B_{B}^{T} D^{T} B_{A} d \\\\Omega e_{i} \\\\\\\\ & =e_{j}^{T} \\\\int_{\\\\Omega} B_{B}^{T} D B_{A} d \\\\Omega e_{i} \\\\quad \\\\text {(symmetry of D)} \\\\\\\\ & =K_{Q P} \\\\end{aligned} $$', \"Note that the symmetry of $\\\\boldsymbol{K}$ followed from the symmetry of $\\\\boldsymbol{D}$, which was a consequence of the major symmetry of the $c_{i j k l}$ 's (see (2.7.3)). Proof of 2. Positive definite (Recall from Sec. 1.9 that we must show (i) $\\\\boldsymbol{c}^{T} \\\\boldsymbol{K} c \\\\geq 0$ and (ii) $\\\\boldsymbol{c}^{T} \\\\boldsymbol{K} \\\\boldsymbol{c}=0$ implies $\\\\boldsymbol{c}=0$.) Let $w_{i}^{h}=\\\\Sigma_{A \\\\in \\\\eta_{-\\\\eta_{g_i}}} N_{A} c_{iA}$ be a member of $\\\\mathcal{V}_{i}^{h}$. Then $c_{P}=c_{iA}$, where $P=$ $\\\\operatorname{ID}(i, A), 1 \\\\leq P \\\\leq n_{e q}$, defines the components of an $n_{e q}$-vector $c$.\\\\\\\\ i. $$ \\\\begin{aligned} & \\\\boldsymbol{c}^{T} K c=\\\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{P Q} c_{Q} \\\\\\\\ & =\\\\sum_{i, j=1}^{n_{dof}}\\\\left(\\\\sum_{\\\\substack{A \\\\in \\\\eta-\\\\eta_{g_i} \\\\\\\\ B \\\\in \\\\eta-\\\\eta_{g_j}}} c_{i A} a\\\\left(N_{A} e_{i}, N_{B} e_{j}\\\\right) c_{j B}\\\\right) \\\\quad \\\\text { (definition of } K_{P Q} \\\\text { ) } \\\\\\\\ & =a\\\\left(\\\\sum_{i=1}^{n_{dof}}\\\\left(\\\\sum_{A \\\\in \\\\eta-\\\\eta_{g_i}} c_{iA} N_{A} e_{i}\\\\right), \\\\sum_{j=1}^{n_ {dof}}\\\\left(\\\\sum_{B \\\\in \\\\eta-\\\\eta_{g_j}} c_{j B} N_{B} e_{j}\\\\right)\\\\right) \\\\quad \\\\text { (bilinearity of } a(\\\\cdot, \\\\cdot)) \\\\\\\\ & =a\\\\left(w^{h}, w^{h}\\\\right) \\\\quad \\\\text { (definition of } w^{h} \\\\text { ) } \\\\\\\\ & =\\\\int_{\\\\Omega} \\\\underbrace{w_{(i, j)}^{h} c_{ijkl} w_{(k, l)}^{h} d \\\\Omega}_{\\\\geq 0} \\\\quad \\\\text { (by (2.7.5) and (2.7.17)) } \\\\\\\\ & \\\\geq 0 \\\\end{aligned} $$ ii. Assume $\\\\boldsymbol{c}^{\\\\boldsymbol{T}} \\\\boldsymbol{K} \\\\boldsymbol{c}=\\\\mathbf{0}$. By the proof of part ( $\\\\left.\\\\mathbf{(}\\\\right)$, we deduce that $$ w_{(i,j)}^{h} c_{i j k l} w_{(k, l)}^{h}=0 $$ From (2.7.6), this means that $w_{(i, j)}^{h}=0$, and so $w^{h}$ is an infinitesimal rigid motion. By Assumption $R, \\\\boldsymbol{w}^{\\\\boldsymbol{h}}=\\\\mathbf{0}$, from which it follows that $c_{p}=0$; hence $\\\\boldsymbol{c}=\\\\mathbf{0}$.\", 'Positive definiteness of $\\\\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\\\\mathcal{V}^{h}$. \\\\subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\\\\boldsymbol{k}^{e}$ and $f^{\\\\boldsymbol{c}}$ : \\\\begin{align*} & \\\\boldsymbol{k}^{e}=\\\\left[k_{p q}^{e}\\\\right], \\\\quad \\\\boldsymbol{f}^{e}=\\\\left\\\\{f_{p}^{e}\\\\right\\\\}, \\\\quad 1 \\\\leq p, q \\\\leq n_{ee}=n_{ed} n_{e n} \\\\tag{2.9.1}\\\\footnotemark\\\\\\\\ & k_{p q}^{e}=e_{i}^{T} \\\\int_{\\\\Omega e} B_{a}^{T} D B_{b} d \\\\Omega e_{j}, \\\\quad p=n_{e d}(a-1)+i, \\\\\\\\ & q=n_{e d}(b-1)+j \\\\tag{2.9.2}\\\\\\\\ & \\\\left(n_{s d}=2\\\\right) \\\\quad B_{a}=\\\\left[\\\\begin{array}{cc} N_{a, 1} & 0 \\\\\\\\ 0 & N_{a, 2} \\\\\\\\ N_{a, 2} & N_{a, 1} \\\\end{array}\\\\right] \\\\tag{2.9.3}\\\\\\\\ & \\\\left(n_{s d}=3\\\\right) \\\\quad \\\\boldsymbol{B}_{a}=\\\\left[\\\\begin{array}{ccc} N_{a, 1} & 0 & 0 \\\\\\\\ 0 & N_{a, 2} & 0 \\\\\\\\ 0 & 0 & N_{a, 3} \\\\\\\\ 0 & N_{a, 3} & N_{a, 2} \\\\\\\\ N_{a, 3} & 0 & N_{a, 1} \\\\\\\\ N_{a, 2} & N_{a, 1} & 0 \\\\end{array}\\\\right] \\\\tag{2.9.4} \\\\end{align*} \\\\footnotetext{$n_{\\\\text{ee}}$ stands for the number of element equations and $n_{\\\\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\\\\text {ed}} \\\\leq n_{\\\\text{dof}}$, although they are usually equal. } and \\\\[ f_{p}^{e}=\\\\int_{\\\\Omega^{e}} N_{a} \\\\ell_{i} d \\\\Omega+\\\\int_{\\\\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \\\\Gamma-\\\\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \\\\quad \\\\begin{array}{r} \\\\Gamma_{h_{i}}^{e}=\\\\Gamma_{h_{i}} \\\\cap \\\\Gamma^{e} \\\\tag{2.9.5}\\\\\\\\ (\\\\text { no sum on } i) \\\\end{array} \\\\] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\\\\left(x_{b}^{e}\\\\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \\\\begin{equation*} \\\\underbrace{\\\\boldsymbol{k}_{ab}^{e}}_{n_{e d} \\\\times n_{e d}}=\\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} B_{a}^{T} \\\\boldsymbol{D} B_{b} d \\\\boldsymbol{\\\\Omega} \\\\tag{2.9.6a} \\\\end{equation*} From (2.9.2) we see that \\\\begin{equation*} k_{p q}^{e}=e_{i}^{T} k_{a b}^{e} e_{j} \\\\tag{2.9.6b} \\\\end{equation*} This means, \"the $p q$-component of $\\\\boldsymbol{k}^{e}$ is the $i j$-component of the submatrix $\\\\boldsymbol{k}_{\\\\text {ab. }}^{e}$ \" By (2.9.1) through (2.9.4), we see that $k^{c}$ may be written as \\\\begin{equation*} k^{c}=\\\\int_{\\\\Omega^{e}} B^{T} D B d \\\\Omega \\\\tag{2.9.7} \\\\end{equation*} where \\\\begin{equation*} \\\\boldsymbol{B}=\\\\left[\\\\boldsymbol{B}_{1}, \\\\boldsymbol{B}_{2}, \\\\ldots, \\\\boldsymbol{B}_{n_{en}}\\\\right] \\\\tag{2.9.8} \\\\end{equation*} For example, in the case of a two-dimensional (i.e., $n_{s d}=n_{e d}=2$ ), four-noded element, $\\\\boldsymbol{k}^{\\\\boldsymbol{c}}$ takes the form \\\\[ \\\\underbrace{k^e}_{8 \\\\times 8} = \\\\left[\\\\begin{array}{cccc} k_{11}^e & k_{12}^e & k_{13}^e & k_{14}^e \\\\\\\\ k_{21}^e & k_{22}^e & k_{23}^e & k_{24}^e \\\\\\\\ k_{31}^e & k_{32}^e & k_{33}^e & k_{34}^e \\\\\\\\ k_{41}^e & k_{42}^e & k_{43}^e & k_{44}^e \\\\end{array}\\\\right] \\\\] In practice, the submatrices above the dashed line are computed and those below, if required, are determined by symmetry. The global arrays $\\\\boldsymbol{K}$ and $\\\\boldsymbol{F}$ may be formed from the element arrays $\\\\boldsymbol{k}^{\\\\boldsymbol{e}}$ and $\\\\boldsymbol{f}^{\\\\boldsymbol{e}}$, respectively, by way of an assembly algorithm as outlined in Sec. 1.14. \\\\subsection*{Exercise 1.} Let \\\\begin{align*} & \\\\underset{n_{ee} \\\\times 1}{d^{e}}=\\\\left\\\\{d_{a}^{e}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} d_{1}^e \\\\\\\\ d_{2}^{e} \\\\\\\\ \\\\vdots \\\\\\\\ d^{e}_{n_{en}} \\\\end{array}\\\\right\\\\} \\\\tag{2.9.10}\\\\\\\\ & \\\\left(n_{\\\\text{ed }}=2\\\\right) \\\\quad d_{a}^{e}=\\\\left\\\\{\\\\begin{array}{l} d_{1a}^e \\\\\\\\ d_{2a}^{e} \\\\end{array}\\\\right\\\\} \\\\tag{2.9.11}\\\\\\\\ & \\\\left(n_{e d}=3\\\\right) \\\\quad d_{a}^{e}=\\\\left\\\\{\\\\begin{array}{l} d_{1a}^e \\\\\\\\ d_{2a}^e \\\\\\\\ d_{3a}^e \\\\end{array}\\\\right\\\\} \\\\tag{2.9.12} \\\\end{align*} where \\\\begin{equation*} d_{i a}^{e}=u_{i}^{h}\\\\left(x_{a}^{e}\\\\right) \\\\tag{2.9.13} \\\\end{equation*} $d^{e}$ is called the element displacement vector. Show that the stress vector (see Exercise 4, Sec. 2.7.) at point $x \\\\in \\\\Omega^{\\\\boldsymbol{c}}$ can be calculated from the formula \\\\begin{equation*} \\\\sigma(x)=D(x) B(x) d^{e}=D(x) \\\\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e} \\\\tag{2.9.14} \\\\end{equation*} \\\\subsection*{2.10 ELASTOSTATICS: DATA PROCESSING ARRAYS ID, IEN, AND LM} We have already noted that the definition of the ID array must be generalized for the present case as indicated in Sec. 2.8. We iniust also generalize our definition of the LM array. However, the IEN array remains the same as before. In the present and fully general cases, the LM array is three-dimensional, with dimensions $\\\\boldsymbol{n}_{e d} \\\\times \\\\boldsymbol{n}_{e n} \\\\times \\\\boldsymbol{n}_{e l}$, and is defined by\\\\\\\\ \\\\[ \\\\text{LM}(i, a, e) = \\\\text{ID}(i, \\\\text{IEN}(a, e)) \\\\] \\\\begin{itemize} \\\\item $i$: Degrees of freedom number \\\\item $a$: Local node number \\\\item $e$: Element number \\\\end{itemize} Alternatively, it is sometimes convenient to think of LM as two-dimensional, with dimensions $n_{e e} \\\\times n_{e l}$, viz., ${ }^{17}$ \\\\begin{align*} & \\\\mathrm{LM}(\\\\underbrace{p}_{\\\\text{Local equation number}}, \\\\underbrace{e}_{\\\\text{Element number}})=\\\\mathrm{LM}(i, a, e), \\\\quad p=n_{e d}(a-1)+i \\\\end{align*} To see how everything works in practice, it is helpful to run through a simple example.', 'Consider the mesh of four-node, rectangular elements illustrated in Fig. 2.10.1. We assume that the local node numbering begins in the lower left-hand corner for each element and proceeds in counterclockwise fashion. \\\\footnotetext[16]{The $g$-boundary conditions are accounted for in this definition.\\\\\\\\ ${ }^{17}$ The reader knowledgeable in FORTRAN will realize that the intemal computer storage of (2.10.1) and (2.10.2) is identical. }This is shown for element 4, which is typical. Four displacement (i.e., \" $g$-type\") boundary conditions are specified; namely, the horizontal displacement is specified at nodes 1 and 10 , and the vertical displacement is specified at nodes 1 and 3. Since $n_{\\\\text {np}}=12, n_{\\\\text {dof }}=n_{\\\\text {ed }}=2$, and 4 displacement degrees of freedom are specified, we have $n_{e q}=20$. As is usual, we adopt the convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. ${ }^{18}$ The ID, IEN, and LM arrays are given in Figure 2.10.2. The reader is urged to verify the results.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{images/chapter2.10.1.png} Figure 2.10.1 Mesh of four-node, rectangular, elasticity elements; global and local node numbers, element numbers, and displacement boundary conditions. In terms of the IEN and LM arrays, a precise definition of the $g_{p}^{e}$ \\'s may be given (see (2.9.5)): \\\\[ g_{p}^{e}=g_{i a}^{e}= \\\\begin{cases}0, & \\\\text { if } \\\\mathrm{LM}(i, a, e) \\\\neq 0 \\\\tag{2.10.3}\\\\\\\\ g_{i A}, & \\\\text { where } A=\\\\operatorname{IEN}(a, e), \\\\text { if } \\\\mathrm{LM}(i, a, e)=0\\\\end{cases} \\\\] This definition may be easily programmed.\\\\\\\\ ${ }^{18}$ In practice, equation numbers are often renumbered internally to minimize the bandwidth of $\\\\boldsymbol{K}$ and thus decrease storage and solution effort. This is especially important in analyzing large-scale systems involving tens of thousands of equations. An algorithm for reducing bandwidth is presented in [8]. \\\\includegraphics[max width=\\\\textwidth, center]{images/chapter2.10.2.png}', \"As a final example, we consider a typical four-node, elasticity element in some large mesh; see Fig. 2.10.3. We assume the pertinent entries of the ID array are given as follows: \\\\[ \\\\left.\\\\begin{array}{l} \\\\operatorname{ID}(1,32)=0 \\\\\\\\ \\\\operatorname{ID}(2,32)=0 \\\\\\\\ \\\\operatorname{ID}(1,59)=115 \\\\\\\\ \\\\operatorname{ID}(2,59)=116 \\\\\\\\ \\\\operatorname{ID}(1,164)=0 \\\\tag{2.10.4}\\\\\\\\ \\\\operatorname{ID}(2,164)=325 \\\\\\\\ \\\\operatorname{ID}(1,168)=332 \\\\\\\\ \\\\operatorname{ID}(2,168)=333 \\\\end{array}\\\\right\\\\} \\\\] The entries of IEN follow from Fig. 2.10.3: \\\\[ \\\\left.\\\\begin{array}{l} \\\\operatorname{IEN}(1, e)=164 \\\\\\\\ \\\\operatorname{IEN}(2, e)=32 \\\\\\\\ \\\\operatorname{IEN}(3, e)=168 \\\\tag{2.10.5}\\\\\\\\ \\\\operatorname{IEN}(4, e)=59 \\\\end{array}\\\\right\\\\} \\\\] \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-39}\\\\\\\\ (i) - Local node numbers Tigure 2.10.3 Typical four-node elasticity element; global and local node numbers. Combining (2.10.4) and (2.10.5), by way of (2.10.1), yields entries of the LM array: \\\\[ \\\\left.\\\\begin{array}{l} \\\\operatorname{LM}(1,1, e)=0 \\\\tag{2.10.6}\\\\\\\\ \\\\operatorname{LM}(2,1, e)=325 \\\\\\\\ \\\\operatorname{LM}(1,2, e)=0 \\\\\\\\ \\\\operatorname{LM}(2,2, e)=0 \\\\\\\\ \\\\operatorname{LM}(1,3, e)=332 \\\\\\\\ \\\\operatorname{LM}(2,3, e)=333 \\\\\\\\ \\\\operatorname{LM}(1,4, e)=115 \\\\\\\\ \\\\operatorname{LM}(2,4, e)=116 \\\\end{array}\\\\right\\\\} \\\\] The contribution to the global arrays may be deduced from LM:\\\\\\\\ Stiffness (due to symmetry, only the upper triangular portion need be assembled.) \\\\[ \\\\left.\\\\begin{array}{l} K_{115,115} \\\\leftarrow K_{115,115}+k_{77}^{e} \\\\\\\\ K_{115,116}^{e} \\\\leftarrow K_{115,116}+k_{78}^{e} \\\\\\\\ K_{115,325} \\\\leftarrow K_{115,325}+k_{72}^{e} \\\\\\\\ K_{115,332} \\\\leftarrow K_{115,332}+k_{75}^{e} \\\\\\\\ K_{115,333} \\\\leftarrow K_{115,333}+k_{76}^{e} \\\\\\\\ K_{116,116} \\\\leftarrow K_{116,116}+k_{88}^{e} \\\\\\\\ K_{116,325} \\\\leftarrow K_{116,325}+k_{82}^{e} \\\\\\\\ K_{116,332} \\\\leftarrow K_{116,332}+k_{85}^{e} \\\\tag{2.10.7}\\\\\\\\ K_{116,333} \\\\leftarrow K_{116,333}+k_{86}^{e} \\\\\\\\ K_{325,325} \\\\leftarrow K_{325,325}+k_{22}^{e} \\\\\\\\ K_{325,332} \\\\leftarrow K_{325,332}+k_{25}^{e} \\\\\\\\ K_{325,333} \\\\leftarrow K_{325,333}+k_{26}^{e} \\\\\\\\ K_{332,332} \\\\leftarrow K_{332,332}+k_{55}^{e} \\\\\\\\ K_{332,333} \\\\leftarrow K_{332,333}+k_{56}^{e} \\\\\\\\ K_{333,333} \\\\leftarrow K_{333,333}+k_{66}^{e} \\\\end{array}\\\\right\\\\} \\\\] Force \\\\[ \\\\left.\\\\begin{array}{l} F_{115} \\\\leftarrow F_{115}+f_{7}^{e} \\\\tag{2.10.8}\\\\\\\\ F_{116} \\\\leftarrow F_{116}+f_{8}^{e} \\\\\\\\ F_{325} \\\\leftarrow F_{325}+f_{2}^{e} \\\\\\\\ F_{332} \\\\leftarrow F_{332}+f_{5}^{e} \\\\\\\\ F_{333} \\\\leftarrow F_{333}+f_{6}^{e} \\\\end{array}\\\\right\\\\} \\\\] where \\\\begin{equation*} f_{p}^{e}=\\\\cdots-\\\\sum_{q=1}^{n_{eq}} k_{p q}^{e} g_{q}^{e} \\\\tag{2.10.9} \\\\end{equation*} (We have omitted the first two terms in the right-hand side of (2.9.5) in writing (2.10.9).) In the present example, only $g_{1}^{e}, g_{3}^{e}$ and $g_{4}^{e}$ may be nonzero. Therefore (2.10.9) may be simplified to \\\\begin{equation*} f_{p}^{e}=\\\\cdots-k_{p 1}^{e} g_{1}^{e}-k_{p 3}^{e} g_{3}^{e}-k_{p 4}^{e} g_{4}^{e} \\\\tag{2.10.10} \\\\end{equation*} The multiplications indicated in (2.10.10) are only performed in practice if the $g_{p}^{e}$ 's are nonzero. A schematic representation of the contributions of $k^{e}$ and $f^{e}$ to $K$ and $F$ is shown in Figure 2.10.4.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-41} Figure 2.10.4 Contributions of elasticity element in Example 2 to global arrays. \\\\subsection*{Exercise 1.} Consider a two-dimensional elastostatic boundary-value problem. Set up the ID, IEN, and LM arrays for the following mesh of four-node quadrilaterals:\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-42} \\\\subsection*{2.11 SUMMARY OF IMPORTANT EQUATIONS FOR PROBLEMS CONSIDERED IN CHAPTERS 1 AND 2}\", '$(\\\\boldsymbol{S})$ $$ \\\\begin{aligned} \\\\sigma_{i j, j}+f_{i} & =0 & & \\\\text { on } \\\\Omega \\\\\\\\ u_{i} & =g_{i} & & \\\\text { on } \\\\Gamma_{g_{i}} \\\\\\\\ \\\\sigma_{i j} n_{j} & =h_{i} & & \\\\text { on } \\\\Gamma_{h_{i}} \\\\end{aligned} $$ where $\\\\sigma_{i j}=c_{ijkl} \\\\epsilon_{k l}=c_{i j k l} u_{(k, l)}$\\\\\\\\ $(W)^{19}$Find $u \\\\in \\\\delta$, such that $\\\\forall w \\\\in \\\\mathcal{V}$ $$ a(w, u)=(w, \\\\ell)+(w, k)_{\\\\Gamma} $$ where $$ \\\\begin{aligned} a(w, u) & =\\\\int_{\\\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\\\Omega \\\\\\\\ (w, \\\\ell) & =\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega \\\\\\\\ (w, k)_{\\\\Gamma} & =\\\\sum_{i=1}^{n_{sd}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right) \\\\end{aligned} $$ (G) Find $v^{h} \\\\in \\\\mathcal{V}^{h}$, such that $\\\\forall \\\\boldsymbol{w}^{h} \\\\in \\\\mathcal{V}^{h}$ $$ a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, \\\\ell \\\\right)+\\\\left(w^{h}, k\\\\right)_{\\\\Gamma}-a\\\\left(w^{h}, g^{h}\\\\right) $$ ${ }^{19}$ The notation $\\\\forall$ means \"for all.\"\\\\\\\\ (M) $K d=F$, where $K=A_{e=1}^{n_{el}}\\\\left(k^{e}\\\\right), F=F_{\\\\text {nodal }}+A_{e=1}^{n_{el}}\\\\left(f^{e}\\\\right)^{20}$ $$ \\\\begin{aligned} k_{p q}^{e} & =e_{i}^{T} k_{a b}^{e} e_{j}, \\\\quad k_{a b}^{e}=\\\\int_{\\\\Omega^{e}} B_{a}^{T} D B_{b} d \\\\Omega \\\\\\\\ f_{p}^{e} & =\\\\int_{\\\\Omega^{e}} N_{a} \\\\ell_{i} d \\\\Omega+\\\\int_{\\\\Gamma^e_{h_{i}}} N_{a} h_{i} d \\\\Gamma-\\\\sum_{q=1}^{n_{el}} k_{p q}^{e} g_{q}^{e} \\\\quad \\\\text { (no sum on } i \\\\text { ) } \\\\\\\\ p & =n_{e d}(a-1)+i \\\\\\\\ q & =n_{e d}(b-1)+j \\\\end{aligned} $$ Stress at a point: $\\\\sigma(x)=D(x) \\\\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e}$', '(S) $$ \\\\begin{aligned} q_{i, i} & =\\\\ell & & \\\\text { in } \\\\Omega \\\\\\\\ u & =g & & \\\\text { on } \\\\Gamma_{g} \\\\\\\\ -q_{i} n_{i} & =h & & \\\\text { on } \\\\Gamma_{h} \\\\end{aligned} $$ where $q_{i}=-\\\\kappa_{i j} u_{, j}$\\\\\\\\ (W) Find $u \\\\in \\\\mathcal{\\\\delta}$, such that $\\\\forall w \\\\in \\\\mathcal{V}$ $$ a(w, u)=(w, \\\\ell)+(w, h)_\\\\Gamma $$ where $$ \\\\begin{aligned} a(w, u) & =\\\\int_{\\\\Omega} w_{, i} \\\\kappa_{i j} u_{, j} d \\\\Omega \\\\\\\\ (w, \\\\ell) & =\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega \\\\\\\\ (w, h)_{\\\\Gamma} & =\\\\int_{\\\\Gamma_{h}} w h d \\\\Gamma \\\\end{aligned} $$ (G) Find $v^{h} \\\\in \\\\delta^{h}$, such that $\\\\forall w^{h} \\\\in \\\\mathcal{V}^{h}$ $$ a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h},\\\\ell \\\\right)+\\\\left(w^{h}, h\\\\right)_{\\\\Gamma}-a\\\\left(w^{h}, g^{h}\\\\right) $$ \\\\footnotetext[20]{In defining $\\\\boldsymbol{P}$ we have added to the element contributions the term $\\\\boldsymbol{F}_{\\\\text {nodal}}$, which is a vector of nodal applied forces. The reason for this is that it is often easier in practice to directly input concentrated forces at nodes rather than go through the element-by-element form and assemble procedure. The expression for $F$ then emphasizes that both modes of constructing $F$ are to be accommodated in the computer implementation of problems of this type. } (M) $\\\\quad K d=\\\\boldsymbol{F}$, where $K=\\\\boldsymbol{A}_{e=1}^{n_{e l}}\\\\left(k^{e}\\\\right), \\\\quad \\\\boldsymbol{F}=\\\\boldsymbol{F}_{\\\\text {nodal }}+A_{e=1}^{n_{e l}}\\\\left(f^{e}\\\\right)^{20}$\\\\\\\\ \\\\[ \\\\begin{aligned} k_{a b}^{e} & =\\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} B_{a}^{T} D B_{b} d \\\\Omega \\\\\\\\ f_{a}^{e} & =\\\\int_{\\\\Omega^{e}} N_{a} \\\\ell d \\\\Omega+\\\\int_{\\\\Gamma_{h}^{e}} N_{a} h d \\\\Gamma-\\\\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \\\\end{aligned} \\\\] Heat flux vector at a point: $q(x)=-D(x) \\\\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e}$', '\\\\[ \\\\begin{array}{rlr} u_{, x x}+\\\\ell=0 & & \\\\text { on } \\\\Omega=] 0,1[ \\\\tag{S}\\\\\\\\ u(1)=g & & \\\\left(\\\\Gamma_{g}=\\\\{1\\\\}\\\\right) \\\\\\\\ -u_{, x}(0)=h & & \\\\left(\\\\Gamma_{h}=\\\\{0\\\\}\\\\right) \\\\end{array} \\\\] (W) Find $u \\\\in \\\\delta$, such that $\\\\forall w \\\\in \\\\mathcal{V}$ $$ a(w, u)=(w, \\\\ell)+w(0) h $$ where $$ \\\\begin{aligned} a(w, u) & =\\\\int_{0}^{1} w_{, x} u_{, x} d x \\\\\\\\ (w, \\\\ell) & =\\\\int_{0}^{1} w \\\\ell d x \\\\end{aligned} $$ (G) Find $v^{h} \\\\in \\\\mathcal{V}^{h}$, such that $\\\\forall w^{h} \\\\in \\\\mathcal{V}^{h}$ $$ a\\\\left(w^{h}, v^{h}\\\\right)=\\\\left(w^{h}, \\\\ell\\\\right)+w^{h}(0) h-a\\\\left(w^{h}, g^{h}\\\\right) $$ (M) $\\\\quad K d=\\\\boldsymbol{F}$, where $K=\\\\boldsymbol{A}_{e=1}^{n_{e l}}\\\\left(k^{e}\\\\right), \\\\quad \\\\boldsymbol{F}=\\\\boldsymbol{F}_{\\\\text {nodal }}+A_{e=1}^{n_{e l}}\\\\left(f^{e}\\\\right)^{20}$\\\\\\\\ $$ \\\\begin{aligned} & k_{a b}^{e}=\\\\int_{\\\\Omega^{e}} N_{a, x} N_{b, x} d x \\\\\\\\ & f_{a}^{e}=\\\\int_{\\\\mathbf{\\\\Omega}^{e}} N_{a} \\\\ell d x+ \\\\begin{cases}k \\\\delta_{a 1} & e=1 \\\\\\\\ 0 & e=2, \\\\ldots, n_{e l}-1 \\\\\\\\ -g k_{2 a}^{e} & e=n_{el}\\\\end{cases} \\\\end{aligned} $$ \\\\subsection*{2.12 AXISYMMETRIC FORMULATIONS AND ADDITIONAL EXERCISES} Axisymmetric formulations are expressed in terms of cylindrical coordinates: $$ \\\\begin{aligned} & x_{1}=r=\\\\text { the radial coordinate } \\\\\\\\ & x_{2}=z=\\\\text { the axial coordinate } \\\\\\\\ & x_{3}=\\\\theta=\\\\text { the circumferential coordinate } \\\\end{aligned} $$ The basic hypothesis of axisymmetry is that all functions under consideration are independent of $\\\\theta$. That is, they are functions of $r$ and $z$ only. Thus three-dimensional problem classes are reduced to two-dimensional ones.', 'The axisymmetric formulation for heat conduction is almost identical to the twodimensional Cartesian case considered previously. The only difference is that a factor of $2 \\\\pi r$ needs to be included in each integrand of the variational equation to account for the correct volumetric weighting (e.g., $d \\\\Omega=2 \\\\pi r d r d z$ replaces $d \\\\Omega=$ $d x_{1} d x_{2}$ ). Since the constant $2 \\\\pi$ is common to all terms, it may be cancelled throughout if desired.', 'The displacement components in cylindrical coordinates are: $$ \\\\begin{aligned} & u_{1}=u_{r}=\\\\text { the radial displacement } \\\\\\\\ & u_{2}=u_{2}=\\\\text { the axial displacement } \\\\\\\\ & u_{3}=u_{\\\\theta}=\\\\text { the circumferential displacement } \\\\end{aligned} $$ In addition to the basic hypothesis of axisymmetry, we further assume that $u_{\\\\theta}=0$, and thus \\\\begin{equation*} \\\\epsilon_{r \\\\theta}=\\\\epsilon_{z \\\\theta}=0 \\\\tag{2.12.1} \\\\end{equation*} Note that $\\\\epsilon_{\\\\theta \\\\theta}=u_{r} / r$ and therefore it is generally not zero. The constitutive moduli are assumed to be such that the preceding kinematical hypotheses result in \\\\begin{equation*} \\\\sigma_{r \\\\theta}=\\\\sigma_{z \\\\theta}=0 \\\\tag{2.12.2} \\\\end{equation*} The preceding assumptions lead to what is called the torsionless axisymmetric case. This formulation is similar to but somewhat more complicated than the twodimensional cases of plane strain and plane stress. Here there are four nonzero components of stress and strain: \\\\begin{align*} & \\\\boldsymbol{\\\\sigma}=\\\\left\\\\{\\\\begin{array}{l} \\\\sigma_{11} \\\\\\\\ \\\\sigma_{22} \\\\\\\\ \\\\sigma_{12} \\\\\\\\ \\\\sigma_{33} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} \\\\sigma_{r r} \\\\\\\\ \\\\sigma_{z z} \\\\\\\\ \\\\sigma_{r z} \\\\\\\\ \\\\sigma_{\\\\theta \\\\theta} \\\\end{array}\\\\right\\\\} \\\\tag{2.12.3}\\\\\\\\ & \\\\epsilon=\\\\left\\\\{\\\\begin{array}{c} \\\\epsilon_{11} \\\\\\\\ \\\\epsilon_{22} \\\\\\\\ 2 \\\\epsilon_{12} \\\\\\\\ \\\\epsilon_{33} \\\\end{array}\\\\right\\\\}=\\\\left\\\\{\\\\begin{array}{c} \\\\epsilon_{r r} \\\\\\\\ \\\\epsilon_{z z} \\\\\\\\ 2 \\\\epsilon_{r z} \\\\\\\\ \\\\epsilon_{\\\\theta \\\\theta} \\\\end{array}\\\\right\\\\} \\\\tag{2.12.4} \\\\end{align*} The ordering emanates from the following generalization of Table 2.7.1: \\\\begin{table}[ht] \\\\centering \\\\begin{tabular}{|c|c|c|c|} \\\\hline \\\\diagbox[width=2em]{\\\\textit{J}}{\\\\textit{I}} & \\\\diagbox[width=2em]{\\\\textit{k}}{\\\\textit{i}} & \\\\diagbox[width=2em]{\\\\textit{l}}{\\\\textit{j}} \\\\\\\\ \\\\hline 1 & 1 & 1 \\\\\\\\ 3 & 1 & 2 \\\\\\\\ 3 & 2 & 1 \\\\\\\\ 2 & 2 & 2 \\\\\\\\ \\\\hline 4 & 3 & 3 \\\\\\\\ \\\\hline \\\\end{tabular} \\\\end{table} The $D$ array takes on the following form: \\\\[ D=\\\\left[D_{I J}\\\\right]=\\\\underbrace{\\\\left[\\\\begin{array}{ll} D_{33} & D_{3} \\\\tag{2.12.5}\\\\\\\\ D_{3}^{T} & D_{44} \\\\end{array}\\\\right]}_{4 \\\\times 4} \\\\] \\\\[ D_{33}=\\\\left[\\\\begin{array}{lll} D_{11} & D_{12} & D_{13} \\\\tag{2.12.6}\\\\\\\\ & D_{22} & D_{23} \\\\\\\\ \\\\text { symmetric } & D_{33} \\\\end{array}\\\\right] \\\\] \\\\[ D_{3}=\\\\left\\\\{\\\\begin{array}{l} D_{14} \\\\tag{2.12.7}\\\\\\\\ D_{24} \\\\\\\\ D_{34} \\\\end{array}\\\\right\\\\} \\\\] where $D_{I J}=c_{i j k l}$, in which the indices are related by the table. The $\\\\boldsymbol{B}_{a}$-matrix takes on the form \\\\[ \\\\boldsymbol{B}_{a}=\\\\left[\\\\begin{array}{cc} N_{a, 1} & 0 \\\\tag{2.12.8}\\\\\\\\ 0 & N_{a, 2} \\\\\\\\ N_{a, 2} & N_{a, 1} \\\\\\\\ \\\\hline \\\\frac{N_{a}}{r} & 0 \\\\end{array}\\\\right] \\\\] Again, a factor of $2 \\\\pi r$ needs to be included in all integrands.', 'The plane strain case may be obtained from the axisymmetric formulation by\\\\\\\\ i. Ignoring the $2 \\\\pi$ factors; and\\\\\\\\ ii. Ignoring the fourth row of $B_{a}$ and the fourth row and column of $\\\\boldsymbol{D}$. Furthermore, the plane stress case may be similarly obtained if, in addition, $\\\\boldsymbol{D}_{33}$ is replaced by \\\\begin{equation*} D_{33}-D_{3} D_{44}^{-1} D_{3}^{T} \\\\tag{2.12.9} \\\\end{equation*} which directly follows from the plane stress condition, $\\\\sigma_{33}=0$. (References [6] and [7] may be consulted for further elaboration on the physical ideas.) Sometimes (2.12.9) is referred to as the statically condensed elastic coefficient matrix. Consequently, in programming the axisymmetric case, for a small amount of additional effort both plane strain and plane stress may also be included. \\\\subsection*{Exercise 1.} Under the assumption of isotropy, show that $D_{33}$ is the same as the $\\\\boldsymbol{D}$-matrix in (2.7.34). Furthermore, show that $D_{44}=\\\\lambda+2 \\\\mu$ and \\\\[ D_{3}=\\\\left\\\\{\\\\begin{array}{l} \\\\lambda \\\\tag{2.12.10}\\\\\\\\ \\\\lambda \\\\\\\\ 0 \\\\end{array}\\\\right\\\\} \\\\] \\\\subsection*{Exercise 2.} Verify that for the isotropic case, (2.12.9) achieves a similar end to the procedure described in Remark 9 of Sec. 2.7. \\\\subsection*{Exercise 3.} Consider the one-dimensional model problem discussed previously. Obtain exact expressions for $f=\\\\left\\\\{f_{a}^{e}\\\\right\\\\}, a=1,2$, for the following cases (ignore $g$ and $h$ contributions):\\\\\\\\ i. $\\\\boldsymbol{\\\\ell}=$ constant.\\\\\\\\ ii. $\\\\ell=\\\\delta(x-\\\\bar{x})$, the delta function, where $x_{1}^{e} \\\\leq \\\\bar{x} \\\\leq x_{2}^{e}$. Specialize for the cases $\\\\bar{x}=x_{b}^{e}$ and $\\\\bar{x}=\\\\left(x_{1}^{e}+x_{2}^{e}\\\\right) / 2$. Solution $$ \\\\begin{aligned} & \\\\text { i. } f_{a}^{e}=\\\\ell \\\\int_{x_{1}^{e}}^{x^ e_{2}} N_{a}(x) d x=\\\\frac{\\\\ell h^{e}}{2} \\\\underbrace{\\\\int_{-1}^{+1} N_{a}(\\\\xi) d \\\\xi}_{1} \\\\\\\\ & f^{e}=\\\\frac{\\\\ell h^{e}}{2}\\\\left\\\\{\\\\begin{array}{l} 1 \\\\\\\\ 1 \\\\end{array}\\\\right\\\\} \\\\end{aligned} $$ ii. $f_{a}^{e}=\\\\int_{x_{1}^{e}}^{x_{2}^{e}} N_{a}(x) \\\\delta(x-\\\\bar{x}) d x=N_{a}(\\\\bar{x})$ For $x=x_{b}^{e}$, $$ \\\\begin{aligned} f_{a}^{e} & =N_{a}(\\\\bar{x})=N_{a}\\\\left(x_{b}^{e}\\\\right)=\\\\delta_{a b} \\\\quad \\\\text { (Kronecker delta) } \\\\\\\\ f^{e} & =\\\\left\\\\{\\\\begin{array}{l} \\\\delta_{1 b} \\\\\\\\ \\\\delta_{2 b} \\\\end{array}\\\\right\\\\} \\\\end{aligned} $$ For $\\\\bar{x}=\\\\left(x_{1}^{e}+x_{2}^{e}\\\\right) / 2$, $$ f_{a}^{e}=N_{a}(\\\\bar{x})=N_{a}\\\\left(\\\\frac{x_{1}^{e}+x_{2}^{e}}{2}\\\\right)=\\\\frac{1}{2} $$ Therefore, $$ f^{e}=\\\\frac{1}{2}\\\\left\\\\{\\\\begin{array}{l} 1 \\\\\\\\ 1 \\\\end{array}\\\\right\\\\} $$ \\\\subsection*{Exercise 4.} Consider the boundary-value problem for classical linear elastostatics discussed previously. In the linearized theory of small displacements superposed upon large, the stiffness term in the variational equation, $$ \\\\int_{\\\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\\\Omega $$ is replaced by $$ \\\\int_{\\\\Omega} w_{i, j} d_{i j k l} u_{k, l} d \\\\Omega $$ where $$ \\\\begin{aligned} d_{i j k l} & =c_{i j k l}+\\\\delta_{i k} \\\\sigma_{jl}^{0} \\\\\\\\ \\\\sigma_{j l}^{0} & =\\\\sigma_{lj}^{0} \\\\end{aligned} $$ and the $\\\\sigma_{j l}^{0}$ \\'s (i.e., initial stresses) are given functions of $\\\\boldsymbol{x} \\\\in \\\\boldsymbol{\\\\Omega}$. It follows from the symmetries of $c_{i j k l}$ and $\\\\sigma_{jl}^{0}$ that $$ d_{i j k l}=d_{k li j} $$ Assume $\\\\boldsymbol{n}_{s d}=2$. An index-free formulation of the stiffness term is given by $$ \\\\int_{\\\\Omega}\\\\left\\\\{\\\\begin{array}{c} w_{1,1} \\\\\\\\ w_{2,2} \\\\\\\\ w_{1,2}+w_{2,1} \\\\\\\\ w_{1,2}-w_{2,1} \\\\end{array}\\\\right\\\\}^{T} \\\\quad \\\\underbrace{D}_{4 \\\\times 4}\\\\left\\\\{\\\\begin{array}{c} u_{1,1} \\\\\\\\ u_{2,2} \\\\\\\\ u_{1,2}+u_{2,1} \\\\\\\\ u_{1,2}-u_{2,1} \\\\end{array}\\\\right\\\\} d \\\\Omega $$ which leads to the following definition of the element stiffness matrix: $$ k_{p q}^{e}=e_{i}^{T} \\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} \\\\underbrace{B_{a}^{T}}_{2 \\\\times 4} \\\\underbrace{D}_{4 \\\\times 4} \\\\underbrace{B_{b}}_{4 \\\\times 2} d \\\\Omega e_{j} $$ Set up $B_{a}$ in terms of the shape function $N_{a}$. Define the components of $D$ in terms of the $d_{i j kl}$ \\'s. (The $\\\\sigma_{j l}^{0}$-contribution to the stiffness is sometimes called the initial-stress stiffiness matrix. It is important to account for it in the solution of many nonlinear problems.) \\\\subsection*{Exercise 5.}Let $\\\\boldsymbol{\\\\Omega}$ be a region in $\\\\mathbb{R}^{2}$ and let its boundary $\\\\Gamma=\\\\overline{\\\\Gamma_{1} \\\\cup \\\\Gamma_{2} \\\\cup \\\\Gamma_{3} \\\\cup \\\\Gamma_{4}}$ where $\\\\Gamma_{1}, \\\\ldots, \\\\Gamma_{4}$ are nonoverlapping subregions of $\\\\Gamma$. Let $n$ be the unit outward normal vector to $\\\\Gamma$ such that $\\\\boldsymbol{s}$ and $\\\\boldsymbol{n}$ form a right-hand rule basis; see Fig. 2.12.1. Consider the following boundary-value problem in classical linear elastostatics: Given $\\\\ell_i: \\\\Omega \\\\rightarrow \\\\mathbb{R} ; g_{i}: \\\\Gamma_{1} \\\\rightarrow \\\\mathbb{R} ; h_{i}: \\\\Gamma_{2} \\\\rightarrow \\\\mathbb{R} ; g_{n}$ and $h_{3}: \\\\Gamma_{3} \\\\rightarrow \\\\mathbb{R}$; and $g_{s}$ and $h_{n}: \\\\Gamma_{4} \\\\rightarrow \\\\mathbb{R}$; find $u_{i}: \\\\overline{\\\\boldsymbol{\\\\Omega}} \\\\rightarrow \\\\mathbf{R}$ such that $$ \\\\begin{aligned} & \\\\sigma_{ij, j}+f_{i}=0 \\\\quad \\\\text { in } \\\\Omega \\\\\\\\ & u_{i}=q_{i} \\\\quad \\\\text { on } \\\\Gamma_{1} \\\\\\\\ & \\\\sigma_{i j} n_{j}=h_{i} \\\\quad \\\\text { on } \\\\Gamma_{2} \\\\\\\\ & \\\\left.\\\\left.\\\\begin{array}{rl} u_{i} n_{i} & =g_{n} \\\\\\\\ \\\\sigma_{y} n_{j} s_{i} & =h_{s} \\\\end{array}\\\\right\\\\} \\\\quad \\\\text { on } \\\\Gamma_{3} \\\\quad \\\\text { ( } \\\\begin{array}{l} \\\\text { normal displacement } \\\\\\\\ \\\\text { tangential traction } \\\\end{array}\\\\right) \\\\\\\\ & \\\\left.\\\\begin{array}{rl} u_{i} s_{i} & =g_{s} \\\\\\\\ \\\\sigma_{i j} n_{j} n_{i} & =h_{n} \\\\end{array}\\\\right\\\\} \\\\quad \\\\text { on } \\\\Gamma_{4} \\\\quad\\\\binom{\\\\text { tangential displacement }}{\\\\text { normal traction }} \\\\end{aligned} $$ where $\\\\sigma_{ij}=c_{ijkl} u_{(k, l)}$. Establish a weak formulation for this problem in which all \" $g$-type\" boundary conditions are essential and all \"h-type\" boundary conditions are natural. State all requirements on the spaces $\\\\delta$ and $\\\\mathcal{V}$. Hint: $w=w_{n} n+w_{s} s$; i.e., $w_{i}=w_{n} n_{i}+w_{s} s_{i}$.\\\\\\\\ \\\\includegraphics[max width=\\\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-49} Figure 2.12.1 \\\\subsection*{Exercise 6.} In practice, it is often useful to generalize the constitutive equation of classical elasticity to the form \\\\begin{equation*} \\\\sigma_{i j}=c_{i j k l}\\\\left(\\\\epsilon_{k l}-\\\\epsilon_{k l}^{0}\\\\right)+\\\\sigma_{i j}^{0} \\\\tag{2.12.11} \\\\end{equation*} where $\\\\epsilon_{i j}^{0}$ and $\\\\sigma_{i j}^{0}$ are the initial strain and initial stress, both given functions of $x$. The initial strain term may be used to represent thermal expansion effects by way of \\\\begin{equation*} \\\\epsilon_{k l}^{0}=-\\\\theta c_{k l} \\\\tag{2.12.12} \\\\end{equation*} where $\\\\theta$ is the temperature and the $c_{k i}$ \\'s are the thermal expansion coefficients (both given functions). Clearly, (2.12.11) will in no way change the stiffness matrix. However there will be additional contributions to $f_{p}^{e}$. Generalize the definition of $f_{p}^{e}$ to account for these additional terms.\\\\\\\\ \\\\textbf{Solution} We begin with the weak form $$ \\\\int_{\\\\Omega} w_{(i,j)} \\\\sigma_{i j} d \\\\Omega=\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega+\\\\sum_{i=1}^{n_{sd}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right) $$ Substituting (2.12.11) and (2.12.12) into the weak form leads to $$ \\\\begin{aligned} & \\\\int_{\\\\Omega} w_{(i,j)} c_{ijkl} \\\\epsilon_{k l} d \\\\Omega=\\\\int_{\\\\Omega} w_{i} \\\\ell_{i} d \\\\Omega+\\\\sum_{i=1}^{n_{sd}}\\\\left(\\\\int_{\\\\Gamma_{h_{i}}} w_{i} h_{i} d \\\\Gamma\\\\right) \\\\end{aligned} $$ $$ \\\\begin{aligned} & \\\\text { to right-hand side: } \\\\\\\\ & +\\\\int_{\\\\Omega} w_{(i, j)} c_{i jk l} \\\\epsilon_{k l}^{0} d \\\\Omega \\\\\\\\ & -\\\\int_{\\\\boldsymbol{\\\\Omega}} w_{(i,j)} \\\\sigma_{i j}^{0} d \\\\boldsymbol{\\\\Omega} \\\\end{aligned} $$ from which the additional terms in $f_{p}^{e}$ may be deduced: $$ f_{p}^{\\\\varepsilon}=\\\\cdots+e_{i}^{T} \\\\int_{\\\\boldsymbol{\\\\Omega}^{e}} B_{a}^{T} D \\\\theta c d \\\\Omega-e_{i}^{T} \\\\int_{\\\\Omega^{e}} B_{a}^{T} \\\\sigma^{0} d \\\\Omega $$ where $$ \\\\begin{aligned} c= & \\\\left\\\\{\\\\begin{array}{c} c_{11} \\\\\\\\ c_{22} \\\\\\\\ 2 c_{12} \\\\end{array}\\\\right\\\\} ; \\\\quad \\\\sigma^{0}=\\\\left\\\\{\\\\begin{array}{l} \\\\sigma_{11}^{0} \\\\\\\\ \\\\sigma_{22}^{0} \\\\\\\\ \\\\sigma_{12}^{0} \\\\end{array}\\\\right\\\\} ; \\\\ldots \\\\\\\\ & \\\\\\\\ & \\\\text { Without loss of generality, we } \\\\\\\\ & \\\\text { may assume symmetry, i.e., } c_{12}+c_{21}=2 c_{12} \\\\end{aligned} $$ \\\\subsection*{Exercise 7.} Consider the following boundary-value problem: $$ \\\\begin{aligned} u_{,xx}-p(p-1) x^{p-2} & =0, \\\\quad 0<x<1 \\\\\\\\ -u_{, x}(0) & =0 \\\\\\\\ u(1) & =1 \\\\end{aligned} $$ where $p$ is a given constant.\\\\\\\\ i. Obtain the exact solution to this problem for $\\\\boldsymbol{p}=5$. Sketch.\\\\\\\\ ii. State the weak formulation of the problem.\\\\\\\\ iii. State the Galerkin formulation.\\\\\\\\ iv. State the matrix formulation.\\\\\\\\ v. Solve the matrix problem assuming $p=5$ and using the piecewise linear finite element space for the following cases:\\\\\\\\ a. one element\\\\\\\\ b. two equal-length elements\\\\\\\\ vi. Compare the exact value of $u_{, x}(1)$ with the approximate values computed in part v. Explain why it is impossible for these results to compare favorably. \\\\subsection*{Exercise 8.} In heat conduction, it is often of interest to accurately calculate the boundary heat flux over a portion of the boundary where temperature is specified. Suppose we use the usual Galerkin finite element formulation to calculate the temperature. However, instead of calculating the heat flux in the usual way (i.e., by differentiating the temperature), we introduce a post-processing which derives from the following weak formulation:\\\\\\\\ Find $u \\\\in \\\\delta$ and $h \\\\in L_{2}\\\\left(\\\\Gamma_{g}\\\\right)$ such that for all $w \\\\in \\\\mathcal{V}$, $$ -\\\\int_{\\\\Omega} w_{,i}q_{i} d \\\\Omega=\\\\int_{\\\\Omega} w \\\\ell d \\\\Omega+\\\\int_{\\\\Gamma_{h}} w h d \\\\Gamma+\\\\int_{\\\\Gamma_{g}} w h d \\\\Gamma $$ where $h$ is the unknown heat flux on $\\\\Gamma_{g}$\\\\\\\\ (Note: In this formulation, it is not assumed that $w=0$ on $\\\\Gamma_{g}!$ )\\\\\\\\ i. Show, in addition to the usual differential equations and boundary conditions, that $$ h=-q_{i} n_{i} \\\\text { on } \\\\Gamma_{g} $$ arises naturally from the new weak formulation.\\\\\\\\ ii. State the Galerkin and matrix formulations corresponding to the new weak formulation assuming $h$ is approximated in the usual way, namely $$ h^{h}(x)=\\\\sum_{A \\\\in \\\\eta_{g}} N_{A}(x) h_{A} $$ (Hint: The equations governing the temperature are unchanged.)\\\\\\\\ iii. Specialize this formulation to the one-dimensional problem described in Exercise 7 and calculate the boundary flux at $x=1$ by the new procedure (cf. parts $v$ and vi of Exercise 7).\\\\\\\\ (Hint: The new method should produce exact results for these cases.)\\\\\\\\ iv. Develop a counterpart of the new formulation for elasticity. That is, introduce the $i$th component of traction as an independent unknown on $\\\\Gamma_{g_i}$ and carefully state the weak formulation.\\\\\\\\ v. Prove that the new method is exact for the one-dimensional model problem of Chapter 1.', '\\\\begin{enumerate} \\\\item J. E. Marsden and A. J. Tromba, Vector Calculus. San Francisco: W. H. Freeman, 1976. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{1} \\\\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N.J.: Prentice-Hall, 1973. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{2} \\\\item G. Duvaut and J. L. Lions, Les Inquations en Mcanique et en Physique. Paris: Dunod, 1972. \\\\item G. Fichera, \"Existence Theorems in Elasticity,\" in Handbuch der Physik, Volume V1a/2, Mechanics of Solids II, ed. C. Truesdell. New York: Springer-Verlag, 1972. \\\\item M. Gurtin, \"The Linear Theory of Elasticity,\" in Handbuch der Physik, Volume V1a/2, Mechanics of Solids II, ed. C. Truesdell. New York: Springer-Verlag, 1972. \\\\item I. S. Sokolnikoff, Mathematical Theory of Elasticity (2nd ed.). New York: McGraw-Hill, 1956. \\\\item S. Timoshenko and J. N. Goodier, Theory of Elasticity (3rd ed.). New York: McGraw-Hill, 1969. \\\\end{enumerate}', '\\\\begin{enumerate} \\\\setcounter{enumi}{7} \\\\item N. E. Gibbs, W. G. Poole, Jr., and P. K. Stockmeyer, \"An Algorithm for Reducing the Bandwidth and Profile of a Sparse Matrix,\" SIAM Journal of Numerical Analysis, 13 (1976), 236-250. \\\\end{enumerate}']\n",
      "chunk word length: 1, chunk char length: 10, chunk = \\maketitle\n",
      "chunk word length: 525, chunk char length: 3363, chunk = The main constituents of a finite element method for the solution of a boundary-value problem are\\\\ i. The variational or weak statement of the problem; and\\\\ ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple example.\\\\ Suppose we want to solve the following differential equation for $u$ : \\begin{equation*} u_{, x x}+f=0 \\tag{1.1.1} \\end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \\begin{equation*} f: [0,1] \\to \\mathbb{R} \\tag{1.1.2} \\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \\leq x \\leq 1$ ) and $\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\in[0,1], f(x) \\in \\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\textbf{\\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\[ \\boldsymbol{\\Omega}=] 0,1[ \\quad \\text { (open) } \\tag{1.1.3} \\] \\[ \\overline{\\boldsymbol{\\Omega}}=[0,1] \\quad \\text { (closed) } \\tag{1.1.4} \\] See Fig. 1.1.1. \\begin{figure}[h] \\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\vspace{0.5em} \\textbf{Figure 1.1.1} \\end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work.\n",
      "chunk word length: 299, chunk char length: 1996, chunk = A boundary-value problem for (1.1.1) involves imposing \\textbf{\\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \\begin{align} u(1) &= g \\tag{1.2.1} \\\\ -u_{, x}(0) &= h \\tag{1.2.2} \\end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \\textbf{\\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \\[ \\text{(S)} \\quad \\left\\{ \\parbox{0.8\\textwidth}{ \\text{Given } $f:\\overline{\\boldsymbol{\\Omega}} \\to \\mathbb{R}$ \\text{ and constants } $g$ \\text{ and } $h$, \\text{ find } $u:\\overline{\\boldsymbol{\\Omega}} \\to \\mathbb{R}$, \\text{ such that:} \\begin{align*} u_{,xx} + f &= 0 \\quad \\text{on } \\Omega \\\\ u(1) &= g \\\\ -u_{,x}(0) &= h \\end{align*} } \\right. \\] When we write $u_{, x x}+f=0$ on $\\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \\in \\Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \\begin{equation*} u(x)=g+(1-x) h+\\int_{x}^{1}\\left\\{\\int_{0}^{y} f(z) d z\\right\\} d y \\tag{1.2.3} \\end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section.\n",
      "chunk word length: 445, chunk char length: 3162, chunk = To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \\begin{equation*} \\int_{0}^{1}(u_{,x})^{2} d x<\\infty \\tag{1.3.1} \\end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\\boldsymbol{u} \\in \\boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \\in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \\begin{equation*} \\mathcal{S}=\\left\\{u \\mid u \\in H^{1}, u(1)=g\\right\\} \\quad \\text { (trial solutions) } \\tag{1.3.2} \\end{equation*} The fact that $\\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \\textbf{\\textit{weighting functions}}, or \\textbf{\\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\\mathcal{U}$ and defined by\\\\ \\begin{equation*} \\mathcal{U} = \\{w \\mid w \\in H^{1}, w(1)=0\\} \\quad \\text{(weighting functions)} \\tag{1.3.3} \\end{equation*} It simplifies matters somewhat to continue to think of $f: \\Omega \\rightarrow \\mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \\[ (\\mathcal{W}) \\quad \\left\\{ \\parbox{0.8\\textwidth}{ \\text{Given } $f,$q and $h$,\\text{ as before. Find } $u \\in \\mathfrak{f}$, \\text{ such that for all} \\mbox{$w \\in \\mathcal{U}$} \\begin{align*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.3.4} \\end{align*} } \\right. \\] Formulations of this type are often called \\textbf{\\textit{virtual work}}, or \\textbf{\\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \\textbf{\\textit{virtual displacements}}. Equation (1.3.4) is called the \\textbf{\\textit{variational equation}}, or (especially in mechanics) the \\textbf{\\textit{equation of virtual work.}} The solution of $(W)$ is called the \\textbf{\\textit{weak}}, or \\textbf{\\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider.\n",
      "chunk word length: 571, chunk char length: 3826, chunk = Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. \"Proofs\" of this kind are sometimes euphemistically referred to as \"formal proofs.\" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall \"prove\" the following. \\subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \\subsection*{Formal Proof} \\bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \\begin{equation*} 0=-\\int_{0}^{1} w(u_{, x x}+f) d x \\tag{1.4.1} \\end{equation*} for any $w \\in \\mathcal{U}$. Integrating (1.4.1) by parts results in \\begin{equation*} 0=\\int_{0}^{1} w_{, x} u_{, x} d x-\\int_{0}^{1} w f d x-\\left.w u_{, x}\\right|_{0} ^{1} \\tag{1.4.2} \\end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \\begin{equation*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.4.3} \\end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \\in \\mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\\\ \\\\ b. Now $u$ is assumed to be a weak solution. Thus $u \\in \\mathcal{S}$; consequently $u(1)=g$, and $$ \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h $$ for all $w \\in \\mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \\begin{equation*} 0=\\int_{0}^{1} w\\left(u_{, x x}+f\\right) d x+w(0)\\left[u_{, x}(0)+h\\right]. \\tag{1.4.4} \\end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\\\ i. $u_{, x x}+f=0$ on $\\Omega$; and\\\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\\boldsymbol{w}$ in (1.4.4) by \\begin{equation*} w=\\phi\\left(u_{, x x}+f\\right) \\tag{1.4.5} \\end{equation*} where $\\phi$ is smooth; $\\phi(x)>0$ for all $x \\in \\Omega=] 0,1[$; and $\\phi(0)=\\phi(1)=0$. For example, we can take $\\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \\in \\mathcal{U}$, so (1.4.5) defines a\\\\ \\begin{figure}[h] \\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \\vspace{0.5em} \\textbf{Figure 1.4.1} \\end{figure} \\footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\\\ \\begin{equation*} 0=\\int_{0}^{1} \\phi \\underbrace{\\left(u_{, x x}+f\\right)^{2}}_{\\geq 0} d x+0 \\tag{1.4.6} \\end{equation*} Since $\\phi>0$ on $\\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \\begin{equation*} 0=w(0)\\left[u_{, x}(0)+h\\right] \\tag{1.4.7} \\end{equation*} That $w \\in \\mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \\neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition.\n",
      "chunk word length: 468, chunk char length: 3228, chunk = \\begin{enumerate} \\item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \\textbf{\\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \\textbf{\\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \\item The method used to prove part (b) of the proposition goes under the name of the \\textbf{\\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \\end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \\begin{align*} a(w, u) & =\\int_{0}^{1} w_{, x} u_{, x} d x \\tag{1.4.8}\\\\ (w, f) & =\\int_{0}^{1} w f d x \\tag{1.4.9} \\end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \\begin{equation*} a(w, u)=(w, f)+w(0) h \\tag{1.4.10} \\end{equation*} Here, $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ are examples of \\textbf{\\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \\begin{align*} a(u, v) & =a(v, u) \\tag{1.4.11}\\\\ (u, v) & =(v, u) \\tag{1.4.12} \\end{align*} Bilinearity means linearity in each \"slot\"; for example, \\begin{align*} a\\left(c_{1} u+c_{2} v, w\\right) & =c_{1} a(u, w)+c_{2} a(v, w) \\tag{1.4.13}\\\\ \\left(c_{1} u+c_{2} v, w\\right) & =c_{1}(u, w)+c_{2}(v, w) \\tag{1.4.14} \\end{align*} \\\\ Exercise 1. Use the definitions of $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ to verify the properties of symmetry and bilinearity.\\\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability.\n",
      "chunk word length: 342, chunk char length: 2364, chunk = We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\\mathcal{U}^{h}$ with a \\textit{\\textbf{mesh}}, or \\textit{\\textbf{discretization}}, of the domain $\\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\\mathcal{U}^{h}$ as being subsets of $f$ and $\\mathcal{U}$, respectively. This is written as \\begin{align*} f^{h} \\subset f & \\text { (i.e., if } \\left.u^{h} \\in f^{h}, \\text { then } u^{h} \\in f\\right) \\tag{1.5.1}\\\\ \\mathcal{U}^{h} \\subset \\mathcal{U} & \\text { (i.e., if } w^{h} \\in \\mathcal{U}^{h}, \\text { then } w^{h} \\in\\mathcal{U}) \\tag{1.5.2} \\end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \\in f^{h}$ and $w^{h} \\in \\mathcal{U}^{h}$, then \\begin{align*} & u^{h}(1)=q \\tag{1.5.3}\\\\ & w^{h}(1)=0 \\tag{1.5.4} \\end{align*} The collections, $f, \\mathcal{U}, d^{h}$, and $\\mathcal{U}^{h}$, are often referred to as \\textit{\\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\\mathcal{U}$. Both $\\mathcal{U}$ and $\\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \\notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$.\n",
      "chunk word length: 500, chunk char length: 3655, chunk = Assume the collection $\\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \\in \\mathcal{U}^{h}$, we construct a function $u^{h} \\in f^{h}$ by \\begin{equation*} u^{h}=v^{h}+g^{h} \\tag{1.5.5} \\end{equation*} where $g^{\\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \\begin{equation*} g^{h}(1)=g \\tag{1.5.6} \\end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \\begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \\tag{1.5.7}\\\\ & =0+g \\end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \\in \\mathcal{W}^{h}$ and $u^{h} \\in \\delta^{h}$ \\begin{equation*} a\\left(w^{h}, u^{h}\\right)=\\left(w^{h}, A\\right)+w^{h}(0) h \\tag{1.5.8} \\end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\\boldsymbol{h}}$. \\footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology \"variational crimes\" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many \"variational crimes\" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\\cdot, \\cdot)$ enables us to write \\begin{equation*} a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, f\\right)+w^{h}(0) h-a\\left(w^{h}, g^{h}\\right) \\tag{1.5.9} \\end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\\\ \\[ \\text{(G)} \\quad \\left\\{ \\parbox{0.8\\textwidth}{ \\text { Given } $f, q$, \\text { and } $h$, \\text {, as before, find } $u^{h}=v^{h}+q^{h}$ \\text {, where } $v^{h} \\in \\mathcal{U}^{h}$ \\text { such that for all } $w^{h} \\in \\mathcal{U}^{h}$ \\\\ \\begin{align*} a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, f\\right)+w^{h}(0) h-a\\left(w^{h}, g^{h}\\right) \\end{align*} } \\right. \\] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\\mathcal{U}^\\text{h}$. To make matters more specific, $g^{h}$ and $\\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \\textit{\\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\\mathcal{U^\\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \\textit{\\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \\textit{\\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4].\n",
      "chunk word length: 686, chunk char length: 5347, chunk = The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\\mathcal{U}^{h}$. Let $\\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \\bar{\\Omega} \\rightarrow \\mathbb{R}$, where $A=1,2, \\ldots, n$. By this we mean that if $w^{h} \\in \\mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \\ldots, n$, such that \\begin{align*} w^{h} & =\\sum_{A=1}^{n} c_{A} N_{A} \\\\ & =c_{1} N_{1}+c_{2} N_{2}+\\cdots+c_{n} N_{n} \\tag{1.6.1} \\end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \\begin{equation*} N_{A}(1)=0, \\quad A=1,2, \\ldots, n \\tag{1.6.2} \\end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \\bar{\\Omega} \\rightarrow \\mathbb{R}$, which has the property \\begin{equation*} N_{n+1}(1)=1 \\tag{1.6.3} \\end{equation*} (Note $N_{n+1} \\notin \\mathcal{U}^{h}$. ) Then $g^{h}$ is given by \\begin{equation*} g^{h}=g N_{n+1} \\tag{1.6.4} \\end{equation*} and thus \\begin{equation*} g^{h}(1)=g \\tag{1.6.5} \\end{equation*} With these definitions, a typical $u^{h} \\in f^{h}$ may be written as \\begin{align*} u^{h} & =v^{h}+g^{h} \\\\ & =\\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \\tag{1.6.6} \\end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \\begin{align*} a\\left(\\sum_{A=1}^{n} c_{A} N_{A}, \\sum_{B=1}^{n} d_{B} N_{B}\\right)=\\left(\\sum_{A=1}^{n} c_{A} N_{A}, f\\right)+ & {\\left[\\sum_{A=1}^{n} c_{A} N_{A}(0)\\right] h } \\\\ & -a\\left(\\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\\right) \\tag{1.6.7} \\end{align*} By using the bilinearity of $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot),(1.6 .7)$ becomes \\begin{equation*} 0=\\sum_{A=1}^{n} c_{A} G_{A} \\tag{1.6.8} \\end{equation*} where \\begin{equation*} G_{A}=\\sum_{B=1}^{n} a\\left(N_{A}, N_{B}\\right) d_{B}-\\left(N_{A}, f\\right)-N_{A}(0) h+a\\left(N_{A}, N_{n+1}\\right) q \\tag{1.6.9} \\end{equation*} Now the Galerkin equation is to hold for all $w^{h} \\in \\mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \\ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \\ldots, n$, must be identically zero, i.e., from (1.6.9) \\begin{equation*} \\sum_{B=1}^{n} a\\left(N_{A}, N_{B}\\right) d_{B}=\\left(N_{A}, f\\right)+N_{A}(0) h-a\\left(N_{A}, N_{n+1}\\right) g \\tag{1.6.10} \\end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \\begin{align*} K_{A B} & =a\\left(N_{A}, N_{B}\\right) \\tag{1.6.11}\\\\ F_{A} & =\\left(N_{A}, f\\right)+N_{A}(0) h-a\\left(N_{A}, N_{n+1}\\right) g \\tag{1.6.12} \\end{align*} Then (1.6.10) becomes \\begin{equation*} \\sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \\quad A=1,2, \\ldots, n \\tag{1.6.13} \\end{equation*} Further simplicity is gained by adopting a matrix notation. Let \\begin{align*} & \\boldsymbol{K}=\\left[K_{A B}\\right]=\\left[\\begin{array}{cccc} K_{11} & \\boldsymbol{K}_{12} & \\cdots & K_{1 n} \\\\ \\boldsymbol{K}_{21} & K_{22} & \\cdots & K_{2 n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ K_{n 1} & K_{n 2} & \\cdots & K_{n n} \\end{array}\\right] \\tag{1.6.14}\\\\ & \\boldsymbol{F}=\\left\\{F_{A}\\right\\}=\\left\\{\\begin{array}{c} F_{1} \\\\ F_{2} \\\\ \\vdots \\\\ F_{n} \\end{array}\\right\\} \\tag{1.6.15} \\end{align*} and \\[ d=\\left\\{d_{B}\\right\\}=\\left\\{\\begin{array}{c} d_{1} \\tag{1.6.16}\\\\ d_{2} \\\\ \\vdots \\\\ d_{n} \\end{array}\\right\\} \\] Now (1.6.13) may be written as \\begin{equation*} \\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F} \\tag{1.6.17} \\end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \\begin{aligned} \\boldsymbol{K} & =\\text { stiffness matrix } \\\\ \\boldsymbol{F} & =\\text { force vector } \\\\ \\boldsymbol{d} & =\\text { displacement vector } \\end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\\\ (M) $\\left\\{\\begin{array}{c}\\text { Given the coefficient matrix } \\boldsymbol{K} \\text { and vector } \\boldsymbol{F}, \\text { find } \\boldsymbol{d} \\text { such that } \\\\ \\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}\\end{array}\\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \\boldsymbol{F}$ (assuming the inverse of $\\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \\in \\bar{\\Omega}$ by employing (1.6.6), viz., \\begin{equation*} u^{h}(x)=\\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \\tag{1.6.18} \\end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$.\n",
      "chunk word length: 188, chunk char length: 1420, chunk = \\begin{enumerate} \\item The matrix $K$ is symmetric. This follows from the symmetry of $a(\\cdot, \\cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \\end{enumerate} \\begin{align*} K_{A B} & =a\\left(N_{A}, N_{B}\\right) \\\\ & =a\\left(N_{B}, N_{A}\\right) \\\\ & =K_{B A} \\tag{1.6.19} \\end{align*} In matrix notation \\begin{equation*} K=K^{\\boldsymbol{T}} \\tag{1.6.20} \\end{equation*} where the superscript $\\boldsymbol{T}$ denotes transpose. The symmetry of $\\boldsymbol{K}$ has important computational consequences.\\\\ \\\\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \\begin{equation*} (S) \\Leftrightarrow(W) \\approx(G) \\Leftrightarrow(M) \\tag{1.6.21} \\end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\\\ 3. It is sometimes convenient to write \\begin{equation*} u^{h}(x)=\\sum_{A=1}^{n+1} N_{A}(x) d_{A} \\tag{1.6.22} \\end{equation*} where $d_{n+1}=g$.\n",
      "chunk word length: 591, chunk char length: 4630, chunk = In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \\subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \\begin{align*} K & =\\left[K_{11}\\right]=K_{11} \\tag{1.7.1}\\\\ F & =\\left\\{F_{1}\\right\\}=F_{1} \\tag{1.7.2}\\\\ d & =\\left\\{d_{1}\\right\\}=d_{1} \\tag{1.7.3}\\\\ K_{11} & =a\\left(N_{1}, N_{1}\\right)=\\int_{0}^{1} \\underbrace{N_{1,x}}_{-1} \\underbrace{N_{1, x}}_{-1} d x=1 \\tag{1.7.4}\\\\ F_{1} & =\\left(N_{1}, f\\right)+N_{1}(0) h-a\\left(N_{1}, N_{2}\\right) q \\\\ & =\\int_{0}^{1}(1-x) f(x) d x+h-\\int_{0}^{1} \\underbrace{N_{1, x}}_{-1} \\underbrace{N_{2, x}}_{+1} d x g \\\\ & =\\int_{0}^{1}(1-x) f(x) d x+h+q \\tag{1.7.5}\\\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \\tag{1.7.6} \\end{align*} Consequently \\begin{equation*} u^{h}(x)=[\\underbrace{\\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \\tag{1.7.7} \\end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\\\ i. Let $f=0$. Then \\begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \\tag{1.7.8} \\end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \\neq 0$ ).\\\\ \\\\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \\begin{equation*} u_{\\text {part }}(x)=\\frac{p\\left(1-x^{2}\\right)}{2} \\tag{1.7.9} \\end{equation*} and \\begin{equation*} u_{\\text {part }}^{h}(x)=\\frac{p(1-x)}{2} \\tag{1.7.10} \\end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\\text {part, } x}^{h}$ is exact at $x=\\frac{1}{2}$. (It should be clear that it is impossible for $u_{\\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \\begin{equation*} u_{\\text {part }}(x)=\\frac{q\\left(1-x^{3}\\right)}{6} \\tag{1.7.11} \\end{equation*} and \\begin{equation*} u_{\\text {pata }}^{h}(x)=\\frac{q(1-x)}{6} \\tag{1.7.12} \\end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \\sqrt{3}$, at which $u_{\\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii).\n",
      "chunk word length: 806, chunk char length: 6345, chunk = In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \\begin{align*} & N_{1}(x)=\\left\\{\\begin{array}{cc} 1-2 x & 0 \\leq x \\leq \\frac{1}{2} \\\\ 0 & \\frac{1}{2} \\leq x \\leq 1 \\end{array}\\right. \\tag{1.7.6}\\\\ & N_{2}(x)=\\left\\{\\begin{array}{cc} 2 x & 0 \\leq x \\leq \\frac{1}{2} \\\\ 2(1-x) & \\frac{1}{2} \\leq x \\leq 1 \\end{array}\\right. \\tag{1.7.7}\\\\ & N_{3}(x)=\\left\\{\\begin{array}{cc} 0 & 0 \\leq x \\leq \\frac{1}{2} \\\\ 2 x-1 & \\frac{1}{2} \\leq x \\leq 1 \\end{array}\\right. \\tag{1.7.8} \\end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \\in \\mathcal{U}^{h}$ and $u^{h} \\in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \\begin{align*} & K=\\left[\\begin{array}{ll} K_{11} & K_{12} \\\\ K_{21} & K_{22} \\end{array}\\right] \\tag{1.7.9}\\\\ & F=\\left\\{\\begin{array}{l} F_{1} \\\\ F_{2} \\end{array}\\right\\} \\tag{1.7.10}\\\\ & d=\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\end{array}\\right\\} \\tag{1.7.11}\\\\ & K_{A B}=a\\left(N_{A}, N_{B}\\right)=\\int_{0}^{1} N_{A, x} N_{B, x} d x=\\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \\tag{1.7.12}\\\\ & K_{11}=2, \\quad K_{12}=K_{21}=-2, \\quad K_{22}=4 \\tag{1.7.13}\\\\ & K=2\\left[\\begin{array}{rr} 1 & -1 \\\\ -1 & 2 \\end{array}\\right] \\tag{1.7.14}\\\\ & F_{A}=\\left(N_{A}, f\\right)+N_{A}(0) h-a\\left(N_{A}, N_{3}\\right) g \\\\ &=\\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \\tag{1.7.15} \\end{align*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \\end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \\begin{align*} & F_{1}=\\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \\tag{1.7.16}\\\\ & F_{2}=2 \\int_{0}^{1 / 2} x f(x) d x+2 \\int_{1 / 2}^{1}(1-x) f(x) d x+2 g \\tag{1.7.17} \\end{align*} Note that due to the shape functions' discontinuities in slope at $x=\\frac{1}{2}$, it is convenient to express integrals over the subintervals $\\left[0, \\frac{1}{2}\\right]$ and $\\left[\\frac{1}{2}, 1\\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\\\ i. $f=0$. \\begin{align*} F & =\\left\\{\\begin{array}{c} h \\\\ 2 g \\end{array}\\right\\} \\tag{1.7.18}\\\\ d & =K^{-1} F \\end{align*} \\begin{align*} & =\\left[\\begin{array}{ll} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{array}\\right]\\left\\{\\begin{array}{l} h \\\\ 2 g \\end{array}\\right\\} \\\\ & =\\left\\{\\begin{array}{l} g+h \\\\ g+\\frac{h}{2} \\end{array}\\right\\} \\tag{1.7.19} \\end{align*} This results in \\begin{align*} u^{h} & =(g+h) N_{1}+\\left(g+\\frac{h}{2}\\right) N_{2}+g N_{3} \\\\ & =g\\left(N_{1}+N_{2}+N_{3}\\right)+h\\left(N_{1}+\\frac{N_{2}}{2}\\right) \\tag{1.7.20}\\\\ u^{h}(x) & =g+h(1-x) \\tag{1.7.21} \\end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\\\ ii. $f(x)=p=$ constant. \\begin{align*} & F_{1}=\\frac{p}{4}+h \\tag{1.7.22}\\\\ & F_{2}=\\frac{p}{2}+2 g \\tag{1.7.23}\\\\ & d=\\left[\\begin{array}{ll} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{array}\\right]\\left\\{\\begin{array}{l} \\frac{p}{4}+h \\\\ \\frac{p}{2}+2 g \\end{array}\\right\\}=\\left\\{\\begin{array}{l} \\frac{p}{2}+g+h \\\\ \\frac{3 p}{8}+g+\\frac{h}{2} \\end{array}\\right\\} \\tag{1.7.24} \\end{align*} The solution takes the form \\begin{align*} & u^{h}(x)=g+h(1-x)+u_{\\text {part }}^{h}(x) \\tag{1.7.25}\\\\ & u_{\\text {part }}^{h}=\\frac{p}{2} N_{1}+\\frac{3 p}{8} N_{2} \\tag{1.7.26} \\end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \\frac{1}{2}$ and 1 , and derivatives coincide at $x=\\frac{1}{4}$ and $\\frac{3}{4}$.\\\\ iii. $f(x)=q x, q=$ constant. \\begin{align*} & F_{1}=\\frac{q}{24}+h \\tag{1.7.27}\\\\ & F_{2}=\\frac{q}{4}+2 g \\tag{1.7.28} \\end{align*} \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \\[ d=\\left\\{\\begin{array}{l} \\frac{q}{6}+g+h \\tag{1.7.29}\\\\ \\frac{7 q}{48}+g+\\frac{h}{2} \\end{array}\\right\\} \\] Again $u^{h}$ may be expressed in the form (1.7.25), where \\begin{equation*} u_{\\text {part }}^{h}=\\frac{q}{6} N_{1}+\\frac{7 q}{48} N_{2} \\tag{1.7.30} \\end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \\frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details.\n",
      "chunk word length: 680, chunk char length: 5021, chunk = The examples of the preceding section employed definitions of $\\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\\left[x_{A}, x_{A+1}\\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \\ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\\max h_{A}, A=1,2, \\ldots, n$ ). The smaller $h$, the more \"refined\" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \\leq A \\leq n$ ) \\[ N_{A}(x)=\\left\\{\\begin{array}{cl} \\frac{\\left(x-x_{A-1}\\right)}{h_{A-1}}, & x_{A-1} \\leq x \\leq x_{A} \\tag{1.8.1}\\\\ \\frac{\\left(x_{A+1}-x\\right)}{h_{A}}, & x_{A} \\leq x \\leq x_{A+1} \\\\ 0, & \\text { elsewhere } \\end{array}\\right. \\] whereas for the boundary nodes we have \\begin{align*} & N_{1}(x)=\\frac{x_{2}-x}{h_{1}}, \\quad x_{1} \\leq x \\leq x_{2} \\tag{1.8.2}\\\\ & N_{n+1}(x)=\\frac{x-x_{n}}{h_{n}}, \\quad x_{n} \\leq x \\leq x_{n+1} \\tag{1.8.3} \\end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as \"hat,\" \"chapeau,\" and \"roof\" functions. Note that $N_{A}\\left(x_{B}\\right)=\\delta_{A B}$, where\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\\\ $\\delta_{A B}$ is the Kronecker delta (i.e., $\\delta_{A B}=1$ if $A=B$, whereas $\\delta_{A B}=0$ if $A \\neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \\in \\mathcal{U}^{h}$ has the form $\\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\\boldsymbol{n}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\\boldsymbol{h}} \\in \\mathcal{U}^{\\boldsymbol{k}}$.\\\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \\begin{equation*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.8.4} \\end{equation*} where $w \\in \\mathcal{U}$ and $u \\in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \\ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \\begin{align*} 0= & \\sum_{A=1}^{n} \\int_{x_{A}}^{x_{A}+1} w\\left(u_{, x x}+f\\right) d x+w(0)\\left[u_{, x}\\left(0^{+}\\right)+h\\right] \\\\ & +\\sum_{A=2}^{n} w\\left(x_{A}\\right)\\left[u_{, x}\\left(x_{A}^{+}\\right)-u_{, x}\\left(x_{A}^{-}\\right)\\right] \\tag{1.8.5} \\end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\\\ i. $u_{, x x}(x)+f(x)=0$, where $\\left.x \\in\\right] x_{A}, x_{A+1}[$ and $A=1,2, \\ldots, n$,\\\\ ii. $-u_{, x}\\left(0^{+}\\right)=h$; and\\\\ iii. $u_{, x}\\left(x_{A}^{-}\\right)=u_{, x}\\left(x_{A}^{+}\\right)$, where $A=2,3, \\ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained.\n",
      "chunk word length: 433, chunk char length: 3075, chunk = The shape functions $N_{A}, A=1,2, \\ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \\begin{equation*} K_{A B}=\\int_{0}^{1} \\underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \\tag{1.9.1} \\end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\\boldsymbol{K}$.\\\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\\boldsymbol{n} \\times \\boldsymbol{n}$ matrix $\\boldsymbol{A}$ is said to be positive definite if\\\\ i. $c^{T} A c \\geq 0$ for all $n$-vectors $c$; and\\\\ ii. $c^{\\boldsymbol{T}} \\boldsymbol{A c}=0$ implies $\\boldsymbol{c}=0$. \\subsection*{Remarks} \\begin{enumerate} \\item A symmetric positive-definite matrix posesses a unique inverse. \\item The eigenvalues of a positive-definite matrix are real and positive. \\end{enumerate} Theorem. The $\\boldsymbol{n} \\times n$ matrix $K$ defined by (1.6.11) is positive definite.\\\\ Proof\\\\ i. Let $c_{A}, A=1,2, \\ldots, n$, be the components of $c$ (i.e., $c=\\left\\{c_{A}\\right\\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\\mathcal{U}^{h}, w^{h}=\\sum_{A=1}^{n} c_{A} N_{A}$, where\\\\ the $N_{A}$ 's are the basis functions for $\\mathcal{U}^{h}$. Then $$ \\begin{array}{rlrl} c^{T} K c & =\\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\\\ & =\\sum_{A, B=1}^{n} c_{A} a\\left(N_{A}, N_{B}\\right) c_{B} & & \\text { (definition of } \\left.K_{A B}\\right) \\\\ & =a\\left(\\sum_{A=1}^{n} c_{A} N_{A}, \\sum_{B=1}^{n} c_{B} N_{B}\\right) & & \\text { (bilinearity of } a(\\cdot, \\cdot)) \\\\ & =a\\left(w^{h}, w^{h}\\right) & & \\text { (definition of } \\left.w^{h}\\right) \\\\ & =\\int_{0}^{1} \\underbrace{\\left(w_{0}^{h} x\\right.}_{\\geq 0})^{2} d x & & \\text { (by (1.4.8) } \\\\ & \\geq 0 & \\end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \\int_{0}^{1}\\left(w_{, x}^{h}\\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \\in \\mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \\in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \\ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\\\ i. Symmetric\\\\ ii. Banded\\\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ may be performed.\n",
      "chunk word length: 858, chunk char length: 6304, chunk = In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\\boldsymbol{u}^{\\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\\delta_{y}(x)=\\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \\begin{align*} \\left(w, \\delta_{y}\\right) & =\\int_{0}^{1} w(x) \\delta(x-y) d x \\tag{1.10.1}\\\\ & =w(y) \\end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \\begin{align*} g_{, x x}+\\delta_{y} & =0 \\quad \\text { on } \\Omega \\tag{1.10.2}\\\\ g(1) & =0 \\tag{1.10.3}\\\\ g_{, x}(0) & =0 \\tag{1.10.4} \\end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\\delta_{y}$ is the Heaviside, or unit step, function: \\[ H_{y}(x)=H(x-y)= \\begin{cases}0, & x<y \\tag{1.10.5}\\\\ 1, & x>y\\end{cases} \\] The integral of $\\dot{H}_{y}$ is the Macaulay bracket: \\[ \\langle x-y\\rangle=\\left\\{\\begin{array}{cl} 0, & x \\leq y \\tag{1.10.6}\\\\ x-y, & x>y \\end{array}\\right. \\] The preceding functions are depicted in Fig. 1.10.1.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \\begin{equation*} g_{, x}+H_{y}=c_{1} \\tag{1.10.7} \\end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \\begin{equation*} g(x)+\\langle x-y\\rangle=c_{1} x+c_{2} \\tag{1.10.8} \\end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \\begin{equation*} g(x)=(1-y)-\\langle x-y\\rangle \\tag{1.10.9} \\end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \\in \\mathcal{U}^{h}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\\delta_{y}$, and $g$ and $h$ by 0 , viz., \\begin{equation*} a(w, g)=\\left(w, \\delta_{y}\\right)=w(y) \\tag{1.10.10} \\end{equation*} Equation (1.10.10) holds for all continuous $w \\in \\mathcal{U}$. The square-integrability of derivatives of functions $w \\in \\mathcal{U}$ actually implies the continuity of all $w \\in \\mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\\left(x_{A}\\right)=u\\left(x_{A}\\right), A=1,2, \\ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\\left(u-u^{h}, w^{h}\\right)=0$ for all $w^{h} \\in \\mathcal{U}^{h}$.\\\\ Proof. We have observed previously that $\\mathcal{U}^{h} \\in \\mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \\begin{equation*} a\\left(w^{h}, u\\right)=\\left(w^{h}, f\\right)+w^{h}(0) h \\tag{1.10.11} \\end{equation*} Equation (1.10.11) holds for all $w^{h} \\in \\mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\\\ from (1.10.11) and using the bilinearity and symmetry of $a(\\cdot, \\cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\\left(u-u^{h}, g\\right)$, where $g$ is the Green's function.\\\\ Proof $$ \\begin{aligned} u(y)-u^{h}(y) & =\\left(u-u^{h}, \\delta_{y}\\right) & & \\text { (definition of } \\left.\\delta_{y}\\right) \\\\ & =a\\left(u-u^{h}, g\\right) & & \\text { (by (1.10.10)) } \\end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\\mathcal{U}$.\\\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \\in \\mathcal{U}^{h}$. Let us take this to be the case. Then $$ \\begin{aligned} u\\left(x_{A}\\right)-u^{h}\\left(x_{A}\\right) & =a\\left(u-u^{h}, g\\right) & & \\text { (Lemma 2) } \\\\ & =0 & & \\text { (Lemma 1) } \\end{aligned} $$ The theorem is valid for $A=1,2, \\ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed.\n",
      "chunk word length: 541, chunk char length: 5005, chunk = In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \\subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \\rightarrow \\mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \\begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \\frac{1}{2}(z-y)^{2} f_{,xx}(y) \\\\ & + \\frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \\cdots + \\tag{1.10.12} \\\\ & + \\frac{1}{k!}(z-y)^{k} f_{,\\underbrace{x \\dots x}_{k \\text{ times}}}(c) \\end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \\subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \\geq 1$ (i.e., $f$ is continuously differentiable): \\begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \\tag{1.10.13} \\end{equation*} Consider a typical subinterval $\\left[x_{A}, x_{A+1}\\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \\begin{equation*} \\left.u_{, x}^{h}(x)=\\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}, \\quad x \\in\\right] x_{A}, x_{A+1}[ \\tag{1.10.14} \\end{equation*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \\end{center} Figare 1.10.3\\\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \\in] x_{A}, x_{A+1}[$ such that \\begin{equation*} \\frac{u\\left(x_{A+1}\\right)-u\\left(x_{A}\\right)}{h_{A}}=u_{, x}(c) \\tag{1.10.15} \\end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\\left(x_{A}\\right)=u^{h}\\left(x_{A}\\right)$ and $u\\left(x_{A+1}\\right)=u^{h}\\left(x_{A+1}\\right)$, we may rewrite (1.10.15) as \\begin{equation*} \\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}=u_{, x}(c) \\tag{1.10.16} \\end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \\subsection*{Remarks} \\begin{enumerate} \\item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \\item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \\end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\\alpha) \\stackrel{\\operatorname{def} .}{=} u_{, x}^{h}(\\alpha)-u_{, x}(\\alpha)=\\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}-u_{, x}(\\alpha) $$ the error in the derivative at $\\alpha \\in\\left[x_{A}, x_{A+1}\\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\\boldsymbol{u}$ is three times continuously differentiable. Then \\begin{align*} e_{, x}(\\alpha)= & \\left(\\frac{x_{A+1}+x_{A}}{2}-\\alpha\\right) u_{, x x}(\\alpha) \\\\ & +\\frac{1}{3!h_{A}}\\left[\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right)-\\left(x_{A}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{2}\\right)\\right] \\tag{1.10.17} \\end{align*} where $c_{1}$ and $c_{2}$ are in $\\left[x_{A}, x_{A+1}\\right]$.\\\\ Proof. Expand $u\\left(x_{A+1}\\right)$ and $u\\left(x_{A}\\right)$ in finite Taylor expansions about $\\alpha \\in\\left[x_{A}, x_{A+1}\\right]$, viz., $$ \\begin{aligned} u\\left(x_{A+1}\\right)= & u(\\alpha)+\\left(x_{A+1}-\\alpha\\right) u_{, x}(\\alpha)+\\frac{1}{2}\\left(x_{A+1}-\\alpha\\right)^{2} u_{, x x}(\\alpha) \\\\ & +\\frac{1}{3!}\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right), \\quad c_{1} \\in\\left[\\alpha, x_{A+1}\\right] \\\\ u\\left(x_{A}\\right)= & u(\\alpha)+\\left(x_{A}-\\alpha\\right) u_{, x}(\\alpha)+\\frac{1}{2}\\left(x_{A}-\\alpha\\right)^{2} u_{, x x}(\\alpha) \\\\ & +\\frac{1}{3!}\\left(x_{A}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{2}\\right), \\quad c_{2} \\in\\left[x_{A}, \\alpha\\right] \\end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \\begin{aligned} \\frac{u\\left(x_{A+1}\\right)-u\\left(x_{A}\\right)}{h_{A}}= & u_{, x}(\\alpha)+\\left(\\frac{x_{A+1}+x_{A}}{2}-\\alpha\\right) u_{, x x}(\\alpha) \\\\ & +\\frac{1}{3!h_{A}}\\left[\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right)-\\left(x_{A}-\\alpha\\right)^{3} u_{1, x x x}\\left(c_{2}\\right)\\right] \\end{aligned} $$ Replacing $u\\left(x_{A+1}\\right)$ by $u^{h}\\left(x_{A+1}\\right)$ and $u\\left(x_{A}\\right)$ by $u^{h}\\left(x_{A}\\right)$ in the left-hand side and rearranging terms completes the proof.\n",
      "chunk word length: 279, chunk char length: 2009, chunk = To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\\left(h_{\\mathrm{A}}^{2}\\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\\left(h_{A}\\right)$. The exponent of $h_{\\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\\alpha \\rightarrow x_{A}$. Then $$ e_{, x}\\left(x_{A}\\right)=\\frac{h_{A}}{2} u_{, x x}\\left(x_{A}\\right)+\\frac{h_{A}^{2}}{3!} u_{, x x x}\\left(c_{1}\\right)=O\\left(h_{A}\\right) $$ As $\\boldsymbol{h}_{A} \\rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\\alpha \\in\\left[x_{A}, x_{A+1}\\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \\equiv\\left(x_{A}+x_{A+1}\\right) / 2$ (i.e., the midpoint). Then $$ \\begin{aligned} e_{, x}\\left(x_{A+1 / 2}\\right) & =\\frac{h_{A}^{2}}{24} u_{, x x x}(c), \\quad c \\in\\left[x_{A}, x_{A+1}\\right] \\\\ & =O\\left(h_{A}^{2}\\right) \\end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\\left(x_{A+1 / 2}\\right)=\\frac{h_{A}^{2}}{48}\\left[u_{, x x x}\\left(c_{1}\\right)+u_{, x x x}\\left(c_{2}\\right)\\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\\frac{1}{2}\\left[u_{, x x x}\\left(c_{1}\\right)+u_{, x x x}\\left(c_{2}\\right)\\right] $$ Combining these facts completes the proof.\n",
      "chunk word length: 332, chunk char length: 2230, chunk = \\begin{enumerate} \\item From the corollary we see that the derivatives are second-order accurate at the midpoints. \\end{enumerate} \\footnotetext{${ }^{3} \\mathrm{~A}$ function $f(x)$ is said to be $O\\left(x^{k}\\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \\rightarrow$ a constant as $x \\rightarrow 0$. For example, $f(x)=x^{k}$ is $O\\left(x^{k}\\right)$, as is $f(x)=\\sum_{j=k}^{k+1} x^{j}, l \\geq 0$. But neither is $O\\left(x^{k+1}\\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear \"elements\" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \\ldots, n$ ). Consider the standard finite difference \"stencil\" for $u_{, x x}+\\phi=0$ at a typical internal node, namely, \\begin{equation*} \\frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \\tag{1.10.18} \\end{equation*} Assuming \\& varies in piecewise linear fashion and so can be expanded as \\begin{equation*} f=\\sum_{A=1}^{n+1} f_{A} N_{A} \\tag{1.10.19} \\end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\\\ i. It is exact at the nodes.\\\\ ii. There exists at least one point in each element at which the derivative is exact.\\\\ iii. The derivative is second-order accurate at the midpoints of the elements.\n",
      "chunk word length: 527, chunk char length: 3532, chunk = It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\\\ $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \\subsection*{Gauss Elimination} \\begin{itemize} \\item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \\item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \\item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \\item Solve the $n$-th equation for $d_{n}$. \\end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \\left[\\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\\\ K_{21} & K_{22} & K_{23} & K_{24} \\\\ K_{31} & K_{32} & K_{33} & K_{34} \\\\ K_{41} & K_{42} & K_{43} & K_{44} \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} F_{1} \\\\ F_{2} \\\\ F_{3} \\\\ F_{4} \\end{array}\\right\\} $$ The augmented matrix corresponding to this system is \\[ \\left[ \\begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\\\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\\\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\\\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{K}} & \\underbrace{\\phantom{F_1}}_{F} \\end{array} \\right] \\] After the forward reduction, the augmented matrix becomes\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\\\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\\\ 0 & 0 & 1 & K'_{34} & F'_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{U}} & \\underbrace{\\phantom{F_1}}_{F'} \\end{array} \\tag{1.11.1} \\right] \\] corresponding to the upper triangular system $\\boldsymbol{U} \\boldsymbol{d}=\\boldsymbol{F}^{\\prime} \\cdot{ }^{4}$ It is a simply verified fact that if $\\boldsymbol{K}$ is banded, then $\\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \\begin{itemize} \\item Eliminate $d_{n}$ from equations $n-1, n-2, \\ldots, 1$.\\\\ \\footnotetext{${ }^{4} \\text{Primes will be used to denote intermediate quantities throughout this section}.$} \\item Eliminate $d_{n-1}$ from equations $n-2, n-3, \\ldots, 1$. \\item Eliminate $d_{2}$ from the first equation. \\end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\\\ 0 & 1 & 0 & 0 & d_2 \\\\ 0 & 0 & 1 & 0 & d_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{I}} & \\underbrace{\\phantom{F_1}}_{d} \\end{array} \\tag{1.11.2} \\right] \\] corresponding to the identity $1 \\boldsymbol{d}=\\boldsymbol{d}$. The solution winds up in the last column.\n",
      "chunk word length: 612, chunk char length: 4473, chunk = In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \\subsection*{Forward reduction} \\begin{itemize} \\item Divide row 1 by $K_{11}$. \\item Subtract $K_{21} \\times$ row 1 from row 2. \\item Subtract $K_{31} \\times$ row 1 from row 3. \\item Subtract $K_{n 1} \\times$ row 1 from row $n$. \\end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \\left[\\begin{array}{llll|l} 1 & \\boldsymbol{K}_{12}^{\\prime} & \\boldsymbol{K}_{3}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & \\boldsymbol{K}_{22}^{\\prime \\prime} & \\boldsymbol{K}_{23}^{\\prime \\prime} & \\boldsymbol{K}_{24}^{\\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime} \\\\ 0 & \\boldsymbol{K}_{32}^{\\prime} & \\boldsymbol{K}_{33}^{\\prime 3} & \\boldsymbol{K}_{34}^{\\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime} \\\\ \\mathbf{0} & \\boldsymbol{K}_{42}^{\\prime} & \\boldsymbol{K}_{43}^{3} & \\boldsymbol{K}_{44}^{\\prime \\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime} \\end{array}\\right] $$ Note that if $\\boldsymbol{K}_{\\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \\begin{itemize} \\item Divide row 2 by $K_{22}^{\\prime \\prime}$. \\item Subtract $K_{32}^{\\prime \\prime} \\times$ row 2 from row 3. \\item Subtract $K_{42}^{n} \\times$ row 2 from row 4. \\item Subtract $K_{n 2}^{\\prime \\prime} \\times$ row 2 from row $n$. \\end{itemize} The result for the example will look like\\\\ $\\left[\\begin{array}{cccc|c}1 & \\boldsymbol{K}_{12}^{\\prime} & \\boldsymbol{K}_{13}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & 1 & \\boldsymbol{K}_{23}^{\\prime \\prime \\prime} & \\boldsymbol{K}_{24}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{33}^{\\prime \\prime \\prime} & \\boldsymbol{K}_{34}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{43}^{\\prime \\prime \\prime} & \\boldsymbol{K}_{44}^{\\prime \\prime\\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime \\prime} \\\\ & & & & \\end{array}\\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \\subsection*{Back substitution} \\begin{itemize} \\item Subtract $K_{n-1, n}^{\\prime} \\times$ row $n$ from row $n-1$. \\item Subtract $K_{n-2, n}^{\\prime} \\times$ row $n$ from row $n-2$.\\\\ \\vdots \\item Subtract $K_{1, n}^{\\prime} \\times$ row $n$ from row 1 \\end{itemize} After these steps the augmented matrix, for this example, will look like $$ \\left[\\begin{array}{cccc|c} 1 & \\bar{K}_{12}^{\\prime} & \\bar{K}_{3}^{\\prime} & 0 & F_{1}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 1 & K_{23}^{\\prime} & 0 & F_{2}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 0 & 1 & 0 & d_{3} \\\\ 0 & 0 & 0 & 1 & d_{4} \\end{array}\\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \\begin{itemize} \\item Subtract $K_{n-2, n-1}^{\\prime} \\times$ row $n-1$ from row $n-2$. \\item Subtract $K_{n-3, n-1}^{\\prime} \\times$ row $n-1$ from row $n-3$.\\\\ \\vdots \\item Subtract $K_{1 . n-1}^{\\prime} \\times$ row $n-1$ from row 1. \\end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\\boldsymbol{F}$ ). Repeat as above until columns $\\boldsymbol{n}-2, n-3, \\ldots, 2$ are cleared. The result is (1.11.2). \\subsection*{Remarks} \\begin{enumerate} \\item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \\item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \\end{enumerate}\n",
      "chunk word length: 609, chunk char length: 2923, chunk = $$ \\left[\\begin{array}{rrrr} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right\\} $$ \\subsection*{Augmented matrix} $$ \\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right] $$ Forward reduction $$ \\begin{aligned} & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\end{aligned} $$ \\subsection*{Back substitution} $$ \\begin{aligned} & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\\\ \\begin{array}{l} \\left\\{ \\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4} \\end{array} \\right\\} = \\left\\{ \\begin{array}{l} 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{array} \\right\\} \\end{array} \\end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \\begin{aligned} u_{, x x}(x)+f(x) & =0 \\quad x \\in] 0,1[ \\\\ u(1) & =g \\\\ -u_{, x}(0) & =h \\end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\\left(h=\\right.$ mesh parameter $\\left.=\\frac{1}{4}\\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\\left(h=1\\right.$ and $h=\\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\\\ b. Let $r e_{, x}=\\left|u_{, x}^{h}-u_{. x}\\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\\\ c. Employing the data for $h=1, \\frac{1}{2}$, and $\\frac{1}{4}$, plot $\\ln r e_{, x}$ versus $\\ln h$.\\\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\\\ i. What is the significance of the slope of the graph in part (c)?\\\\ ii. What is the significance of the $y$-intercept?\n",
      "chunk word length: 771, chunk char length: 5825, chunk = So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and \"local support\" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \\begin{center} \\begin{tabular}{lll} $(g 1)$ & Domain: & $\\left[x_{A}, x_{A+1}\\right]$ \\\\ $(g 2)$ & Nodes: & $\\left\\{x_{A}, x_{A+1}\\right\\}$ \\\\ $(g 3)$ & Degrees of freedom: & $\\left\\{d_{A}, d_{A+1}\\right\\}$ \\\\ $(g 4)$ & Shape functions: & $\\left\\{N_{A}, N_{A+1}\\right\\}$ \\\\ $(g 5)$ & Interpolation function: & \\\\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \\in\\left[x_{A}, x_{A+1}\\right]$ & \\\\ \\end{tabular} \\end{center} (Recall $d_{A}=u^{h}\\left(x_{A}\\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\\\ (l1) Domain: $\\left[\\xi_{1}, \\xi_{2}\\right]$ \\footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\\left\\{\\xi_{1}, \\xi_{2}\\right\\}$\\\\ (l3) Degrees of freedom: $\\left\\{d_{1}, d_{2}\\right\\}$\\\\ (I4) Shape functions: $\\left\\{N_{1}, N_{2}\\right\\}$\\\\ (15) Interpolation function: $$ u^{h}(\\xi)=N_{1}(\\xi) d_{1}+N_{2}(\\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\\\ We shall relate the domains of the global and local descriptions by an \"affine\" transformation $\\xi:\\left[x_{A}, x_{A+1}\\right] \\rightarrow\\left[\\xi_{1}, \\xi_{2}\\right]$, such that $\\xi\\left(x_{A}\\right)=\\xi_{1}$ and $\\xi\\left(x_{A+1}\\right)=\\xi_{2}$. It is standard practice to take $\\xi_{1}=-1$ and $\\xi_{2}=+1$. Thus $\\xi$ may be represented by the expression \\begin{equation*} \\xi(x)=c_{1}+c_{2} x \\tag{1.12.1} \\end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \\[ \\left.\\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \\tag{1.12.2}\\\\ 1 & =c_{1}+x_{A+1} c_{2} \\end{array}\\right\\} \\] Solving this system yields \\begin{equation*} \\xi(x)=\\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \\tag{1.12.3} \\end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\\xi$ is obtained by solving for $x$ : \\begin{equation*} x(\\xi)=\\frac{h_{A} \\xi+x_{A}+x_{A+1}}{2} \\tag{1.12.4} \\end{equation*} In (1.12.1), $\\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \\ldots$ pertain to the local numbering system. The subscripts $A, B, C, \\ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\\xi)=N_{A}\\left(x^{e}(\\xi)\\right)$, where $x^{e}:\\left[\\xi_{1}, \\xi_{2}\\right] \\rightarrow$ $\\left[x_{1}^{e}, x_{2}^{e}\\right]=\\left[x_{A}, x_{A+1}\\right]$, etc.). In terms of $\\xi$, the shape functions in the local description take on a standard form \\begin{equation*} N_{a}(\\xi)=\\frac{1}{2}\\left(1+\\xi_{a} \\xi\\right), \\quad a=1,2 \\tag{1.12.5} \\end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \\begin{equation*} x^{e}(\\xi)=\\sum_{a=1}^{2} N_{a}(\\xi) x_{a}^{e} \\tag{1.12.6} \\end{equation*} This has the same form as the interpolation function (cf. 15).\\\\ For future reference, we note the following results: \\begin{gather*} N_{a, \\xi}=\\frac{\\xi_{a}}{2}=\\frac{(-1)^{a}}{2} \\tag{1.12.7}\\\\ x_{, \\xi}^{e}=\\frac{h^{e}}{2} \\tag{1.12.8} \\end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \\begin{equation*} \\xi_{, x}^{e}=\\left(x_{, \\xi}^{e}\\right)^{-1}=\\frac{2}{h^{e}} \\tag{1.12.9} \\end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element.\n",
      "chunk word length: 477, chunk char length: 3657, chunk = To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \\leq e \\leq n_{e l}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\\\ Now recall the definitions of the (global) stiffness matrix and force vector\\\\ \\\\ \\[ K = \\underbrace{\\left[ K_{AB} \\right]}_{n \\times n}, \\quad F = \\underbrace{\\left\\{ F_A \\right\\}}_{n \\times 1} \\tag{1.13.1} \\] where \\begin{gather*} K_{A B}=a\\left(N_{A}, N_{B}\\right)=\\int_{0}^{1} N_{A, x} N_{B, x} d x \\tag{1.13.2}\\\\ F_{A}=\\left(N_{A}, f\\right)+\\delta_{A 1} h-a\\left(N_{A}, N_{n+1}\\right) g \\\\ =\\int_{0}^{1} N_{A} f d x+\\delta_{A 1} h-\\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \\tag{1.13.3} \\end{gather*} ( $\\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\\left(x_{1}\\right)=\\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \\[ \\begin{array}{ll} \\boldsymbol{K}=\\sum_{e=1}^{n_{e l}} \\boldsymbol{K}^{e}, & \\boldsymbol{K}^{e}=\\left[K_{A B}^{e}\\right] \\\\ \\boldsymbol{F}=\\sum_{e=1}^{n_{e l}} \\boldsymbol{F}^{e}, & \\boldsymbol{F}^{e}=\\left\\{F_{\\hat{A}}^{e}\\right\\} \\tag{1.13.5} \\end{array} \\] where \\begin{align*} K_{A B}^{e} & =a\\left(N_{A}, N_{B}\\right)^{e}=\\int_{\\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \\tag{1.13.6}\\\\ F_{A}^{e} & =\\left(N_{A}, f\\right)^{e}+\\delta_{e 1} \\delta_{A 1} h-a\\left(N_{A}, N_{n+1}\\right)^{e} g \\\\ & =\\int_{\\Omega^{e}} N_{A} f d x+\\delta_{e 1} \\delta_{A 1} h-\\int_{\\Omega^{e}} N_{A, x} N_{n+1, x} d x g \\tag{1.13.7} \\end{align*} and $\\Omega^{e}=\\left[x_{1}^{e}, x_{2}^{e}\\right]$, the domain of the eth element.\\\\ The important observation to make is that $\\boldsymbol{K}$ and $\\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \\begin{equation*} K_{A B}^{e}=0, \\quad \\text { if } A \\neq e \\text { or } e+1 \\text { or } B \\neq e \\text { or } e+1 \\tag{1.13.8} \\end{equation*} and \\begin{equation*} F_{A}^{e}=0, \\quad \\text { if } A \\neq e \\text { or } e+1 \\tag{1.13.9} \\end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \\begin{align*} & k^{e}=\\underbrace{\\left[k_{a b}^{e}\\right]}_{2 \\times 2}, \\quad f^{e}=\\underbrace{\\left\\{f_{a}^{e}\\right\\}}_{2 \\times 1} \\tag{1.13.10}\\\\ & k_{a b}^{e}=a\\left(N_{a}, N_{b}\\right)^{e}=\\int_{\\Omega^{e}} N_{a, x} N_{b, x} d x \\tag{1.13.11}\\\\ & f_{a}^{e}=\\int_{\\Omega^{e}} N_{a} f d x+\\left\\{\\begin{array}{cl} \\delta_{a 1} h & e=1 \\\\ 0 & e=2,3, \\ldots, n_{e l}-1 \\\\ -k_{a 2 }^{e} g & e=n_{e l} \\end{array}\\right. \\tag{1.13.12} \\end{align*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \\end{center} Here $\\boldsymbol{k}^{e}$ and $\\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\\boldsymbol{K}^{e}$ and $\\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ \"go\" in $\\boldsymbol{K}$ and $\\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section.\n",
      "chunk word length: 511, chunk char length: 3473, chunk = In a finite element computer program, it is the task of a \"finite element subroutine\" to produce $k^{e}$ and $f^{e}, e=1,2, \\ldots, n_{e l}$, from given data and to provide an \"assembly subroutine\" enough information so that the terms in $\\boldsymbol{k}^{e}$ and $\\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\\boldsymbol{K}$ and $\\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \\[ A=\\mathrm{LM}(a, e)=\\left\\{\\begin{array}{cc} e & \\text { if } a=1 \\tag{1.14.1}\\\\ e+1 & \\text { if } a=2 \\end{array}\\right. \\] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\\mathrm{LM}\\left(2, n_{e l}\\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \\leq e \\leq n_{e l}$ \\begin{center} \\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \\hline \\multirow[b]{3}{*}{\\( \\begin{aligned} & \\text { Local } \\\\ & \\text { node } \\\\ & \\text { number } \\end{aligned} \\)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\\\ \\hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\\\ \\hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\\\ \\hline \\end{tabular} \\end{center} Figure 1.14.1 LM array for example problem.\\\\ As an example, assume we want to add the eth elemental contributions, where $1 \\leq e \\leq n_{e l-1}$, to the partially assembled $\\boldsymbol{K}$ and $\\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \\begin{align*} K_{e e} & \\leftarrow K_{e e}+k_{11}^{e} \\tag{1.14.2}\\\\ K_{e, e+1} & \\leftarrow K_{e, e+1}+k_{12}^{e} \\tag{1.14.3}\\\\ K_{e+1, e} & \\leftarrow K_{e+1, e}+k_{21}^{e} \\tag{1.14.4}\\\\ K_{e+1, e+1} & \\leftarrow K_{e+1, e+1}+k_{22}^{e} \\tag{1.14.5} \\end{align*} \\footnote{${ }^{6}$ Due to symmetry $k_{21}^{\\prime}$ would not actually be assembled in practice.} \\begin{align*} F_{e} & \\leftarrow F_{e}+f_{i}^{e} \\tag{1.14.6}\\\\ F_{e+1} & \\leftarrow F_{e+1}+f_{2}^{e} \\tag{1.14.7} \\end{align*} where the arrow $(\\leftarrow)$ is read \"is replaced by.\"\\\\ For element $n_{e l}$ we have only that \\begin{align*} K_{n n} & \\leftarrow K_{m n}+k_{11}^{n_{el}} \\tag{1.14.8}\\\\ F_{n} & \\leftarrow F_{n}+f_{1}^{n_{t}} \\tag{1.14.9} \\end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\\boldsymbol{K}$ and $\\boldsymbol{F}$; see Fig. 1.14.2.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\\mathbf{A}$, the assembly operator, vis., \\begin{equation*} \\boldsymbol{K}=\\boldsymbol{A}_{e=1}^{n_{el}}\\left(k^{e}\\right), \\quad \\boldsymbol{F}=A_{e=1}^{n_{el}}\\left(f^{e}\\right) \\tag{1.14.10} \\end{equation*}\n",
      "chunk word length: 76, chunk char length: 752, chunk = The explicit computation of $\\boldsymbol{k}^{\\boldsymbol{e}}$ and $\\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \\subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\\left[x_{1}, x_{2}\\right] \\rightarrow \\mathbb{R}$ be an integrable function and let $x:\\left[\\xi_{1}, \\xi_{2}\\right] \\rightarrow\\left[x_{1}, x_{2}\\right]$ be continuously differentiable, with $x\\left(\\xi_{1}\\right)=x_{1}$ and $x\\left(\\xi_{2}\\right)=x_{2}$. Then \\begin{equation*} \\int_{x_{1}}^{x_{2}} f(x) d x=\\int_{\\xi_{1}}^{\\xi_{2}} f(x(\\xi)) x_{, \\xi}(\\xi) d \\xi \\tag{1.15.1} \\end{equation*}\n",
      "chunk word length: 895, chunk char length: 6470, chunk = Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \\begin{equation*} \\frac{\\partial}{\\partial \\xi} f(x(\\xi))=f_{, x}(x(\\xi)) x_{, \\xi}(\\xi) \\tag{1.15.2} \\end{equation*} Proofs of these results may be found in [11].\\\\ The computation of $k^{e}$ proceeds as follows: $$ \\begin{aligned} k_{a b}^{e} & =\\int_{\\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \\quad \\text { (by definition) } \\\\ & =\\int_{-1}^{+1} N_{a, x}(x(\\xi)) N_{b, x}(x(\\xi)) x_{, \\xi}(\\xi) d \\xi \\end{aligned} $$ (Change of variables, where $x(\\xi)$ is defined by (1.12.6)) $$ =\\int_{-1}^{+1} N_{a, \\xi}(\\xi) N_{b, \\xi}(\\xi)\\left(x_{, \\xi}(\\xi)\\right)^{-1} d \\xi $$ (Chain rule; $\\left.N_{a, \\xi}(\\xi)=(\\partial / \\partial \\xi) N_{a}(x(\\xi))=N_{a, x}(x(\\xi)) x_{, \\xi}(\\xi)\\right)$ $$ =(-1)^{a+b} / h^{e} \\quad(\\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \\[ k^{e}=\\frac{1}{h^{e}}\\left[\\begin{array}{rr} 1 & -1 \\tag{1.15.3}\\\\ -1 & 1 \\end{array}\\right] \\] Observe that $N_{a, \\xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\\boldsymbol{x}_{, \\xi}$ and $\\boldsymbol{\\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \\begin{equation*} f^{h}=\\sum_{a=1}^{2} f_{a} N_{a} \\tag{1.15.4} \\end{equation*} where $f_{a}=f\\left(x\\left(\\xi_{a}\\right)\\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear \"loading\" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \\begin{align*} \\int_{\\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\\int_{-1}^{+1} N_{a}(x(\\xi)) f^{h}(x(\\xi)) x_{, \\xi}(\\xi) d \\xi \\quad \\text { (change of variables) } \\\\ & =\\frac{h^{e}}{2} \\sum_{b=1}^{2} \\int_{-1}^{+1} N_{a}(\\xi) N_{b}(\\xi) d \\xi f_{b} \\quad \\text { (by (1.12.8)) } \\tag{1.15.5} \\end{align*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \\end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\\left(\\int_{-1}^{+1} N_{a} N_{b} d \\xi=\\left(1+\\delta_{a b}\\right) / 3\\right)$ yields \\[ \\begin{array}{rlr} \\mathfrak{f}^{e} & =\\frac{h^{e}}{6}\\left[\\begin{array}{cc} 2 & 1 \\\\ 1 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} f_{1} \\\\ f_{2} \\end{array}\\right\\} & \\begin{array}{c} \\text { (+ boundary terms } \\\\ \\text { cf. (1.13.12)) } \\end{array} \\\\ & =\\frac{h^{e}}{6}\\left\\{\\begin{array}{l} 2 f_{1} + f_{2} \\\\ f_{1} + 2 f_{2} \\end{array}\\right\\} & \\text { (+ boundary terms) } \\tag{1.15.6} \\end{array} \\] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\\left(h^{2}\\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\\left(h^{2}\\right)$ errors in $u^{\\boldsymbol{h}}$ and $u_{, x}^{\\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \\subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \\left.u_{,xx}-\\lambda u+f=0 \\quad \\text { on } \\Omega=\\right] 0,1[ $$ where $\\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \\int_{\\Omega}\\left(w_{\\cdot x} u_{, x}+w \\lambda u\\right) d x=\\int_{\\Omega} w f d x+w(0) h $$ where $u \\in \\mathfrak{f}, w \\in \\mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \\lambda u)=(w, f)+w(0) h $$ i. Let $\\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \\begin{array}{r} a\\left(w^{h}, v^{h}\\right)+\\square= \\\\ \\left(w^{h}, f\\right)+w^{h}(0) h-a\\left(w^{h}, gS^{h}\\right) \\\\ -\\square \\end{array} $$ ii. Define $K_{A B}=a\\left(N_{A}, N_{B}\\right)+\\square$\\\\ and $$ k_{a b}^{e}=a\\left(N_{a}, N_{b}\\right)^{e}+\\square $$ iii. Determine $\\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\\left[k_{a b}^{e}\\right]=[\\square] $$ iv. Show that $\\boldsymbol{K}$ is symmetric.\\\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\\lambda g+\\delta_{y}=0 $$ and can be written as $$ g(x)= \\begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \\leq x \\leq y \\\\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \\leq x \\leq 1\\end{cases} $$ where $p=\\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \\begin{aligned} g(1) & =0 \\\\ g_{, x}(0) & =0 \\\\ g\\left(y^{+}\\right) & =g\\left(y^{-}\\right) \\\\ g_{, x}\\left(y^{+}\\right) & =g_{. x}\\left(y^{-}\\right)-1 \\end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \\quad x \\in \\Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\\left(x_{a}^{e}\\right), \\quad a=1,2 $$ What is the attribute which this choice of functions attains?\n",
      "chunk word length: 1148, chunk char length: 8606, chunk = This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\\Omega=] 0,1[, \\bar{\\Omega}=[0,1]$ ).\\\\ \\[ \\text{(S)} \\quad \\left\\{ \\begin{minipage}{0.8\\textwidth} \\text {Given } $f: \\Omega \\rightarrow \\mathbb{R}$ \\text { and constants } $M$ \\text { and } $f$, \\text { find } $u: \\bar{\\Omega} \\rightarrow \\mathbb{R}$ \\text { such that:} \\begin{align*} E I u_{,xxxx} = f \\quad \\text {on } \\Omega \\quad \\text {(transverse equilibrium)}\\\\ u(1) &= 0 & \\text{(zero transverse displacement)} \\\\ u_{x}(1) &= 0 & \\text{(zero slope)} \\\\ E I u_{,xx}(0) &= M & \\text{(prescribed moment)} \\\\ E I u_{,xxx}(0) &= Q & \\text{(prescribed shear)} \\end{align*} \\end{minipage} \\right. \\] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\\\ The setup is shown in Fig. 1.16.1.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\\\ Let $\\mathfrak{f}=\\mathcal{U}=\\left\\{w \\mid w \\in H^{2}(\\Omega), w(1)=w_{x}(1)=0\\right\\}^{7}$. Then a corresponding weak form of the problem is:\\\\ \\[ \\text{(W)} \\quad \\left\\{ \\begin{minipage}{0.8\\textwidth} \\text{Given} $f, M$, \\text{, and} $Q$, find $u \\in \\mathfrak{f}$ \\text{such that for all} $w \\in \\mathcal{U}$\\\\ \\begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \\end{align*} \\end{minipage} \\right. \\] where $$ \\begin{aligned} a(w, u) & =\\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\\\ (w, f) & =\\int_{0}^{1} w f d x \\end{aligned} $$ \\footnotetext{${ }^{7} w \\in H^{2}(\\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\\left.f_{0}^{1}\\left(w_{, x x}\\right)^{2} d x<\\infty\\right)$. }The collection of functions, $\\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \\in \\mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\\mathfrak{d}^{h}=\\mathcal{U}^{h}$ be a finite-dimensional approximation of $\\mathfrak{f}$. In particular, we assume $w^{h} \\in \\mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\\\ (G) $\\left\\{\\begin{array}{c}\\text { Given } f, M, \\text { and } Q, \\text { find } u^{h} \\in \\delta^{h} \\text { such that for all } w^{h} \\in \\mathcal{U}^{h} \\\\ a\\left(w^{h}, u^{h}\\right)=\\left(w^{h}, f\\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\\end{array}\\right.$\\\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\\\ b. Assume $0=x_{1}<x_{2}<\\cdots<x_{n+1}=1$ and $\\mathcal{U}^{h}=\\left\\{w^{h} \\mid w^{h} \\in C^{1}(\\bar{\\Omega})\\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\\left[x_{A}, x_{A+1}\\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\\left.\\left.1, x, x^{2}, x^{3}\\right)\\right\\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \\in \\mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\\mathrm{A}}$ and $x_{\\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\\left(x_{1}\\right)+N_{3}(x) w^{h}\\left(x_{2}\\right)+N_{2}(x) w_{. x}^{h}\\left(x_{1}\\right)+N_{4}(x) w_{, x}^{h}\\left(x_{2}\\right) $$ where $$ \\begin{aligned} & N_{1}(x)=\\frac{-\\left(x-x_{2}\\right)^{2}\\left[-h+2\\left(x_{1}-x\\right)\\right]}{h^{3}} \\\\ & N_{2}(x)=\\frac{\\left(x-x_{1}\\right)^{\\prime}\\left(x-x_{2}\\right)^{2}}{h^{2}} \\\\ & N_{3}(x)=\\frac{\\left(x-x_{1}\\right)^{2}\\left[h+2\\left(x_{2}-x\\right)\\right]}{h^{3}} \\\\ & N_{4}(x)=\\frac{\\left(x-x_{1}\\right)^{2}\\left(x-x_{2}\\right)}{h^{2}} \\end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \\begin{aligned} w^{h}\\left(x_{1}\\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\\\ w^{h}\\left(x_{2}\\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\\\ w_{,x}^{h}\\left(x_{1}\\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\\\ w_{,x}^{h}\\left(x_{2}\\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \\end{aligned} $$ \\footnote{${ }^{8}$ The notation $w^{k} \\in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\\xi$-element coordinate system introduced in Sec. 1.12 (recall $\\left.\\xi=\\left(2 x-x_{A}-x_{A+1}\\right) / h_{A}\\right)$, the location of the Barlow curvature points may be expressed as $\\xi= \\pm 1 / \\sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\\left(h^{3}\\right)$ ).\\\\ e. If the segment of the beam $\\left[x_{A}, x_{A+1}\\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\\\ g. Prove that $$ \\begin{gathered} u^{h}\\left(x_{A}\\right)=u\\left(x_{A}\\right) \\\\ u_{, x}^{h}\\left(x_{A}\\right)=u_{, x}\\left(x_{A}\\right) \\end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\\delta_{x}\\left(x-x_{A}\\right)$, which is the generalized derivative of the delta function.\\\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\\\ i. Why do we require that the functions in $\\mathcal{U}^{h}$ have continuous first derivatives?\\\\ j. Calculate the $4 \\times 4$ element stiffness matrix, $$ k_{p q}^{e}=\\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \\quad 1 \\leq p, q \\leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\\mathrm{e}}$.\\\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \\in \\mathcal{U}$ and $u \\in \\mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \\begin{aligned} 0= & \\sum_{A=1}^{n} \\int_{x_{A}}^{x_{A}+1} w\\left(E I u_{, x x x x}-f\\right) d x \\\\ & -w_{, x}(0)\\left(E I u_{, x x}\\left(0^{+}\\right)-M\\right) \\\\ & +w(0)\\left(E I u_{, x x x}\\left(0^{+}\\right)-Q\\right) \\\\ & -\\sum_{A=2}^{n} w_{, x}\\left(x_{A}\\right) E I\\left(u_{, x x}\\left(x_{A}^{+}\\right)-u_{, x x}\\left(x_{A}^{-}\\right)\\right) \\\\ & +\\sum_{A=2}^{n} w\\left(x_{A}\\right) E I\\left(u_{, x x x}\\left(x_{A}^{+}\\right)-u_{, x x x}\\left(x_{A}^{-}\\right)\\right) \\end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\\left.x \\in\\right] x_{A}, x_{A+1}[$ and $A=1,2, \\ldots, n$\\\\ ii. $E I u_{, x x}\\left(0^{+}\\right)=M$\\\\ iii. EI $u_{, x x x}\\left(0^{+}\\right)=Q$\\\\ iv. $E I u_{, x x}\\left(x_{A}^{+}\\right)=E I u_{, x x}\\left(x_{A}^{-}\\right)$, where $A=2,3, \\ldots, n$\\\\ v. $E I u_{, x x x}\\left(x_{A}^{+}\\right)=E I u_{, x x x}\\left(x_{A}^{-}\\right)$, where $A=2,3, \\ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v).\n",
      "chunk word length: 1096, chunk char length: 7512, chunk = \\subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \\Omega \\rightarrow \\mathbb{R}$ (recall $\\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\\Omega)$, if its derivatives of order $j$, where $0 \\leq j \\leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \\ldots, \\infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \\in \\Omega$ ). \\subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\\infty}$. \\subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\\Omega$, as are all its derivatives; hence it is $C^{\\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \\in \\Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \\geq 0$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \\subsection*{Example 3} The function \\[ f(x)= \\begin{cases}x, & x \\leq \\frac{1}{2} \\tag{1.I.1}\\\\ 1 / 2, & x>\\frac{1}{2}\\end{cases} \\] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \\geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \\[ f(x)=\\left\\{\\begin{array}{cc} \\frac{x^{2}}{2}, & x \\leq \\frac{1}{2} \\tag{1.I.2}\\\\ \\frac{\\left(x-\\frac{1}{4}\\right)}{2}, & x>\\frac{1}{2} \\end{array}\\right. \\] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a \"smooth\" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\\overline{\\boldsymbol{\\Omega}}=[0,1]$, instead of $\\boldsymbol{\\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\\Omega)$ and $C_{b}^{k}(\\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally \"rough.\" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a \"generalized derivative,\" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \\begin{equation*} H^{k}=H^{k}(\\Omega)=\\left\\{w \\mid w \\in L_{2} ; w_{, x} \\in L_{2} ; \\ldots ; \\underbrace{w_{x \\ldots x}}_{\\text{k times}} \\in L_{2}\\right\\} \\tag{1.I.3} \\end{equation*} where \\begin{equation*} L_{2}=L_{2}(\\Omega)=\\left\\{w \\mid \\int_{0}^{1} w^{2} d x<\\infty\\right\\} \\tag{1.I.4} \\end{equation*} In words, the Sobolev space of degree $k$, denoted by $\\boldsymbol{H}^{\\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \\subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \\subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain \"singularities\" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in \"functional analysis.\" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein.\n",
      "chunk word length: 23, chunk char length: 167, chunk = \\begin{enumerate} \\item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \\end{enumerate}\n",
      "chunk word length: 60, chunk char length: 424, chunk = \\begin{enumerate} \\setcounter{enumi}{1} \\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \\item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \\item B. A. Finlayson and L. E. Scriven, \"The Method of Weighted Residuals-A Review,\" Applied Mechanics Reviews, 19, (1966), 735-738. \\end{enumerate}\n",
      "chunk word length: 86, chunk char length: 622, chunk = \\begin{enumerate} \\setcounter{enumi}{4} \\item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \\item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \\item J. Barlow, \"Optimal Stress Locations in Finite Element Models,\" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \\item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \\end{enumerate}\n",
      "chunk word length: 31, chunk char length: 232, chunk = \\begin{enumerate} \\setcounter{enumi}{9} \\item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, \"Stiffness and deflection analysis of complex structures,\" Journal of Aeronautical Sciences, 23 (1956), 805-823. \\end{enumerate}\n",
      "chunk word length: 27, chunk char length: 223, chunk = \\begin{enumerate} \\setcounter{enumi}{10} \\item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \\item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \\end{enumerate}\n",
      "chunk word length: 54, chunk char length: 389, chunk = \\begin{enumerate} \\setcounter{enumi}{12} \\item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \\item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \\item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \\end{enumerate} \\maketitle\n",
      "chunk word length: 501, chunk char length: 3959, chunk = \\subsection*{2.1 INTRODUCTORY REMARKS} It makes no sense to attempt to \"solve\" a boundary-value problem without a precise knowledge of what the problem is. The truth of this statement seems self-evident. Unfortunately, attempts are often made to solve vaguely defined problems, creating considerable confusion and, sometimes, totally erroneous results. In this chapter we present precise statements of multidimensional boundary-value problems in classical linear heat conduction and elastostatics. The presentation is similar in many respects to that for the one-dimensional model problem of Chapter 1. In particular, we discuss strong and weak forms, their equivalence, corresponding Galerkin formulations, the definitions of element arrays, and pertinent data processing concepts. In multidimensions, the data processing ideas necessarily become more involved. The reader is urged to study them carefully as they are necessary in order to understand the computer implementation of finite element techniques. \\subsection*{2.2 PRELIMINARIES} Let $n_{s d}(=2$ or 3 ) denote the number of space dimensions of the problem under consideration. Let $\\Omega \\subset \\mathbb{R}^{n_{sd}}$ be an open set\\footnote{For our purposes, it is sufficient to think of an open set as one without its boundary. }. We shall employ the following alternative representations for $\\boldsymbol{x}$ and $\\boldsymbol{n}$ : with piecewise smooth boundary $\\Gamma$. A general point in $\\mathbb{R}^{n_{sd}}$ is denoted by $\\boldsymbol{x}$. We will identify the point $\\boldsymbol{x}$ with its position vector emanating from the origin of ${R}^{n_{sd}}$. The unit outward normal vector to $\\Gamma$ is denoted by $\\boldsymbol{n}$. \\begin{align*} & \\left(n_{s d}=2\\right): \\quad x=\\left\\{x_{i}\\right\\}=\\left\\{\\begin{array}{l} x_{1} \\\\ x_{2} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} x \\\\ y \\end{array}\\right\\} \\quad n=\\left\\{n_{i}\\right\\}=\\left\\{\\begin{array}{l} n_{1} \\\\ n_{2} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} n_{x} \\\\ n_{y} \\end{array}\\right\\} \\tag{2.2.1}\\\\ & \\left(n_{s d}=3\\right): \\quad x=\\left\\{x_{i}\\right\\}=\\left\\{\\begin{array}{l} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} x \\\\ y \\\\ z \\end{array}\\right\\} \\quad n=\\left\\{n_{i}\\right\\}=\\left\\{\\begin{array}{l} n_{1} \\\\ n_{2} \\\\ n_{3} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} n_{x} \\\\ n_{y} \\\\ n_{z} \\end{array}\\right\\} \\tag{2.2.2} \\end{align*} where $x_{i}$ and $n_{i}, 1 \\leq i \\leq n_{s d}$, are the Cartesian components of $\\boldsymbol{x}$ and $\\boldsymbol{n}$, respectively; see Figure 2.2.1. Unless otherwise specified we shall work in terms of Cartesian components of vectors and tensors.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-02} Figure 2.2.1\\\\ We assume that $\\Gamma$ admits the decomposition \\begin{equation*} \\Gamma=\\overline{\\Gamma_{g} \\cup \\Gamma_{h}} \\tag{2.2.3} \\end{equation*} where \\begin{equation*} \\Gamma_{g} \\cap \\Gamma_{h}=\\varnothing \\tag{2.2.4} \\end{equation*} and $\\Gamma_{g}$ and $\\Gamma_{h}$ are open sets in $\\Gamma$. The notations are defined as follows: $\\cup$ is the set union symbol. Thus $\\Gamma_{g} \\cup \\Gamma_{h}$ means the set of all points $x$ contained in either $\\Gamma_{g}$ or $\\Gamma_{h}$. Also, $\\cap$ is the set intersection symbol. Thus $\\Gamma_{g} \\cap \\Gamma_{h}$ means the set of all points contained in both $\\Gamma_{g}$ and $\\Gamma_{h}$. The empty set is denoted by $\\varnothing$. Thus (2.2.4) means that there is no point $x$ contained in both $\\Gamma_{g}$ and $\\Gamma_{h}$ (i.e., $\\Gamma_{g}$ and $\\Gamma_{h}$ do not intersect or overlap). A bar above a set means set closure, i.e., the union of the set with its boundary. Thus \\begin{equation*} \\bar{\\Omega}={\\Omega} \\cup \\Gamma \\tag{2.2.5} \\end{equation*} To understand the meaning of $\\overline{\\Gamma_{g} \\cup \\Gamma_{h}}$, we must define the boundaries of $\\Gamma_{g}$ and $\\Gamma_{h}$ in $\\Gamma$. We shall do this with the aid of an example.\n",
      "chunk word length: 2562, chunk char length: 18926, chunk = Let $\\Omega=\\left\\{x \\in \\mathbb{R}^{2} \\mid x^{2}+y^{2}<1\\right.$, i.e., the interior of the unit disc $\\}$. The boundary of $\\Omega$ is $\\Gamma=\\left\\{x \\in R^{2} \\mid x^{2}+y^{2}=1\\right.$, i.e., the unit circle $\\}$. Let \\begin{equation*} \\Gamma_{g}=\\Gamma \\cap\\left\\{x \\in \\mathbb{R}^{2} \\mid y>0\\right\\} \\tag{2.2.6} \\end{equation*} The \"boundary of $\\Gamma_{g}$ \" consists of the endpoints of the upper semicircle, i.e., $\\left\\{x \\in R^{2} \\mid x=-1, y=0\\right.$, and $\\left.x=+1, y=0\\right\\}$. Thus \\begin{equation*} \\overline{\\Gamma_{g}}=\\Gamma \\cap\\left\\{x \\in R^{2} \\mid y \\geq 0\\right\\} \\tag{2.2.7} \\end{equation*} Similarly, let \\begin{equation*} \\Gamma_{h}=\\Gamma \\cap\\left\\{x \\in \\mathbb{R}^{2} \\mid y<0\\right\\} \\tag{2.2.8} \\end{equation*} Thus \\begin{equation*} \\overline{\\Gamma_{h}}=\\Gamma \\cap\\left\\{x \\in R^{2} \\mid y \\leq 0\\right\\} \\tag{2.2.9} \\end{equation*} Clearly \\begin{equation*} \\overline{\\Gamma_{g} \\cup \\Gamma_{h}}=\\overline{\\Gamma_{g}} \\cup \\overline{\\Gamma_{h}}=\\bar{\\Gamma} \\tag{2.2.10} \\end{equation*} These sets are depicted in Fig. 2.2.2.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-03} Figure 2.2.2 We shall assume throughout that $\\Gamma_{g} \\neq \\varnothing$ but allow for the case $\\Gamma_{h}=\\varnothing$.\\\\ Let the indices $i, j, k, l$, run over the values $1, \\ldots, n_{s d}$. Differentiation is denoted by a comma (e.g., $u_{, i}=u_{, x_{i}}=\\partial u / \\partial x_{i}$ ) and repeated indices imply summation (e.g., in $\\mathbb{R}^{3}, u_{, i i}=u_{, 11}+u_{, 22}+u_{, 33}=\\partial^{2} u / \\partial x^{2}+\\partial^{2} u / \\partial y^{2}+\\partial^{2} u / \\partial z^{2}$ ). The summation convention only applies to the indices $i, j, k$, and $l$ and only to two repeated indices. If there are three, or more, repeated indices in an expression, then\\\\ the summation convention is not in effect. (This is in keeping with the usual convention.) Divergence theorem. Let $f: \\bar{\\Omega} \\rightarrow \\mathbb{R}$ be $C^{1}$. Then \\begin{equation*} \\int_{\\boldsymbol{\\Omega}} f_{,i} d \\boldsymbol{\\Omega}=\\int_{\\boldsymbol{\\Gamma}} f \\boldsymbol{n_{i}} d \\boldsymbol{\\Gamma} \\tag{2.2.11} \\end{equation*} The proof may be found in [1].\\\\ \\textbf{Integration by parts.} Let $f$ be as above and also let $g: \\overline{\\Omega} \\rightarrow \\mathbb{R}$ be $C^{\\mathbf{1}}$. Then \\begin{equation*} \\int_{\\mathbf{\\Omega}} f_{,i} g d \\mathbf{\\Omega}=-\\int_{\\mathbf{\\Omega}} f g_{,i} d \\mathbf{\\Omega}+\\int_{\\mathbf{\\Gamma}} f g \\mathbf{n_{i}} d \\mathbf{\\Gamma} \\tag{2.2.12} \\end{equation*} \\textit{Proof}. We integrate the identity (i.e., \"product rules of differentiation,\" see [1]) $$ (f g)_{,i}=f_{,i} g+f g_{,i} $$ to get $$ \\int_{\\mathbf{\\Omega}}(f g)_{,i} d \\mathbf{\\Omega}=\\int_{\\mathbf{\\Omega}} f_{,i} g d \\mathbf{\\Omega}+\\int_{\\mathbf{\\Omega}} f g_{,i} d \\mathbf{\\Omega} $$ and then use the divergence theorem to convert the left-hand side into a boundary integral. \\subsection*{2.3 CLASSICAL LINEAR HEAT CONDUCTION: STRONG AND WEAK FORMS; EQUIVALENCE} Let $q_{i}$ denote (Cartesian components of) the heat flux vector, let $u$ be the temperature, and let $\\ell$ be the heat supply per unit volume. Assume the heat flux vector is defined in terms of the temperature gradient by the generalized Fourier law\\footnote{The generalized fourier law is a constitutive equation, or equation of state, which reflects the heat conduction properties of the body (i.e., $\\Omega$ ) under consideration. }: \\begin{equation*} q_{i}=-\\kappa_{i j} u_{, j}, \\quad \\kappa_{i j}=\\kappa_{j i} \\quad \\text { (symmetry) } \\tag{2.3.1} \\end{equation*} where the conductivities, $\\kappa_{i j}$ 's, are given functions of $x$. (If the $\\kappa_{i j}$ 's are constant throughout $\\Omega$, the body is said to be homogeneous.) The conductivity matrix, $\\boldsymbol{\\kappa}=\\left[\\kappa_{i j}\\right]$, is assumed positive definite (see the definition in Sec. 1.9). The most common situation in practice is the isotropic case in which $\\kappa_{i j}(x)=\\kappa(x) \\delta_{i j}$, where $\\delta_{i j}$ is the Kronecker delta. A formal statement\\footnote{By a formal statement, we mean one in which we do not precisely delineate the spaces to which the functions involved belong.} of the strong form of the boundary-value problem is as follows: \\[ (S) \\begin{cases} \\begin{alignedat}{3} &\\text{Given } \\ell : \\Omega \\to \\mathbb{R},\\ g : \\Gamma_g \\to \\mathbb{R},\\ \\text{and } h : \\Gamma_h \\to \\mathbb{R},\\ \\text{find } u : \\overline{\\Omega} \\to \\mathbb{R}\\ \\text{such that} & & \\\\ & q_{i,i} = \\ell \\quad \\quad \\text{in } \\Omega \\quad \\text{(heat equation)} \\quad \\text{(2.3.2)} \\\\ & u = g \\quad \\quad \\text{on } \\Gamma_g \\quad \\quad \\quad \\quad \\quad \\text{(2.3.3)}\\footnotemark \\\\ & - q_i n_i = h \\quad \\quad \\text{on } \\Gamma_h \\quad \\quad \\quad \\text{(2.3.4)} \\\\ & \\text{where $q_i$ is defined by (2.3.1).} \\end{alignedat} \\end{cases} \\] \\footnotetext{The statement $u=g$ on $\\Gamma_{g}$ means $u(x)=g(x)$ for all $x \\in \\Gamma_{g}$, and so on.} The functions $g$ and $h$ are the prescribed boundary temperature and heat flux, respectively. This problem possesses a unique solution for appropriate restrictions on the given data. In more mathematical terminology, (2.3.2) is a generalized Poisson equation, (2.3.3) is a Dirichlet boundary condition, and (2.3.4) is a Neumann boundary condition. We shall now construct a weak formulation of the boundary-value problem analogous to that for the one-dimensional problem of Chapter 1. In particular, (2.3.3) and (2.3.4) will be treated as essential and natural boundary conditions, respectively. As before, let $\\delta$ denote the trial solution space and $\\underline{\\mathcal{V}}$ the variation space. This time $\\delta$ and $\\mathcal{V}$ consist of real-valued functions defined on $\\bar{\\Omega}$ satisfying certain smoothness requirements, such that all members of $\\delta$ satisfy (2.3.3), whereas if $w \\in \\mathcal{V}$, then \\begin{equation*} w=0 \\quad \\text { on } \\Gamma_{g} \\tag{2.3.5} \\end{equation*} The weak formulation of the problem goes as follows: \\[ (W) \\begin{cases} \\begin{alignedat}{3} & \\text{Given } \\ell : \\Omega \\to \\mathbb{R}, g : \\Gamma_g \\to \\mathbb{R}, \\text{and } h : \\Gamma_h \\to \\mathbb{R}, \\text{find } u \\in \\delta \\text{ such that for all } w \\in \\mathcal{V}, \\\\ & - \\int_{\\Omega} w_{,i} q_i \\, d\\Omega = \\int_{\\Omega} w \\ell \\, d\\Omega + \\int_{\\Gamma_h} w h \\, d\\Gamma \\quad \\quad \\quad \\text{(2.3.6)} \\\\ & \\text{where \\(q_i\\) is defined by (2.3.1).} \\end{alignedat} \\end{cases} \\] \\textbf{Theorem.} Assume all functions involved are smooth enough to justify the manipulations. Then a solution of $(S)$ is a solution of $(W)$ and vice versa. \\textbf{Proof. 1.} Assume $u$ is the solution of ($S$). By virtue of (2.3.3), $u \\in \\delta$. Pick any $w \\in \\mathcal{V}$ and proceed as follows: \\[ \\begin{aligned} & 0 = \\int_{\\Omega} w \\underbrace{(q_{i,i} - \\ell)}_{0} \\, d\\Omega \\quad \\quad \\quad \\text{(heat equation, i.e., (2.3.2))}\\\\ & =-\\int_{\\Omega} w_{,i} q_{i} d \\Omega+\\int_{\\Gamma} w q_{i} n_{i} d \\Gamma-\\int_{\\Omega} w \\ell d \\Omega \\quad \\text { (integration by parts) } \\\\ & =-\\int_{\\Omega} w_{,i} q_{i} d \\Omega-\\int_{\\Gamma_{h}} w h d \\Gamma-\\int_{\\Omega} w \\ell d \\Omega \\quad \\begin{array}{l} \\left(w=0 \\text { on } \\Gamma_{g},\\right. \\text { and heat flux } \\\\ \\text { boundary condition, i.e. (2.3.4)) } \\end{array} \\end{aligned} \\] Therefore (2.3.6) is satisfied and so $u$ is a solution of ($W$).\\\\ 2. Assume $u$ is the solution of (W). Then $u=g$ on $\\Gamma_{g}$, and for all $w \\in \\mathcal{V}$ \\begin{align*} 0 & =\\int_{\\Omega} w_{,i} q_{i} d \\Omega+\\int_{\\Omega} w \\ell d \\Omega+\\int_{\\Gamma_{h}} w h d \\Gamma \\text{(by (2.3.6))}\\\\ & =\\int_{\\Omega} w\\left(-q_{i, i}+\\ell) d \\Omega+\\int_{\\Gamma_{h}} w\\left(n_{i} q_{i}+h\\right) d \\Gamma\\right. \\tag{2.3.7} \\end{align*} Let $$ \\begin{aligned} & \\alpha=-q_{i, i}+\\ell \\\\ & \\beta=q_{i} n_{i}+h \\end{aligned} $$ To show (2.3.2) and (2.3.4) are satisfied and thus complete the proof, we must prove that $$ \\begin{array}{ll} \\alpha=0 & \\text { on } \\Omega \\\\ \\beta=0 & \\text { on } \\Gamma_{h} \\end{array} $$ First pick $w=\\alpha \\phi$ where\\\\ i. $\\phi>0$ on $\\Omega$;\\\\ ii. $\\phi=0$ on $\\Gamma$; and\\\\ iii. $\\phi$ is smooth.\\\\ (These conditions insure that $w \\in \\mathcal{V}$.) With this choice for $w,(2.3 .7)$ becomes $$ 0=\\int_{\\Omega} \\alpha^{2} \\phi d \\Omega $$ which implies $\\alpha=0$ on $\\Omega$.\\\\ Now pick $w=\\beta \\psi$, where\\\\ i'. $\\psi>0$ on $\\Gamma_{h}$;\\\\ ii'. $\\psi=0$ on $\\Gamma ;$ and\\\\ iii'. $\\psi$ is smooth.\\\\ (These conditions insure that $w \\in \\mathcal{V}$.) With this choice for $w,(2.3 .7)$ becomes\\\\ (making use of $\\boldsymbol{\\alpha}=0$ ): $$ 0=\\int_{\\Gamma_{h}} \\beta^{2} \\psi d \\Gamma $$ from which it follows that $\\beta=0$ on $\\Gamma_{h}$. Thus $u$ is a solution of $(S)$.\\\\ It is convenient to introduce an abstract version of (2.3.6). Let \\begin{gather*} a(w, u)=\\int_{\\Omega} w_{,i} \\kappa_{i j} u_{, j} d \\Omega \\tag{2.3.8}\\\\ (w, \\ell)=\\int_{\\Omega} w \\ell d \\Omega \\tag{2.3.9}\\\\ (w, h)_{\\Gamma}=\\int_{\\Gamma_{h}} w h d \\Gamma \\tag{2.3.10} \\end{gather*} Then (2.3.6) may be written as \\begin{equation*} a(w, u)=(w, \\ell)+(w, h)_{\\Gamma} \\tag{2.3.11} \\end{equation*} \\subsection*{Exercise 1.} Verify that $a(\\cdot, \\cdot),(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)_{r}$, as just defined, are symmetric bilinear forms. (Note that the symmetry of $a(\\cdot, \\cdot)$ follows from the symmetry of the conductivities.) In manipulating terms in theories involving vector and tensor quantities, the indicial notation used is very explicit and convenient. However, when we come to the Galerkin formulation analogous to (2.3.6), additional indices necessarily appear. The situation becomes very complicated due to the greater number of indices involved and due to the ranges of the various indices being different. When we come to elasticity theory, the situation is even worse as the corresponding terms have an even greater number of indices. For these reasons it is useful at this point to adopt an index-free notation for (2.3.6). Aside from stemming the proliferation of indices, we shall find later on that this formulation is conducive to the computer implementation of the element arrays, especially in more complicated situations such as elasticity. In introducing our index-free notation we shall assume for definiteness that $n_{s d}=2$. Let $\\nabla$ denote the gradient operator; thus \\begin{align*} & \\nabla u=\\left\\{u_{, i}\\right\\}=\\left\\{\\begin{array}{l} u_{, 1} \\\\ u_{, 2} \\end{array}\\right\\} \\tag{2.3.12}\\\\ & \\nabla w=\\left\\{w_{,}\\right\\}=\\left\\{\\begin{array}{l} w_{, 1} \\\\ w_{, 2} \\end{array}\\right\\} \\tag{2.3.13} \\end{align*} In the case of two space dimensions, the conductivity matrix may be written as \\[ \\kappa=\\left[\\kappa_{i j}\\right]=\\left[\\begin{array}{ll} \\kappa_{11} & \\kappa_{12} \\tag{2.3.14}\\\\ \\kappa_{21} & \\kappa_{22} \\end{array}\\right] \\quad \\text { (symmetric) } \\] In the isotropic case, (2.3.14) simplifies to \\[ \\kappa=\\kappa\\left[\\delta_{i j}\\right]=\\kappa\\left[\\begin{array}{ll} 1 & 0 \\tag{2.3.15}\\\\ 0 & 1 \\end{array}\\right] \\] In terms of the above expressions, the integrand of (2.3.8) may be written in indexfree fashion: \\begin{equation*} w_{, i} \\kappa_{i j} u_{, j}=(\\nabla w)^{T} \\kappa(\\nabla u) \\tag{2.3.16} \\end{equation*} Thus in place of (2.3.8) we may write \\begin{equation*} a(w, u)=\\int_{\\Omega}(\\nabla w)^{T} \\kappa(\\nabla u) d \\Omega \\tag{2.3.17} \\end{equation*} \\subsection*{Exercise 2.} Verify (2.3.16) for the cases $n_{s d}=2$ and 3. \\subsection*{2.4 HEAT CONDUCTION: GALERKIN FORMULATION; SYMMETRY AND POSITIVE-DEFINITENESS OF K} Let $\\delta^{h}$ and $\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\delta$ and $\\mathcal{V}$, respectively. We assume all members of $\\mathcal{V}^{h}$ vanish, or vanish approximately, on $\\Gamma_{g}$ and that each member of $\\delta^{h}$ admits the representation \\begin{equation*} u^{h}=v^{h}+g^{h} \\tag{2.4.1} \\end{equation*} where $v^{h} \\in \\mathcal{V}^{h}$ and $g^{h}$ results in satisfaction, or at least approximate satisfaction, of the boundary condition $u=g$ on $\\Gamma_{g}$. The Galerkin formulation is given as follows:\\\\ (G) $\\left\\{\\begin{array}{c}\\text { Given } \\ell, g \\text {, and } h\\left[\\text { as in (W)], find } u^{h}=v^{h}+y^{h} \\in \\delta^{h} \\text { such that for all }\\right. \\\\ w^{h} \\in \\mathcal{V}^{h}(\\mathrm{cf} . \\text { Sec. 1.5): } \\\\ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell \\right)+\\left(w^{h}, h\\right)_{\\Gamma}-a\\left(w^{h}, g^{h}\\right) \\\\ \\text { (2.4.2) }\\end{array}\\right.$ We now view our domain as \"discretized\" into element domains $\\Omega^{\\text {e }}$, $1 \\leq e \\leq n_{e l}$. In two dimensions the element domains might be simply triangles and quadrilaterals; see Fig. 2.4.1. Nodal points may exist anywhere on the domain but most frequently appear at the element vertices and interelement boundaries and less often in the interiors.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-09} Figure 2.4.1\\\\ In Sec. 1.9 the global nodal ordering and ordering of equations in the matrix system coincided. In multidimensional applications this would prove to be an inconvenient restriction with regard to data preparation. In what follows, a more flexible scheme is described. Let $\\boldsymbol{\\eta}=\\left\\{1,2, \\ldots, n_{\\text {np}}\\right\\}$, the set of global node numbers where $n_{\\text {np}}$ is the number of nodal points. By the terminology $g$-node we shall mean a node, $A$, at which it is prescribed that $\\boldsymbol{u}^{\\boldsymbol{h}}=\\boldsymbol{g}$. Let $\\eta_{g} \\subset \\eta$ be the set of \" $g$-nodes.\" The complement of $\\eta_{g}$ in $\\eta$, denoted by $\\eta-\\eta_{g}$, is the set of nodes at which $\\boldsymbol{u}^{\\boldsymbol{h}}$ is to be determined. The number of nodes in $\\boldsymbol{\\eta}-\\boldsymbol{\\eta}_{\\boldsymbol{g}}$ equals $n_{\\text {eq }}$, the number of equations. A typical member of $\\mathcal{V}^{h}$ is assumed to have the form \\begin{equation*} w^{h}(x)=\\sum_{A \\in \\eta-\\eta_{g}} N_{A}(x) c_{A} \\tag{2.4.3} \\end{equation*} where $N_{A}$ is the shape function associated with node number $A$ and $c_{A}$ is a constant. We assume throughout that $w^{h}=0$ if and only if $c_{A}=0$ for each $A \\in \\eta-\\eta_{g}$. Likewise \\begin{equation*} v^{h}(x)=\\sum_{A \\in \\eta-\\eta_{g}} N_{A}(x) d_{A} \\tag{2.4.4} \\end{equation*} where $d_{A}$ is the unknown at node $A$ (i.e., temperature) and \\begin{equation*} g^{h}(x)=\\sum_{A \\in \\eta_{g}} N_{A}(x) g_{A}, \\quad g_{A}=g\\left(x_{A}\\right) \\tag{2.4.5} \\end{equation*} From (2.4.5), we see that $g^{h}$ has been defined to be the nodal interpolate of $g$ by way of the shape functions. \\footnote{This is not the only possibility, nor the best from the standpoint of accuracy. However, in practice it is generally the most convenient. } Consequently, $g^{h}$ will be, generally, only an approximation of $g$. See Fig. 2.4.2. Additional sources of error are (1) the use of approximations $\\ell^{h}$ and $h^{h}$ in place of $\\ell$ and $h$, respectively; and (2) domain approximations in which the element boundaries do not exactly coincide with $\\Gamma$. Analyses of these approximations are presented in Strang and Fix [2].\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-10} Figure 2.4.2 Piecewise linear approximation of boundary data (schematic).\\\\ Substituting (2.4.3)-(2.4.5) into (2.4.2) and arguing as in Sec. 1.6, results in \\[ \\begin{array}{r} \\sum_{B \\in \\eta-\\eta_{g}} a\\left(N_{A}, N_{B}\\right) d_{B}=\\left(N_{A}, \\ell\\right)+\\left(N_{A}, h\\right)_{\\Gamma}-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right) g_{B} \\tag{2.4.6}\\\\ A \\in \\eta-\\eta_{g} \\end{array} \\] To define the global stiffness matrix and force vector, we need to first specify the global ordering of equations. For this purpose we introduce the ID array, sometimes called the destination array, which assigns to node $A$ the corresponding global equation number, viz., \\[ \\text{ID}(A) = \\begin{cases} \\overbrace{P}^{\\text{Global equation number}} & \\text{if } A \\in \\eta - \\eta_{g}, \\\\ 0 & \\text{if } A \\in \\eta_{g} \\end{cases} \\] where $1 \\leq P \\leq n_{e q}$. The dimension of ID is $n_{n p}$. As may be seen from (2.4.7), nodes at which $g$ is prescribed are assigned \"equation number\" zero. An example of the setup of ID and other important data-processing arrays is presented in Sec. 2.6. The matrix equivalent of (2.4.6) is given as follows: \\begin{gather*} K d=F \\tag{2.4.8}\\\\ \\boldsymbol{K}=\\left[K_{P Q}\\right], \\quad d=\\left\\{d_{Q}\\right\\}, \\quad F=\\left\\{F_{P}\\right\\}, \\quad 1 \\leq P, Q \\leq n_{e q} \\tag{2.4.9}\\\\ K_{P Q}=a\\left(N_{A}, N_{B}\\right), \\quad P=\\operatorname{ID}(A), \\quad Q=\\operatorname{ID}(B) \\tag{2.4.10}\\\\ F_{P}=\\left(N_{A}, \\ell\\right)+\\left(N_{A}, h\\right)_{\\Gamma}-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right) g_{B} \\tag{2.4.11} \\end{gather*} The main properties of $\\boldsymbol{K}$ are established in the following theorem. \\\\ \\\\ \\textbf{Theorem} \\begin{enumerate} \\item $K$ is symmetric. \\item $K$ is positive definite. \\end{enumerate} \\textbf{Proof} \\begin{enumerate} \\item The symmetry of $\\boldsymbol{K}$ follows directly from the symmetry of $a(\\cdot, \\cdot)$, viz., \\end{enumerate} $$ \\begin{aligned} K_{P Q} & =a\\left(N_{A}, N_{B}\\right) & & \\text { (by definition) } \\\\ & =a\\left(N_{B}, N_{A}\\right) & & \\text { (symmetry of } a(\\cdot, \\cdot)) \\\\ & =K_{Q P} & & \\text { (by definition) } \\end{aligned} $$ \\begin{enumerate} \\setcounter{enumi}{1} \\item (Recall that we must show (i) $c^{T} K c \\geq 0$ and (ii) $c^{T} K c=0$ implies $c=0$.) \\end{enumerate} To each $n_{e q}$-vector $c=\\left\\{c_{p}\\right\\}$, we may associate a member $w^{h} \\in \\mathcal{V}^{h}$ by the expression $\\boldsymbol{w}^{h}=\\Sigma_{A \\in \\eta-\\eta_{g}} N_{A} \\bar{c}_{A}$, where $\\bar{c}_{A}=c_{P}, P=\\operatorname{ID}(A)$.\\\\ i. $$ \\begin{array}{rlrl} \\boldsymbol{c}^{T} K c & =\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{PQ} c_{Q} & \\\\ & =\\sum_{A, B \\in \\eta - \\eta_g} \\bar{c}_{A} a\\left(N_{A}, N_{B}\\right) \\bar{c}_{B} & \\\\ & =a\\left(\\sum_{A \\in \\eta-\\eta_{g}} N_{A} \\bar{c}_{A}, \\sum_{B \\in \\eta-\\eta_{g}} N_{B} \\bar{c}_{B}\\right) & & \\text { (bilinearity of } a(\\cdot, \\cdot)) \\\\ & =a\\left(w^{h}, w^{h}\\right) & & \\text { (definition of } \\left.w^{h}\\right) \\\\ & =\\int_{\\Omega} \\underbrace{w_{,i}^{h} k_{ij} w_{,j}^{h}}_{\\geq 0} d \\Omega & & \\text { (positive-definiteness of conductivities) } \\\\ & \\geq 0 & & \\end{array} $$ ii. Assume $\\boldsymbol{c}^{\\boldsymbol{T}} \\boldsymbol{K c}=\\mathbf{0}$. By the proof of part (i), $$ \\int_{\\Omega} \\underbrace{w_{, i}^{h} \\kappa_{i j} w_{, j}^{h}}_{\\geq 0} d \\Omega=0 $$ and thus it follows that $$ w_{, i}^{h} \\kappa_{i j} w_{, j}^{h}=0 $$ By the positive-definiteness hypothesis on the conductivities, this requires $w_{, i}^{h}=0$ and so $w^{h}$ must be constant. However $w^{h}=0$ on $\\Gamma_{g}$ (which is not empty) and so $w^{h}$ must be zero throughout $\\Omega$. By the definition of\n",
      "chunk word length: 1055, chunk char length: 8732, chunk = $\\boldsymbol{w}^{\\boldsymbol{h}}$, it follows that each $c_{P}=0$; that is $c=0$, which was to be proved. \\\\ \\\\ \\\\ \\\\ \\\\ \\textbf{Remarks} \\begin{enumerate} \\item Observe that it is the positive-definiteness hypothesis on the constitutive coefficients (i.e., $\\kappa_{ij}$ 's) and the boundary condition incorporated in the definition of $\\mathcal{V}^{\\boldsymbol{h}}$ which together result in the positive-definiteness of $\\boldsymbol{K}$ and thus ensure its invertibility. \\item The explicit structure of the shape functions, which will be delineated in Chapter 3, will also result in $\\boldsymbol{K}$ being banded. \\end{enumerate} \\subsection*{Exercise 1.} (This exercise is a multidimensional analog of the one contained in Sec. 1.8.) Let $$ \\Gamma_{\\text{int }}=\\left(\\bigcup_{e=1}^{n_{el}} \\Gamma^{e}\\right)-\\Gamma \\quad \\text { (interior element boundaries) } $$ One side of $\\Gamma_{\\text{int}}$ is (arbitrarily) designated to be the \" + side\" and the other is the \" - side.\" Let $\\boldsymbol{n}^{+}$and $\\boldsymbol{n}^{-}$be unit normals to $\\Gamma_{\\text {int }}$ which point in the minus and plus directions, respectively. Clearly $n^{+}=-n^{-}$. Let $q_{i}^{+}$and $q_{i}^{-}$denote the values of $q_{i}$ obtained by approaching $x \\in \\Gamma_{\\mathrm{int}}$ from + and - sides, respectively. The \"jump\" in $q_{n}=q_{i} n_{i}$ at $x$ is defined to be $$ \\begin{aligned} {\\left[q_{n}\\right] } & =\\left(q_{i}^{+}-q_{i}^{-}\\right) n_{i}^{+} \\\\ & =q_{i}^{+} n_{i}^{+}+q_{i}^{-} n_{i}^{-} \\end{aligned} $$ As may be easily verified, the jump is invariant with respect to reversing the + and - designations. Consider the weak formulation (i.e., (2.3.6)) and assume $w$ and $u$ are smooth on the element interiors but may experience discontinuities in gradient across element boundaries. (Functions of this type contain the standard $C^{0}$ finite element interpolations; see Chapter 3.) Show that $$ 0=\\sum_{e=1}^{n_{e l}} \\int_{\\Omega^{e}} w\\left(q_{i, i}-\\ell\\right) d \\Omega-\\int_{\\Gamma_{h}} w\\left(q_{n}+h\\right) d \\Gamma-\\int_{\\Gamma_{int}} w\\left[q_{n}\\right] d \\Gamma $$ from which the Euler-Lagrange conditions may be readily deduced:\\\\ i. $q_{i, i}= \\ell$ in $\\bigcup_{e=1}^{n_{el}} \\Omega^{e}$\\\\ ii. $-q_{n}=h$ on $\\Gamma_{h}$\\\\ iii. $\\left[q_{n}\\right]=0$ on $\\Gamma_{\\text {int }}$ As may be seen, (i) is the heat equation on the element interiors and (iii) is a continuity condition across element boundaries on the heat flux. Contrast the present results with those obtained assuming $w$ and $u$ are globally smooth. The Galerkin finite element formulation obtains an approximate solution to (i) through (iii). \\subsection*{2.5 HEAT CONDUCTION: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As before, we can break up the global arrays into sums of elemental contributions: \\[ \\begin{array}{ll} \\boldsymbol{K}=\\sum_{e=1}^{n_{el}} \\boldsymbol{K}^{e}, & \\boldsymbol{K}^{e}=\\left[K_{P Q}^{e}\\right] \\\\ \\boldsymbol{F}=\\sum_{e=1}^{n_{el}} \\boldsymbol{F}^{e}, & \\boldsymbol{F}^{e}=\\left\\{F_{P}^{e}\\right\\} \\tag{2.5.2} \\end{array} \\] where \\begin{align*} & K_{P Q}^{e}=a\\left(N_{A}, N_{B}\\right)^{e}=\\int_{\\Omega^{e}}\\left(\\nabla N_{A}\\right)^{T} \\boldsymbol{k}\\left(\\nabla N_{B}\\right) d \\Omega \\tag{2.5.3}\\\\ & F_{P}^{e}=\\left(N_{A}, \\ell\\right)^{e}+\\left(N_{A}, h\\right)_{\\Gamma}^e-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right)^{e} g_{B} \\\\ & =\\int_{\\Omega^{e}} N_{A} \\ell d \\Omega+\\int_{\\Gamma_{h}^{e}} N_{A} h d \\Gamma-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right)^{e} g_{B} \\tag{2.5.4}\\\\ & \\Gamma_{h}^{e}=\\Gamma_{h} \\cap \\Gamma^{e}, \\quad P=\\operatorname{ID}(A), \\quad Q=\\operatorname{ID}(B) \\tag{2.5.5} \\end{align*} See Fig. 2.5.1 for an illustration of $\\Gamma_{h}^{e}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-13} The element stiffness, $\\boldsymbol{k}^{e}$, and element force vector, $\\boldsymbol{f}^{\\boldsymbol{e}}$, may be deduced from these equations: \\begin{align*} & k^{e}=\\left[k_{a b}^{e}\\right], \\quad f^{e}=\\left\\{f_{a}^{e}\\right\\}, \\quad 1 \\leq a, b \\leq n_{e n} \\tag{2.5.6}\\\\ & k_{a b}^{e}=a\\left(N_{a}, N_{b}\\right)^{e}=\\int_{\\Omega^{e}}\\left(\\nabla N_{a}\\right)^{T} \\kappa\\left(\\nabla N_{b}\\right) d \\Omega \\tag{2.5.7}\\\\ & f_{a}^{e}=\\int_{\\boldsymbol{\\Omega}^{e}} N_{a} \\ell d \\Omega+\\int_{\\Gamma_{h}^{e}} N_{a} h d \\Gamma-\\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \\tag{2.5.8} \\end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\\left(x_{b}^{e}\\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all' $x \\in \\bar{\\Omega}^{\\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\\boldsymbol{K}$ and $\\boldsymbol{F}$ may be formed from the element arrays $\\boldsymbol{k}^{\\boldsymbol{e}}$ and $\\boldsymbol{f}^{\\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \\begin{equation*} k^{e}=\\int_{\\boldsymbol{\\Omega}^{e}} B^{T} D B d \\Omega \\tag{2.5.9} \\end{equation*} where, in the present case, \\begin{align*} & \\underbrace{\\boldsymbol{D}}_{\\boldsymbol{n}_{s d} \\times \\boldsymbol{n}_{s d}}=\\boldsymbol{\\kappa} \\tag{2.5.10}\\\\ & \\underbrace{\\boldsymbol{B}}_{\\boldsymbol{n}_{s d} \\times \\boldsymbol{n}_{en}}=\\left[\\boldsymbol{B}_{1}, \\boldsymbol{B}_{2}, \\ldots, \\boldsymbol{B}_{n_{e n}}\\right] \\tag{2.5.11}\\\\ & \\underbrace{B_{a}}_{n_{s d} \\times 1}=\\nabla N_{a} \\tag{2.5.12} \\end{align*} The component version of $(2.5 .9)$ is \\begin{equation*} k_{a b}^{e}=\\int_{\\Omega^{e}} B_{a}^{T} D B_{b} d \\Omega \\tag{2.5.13} \\end{equation*} \\subsection*{Exercise 1.} Let\\\\ \\[ \\underbrace{d^{e}}_{n_{e n} \\times 1}=\\left\\{d_{a}^{e}\\right\\}=\\left\\{\\begin{array}{c} d_{1}^{e} \\tag{2.5.14}\\\\ d_{2}^{e} \\\\ \\vdots \\\\ d_{n_{e n}^{e}}^{e} \\end{array}\\right\\} \\] where \\begin{equation*} d_{a}^{e}=u^{h}\\left(x_{a}^{e}\\right) \\tag{7} \\end{equation*} $d^{e}$ is called the element temperature vector. Show that the heat flux vector at point $\\boldsymbol{x} \\in \\boldsymbol{\\Omega}^{\\boldsymbol{e}}$ can be calculated from the formula \\begin{equation*} q(x)=-D(x) B(x) d^{e}=-D(x) \\sum_{a=1}^{n_{en}} B_{a} d_{a}^{e} \\tag{2.5.16} \\end{equation*} \\subsection*{Exercise 2.} Consider the strong statement of the boundary-value problem in classical linear heat conduction in which the $h$-type boundary condition (i.e., eq. (2.3.4)) is replaced by the following expression: \\begin{equation*} \\lambda u-q_{i} n_{i}=h \\quad \\text { on } \\Gamma_{h} \\tag{2.5.17}\\footnotemark \\end{equation*} \\footnotetext{The $\\boldsymbol{g}$-boundary conditions are accounted for in this definition.} where $\\lambda \\geq 0$ is a given function of $x \\in \\Gamma_{h}$. Generalize the weak formulation to include (2.5.17) as a natural boundary condition. Obtain an expression for the additional contribution to $k_{a b}^{e}$ (cf. (2.5.13)) arising from (2.5.17). Show that $\\boldsymbol{K}$ is positive-definite. The boundary condition (2.5.17) is equivalent to what is often called Newton's law of heat transfer; $\\lambda$ is called the coefficient of heat transfer. This boundary condition applies to the case in which the heat flux is proportional to the difference of the surface temperatures of the body and surrounding medium, the latter formally represented by $h / \\lambda$ in (2.5.17). \\subsection*{2.6 HEAT CONDUCTION: DATA PROCESSING ARRAYS; \\\\ ID, IEN, AND LM} The element nodal data is stored in the array IEN, the element nodes array, which relates local node numbers to global node numbers, viz.,\\\\ \\[ \\text{IEN}(\\underbrace{a}_{\\text{Local node number}}, \\underbrace{e}_{\\text{Element number}}) = \\underbrace{A}_{\\text{Global node number}} \\] The relationship between global node numbers and global equation numbers as well as nodal boundary condition information is stored in the ID array (see 2.4.7). In practice, the IEN and ID arrays are set up from input data. The LM array, which was described in the context of the one-dimensional model problem in Sec. 1.14, may then be constructed from the relation\\\\ \\begin{equation*} \\operatorname{LM}(a, e)=\\operatorname{ID}(\\operatorname{IEN}(a, e)) \\tag{2.6.2} \\end{equation*} Because of the previous relationship, we often think of LM as the element \"localization\" of ID. Strictly speaking the LM array is redundant. However, it is generally convenient in computing to set up LM once and for all, rather than make use of (2.6.2) repeatedly. Example 1 illustrates the structure of the ID, IEN, and LM arrays.\n",
      "chunk word length: 521, chunk char length: 2712, chunk = Consider the mesh of four-node, rectangular elements shown in Fig. 2.6.1. We assume that the local node numbering begins at the lower left-hand node of each element and proceeds in counterclockwise fashion. This is illustrated in Fig. 2.6.1 for element 2, which is typical. We also assume that essential boundary conditions (i.e., \" $g$-type\") are specified at nodes $1,4,7$, and 10 . Thus there will only be eight equations in the global system $\\boldsymbol{K} d=\\boldsymbol{F}$. We adopt the usual convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. The ID, IEN, and LM arrays are given in Fig. 2.6.2. The reader is urged to verify the details.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-16} Figure 2.6.1 Mesh of four-node, rectangular elements; global and local node numbers, element numbers, and essential boundary condition nodes. ID array:\\\\ \\begin{center} \\overbrace{ \\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\hline 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\\\ \\hline $0 *$ & 1 & 2 & 0 & 3 & 4 & 0 & 5 & 6 & 0 & 7 & 8 \\\\ \\hline \\end{tabular}}^{\\text{Global node numbers ($\\boldsymbol{A}$)} } \\end{center}$\\quad\\left(n_{n p}=12\\right)$ \\vfill \\textbf{IEN array:} \\[ \\begin{array}{c|c c c c c c} & \\multicolumn{6}{c}{\\text{Element numbers} \\ (e)} \\\\ \\text{Local node numbers} \\ (a) & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline 1 & 1 & 2 & 4 & 5 & 7 & 8 \\\\ 2 & 2 & 3 & 5 & 6 & 8 & 9 \\\\ 3 & 5 & 6 & 8 & 9 & 11 & 12 \\\\ 4 & 4 & 5 & 7 & 8 & 10 & 11 \\\\ \\end{array} \\] \\[ A = \\text{IEN}(a, e) \\] \\[ (n_{en} = 4) \\quad (n_{el} = 6) \\] \\vspace{1cm} \\textbf{LM array:} \\[ \\begin{array}{c|c c c c c c} & \\multicolumn{6}{c}{\\text{Element numbers} \\ (e)} \\\\ \\text{Local node numbers} \\ (a) & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline 1 & 0^* & 1 & 0 & 3 & 0 & 5 \\\\ 2 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ 3 & 3 & 4 & 5 & 6 & 7 & 8 \\\\ 4 & 0 & 3 & 0 & 5 & 0 & 7 \\\\ \\end{array} \\] \\[ P = \\text{LM}(a, e) = \\text{ID}(\\text{IEN}(a, e)) \\] \\[ (n_{en} = 4) \\quad (n_{el} = 6) \\] \\footnotetext{\"Temperature boundary conditions (\"$g$-type\") denoted by zeros. } Figure 2.6.2. DD, IEN, and LM arrays for the mesh of Fig. 2.6.1. In terms of the IEN and LM arrays, a precise definition of the $g_{a}^{e}$ 's may be given (see (2.5.8)): \\[ g_{a}^{e}=\\left\\{\\begin{array}{ll} 0 & \\text { if } \\operatorname{LM}(a, e) \\neq 0 \\tag{2.6.3}\\\\ g_{A} & \\text { if } \\operatorname{LM}(a, e)=0, \\end{array} \\text { where } A=\\operatorname{IEN}(a, e)\\right. \\] This definition may be easily programmed.\\\\ In our final example of this section we shall illustrate the assembly procedure for a typical element subjected to essential boundary conditions.\n",
      "chunk word length: 531, chunk char length: 3980, chunk = Consider a typical, four-noded element $e$. Assume values of the LM array, for this element, are given as follows: \\[ \\left.\\begin{array}{l} \\mathrm{LM}(1, e)=5 \\\\ \\mathrm{LM}(2, e)=0 \\tag{2.6.4}\\\\ \\mathrm{LM}(3, e)=0 \\\\ \\mathrm{LM}(4, e)=9 \\end{array}\\right\\} \\] We deduce from (2.6.4) that the contributions to the global arrays are given as follows:\\footnote{Due to symmetry, $k_{41}^{e}$ is not actually assembled in practice.} \\[ \\left.\\begin{array}{c} K_{55} \\leftarrow K_{55}+k_{11}^{e} \\\\ K_{59} \\leftarrow K_{99}+k_{14}^{e} \\tag{2.6.5}\\\\ K_{95} \\leftarrow K_{95}+k_{41}^{e} \\\\ K_{99} \\leftarrow K_{99}+k_{44}^{e} \\end{array}\\right\\} \\] \\[ \\left.\\begin{array}{c} F_{5} \\leftarrow F_{5}+f_{1}^{e} \\\\ \\tag{2.6.6}\\\\ F_{9} \\leftarrow F_{9}+f_{4}^{e} \\\\ \\end{array}\\right\\} \\] Note that all terms in the second and third rows and columns of $\\boldsymbol{k}^{\\boldsymbol{e}}$ do not contribute to $K$. However, they may contribute to $F$ via $f_{1}^{f}$ and $f_{4}^{f}$, since, by (2.5.8) we have \\begin{align*} & f_{1}^{e}=\\cdots-k_{12}^{e} g_{2}^{e}-k_{13}^{e} g_{3}^{e} \\tag{2.6.7}\\\\ & f_{4}^{e}=\\cdots-k_{42}^{e} g_{2}^{e}-k_{43}^{e} g_{3}^{e} \\tag{2.6.8} \\end{align*} in which, for clarity, we have omitted the first two terms of the right-hand side of (2.5.8).\\\\ Special subroutines are easily programmed to carry out the operations indicated in (2.6.5)-(2.6.8). It is instructive to visualize the contributions of the \\textit{e}th element to the global stiffness and force. These contributions are depicted in Fig. 2.6.3. We note that all necessary element assembly information is provided by the LM array. Sec. 2.7 Classical Linear Elastostatics: Strong and Weak Forms\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-19(1)} Figure 2.6.3 Contributions of heat conduction element in Example 2 to global arrays. \\subsection*{Exercise 1.} Consider the accompanying mesh. Set up the ID, IEN, and LM arrays.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-19} \\subsection*{2.7 CLASSICAL LINEAR ELASTOSTATICS: \\\\ STRONG AND WEAK FORMS; EQUIVALENCE} Classical elastostatics is a rich subject in its own right. See [3-7] for background and references to the literature. These references range from the very physical to the very\\\\ mathematical. The most physical book is the one by Timoshenko and Goodier. In ascending order of mathematical content are Sokolnikoff, Gurtin, Duvaut-Lions, and Fichera. The reader is reminded that indices $i, j, k$, and $l$ take on values $1, \\ldots, n_{\\text {sd }}$, where $n_{s d}$ is the number of spatial dimensions, and the summation convention applies to repeated indices $i, j, k$, and $l$ only. Let $\\sigma_{i j}$ denote (Cartesian components of) the (Cauchy) stress tensor, let $u_{i}$ denote the displacement vector, and let $\\ell_{i}$ be the prescribed body force per unit volume. The (infinitesimal) strain tensor, $\\epsilon_{i j}$, is defined to be the symmetric part of the displacement gradients, viz., \\begin{equation*} \\epsilon_{i j}=u_{(i, j)} \\stackrel{\\text { def. }}{=} \\frac{u_{i, j}+u_{j, i}}{2} \\quad \\text { (strain-displacement equations) } \\tag{2.7.1} \\end{equation*} The stress tensor is defined in terms of the strain tensor by the generalized Hooke's law: \\footnote{This is another constitutive equation, which reflects the elastic properties of the body under consideration.} \\begin{equation*} \\sigma_{i j}=c_{i j k l} \\epsilon_{k l} \\tag{2.7.2} \\end{equation*} where the $c_{i j k l}$ 's, the elastic coefficients, are given functions of $x$. (If the $c_{i j k l}$ 's are constants throughout, the body is called homogeneous.) The elastic coefficients are assumed to satisfy the following properties: Symmetry \\begin{equation*} c_{i j k l}=c_{k l i j} \\quad \\text { (major symmetry) } \\tag{2.7.3} \\end{equation*} \\[ \\left. \\begin{aligned} c_{ijkl} &= c_{jikl} \\\\ c_{ijkl} &= c_{ijlk} \\end{aligned} \\right\\} \\quad \\quad \\text{(minor symmetries)} \\]\n",
      "chunk word length: 327, chunk char length: 2314, chunk = \\begin{align*} & c_{i j k l}(x) \\psi_{i j} \\psi_{k l} \\geq 0 \\tag{2.7.5}\\\\ & c_{i j k l}(x) \\psi_{i j} \\psi_{k l}=0 \\text { implies } \\psi_{i j}=0 \\tag{2.7.6} \\end{align*} for all $\\boldsymbol{x} \\in \\overline{\\boldsymbol{\\Omega}}$ and all $\\psi_{i j}=\\psi_{j i}$.\\\\ Note. The positive-definiteness condition is in terms of symmetric arrays, $\\psi_{i j}$.\\\\ We shall see in Sec. 2.8 that a consequence of the major symmetry (2.7.3) is that $K$ is symmetric. The first minor symmetry implies the symmetry of the stress tensor (i.e., $\\sigma_{i j}=\\sigma_{j i}$ ).\\footnote{From a fundamental continuum mechanics standpoint, the symmetry of the Cauchy stress tensor emanates from the balance of angular momentum. } The positive-definiteness condition, when combined with appropriate boundary conditions on the displacement, leads to the positive definiteness of $\\boldsymbol{K}$. In the present theory, the unknown is a vector (i.e., the displacement vector). Consequently, a generalization of the boundary conditions considered previously is necessitated. We shall assume that $\\Gamma$ admits decompositions \\[ \\left.\\begin{array}{l} \\Gamma=\\overline{\\Gamma_{g_{i}} \\cup \\Gamma_{h_{i}}} \\tag{2.7.7}\\\\ \\varnothing=\\Gamma_{g_{i}} \\cap \\Gamma_{h_{i}} \\end{array}\\right\\} \\quad i=1, \\ldots, n_{s d} \\] For example, in two dimensions the situation might appear as in Fig. 2.7.1. As can be seen there can be a different decomposition for each $i=1,2, \\ldots, n_{s d}$. \\includegraphics[max width=\\textwidth, center]{images/chapter2_2.7.1.png} A formal statement of the strong form of the boundary-value problem goes as follows:\\\\ \\[ \\text{(S)} \\begin{cases} \\text{Given } \\ell_i : \\Omega \\to \\mathbb{R}, \\, g_i : \\Gamma_{g_i} \\to \\mathbb{R}, \\, \\text{and } h_i : \\Gamma_{h_i} \\to \\mathbb{R}, \\, \\text{find } u_i : \\overline{\\Omega} \\to \\mathbb{R} \\text{ such that}\\\\ \\sigma_{ij,j} + \\ell_i = 0 \\quad \\quad \\quad \\quad \\quad \\quad \\text{in } \\Omega \\quad \\text{(equilibrium equations)} \\quad (2.7.8) \\\\ u_i = g_i \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{on } \\Gamma_{g_i} \\quad (2.7.9) \\\\ \\sigma_{ij} n_j = h_i \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{on } \\Gamma_{h_i} \\quad (2.7.10) \\\\ \\text{where } \\sigma_{ij} \\text{ is defined in terms of } u_i \\text{ by (2.7.1) and (2.7.2)}. \\end{cases} \\]\n",
      "chunk word length: 214, chunk char length: 1655, chunk = \\begin{enumerate} \\item The functions $g_{i}$ and $h_{i}$ are called the prescribed boundary displacements and tractions, respectively. \\item (S) is sometimes referred to as the mixed boundary-value problem of linear elastostatics. Under appropriate hypotheses on the data, $(S)$ possesses a unique solution (see Fichera [4]). \\item The additional complexity of the present theory, when compared with heat conduction, is that the unknown (i.e., $u=\\left\\{u_{i}\\right\\}$ ) is vector-valued rather than scalarvalued. \\item In practice, it is important to be able to deal with somewhat more complicated boundary-condition specification. In order not to encumber the present exposition, this generalization will be considered later on in the form of an exercise (see Exercise 5 in Sec. 2.12). \\end{enumerate} Let $\\delta_{i}$ denote the trial solution space and $\\mathcal{V}_{i}$ the variation space. Each member $u_{i} \\in \\delta_{i}$ satisfies $u_{i}=g_{i}$ on $\\Gamma_{g_i}$, whereas each $w_{i} \\in \\mathcal{V}_{i}$ satisfies $w_{i}=0$ on $\\Gamma_{g_{i}}$. Equation (2.7.10) will be a natural boundary condition. The weak formulation goes as follows:\\\\ Given $\\ell_{i}: \\Omega \\rightarrow \\mathbb{R}, g_{i}: \\Gamma_{g_{i}} \\rightarrow \\mathbb{R}$ and $h_{i}: \\Gamma_{h_{i}} \\rightarrow \\mathbb{R}$, find $u_{i} \\in \\delta_{i}$ such that for all $w_{i} \\in \\mathcal{V}_{i}$ \\begin{equation*} \\int_{\\Omega} w_{(i, j)} \\sigma_{i j} d \\Omega=\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega+\\sum_{i=1}^{n_{sd}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right) \\tag{2.7.11} \\end{equation*} where $\\sigma_{i j}$ is defined in terms of $u_{i}$ by (2.7.1) and (2.7.2).\n",
      "chunk word length: 98, chunk char length: 773, chunk = \\begin{enumerate} \\setcounter{enumi}{4} \\item In the solid mechanics literature, $(W)$ is sometimes referred to as the principle of virtual work, or principle of virtual displacements, $w_{i}$ being the virtual displacements. \\item The existence and uniqueness of weak solutions is discussed in Duvaut-Lions [3]. \\end{enumerate} Note. The boundary integral in (2.7.11) takes on the explicit form: \\begin{equation*} \\sum_{i=1}^{n_{s d}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right)=\\int_{\\Gamma_{h_{1}}} w_{1} h_{1} d \\Gamma+\\cdots+\\int_{\\Gamma_{h_{n_{s d}}}} w_{n_{s d}} h_{n_{s d}} d \\Gamma \\tag{2.7.12} \\end{equation*} Theorem. Assume all functions are smooth enough to justify the manipulations. Then a solution of $(S)$ is a solution of $(W)$, and vice versa.\n",
      "chunk word length: 88, chunk char length: 733, chunk = \\begin{enumerate} \\setcounter{enumi}{6} \\item The proof of the equivalence theorem requires some preliminary results, which we shall establish in the following lemmas. \\end{enumerate} Lemma 1. (Euclidean decomposition of a second-rank tensor.) Let $s_{i j}$ denote a general nonsymmetric, second-rank tensor. Then $s_{i j}=s_{(ij)}+s_{[ij]}$, where $s_{(ij)}$ is symmetric (i.e., $s_{(i j)}=s_{(j i)}$ ) and $s_{[i j]}$ is skew-symmetric (i.e., $s_{[i j}=-s_{[ji]}$ ). Proof. Define \\begin{equation*} s_{(ij)}=\\frac{s_{i j}+s_{j i}}{2} \\tag{2.7.13} \\end{equation*} \\begin{equation*} s_{[ij]}=\\frac{s_{i j}-s_{j i}}{2} \\tag{2.7.14} \\end{equation*} It is easily verified that $s_{(i j)}$ is symmetric and $s_{[i j]}$ is skew-symmetric.\n",
      "chunk word length: 1177, chunk char length: 8793, chunk = \\begin{enumerate} \\setcounter{enumi}{7} \\item $s_{(ij)}$ and $s_{[i j}$ are called the symmetric and skew-symmetric parts of $s_{i j}$, respectively. \\end{enumerate} Lemma 2. Let $s_{i j}$ be a nonsymmetric tensor and let $t_{i j}$ be a symmetric tensor. Then \\begin{equation*} s_{i j} t_{i j}=s_{(i j)} t_{i j} \\tag{2.7.15} \\end{equation*} Proof. By Lemma 1, $s_{i j}=s_{(i j)}+s_{[i j]}$. Since $$ s_{i j} t_{i j}=s_{(i j)} t_{i j}+s_{[i j]} t_{i j} $$ the lemma will be established if we can show that $s_{[i j]} t_{i j}=0$. We proceed as follows: $$ \\begin{aligned} s_{[ij]} t_{i j} & =-s_{[ji]} t_{i j} & & \\text { (skew-symmetry of } \\left.s_{[i j]}\\right) \\\\ & =-s_{[j i]} t_{j i} & & \\text { (symmetry of } \\left.t_{i j}\\right) \\\\ & =-s_{[i j]} t_{i j} & & \\text { (redefinition of dummy indices) } \\end{aligned} $$ from which the result follows.\\\\ Proof of Theorem \\begin{enumerate} \\item Let $u_{i}$ be a solution of (S). Thus $u_{i} \\in \\delta_{1}$. Multiply (2.7.8) by $w_{i} \\in \\mathcal{V}_{i}$ and integrate over $\\Omega$, viz., \\end{enumerate} $$ \\begin{aligned} & 0=\\int_{\\Omega} w_{i}\\left(\\sigma_{i j, j}+\\ell_{i}\\right) d \\Omega=-\\int_{\\Omega} w_{i, j} \\sigma_{i j} d \\Omega+\\int_{\\Gamma} w_{i} \\sigma_{i j} n_{j} d \\Gamma+\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega \\\\ & \\text { (integration by parts) } \\\\ & =-\\int_{\\Omega} w_{(i,j)} \\sigma_{i j} d \\Omega+\\sum_{i=1}^{n_{sd}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right)+\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega \\quad \\begin{array}{l} \\text { (symmetry of } \\sigma_{i j}, \\\\ \\text { Lemma 2, } w_{i}=0 \\end{array} \\\\ & \\text { on } \\Gamma_{g_i} \\text {, and (2.7.10)) } \\end{aligned} $$ Therefore, $u_{i}$ is a solution of (W).\\\\ 2. Assume $u_{i}$ is a solution of (W). Since $u_{i} \\in \\delta_{i}, u_{i}=g_{i}$ on $\\Gamma_{g_{i}}$. From (2.7.11) $$ 0=-\\int_{\\Omega} \\underbrace{w_{(i,j)} \\sigma_{i j}}_{\\left(=w_{i, j} \\sigma_{i j}\\right. \\text { by Lemma 2) }} d \\Omega+\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega+\\sum_{i=1}^{n_{sd}} \\int_{\\Gamma_{k_{i}}} w_{i} h_{i} d \\Gamma $$ \\[ =\\int_{\\Omega} w_{i}\\left(\\sigma_{ij, j}+\\ell_{i}\\right) d \\Omega-\\sum_{i=1}^{n_{s d}} \\int_{\\Gamma_{h_{i}}} w_{i}\\left(\\sigma_{i j} n_{j}-h_{i}\\right) d \\Gamma \\quad \\begin{align*} & \\text { (integration by parts, } \\tag{2.7.16}\\\\ & \\left.w_{i}=0 \\text { on } \\Gamma_{g_{i}}\\right) \\end{align*} \\] Let $$ \\begin{aligned} & \\alpha_{i}=\\sigma_{i j, j}+\\ell_{i} \\\\ & \\beta_{i}=\\sigma_{i j} n_{j}-h_{i} \\end{aligned} $$ Thus to complete the proof we must show that $$ \\begin{array}{ll} \\alpha_{i}=0 & \\text { on } \\Omega \\\\ \\beta_{i}=0 & \\text { on } \\Gamma_{h_{i}} \\end{array} $$ These conditions can be proved by the techniques used in Sec. 2.3. Let $w_{i}=\\alpha_{i} \\phi$, where\\\\ i. $\\phi>0$ on $\\Omega$;\\\\ ii. $\\phi=0$ on $\\Gamma$; and\\\\ iii. $\\phi$ is smooth.\\\\ (These conditions insure that $w_{i} \\in \\mathcal{V}_{i}$.) Substituting this $w_{i}$ into (2.7.16) yields $$ 0=\\int_{\\Omega} \\underbrace{\\alpha_{i} \\alpha_{i}}_{\\geq 0} \\underbrace{\\phi}_{>0} d \\Omega $$ which implies $\\alpha_{i}=0$ on $\\Omega$.\\\\ Now take $w_{i}=\\delta_{i 1} \\beta_{1} \\psi$, where\\\\ i'. $\\psi>0$ on $\\Gamma_{h_{1}}$;\\\\ ii'. $\\psi=0$ on $\\Gamma_{g_{1}}$; and\\\\ iii'. $\\psi$ is smooth.\\\\ (Again, $w_{i} \\in \\mathcal{V}_{1 .}$ ) Substituting this $w_{i}$ into (2.7.16) and making use of $\\alpha_{i}=0$ results in $$ 0=\\int_{\\Gamma_{h_{1}}} \\beta_{1}^{2} \\psi d \\Gamma $$ from which it follows that $\\beta_{1}=0$ on $\\Gamma_{h_{1}}$.\\\\ We may proceed analogously to show $\\beta_{2}=0$, and so on. Consequently, $u_{i}$ is a solution of $(\\boldsymbol{S})$. The abstract notation for the present case is \\begin{align*} a(w, u) & =\\int_{\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\Omega \\tag{2.7.17}\\\\ (w, \\ell) & =\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega \\tag{2.7.18}\\\\ (w, h)_{\\Gamma} & =\\sum_{i=1}^{n_{s d}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right) \\tag{2.7.19} \\end{align*} \\subsection*{Exercise 1.} Verify that $a(\\cdot, \\cdot),(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)_{\\Gamma}$, as just defined, are symmetric, bilinear forms (cf. Sec. 1.4). Let $\\delta=\\left\\{u \\mid u_{i} \\in \\delta_{i}\\right\\}$ and let $\\mathcal{V}=\\left\\{w \\mid w_{i} \\in \\mathcal{V}_{i}\\right\\}$. Then the weak form can be concisely written in terms of (2.7.17-2.7.19) as follows: Given $\\boldsymbol{\\ell}, \\boldsymbol{g}$, and $\\boldsymbol{h}$ (in which the components are defined in (W)), find $\\boldsymbol{u} \\in \\mathcal{\\delta}$ such that for all $w \\in \\mathcal{V}$\\\\ (W) \\begin{equation*} a(w, u)=(w, \\ell)+(w, h)_{\\Gamma} \\tag{2.7.20} \\end{equation*} As discussed in Sec. 2.3, it is desirable to construct index-free counterparts of the expressions on the right-hand sides of (2.7.17)-(2.7.19). For concreteness we shall assume that $n_{s d}=2$; thus $1 \\leq i, j, k, l \\leq 2$. Let\\footnote{According to our previous notational conventions, $\\epsilon=\\left[\\epsilon_{i j}\\right]$, the matrix of strain components. However, we will have no need for this matrix, and consequently we reserve - for the \"strain vector\" defined in (2.7.21). A similar notational conflict occurs with respect to the \"stress vector,\" $\\sigma$, defined in Exercise 4. Note that factors of one-half have been eliminated from the shearing components (i.e., last components) of (2.7.21) and (2.7.22). (Compare (2.7.21) and (2.7.22) with (2.7.1).) This will considerably simplify subsequent writing. } \\begin{gather*} \\epsilon(u)=\\left\\{\\epsilon_{I}(u)\\right\\}=\\left\\{\\begin{array}{c} u_{1,1} \\\\ u_{2,2} \\\\ u_{1,2}+u_{2,1} \\end{array}\\right\\} \\tag{2.7.21}\\\\ \\epsilon(w)=\\left\\{\\epsilon_{I}(w)\\right\\}=\\left\\{\\begin{array}{c} w_{1,1} \\\\ w_{2,2} \\\\ w_{1,2}+w_{2,1} \\end{array}\\right\\} \\tag{2.7.22}\\\\ D=\\left[D_{I J}\\right]=\\left[\\begin{array}{lll} D_{11} & D_{12} & D_{13} \\\\ & D_{22} & D_{23} \\\\ \\text { symmetric } & D_{33} \\end{array}\\right] \\tag{2.7.23} \\end{gather*} where\\\\ \\begin{equation*} D_{I J}=c_{i j k l} \\tag{2.7.24} \\end{equation*} in which the indices are related by the following table: \\begin{table}[ht] \\centering \\begin{tabular}{|c|c|c|} \\hline \\diagbox[width=2em]{\\textit{J}}{\\textit{I}} & \\diagbox[width=2em]{\\textit{k}}{\\textit{i}} & \\diagbox[width=2em]{\\textit{l}}{\\textit{j}} \\\\ \\hline 1 & 1 & 1 \\\\ 3 & 1 & 2 \\\\ 3 & 2 & 1 \\\\ 2 & 2 & 2 \\\\ \\hline \\end{tabular} \\caption{TABLE 2.7.1} \\end{table} As should be clear, we have \"collapsed\" pairs of indices $(i, j$, and $k, l)$ into single indices ($I$ and $J$, respectively) taking account of the symmetries of $c_{i j k l}, u_{(k, l)}$ and $w_{(i, j)}$. Observe that the indices $I$ and $J$ take on values 1, 2, and 3. In $n_{s d}$ dimensions, the $I$ and $J$ indices will take on values $1,2, \\ldots, n_{s d}\\left(n_{s d}+1\\right) / 2$. It can be shown that \\begin{equation*} w_{(i, j)} c_{i j k l} u_{(k, l)}=\\epsilon(w)^{T} D \\epsilon(u) \\tag{2.7.25} \\end{equation*} and so \\begin{equation*} a(w, u)=\\int_{\\Omega} \\epsilon(w)^{T} D \\epsilon(u) d \\Omega \\tag{2.7.26} \\end{equation*} \\subsection*{Exercise 2.} Verify (2.7.25) for $n_{s d}=2$.\\\\ \\subsection*{Exercise 3.} Construct the analog of Table 2.7.1 for the case $n_{s d}=$ 3. For definiteness of the ordering, take \\[ \\epsilon(u)=\\left\\{\\begin{array}{c} u_{1,1} \\tag{2.7.27}\\\\ u_{2,2} \\\\ u_{3,3} \\\\ u_{2,3}+u_{3,2} \\\\ u_{1,3}+u_{3,1} \\\\ u_{1,2}+u_{2,1} \\end{array}\\right\\} \\] \\subsection*{Exercise 4.} Show that \\begin{equation*} \\sigma=\\boldsymbol{D} \\epsilon(u) \\tag{2.7.28} \\end{equation*} where \\[ \\begin{array}{ll} \\sigma=\\left\\{\\begin{array}{ll} \\sigma_{11} \\\\ \\sigma_{22} \\\\ \\sigma_{12} \\end{array}\\right\\}, & n_{s d}=2 \\\\ \\sigma=\\left\\{\\begin{array}{l} \\sigma_{11} \\\\ \\sigma_{22} \\\\ \\sigma_{33} \\\\ \\sigma_{23} \\\\ \\sigma_{13} \\\\ \\sigma_{12} \\end{array}\\right\\}, & n_{s d}=3 \\tag{2.7.30} \\end{array} \\] \\subsection*{Exercise 5.} If the body in question is isotropic, then \\begin{equation*} c_{i j k l}(x)=\\mu(x)\\left(\\delta_{i k} \\delta_{j l}+\\delta_{i l} \\delta_{j k}\\right)+\\lambda(x) \\delta_{i j} \\delta_{k l} \\tag{2.7.31} \\end{equation*} where $\\lambda$ and $\\mu$ are the Lam parameters; $\\mu$ is often referred to as the shear modulus and denoted by G. The relationships of $\\lambda$ and $\\mu$ to $E$, Young's modulus, and $\\nu$, Poisson's ratio, are given by \\begin{align*} & \\lambda=\\frac{\\nu E}{(1+\\nu)(1-2 \\nu)} \\tag{2.7.32}\\\\ & \\mu=\\frac{E}{2(1+\\nu)} \\tag{2.7.33} \\end{align*} (See Sokolnikoff [6], p. 71, for further relations with other equivalent moduli.) If (2.7.31) is not satisfied, the body is said to be anisotropic. Using (2.7.31) set up $D$ for $n_{s d}=2$ and $n_{s d}=3$. Hint: The answer for $n_{s d}=2$ is \\[ D=\\left[\\begin{array}{ccc} \\lambda+2 \\mu & \\lambda & 0 \\tag{2.7.34}\\\\ & \\lambda+2 \\mu & 0 \\\\ \\text { Symmetric } & \\mu \\end{array}\\right] \\] This matrix manifests the plane strain hypothesis. (See Sokolnikoff [6] for elaboration.)\n",
      "chunk word length: 1277, chunk char length: 10218, chunk = \\begin{enumerate} \\setcounter{enumi}{8} \\item The case of isotropic plane stress may be determined from (2.7.34) by replacing ${\\lambda}$ by $\\bar{\\lambda}$, where \\end{enumerate} \\begin{equation*} \\bar{\\lambda}=\\frac{2 \\lambda \\mu}{\\lambda+2 \\mu} \\tag{2.7.35} \\end{equation*} (See Sokolnikoff [6] or Timoshenko and Goodier [7] for elaboration on the physical ideas.) \\subsection*{Exercise 6.} (See Exercise 1, Sec. 2.4 for background and an analogous result.)\\\\ Let the \"jump\" in $\\sigma_{i n}=\\sigma_{i j} n_{j}$ be denoted by [$\\sigma_{i n}$]. Consider the weak formulation (i.e., (2.7.11)) and assume $w_{i}$ and $u_{i}$ are smooth on element interiors, but experience gradient discontinuities across element boundaries. Show that $$ 0=\\sum_{e=1}^{n_{e l}} \\int_{\\Omega^{e}} w_{i}\\left(\\sigma_{ij, j}+\\ell_{i}\\right) d \\Omega-\\sum_{i=1}^{n_{sd}} \\int_{\\Gamma_{h_{i}}} w_{i}\\left(\\sigma_{i n}-h_{i}\\right) d \\Gamma-\\int_{\\Gamma_{int}} w_{i}\\left[\\sigma_{i n}\\right] d \\Gamma $$ from which the Euler-Lagrange conditions may be read: $$ \\begin{aligned} & \\text { i. } \\sigma_{i j, j}+\\ell_{i}=0 \\text { in } \\bigcup_{e=1}^{n_{el}} \\Omega^{e} \\\\ & \\text { ii. } \\sigma_{i n}=h_{i} \\text { on } \\Gamma_{h_{i}} \\\\ & \\text { iii. }\\left[\\sigma_{i n}\\right]=0 \\text { on } \\Gamma_{\\text {int }} \\end{aligned} $$ Here (i) is the equilibrium equation on element interiors, and (iii) is a traction continuity condition across element boundaries. Compare these results with those obtained assuming $w_{i}$ and $u_{i}$ are globally smooth. \\subsection*{2.8 ELASTOSTATICS: GALERKIN FORMULATION, SYMMETRY, AND POSITIVE-DEFINITENESS OF K} Let $\\delta^{h}$ and $\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\delta$ and $\\mathcal{V}$, respectively. We assume members $w^{h} \\in \\mathcal{V}^{h}$ result in satisfaction, or approximate satisfaction, of the boundary condition $w_{i}=0$ on $\\Gamma_{g_i}$, and members of $\\delta^{h}$ admit the decomposition \\begin{equation*} u^{h}=v^{h}+g^{h} \\tag{2.8.1} \\end{equation*} where $\\boldsymbol{g}^{h} \\in \\mathcal{V}^{h}$ and $\\boldsymbol{g}^{h}$ results in satisfaction, or approximate satisfaction, of the boundary condition $u_{i}=g_{i}$ on $\\Gamma_{g_i}$. The Galerkin formulation of our problem is given as follows:\\\\ (G) $\\left\\{\\begin{array}{c}\\text { Given } \\ell, g, \\text { and } h \\text { (as in }(W)) \\text {, find } u^{h}=v^{h}+{g}^{h} \\in \\delta^{h} \\text { such that for all } w^{h} \\in \\mathbb{V}^{h} \\\\ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell\\right)+\\left(w^{h}, h\\right)_{\\Gamma}-a\\left(w^{h},g^{h}\\right) \\quad \\text { (2.8.2) }\\end{array}\\right.$ To define the global stiffness matrix and force vector for elasticity, it is necessary to introduce the ID array. This entails a generalization of the definition given in Sec. 2.6, since in the present case there will be more than 1 degree of freedom per node. For elasticity there are $n_{s d}$ degrees of freedom per node, but in order to include in our\\\\ definition cases such as heat conduction, we shall take the fully general situation in which it is assumed that there are $n_{\\text {dof}}$ degrees of freedom per node.\\footnote{In general, this is taken to mean the maximum number of degrees of freedom per node in the global model. It is possible in practice to have elements with fewer degrees of freedom per node contributing to the model.} In this case\\\\ \\[ \\text{ID}(\\underbrace{i}_{\\text{Degree of freedom number}}, \\underbrace{A}_{\\text{Global node number}}) = \\left\\{ \\begin{aligned} &\\underbrace{P}_{\\text{Global equation number}} &\\quad \\text{if } A \\in \\eta - \\eta_{g_i} \\\\ &0 &\\quad \\text{if } A \\in \\eta_{g_i} \\end{aligned} \\right. \\] where $1 \\leq i \\leq n_{\\text {dof }}$. Thus ID has dimensions $n_{\\mathrm{dof}} \\times n_{n p}$. If $n_{\\mathrm{dof}}=1$, we reduce to the case considered previously in Sec. 2.6 (i.e., $\\operatorname{ID}(i, A)=\\operatorname{ID}(A)$ ). Recall that $\\boldsymbol{\\eta}=\\left\\{1,2, \\ldots, n_{n p}\\right\\}$ denotes the set of global node numbers. Let $\\boldsymbol{\\eta}_{\\boldsymbol{g}_{i}} \\subset \\boldsymbol{\\eta}$ be the set of nodes at which $u_{i}^{h}=\\boldsymbol{g}_{i}$ and let $\\boldsymbol{\\eta}-\\boldsymbol{\\eta}_{\\boldsymbol{g}_{i}}$ be the complement of $\\eta_{g i}$. For each node in $\\eta-\\eta_{g i}$, the nodal value of $u_{i}^{h}$ is to be determined. The explicit representations of $v_{i}^{h}$ and $g_{i}^{h}$, in terms of the shape functions and nodal values are \\begin{align*} & v_{i}^{h}=\\sum_{A \\in \\eta-\\eta_{g_{i}}} N_{A} d_{\\underbrace{i}_{\\text{Degree of freedom number}} \\underbrace{A}_{\\text{Global node number}}} \\quad \\text { (no sum on } i \\text { ) } \\tag{2.8.4}\\\\ & g_{i}^{h}=\\sum_{A \\in \\eta_{g_{i}}} N_{A} g_{iA} \\quad \\text { (no sum on } i \\text { ) } \\end{align*} Let $e_{i}$ denote the $i$th Euclidean basis vector for $\\mathbb{R}^{n_{sd}} ; \\boldsymbol{e}_{i}$ has a 1 in slot $i$ and zeros elsewhere. For example \\[ \\left(n_{s d}=2\\right) \\quad e_{1}=\\left\\{\\begin{array}{l} 1 \\tag{2.8.6}\\\\ 0 \\end{array}\\right\\}, \\quad e_{2}=\\left\\{\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right\\} \\] \\[ \\left(n_{s d}=3\\right) \\quad e_{1}=\\left\\{\\begin{array}{l} 1 \\tag{2.8.7}\\\\ 0 \\\\ 0 \\end{array}\\right\\}, \\quad e_{2}=\\left\\{\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\end{array}\\right\\} \\quad e_{3}=\\left\\{\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right\\} \\] The vector versions of (2.8.4) and (2.8.5) may be defined with the aid of $e_{i}$, viz., \\begin{align*} & v^{h}=v_{i}^{h} e_{i} \\tag{2.8.8}\\\\ & g^{h}=g_{i}^{h} e_{i} \\tag{2.8.9} \\end{align*} Likewise, a typical member $w^{h} \\in \\mathcal{V}^{h}$ has the representation \\begin{equation*} w^{h}=w_{i}^{h} e_{i}, \\quad w_{i}^{h}=\\sum_{A \\in \\eta-\\eta_{g_{i}}} N_{A} c_{i A} \\quad \\text {(no sum on i)}\\tag{2.8.10} \\end{equation*} Substituting (2.8.4), (2.8.5), and (2.8.8)-(2.8.10) into (2.8.2) and arguing along the lines of Sec. 1.6 results in (verify!) \\begin{align*} & \\sum_{j=1}^{n_{\\text {dof }}}\\left(\\sum_{B \\in \\eta-\\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) d_{j B}\\right)=\\left(N_{A} e_{i}, \\ell\\right)+\\left(N_{A} e_{i}, h\\right)_{\\Gamma} \\\\ & \\quad-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{j B}\\right), \\quad A \\in \\eta-\\eta_{g_i}, \\quad 1 \\leq i \\leq n_{s d} \\tag{2.8.11\\footnotemark} \\end{align*} \\footnotetext{For correct interpretation of the meaning of these equations, the sum on $j$ should be taken first. For example, in two dimensions \\[ \\begin{aligned} \\sum_{j=1}^{2}\\left(\\sum_{B \\in \\eta-\\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) d_{jB}\\right)= & \\sum_{B \\in \\eta-\\eta_{g_1}} a\\left(N_{A} e_{i}, N_{B} e_{1}\\right) d_{1 B} \\\\ & +\\sum_{B \\in \\eta-\\eta_{g_2}} a\\left(N_{A} e_{i}, N_{B} e_{2}\\right) d_{2 B} \\end{aligned} \\] and \\[ \\begin{aligned} \\sum_{j=1}^{2}\\left(\\sum_{B \\in n_{g_j}}\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{jB}\\right)= & \\sum_{B \\in \\eta_{g_1}} a\\left(N_{A} e_{i}, N_{B} e_{1}\\right) g_{1 B} \\\\ & +\\sum_{B \\in n_{g_2}} a\\left(N_{A} e_{i}, N_{B} e_{2}\\right) g_{2 B} \\end{aligned} \\] } This is equivalent to the matrix equation\\\\ \\begin{align*} & K d=\\boldsymbol{F} \\tag{2.8.12}\\\\ & \\text { where } \\\\ & K=\\left[K_{P Q}\\right] \\tag{2.8.13}\\\\ & d=\\left\\{d_{Q}\\right\\} \\tag{2.8.14}\\\\ & F=\\left\\{F_{P}\\right\\} \\tag{2.8.15}\\\\ & K_{P Q}=a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) \\tag{2.8.16}\\\\ & F_{P}=\\left(N_{A} e_{i}, \\ell\\right)+\\left(N_{A} e_{i}, h\\right)_{\\Gamma}-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{jB}\\right) \\tag{2.8.17}\\\\ & \\text { in which } \\\\ & P=\\operatorname{ID}(i, A), \\quad Q=\\operatorname{ID}(j, B) \\end{align*} Equation (2.8.16) may be written in more explicit form by using (2.7.26) and noting that (see (2.7.21) and (2.7.22)): \\begin{equation*} \\epsilon\\left(N_{A} e_{i}\\right)=B_{A} e_{i} \\tag{2.8.19} \\end{equation*} where \\begin{align*} & \\left(n_{s d}=2\\right) \\quad B_{A}=\\left[\\begin{array}{cc} N_{A, 1} & 0 \\\\ 0 & N_{A, 2} \\\\ N_{A, 2} & N_{A, 1} \\end{array}\\right] \\tag{2.8.20}\\\\ & \\left(n_{s d}=3\\right) \\quad B_{A}=\\left[\\begin{array}{ccc} N_{A, 1} & 0 & 0 \\\\ 0 & N_{A, 2} & 0 \\\\ 0 & 0 & N_{A, 3} \\\\ 0 & N_{A, 3} & N_{A, 2} \\\\ N_{A, 3} & 0 & N_{A, 1} \\\\ N_{A, 2} & N_{A, 1} & 0 \\end{array}\\right] \\tag{2.8.21} \\end{align*} \\subsection*{Exercise 1.} Verify (2.8.19)-(2.8.21). With these, (2.8.16) becomes\\\\ \\[ K_{PQ} = e_i^T \\int_\\Omega B_A^T D B_B \\, d\\Omega \\, e_j \\] \\begin{itemize} \\item $PQ$: Global equation numbers \\item $i, j$: Degree of freedom numbers \\item $A, B$: Global node numbers \\end{itemize} and the indices are related by (2.8.18). Equation (2.8.17) is also amenable to explication. Note that, by (2.7.18) \\begin{equation*} \\left(N_{A} e_{i}, \\ell\\right)=\\int_{\\Omega} N_{A} \\ell_{i} d \\Omega \\tag{2.8.23} \\end{equation*} and likewise by (2.7.19) \\begin{equation*} \\left(N_{A} e_{i}, h\\right)_{\\Gamma}=\\int_{\\Gamma_{h_{i}}} N_{A} h_{i} d \\Gamma \\quad \\text { (no sum) } \\tag{2.8.24} \\end{equation*} Thus (2.8.17) may be written as \\begin{equation*} F_{P}=\\int_{\\Omega} N_{A} \\ell_{i} d \\Omega+\\int_{\\Gamma_{h_i}} N_{A} h_{i} d \\Gamma-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{j B}\\right) \\tag{2.8.25} \\end{equation*} Now that we have defined $\\boldsymbol{K}$, we can establish its fundamental properties. We shall need the following preliminary results. Let $n_{s d}=2$ or 3 and let $w: \\Omega \\rightarrow \\mathbb{R}^{n_{s d}}$. If $w_{(i, j)}=0$ (\"zero strains\"), then $\\boldsymbol{w}$ admits the representations: \\begin{align*} & \\left(n_{s d}=2\\right) \\quad w(x)=\\overbrace{c}^{\\text{Translation}}+\\overbrace{c_3\\left(x_{1} e_{2}-x_{2} e_{1}\\right)}^{\\text{Rotation}} \\tag{2.8.26}\\\\ & \\left(n_{s d}=3\\right) \\quad w(x)=\\underbrace{c_{1}}_{\\text{Translation}}+\\underbrace{c_{2} \\times x}_{\\text{Rotation}}\\tag{2.8.27} \\end{align*} where \\[ c=\\left\\{\\begin{array}{l} c_{1} \\tag{2.8.28}\\\\ c_{2} \\end{array}\\right\\} \\quad c_{1}=\\left\\{\\begin{array}{l} c_{11} \\\\ c_{12} \\\\ c_{13} \\end{array}\\right\\} \\quad c_{2}=\\left\\{\\begin{array}{l} c_{21} \\\\ c_{22} \\\\ c_{23} \\end{array}\\right\\} \\] and $c_{3}$ are constants; and $\\times$ denotes the vector cross product. Equations (2.8.26) and (2.8.27) define infinitesimal rigid-body motions.\n",
      "chunk word length: 38, chunk char length: 281, chunk = $} We assume that the homogeneous boundary conditions incorporated into the definition of $\\mathcal{V}^{h}$ preclude nontrivial infinitesimal rigid-body motions. In other words, we assume that if $w^{h} \\in \\mathcal{V}^{h}$ is a rigid-body motion, then $w^{h}$ is identically zero.\n",
      "chunk word length: 92, chunk char length: 604, chunk = \\begin{enumerate} \\item $K$ is symmetric. \\item If Assumption $R$ holds, then $K$ is also positive definite. \\end{enumerate} Proof of $1$. Symmetry. We may note that symmetry of $\\boldsymbol{K}$ follows from (2.8.16) and the symmetry of $a(\\cdot, \\cdot)$. However, we shall provide an alternative proof in terms of (2.8.22).\\\\ $$ \\begin{aligned} K_{P Q} & =e_{i}^{T} \\int_{\\Omega} B_{A}^{T} D B_{B} d \\Omega e_{j} \\\\ & =e_{j}^{T} \\int_{\\Omega} B_{B}^{T} D^{T} B_{A} d \\Omega e_{i} \\\\ & =e_{j}^{T} \\int_{\\Omega} B_{B}^{T} D B_{A} d \\Omega e_{i} \\quad \\text {(symmetry of D)} \\\\ & =K_{Q P} \\end{aligned} $$\n",
      "chunk word length: 243, chunk char length: 1837, chunk = Note that the symmetry of $\\boldsymbol{K}$ followed from the symmetry of $\\boldsymbol{D}$, which was a consequence of the major symmetry of the $c_{i j k l}$ 's (see (2.7.3)). Proof of 2. Positive definite (Recall from Sec. 1.9 that we must show (i) $\\boldsymbol{c}^{T} \\boldsymbol{K} c \\geq 0$ and (ii) $\\boldsymbol{c}^{T} \\boldsymbol{K} \\boldsymbol{c}=0$ implies $\\boldsymbol{c}=0$.) Let $w_{i}^{h}=\\Sigma_{A \\in \\eta_{-\\eta_{g_i}}} N_{A} c_{iA}$ be a member of $\\mathcal{V}_{i}^{h}$. Then $c_{P}=c_{iA}$, where $P=$ $\\operatorname{ID}(i, A), 1 \\leq P \\leq n_{e q}$, defines the components of an $n_{e q}$-vector $c$.\\\\ i. $$ \\begin{aligned} & \\boldsymbol{c}^{T} K c=\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{P Q} c_{Q} \\\\ & =\\sum_{i, j=1}^{n_{dof}}\\left(\\sum_{\\substack{A \\in \\eta-\\eta_{g_i} \\\\ B \\in \\eta-\\eta_{g_j}}} c_{i A} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) c_{j B}\\right) \\quad \\text { (definition of } K_{P Q} \\text { ) } \\\\ & =a\\left(\\sum_{i=1}^{n_{dof}}\\left(\\sum_{A \\in \\eta-\\eta_{g_i}} c_{iA} N_{A} e_{i}\\right), \\sum_{j=1}^{n_ {dof}}\\left(\\sum_{B \\in \\eta-\\eta_{g_j}} c_{j B} N_{B} e_{j}\\right)\\right) \\quad \\text { (bilinearity of } a(\\cdot, \\cdot)) \\\\ & =a\\left(w^{h}, w^{h}\\right) \\quad \\text { (definition of } w^{h} \\text { ) } \\\\ & =\\int_{\\Omega} \\underbrace{w_{(i, j)}^{h} c_{ijkl} w_{(k, l)}^{h} d \\Omega}_{\\geq 0} \\quad \\text { (by (2.7.5) and (2.7.17)) } \\\\ & \\geq 0 \\end{aligned} $$ ii. Assume $\\boldsymbol{c}^{\\boldsymbol{T}} \\boldsymbol{K} \\boldsymbol{c}=\\mathbf{0}$. By the proof of part ( $\\left.\\mathbf{(}\\right)$, we deduce that $$ w_{(i,j)}^{h} c_{i j k l} w_{(k, l)}^{h}=0 $$ From (2.7.6), this means that $w_{(i, j)}^{h}=0$, and so $w^{h}$ is an infinitesimal rigid motion. By Assumption $R, \\boldsymbol{w}^{\\boldsymbol{h}}=\\mathbf{0}$, from which it follows that $c_{p}=0$; hence $\\boldsymbol{c}=\\mathbf{0}$.\n",
      "chunk word length: 693, chunk char length: 5518, chunk = Positive definiteness of $\\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\\mathcal{V}^{h}$. \\subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\\boldsymbol{K}$ and $\\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\\boldsymbol{k}^{e}$ and $f^{\\boldsymbol{c}}$ : \\begin{align*} & \\boldsymbol{k}^{e}=\\left[k_{p q}^{e}\\right], \\quad \\boldsymbol{f}^{e}=\\left\\{f_{p}^{e}\\right\\}, \\quad 1 \\leq p, q \\leq n_{ee}=n_{ed} n_{e n} \\tag{2.9.1}\\footnotemark\\\\ & k_{p q}^{e}=e_{i}^{T} \\int_{\\Omega e} B_{a}^{T} D B_{b} d \\Omega e_{j}, \\quad p=n_{e d}(a-1)+i, \\\\ & q=n_{e d}(b-1)+j \\tag{2.9.2}\\\\ & \\left(n_{s d}=2\\right) \\quad B_{a}=\\left[\\begin{array}{cc} N_{a, 1} & 0 \\\\ 0 & N_{a, 2} \\\\ N_{a, 2} & N_{a, 1} \\end{array}\\right] \\tag{2.9.3}\\\\ & \\left(n_{s d}=3\\right) \\quad \\boldsymbol{B}_{a}=\\left[\\begin{array}{ccc} N_{a, 1} & 0 & 0 \\\\ 0 & N_{a, 2} & 0 \\\\ 0 & 0 & N_{a, 3} \\\\ 0 & N_{a, 3} & N_{a, 2} \\\\ N_{a, 3} & 0 & N_{a, 1} \\\\ N_{a, 2} & N_{a, 1} & 0 \\end{array}\\right] \\tag{2.9.4} \\end{align*} \\footnotetext{$n_{\\text{ee}}$ stands for the number of element equations and $n_{\\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\\text {ed}} \\leq n_{\\text{dof}}$, although they are usually equal. } and \\[ f_{p}^{e}=\\int_{\\Omega^{e}} N_{a} \\ell_{i} d \\Omega+\\int_{\\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \\Gamma-\\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \\quad \\begin{array}{r} \\Gamma_{h_{i}}^{e}=\\Gamma_{h_{i}} \\cap \\Gamma^{e} \\tag{2.9.5}\\\\ (\\text { no sum on } i) \\end{array} \\] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\\left(x_{b}^{e}\\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \\begin{equation*} \\underbrace{\\boldsymbol{k}_{ab}^{e}}_{n_{e d} \\times n_{e d}}=\\int_{\\boldsymbol{\\Omega}^{e}} B_{a}^{T} \\boldsymbol{D} B_{b} d \\boldsymbol{\\Omega} \\tag{2.9.6a} \\end{equation*} From (2.9.2) we see that \\begin{equation*} k_{p q}^{e}=e_{i}^{T} k_{a b}^{e} e_{j} \\tag{2.9.6b} \\end{equation*} This means, \"the $p q$-component of $\\boldsymbol{k}^{e}$ is the $i j$-component of the submatrix $\\boldsymbol{k}_{\\text {ab. }}^{e}$ \" By (2.9.1) through (2.9.4), we see that $k^{c}$ may be written as \\begin{equation*} k^{c}=\\int_{\\Omega^{e}} B^{T} D B d \\Omega \\tag{2.9.7} \\end{equation*} where \\begin{equation*} \\boldsymbol{B}=\\left[\\boldsymbol{B}_{1}, \\boldsymbol{B}_{2}, \\ldots, \\boldsymbol{B}_{n_{en}}\\right] \\tag{2.9.8} \\end{equation*} For example, in the case of a two-dimensional (i.e., $n_{s d}=n_{e d}=2$ ), four-noded element, $\\boldsymbol{k}^{\\boldsymbol{c}}$ takes the form \\[ \\underbrace{k^e}_{8 \\times 8} = \\left[\\begin{array}{cccc} k_{11}^e & k_{12}^e & k_{13}^e & k_{14}^e \\\\ k_{21}^e & k_{22}^e & k_{23}^e & k_{24}^e \\\\ k_{31}^e & k_{32}^e & k_{33}^e & k_{34}^e \\\\ k_{41}^e & k_{42}^e & k_{43}^e & k_{44}^e \\end{array}\\right] \\] In practice, the submatrices above the dashed line are computed and those below, if required, are determined by symmetry. The global arrays $\\boldsymbol{K}$ and $\\boldsymbol{F}$ may be formed from the element arrays $\\boldsymbol{k}^{\\boldsymbol{e}}$ and $\\boldsymbol{f}^{\\boldsymbol{e}}$, respectively, by way of an assembly algorithm as outlined in Sec. 1.14. \\subsection*{Exercise 1.} Let \\begin{align*} & \\underset{n_{ee} \\times 1}{d^{e}}=\\left\\{d_{a}^{e}\\right\\}=\\left\\{\\begin{array}{c} d_{1}^e \\\\ d_{2}^{e} \\\\ \\vdots \\\\ d^{e}_{n_{en}} \\end{array}\\right\\} \\tag{2.9.10}\\\\ & \\left(n_{\\text{ed }}=2\\right) \\quad d_{a}^{e}=\\left\\{\\begin{array}{l} d_{1a}^e \\\\ d_{2a}^{e} \\end{array}\\right\\} \\tag{2.9.11}\\\\ & \\left(n_{e d}=3\\right) \\quad d_{a}^{e}=\\left\\{\\begin{array}{l} d_{1a}^e \\\\ d_{2a}^e \\\\ d_{3a}^e \\end{array}\\right\\} \\tag{2.9.12} \\end{align*} where \\begin{equation*} d_{i a}^{e}=u_{i}^{h}\\left(x_{a}^{e}\\right) \\tag{2.9.13} \\end{equation*} $d^{e}$ is called the element displacement vector. Show that the stress vector (see Exercise 4, Sec. 2.7.) at point $x \\in \\Omega^{\\boldsymbol{c}}$ can be calculated from the formula \\begin{equation*} \\sigma(x)=D(x) B(x) d^{e}=D(x) \\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e} \\tag{2.9.14} \\end{equation*} \\subsection*{2.10 ELASTOSTATICS: DATA PROCESSING ARRAYS ID, IEN, AND LM} We have already noted that the definition of the ID array must be generalized for the present case as indicated in Sec. 2.8. We iniust also generalize our definition of the LM array. However, the IEN array remains the same as before. In the present and fully general cases, the LM array is three-dimensional, with dimensions $\\boldsymbol{n}_{e d} \\times \\boldsymbol{n}_{e n} \\times \\boldsymbol{n}_{e l}$, and is defined by\\\\ \\[ \\text{LM}(i, a, e) = \\text{ID}(i, \\text{IEN}(a, e)) \\] \\begin{itemize} \\item $i$: Degrees of freedom number \\item $a$: Local node number \\item $e$: Element number \\end{itemize} Alternatively, it is sometimes convenient to think of LM as two-dimensional, with dimensions $n_{e e} \\times n_{e l}$, viz., ${ }^{17}$ \\begin{align*} & \\mathrm{LM}(\\underbrace{p}_{\\text{Local equation number}}, \\underbrace{e}_{\\text{Element number}})=\\mathrm{LM}(i, a, e), \\quad p=n_{e d}(a-1)+i \\end{align*} To see how everything works in practice, it is helpful to run through a simple example.\n",
      "chunk word length: 298, chunk char length: 2069, chunk = Consider the mesh of four-node, rectangular elements illustrated in Fig. 2.10.1. We assume that the local node numbering begins in the lower left-hand corner for each element and proceeds in counterclockwise fashion. \\footnotetext[16]{The $g$-boundary conditions are accounted for in this definition.\\\\ ${ }^{17}$ The reader knowledgeable in FORTRAN will realize that the intemal computer storage of (2.10.1) and (2.10.2) is identical. }This is shown for element 4, which is typical. Four displacement (i.e., \" $g$-type\") boundary conditions are specified; namely, the horizontal displacement is specified at nodes 1 and 10 , and the vertical displacement is specified at nodes 1 and 3. Since $n_{\\text {np}}=12, n_{\\text {dof }}=n_{\\text {ed }}=2$, and 4 displacement degrees of freedom are specified, we have $n_{e q}=20$. As is usual, we adopt the convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. ${ }^{18}$ The ID, IEN, and LM arrays are given in Figure 2.10.2. The reader is urged to verify the results.\\\\ \\includegraphics[max width=\\textwidth, center]{images/chapter2.10.1.png} Figure 2.10.1 Mesh of four-node, rectangular, elasticity elements; global and local node numbers, element numbers, and displacement boundary conditions. In terms of the IEN and LM arrays, a precise definition of the $g_{p}^{e}$ 's may be given (see (2.9.5)): \\[ g_{p}^{e}=g_{i a}^{e}= \\begin{cases}0, & \\text { if } \\mathrm{LM}(i, a, e) \\neq 0 \\tag{2.10.3}\\\\ g_{i A}, & \\text { where } A=\\operatorname{IEN}(a, e), \\text { if } \\mathrm{LM}(i, a, e)=0\\end{cases} \\] This definition may be easily programmed.\\\\ ${ }^{18}$ In practice, equation numbers are often renumbered internally to minimize the bandwidth of $\\boldsymbol{K}$ and thus decrease storage and solution effort. This is especially important in analyzing large-scale systems involving tens of thousands of equations. An algorithm for reducing bandwidth is presented in [8]. \\includegraphics[max width=\\textwidth, center]{images/chapter2.10.2.png}\n",
      "chunk word length: 384, chunk char length: 3717, chunk = As a final example, we consider a typical four-node, elasticity element in some large mesh; see Fig. 2.10.3. We assume the pertinent entries of the ID array are given as follows: \\[ \\left.\\begin{array}{l} \\operatorname{ID}(1,32)=0 \\\\ \\operatorname{ID}(2,32)=0 \\\\ \\operatorname{ID}(1,59)=115 \\\\ \\operatorname{ID}(2,59)=116 \\\\ \\operatorname{ID}(1,164)=0 \\tag{2.10.4}\\\\ \\operatorname{ID}(2,164)=325 \\\\ \\operatorname{ID}(1,168)=332 \\\\ \\operatorname{ID}(2,168)=333 \\end{array}\\right\\} \\] The entries of IEN follow from Fig. 2.10.3: \\[ \\left.\\begin{array}{l} \\operatorname{IEN}(1, e)=164 \\\\ \\operatorname{IEN}(2, e)=32 \\\\ \\operatorname{IEN}(3, e)=168 \\tag{2.10.5}\\\\ \\operatorname{IEN}(4, e)=59 \\end{array}\\right\\} \\] \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-39}\\\\ (i) - Local node numbers Tigure 2.10.3 Typical four-node elasticity element; global and local node numbers. Combining (2.10.4) and (2.10.5), by way of (2.10.1), yields entries of the LM array: \\[ \\left.\\begin{array}{l} \\operatorname{LM}(1,1, e)=0 \\tag{2.10.6}\\\\ \\operatorname{LM}(2,1, e)=325 \\\\ \\operatorname{LM}(1,2, e)=0 \\\\ \\operatorname{LM}(2,2, e)=0 \\\\ \\operatorname{LM}(1,3, e)=332 \\\\ \\operatorname{LM}(2,3, e)=333 \\\\ \\operatorname{LM}(1,4, e)=115 \\\\ \\operatorname{LM}(2,4, e)=116 \\end{array}\\right\\} \\] The contribution to the global arrays may be deduced from LM:\\\\ Stiffness (due to symmetry, only the upper triangular portion need be assembled.) \\[ \\left.\\begin{array}{l} K_{115,115} \\leftarrow K_{115,115}+k_{77}^{e} \\\\ K_{115,116}^{e} \\leftarrow K_{115,116}+k_{78}^{e} \\\\ K_{115,325} \\leftarrow K_{115,325}+k_{72}^{e} \\\\ K_{115,332} \\leftarrow K_{115,332}+k_{75}^{e} \\\\ K_{115,333} \\leftarrow K_{115,333}+k_{76}^{e} \\\\ K_{116,116} \\leftarrow K_{116,116}+k_{88}^{e} \\\\ K_{116,325} \\leftarrow K_{116,325}+k_{82}^{e} \\\\ K_{116,332} \\leftarrow K_{116,332}+k_{85}^{e} \\tag{2.10.7}\\\\ K_{116,333} \\leftarrow K_{116,333}+k_{86}^{e} \\\\ K_{325,325} \\leftarrow K_{325,325}+k_{22}^{e} \\\\ K_{325,332} \\leftarrow K_{325,332}+k_{25}^{e} \\\\ K_{325,333} \\leftarrow K_{325,333}+k_{26}^{e} \\\\ K_{332,332} \\leftarrow K_{332,332}+k_{55}^{e} \\\\ K_{332,333} \\leftarrow K_{332,333}+k_{56}^{e} \\\\ K_{333,333} \\leftarrow K_{333,333}+k_{66}^{e} \\end{array}\\right\\} \\] Force \\[ \\left.\\begin{array}{l} F_{115} \\leftarrow F_{115}+f_{7}^{e} \\tag{2.10.8}\\\\ F_{116} \\leftarrow F_{116}+f_{8}^{e} \\\\ F_{325} \\leftarrow F_{325}+f_{2}^{e} \\\\ F_{332} \\leftarrow F_{332}+f_{5}^{e} \\\\ F_{333} \\leftarrow F_{333}+f_{6}^{e} \\end{array}\\right\\} \\] where \\begin{equation*} f_{p}^{e}=\\cdots-\\sum_{q=1}^{n_{eq}} k_{p q}^{e} g_{q}^{e} \\tag{2.10.9} \\end{equation*} (We have omitted the first two terms in the right-hand side of (2.9.5) in writing (2.10.9).) In the present example, only $g_{1}^{e}, g_{3}^{e}$ and $g_{4}^{e}$ may be nonzero. Therefore (2.10.9) may be simplified to \\begin{equation*} f_{p}^{e}=\\cdots-k_{p 1}^{e} g_{1}^{e}-k_{p 3}^{e} g_{3}^{e}-k_{p 4}^{e} g_{4}^{e} \\tag{2.10.10} \\end{equation*} The multiplications indicated in (2.10.10) are only performed in practice if the $g_{p}^{e}$ 's are nonzero. A schematic representation of the contributions of $k^{e}$ and $f^{e}$ to $K$ and $F$ is shown in Figure 2.10.4.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-41} Figure 2.10.4 Contributions of elasticity element in Example 2 to global arrays. \\subsection*{Exercise 1.} Consider a two-dimensional elastostatic boundary-value problem. Set up the ID, IEN, and LM arrays for the following mesh of four-node quadrilaterals:\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-42} \\subsection*{2.11 SUMMARY OF IMPORTANT EQUATIONS FOR PROBLEMS CONSIDERED IN CHAPTERS 1 AND 2}\n",
      "chunk word length: 204, chunk char length: 1496, chunk = $(\\boldsymbol{S})$ $$ \\begin{aligned} \\sigma_{i j, j}+f_{i} & =0 & & \\text { on } \\Omega \\\\ u_{i} & =g_{i} & & \\text { on } \\Gamma_{g_{i}} \\\\ \\sigma_{i j} n_{j} & =h_{i} & & \\text { on } \\Gamma_{h_{i}} \\end{aligned} $$ where $\\sigma_{i j}=c_{ijkl} \\epsilon_{k l}=c_{i j k l} u_{(k, l)}$\\\\ $(W)^{19}$Find $u \\in \\delta$, such that $\\forall w \\in \\mathcal{V}$ $$ a(w, u)=(w, \\ell)+(w, k)_{\\Gamma} $$ where $$ \\begin{aligned} a(w, u) & =\\int_{\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\Omega \\\\ (w, \\ell) & =\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega \\\\ (w, k)_{\\Gamma} & =\\sum_{i=1}^{n_{sd}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right) \\end{aligned} $$ (G) Find $v^{h} \\in \\mathcal{V}^{h}$, such that $\\forall \\boldsymbol{w}^{h} \\in \\mathcal{V}^{h}$ $$ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell \\right)+\\left(w^{h}, k\\right)_{\\Gamma}-a\\left(w^{h}, g^{h}\\right) $$ ${ }^{19}$ The notation $\\forall$ means \"for all.\"\\\\ (M) $K d=F$, where $K=A_{e=1}^{n_{el}}\\left(k^{e}\\right), F=F_{\\text {nodal }}+A_{e=1}^{n_{el}}\\left(f^{e}\\right)^{20}$ $$ \\begin{aligned} k_{p q}^{e} & =e_{i}^{T} k_{a b}^{e} e_{j}, \\quad k_{a b}^{e}=\\int_{\\Omega^{e}} B_{a}^{T} D B_{b} d \\Omega \\\\ f_{p}^{e} & =\\int_{\\Omega^{e}} N_{a} \\ell_{i} d \\Omega+\\int_{\\Gamma^e_{h_{i}}} N_{a} h_{i} d \\Gamma-\\sum_{q=1}^{n_{el}} k_{p q}^{e} g_{q}^{e} \\quad \\text { (no sum on } i \\text { ) } \\\\ p & =n_{e d}(a-1)+i \\\\ q & =n_{e d}(b-1)+j \\end{aligned} $$ Stress at a point: $\\sigma(x)=D(x) \\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e}$\n",
      "chunk word length: 242, chunk char length: 1743, chunk = (S) $$ \\begin{aligned} q_{i, i} & =\\ell & & \\text { in } \\Omega \\\\ u & =g & & \\text { on } \\Gamma_{g} \\\\ -q_{i} n_{i} & =h & & \\text { on } \\Gamma_{h} \\end{aligned} $$ where $q_{i}=-\\kappa_{i j} u_{, j}$\\\\ (W) Find $u \\in \\mathcal{\\delta}$, such that $\\forall w \\in \\mathcal{V}$ $$ a(w, u)=(w, \\ell)+(w, h)_\\Gamma $$ where $$ \\begin{aligned} a(w, u) & =\\int_{\\Omega} w_{, i} \\kappa_{i j} u_{, j} d \\Omega \\\\ (w, \\ell) & =\\int_{\\Omega} w \\ell d \\Omega \\\\ (w, h)_{\\Gamma} & =\\int_{\\Gamma_{h}} w h d \\Gamma \\end{aligned} $$ (G) Find $v^{h} \\in \\delta^{h}$, such that $\\forall w^{h} \\in \\mathcal{V}^{h}$ $$ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h},\\ell \\right)+\\left(w^{h}, h\\right)_{\\Gamma}-a\\left(w^{h}, g^{h}\\right) $$ \\footnotetext[20]{In defining $\\boldsymbol{P}$ we have added to the element contributions the term $\\boldsymbol{F}_{\\text {nodal}}$, which is a vector of nodal applied forces. The reason for this is that it is often easier in practice to directly input concentrated forces at nodes rather than go through the element-by-element form and assemble procedure. The expression for $F$ then emphasizes that both modes of constructing $F$ are to be accommodated in the computer implementation of problems of this type. } (M) $\\quad K d=\\boldsymbol{F}$, where $K=\\boldsymbol{A}_{e=1}^{n_{e l}}\\left(k^{e}\\right), \\quad \\boldsymbol{F}=\\boldsymbol{F}_{\\text {nodal }}+A_{e=1}^{n_{e l}}\\left(f^{e}\\right)^{20}$\\\\ \\[ \\begin{aligned} k_{a b}^{e} & =\\int_{\\boldsymbol{\\Omega}^{e}} B_{a}^{T} D B_{b} d \\Omega \\\\ f_{a}^{e} & =\\int_{\\Omega^{e}} N_{a} \\ell d \\Omega+\\int_{\\Gamma_{h}^{e}} N_{a} h d \\Gamma-\\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \\end{aligned} \\] Heat flux vector at a point: $q(x)=-D(x) \\sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e}$\n",
      "chunk word length: 211, chunk char length: 1586, chunk = \\[ \\begin{array}{rlr} u_{, x x}+\\ell=0 & & \\text { on } \\Omega=] 0,1[ \\tag{S}\\\\ u(1)=g & & \\left(\\Gamma_{g}=\\{1\\}\\right) \\\\ -u_{, x}(0)=h & & \\left(\\Gamma_{h}=\\{0\\}\\right) \\end{array} \\] (W) Find $u \\in \\delta$, such that $\\forall w \\in \\mathcal{V}$ $$ a(w, u)=(w, \\ell)+w(0) h $$ where $$ \\begin{aligned} a(w, u) & =\\int_{0}^{1} w_{, x} u_{, x} d x \\\\ (w, \\ell) & =\\int_{0}^{1} w \\ell d x \\end{aligned} $$ (G) Find $v^{h} \\in \\mathcal{V}^{h}$, such that $\\forall w^{h} \\in \\mathcal{V}^{h}$ $$ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell\\right)+w^{h}(0) h-a\\left(w^{h}, g^{h}\\right) $$ (M) $\\quad K d=\\boldsymbol{F}$, where $K=\\boldsymbol{A}_{e=1}^{n_{e l}}\\left(k^{e}\\right), \\quad \\boldsymbol{F}=\\boldsymbol{F}_{\\text {nodal }}+A_{e=1}^{n_{e l}}\\left(f^{e}\\right)^{20}$\\\\ $$ \\begin{aligned} & k_{a b}^{e}=\\int_{\\Omega^{e}} N_{a, x} N_{b, x} d x \\\\ & f_{a}^{e}=\\int_{\\mathbf{\\Omega}^{e}} N_{a} \\ell d x+ \\begin{cases}k \\delta_{a 1} & e=1 \\\\ 0 & e=2, \\ldots, n_{e l}-1 \\\\ -g k_{2 a}^{e} & e=n_{el}\\end{cases} \\end{aligned} $$ \\subsection*{2.12 AXISYMMETRIC FORMULATIONS AND ADDITIONAL EXERCISES} Axisymmetric formulations are expressed in terms of cylindrical coordinates: $$ \\begin{aligned} & x_{1}=r=\\text { the radial coordinate } \\\\ & x_{2}=z=\\text { the axial coordinate } \\\\ & x_{3}=\\theta=\\text { the circumferential coordinate } \\end{aligned} $$ The basic hypothesis of axisymmetry is that all functions under consideration are independent of $\\theta$. That is, they are functions of $r$ and $z$ only. Thus three-dimensional problem classes are reduced to two-dimensional ones.\n",
      "chunk word length: 79, chunk char length: 463, chunk = The axisymmetric formulation for heat conduction is almost identical to the twodimensional Cartesian case considered previously. The only difference is that a factor of $2 \\pi r$ needs to be included in each integrand of the variational equation to account for the correct volumetric weighting (e.g., $d \\Omega=2 \\pi r d r d z$ replaces $d \\Omega=$ $d x_{1} d x_{2}$ ). Since the constant $2 \\pi$ is common to all terms, it may be cancelled throughout if desired.\n",
      "chunk word length: 346, chunk char length: 2704, chunk = The displacement components in cylindrical coordinates are: $$ \\begin{aligned} & u_{1}=u_{r}=\\text { the radial displacement } \\\\ & u_{2}=u_{2}=\\text { the axial displacement } \\\\ & u_{3}=u_{\\theta}=\\text { the circumferential displacement } \\end{aligned} $$ In addition to the basic hypothesis of axisymmetry, we further assume that $u_{\\theta}=0$, and thus \\begin{equation*} \\epsilon_{r \\theta}=\\epsilon_{z \\theta}=0 \\tag{2.12.1} \\end{equation*} Note that $\\epsilon_{\\theta \\theta}=u_{r} / r$ and therefore it is generally not zero. The constitutive moduli are assumed to be such that the preceding kinematical hypotheses result in \\begin{equation*} \\sigma_{r \\theta}=\\sigma_{z \\theta}=0 \\tag{2.12.2} \\end{equation*} The preceding assumptions lead to what is called the torsionless axisymmetric case. This formulation is similar to but somewhat more complicated than the twodimensional cases of plane strain and plane stress. Here there are four nonzero components of stress and strain: \\begin{align*} & \\boldsymbol{\\sigma}=\\left\\{\\begin{array}{l} \\sigma_{11} \\\\ \\sigma_{22} \\\\ \\sigma_{12} \\\\ \\sigma_{33} \\end{array}\\right\\}=\\left\\{\\begin{array}{c} \\sigma_{r r} \\\\ \\sigma_{z z} \\\\ \\sigma_{r z} \\\\ \\sigma_{\\theta \\theta} \\end{array}\\right\\} \\tag{2.12.3}\\\\ & \\epsilon=\\left\\{\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{22} \\\\ 2 \\epsilon_{12} \\\\ \\epsilon_{33} \\end{array}\\right\\}=\\left\\{\\begin{array}{c} \\epsilon_{r r} \\\\ \\epsilon_{z z} \\\\ 2 \\epsilon_{r z} \\\\ \\epsilon_{\\theta \\theta} \\end{array}\\right\\} \\tag{2.12.4} \\end{align*} The ordering emanates from the following generalization of Table 2.7.1: \\begin{table}[ht] \\centering \\begin{tabular}{|c|c|c|c|} \\hline \\diagbox[width=2em]{\\textit{J}}{\\textit{I}} & \\diagbox[width=2em]{\\textit{k}}{\\textit{i}} & \\diagbox[width=2em]{\\textit{l}}{\\textit{j}} \\\\ \\hline 1 & 1 & 1 \\\\ 3 & 1 & 2 \\\\ 3 & 2 & 1 \\\\ 2 & 2 & 2 \\\\ \\hline 4 & 3 & 3 \\\\ \\hline \\end{tabular} \\end{table} The $D$ array takes on the following form: \\[ D=\\left[D_{I J}\\right]=\\underbrace{\\left[\\begin{array}{ll} D_{33} & D_{3} \\tag{2.12.5}\\\\ D_{3}^{T} & D_{44} \\end{array}\\right]}_{4 \\times 4} \\] \\[ D_{33}=\\left[\\begin{array}{lll} D_{11} & D_{12} & D_{13} \\tag{2.12.6}\\\\ & D_{22} & D_{23} \\\\ \\text { symmetric } & D_{33} \\end{array}\\right] \\] \\[ D_{3}=\\left\\{\\begin{array}{l} D_{14} \\tag{2.12.7}\\\\ D_{24} \\\\ D_{34} \\end{array}\\right\\} \\] where $D_{I J}=c_{i j k l}$, in which the indices are related by the table. The $\\boldsymbol{B}_{a}$-matrix takes on the form \\[ \\boldsymbol{B}_{a}=\\left[\\begin{array}{cc} N_{a, 1} & 0 \\tag{2.12.8}\\\\ 0 & N_{a, 2} \\\\ N_{a, 2} & N_{a, 1} \\\\ \\hline \\frac{N_{a}}{r} & 0 \\end{array}\\right] \\] Again, a factor of $2 \\pi r$ needs to be included in all integrands.\n",
      "chunk word length: 1435, chunk char length: 10815, chunk = The plane strain case may be obtained from the axisymmetric formulation by\\\\ i. Ignoring the $2 \\pi$ factors; and\\\\ ii. Ignoring the fourth row of $B_{a}$ and the fourth row and column of $\\boldsymbol{D}$. Furthermore, the plane stress case may be similarly obtained if, in addition, $\\boldsymbol{D}_{33}$ is replaced by \\begin{equation*} D_{33}-D_{3} D_{44}^{-1} D_{3}^{T} \\tag{2.12.9} \\end{equation*} which directly follows from the plane stress condition, $\\sigma_{33}=0$. (References [6] and [7] may be consulted for further elaboration on the physical ideas.) Sometimes (2.12.9) is referred to as the statically condensed elastic coefficient matrix. Consequently, in programming the axisymmetric case, for a small amount of additional effort both plane strain and plane stress may also be included. \\subsection*{Exercise 1.} Under the assumption of isotropy, show that $D_{33}$ is the same as the $\\boldsymbol{D}$-matrix in (2.7.34). Furthermore, show that $D_{44}=\\lambda+2 \\mu$ and \\[ D_{3}=\\left\\{\\begin{array}{l} \\lambda \\tag{2.12.10}\\\\ \\lambda \\\\ 0 \\end{array}\\right\\} \\] \\subsection*{Exercise 2.} Verify that for the isotropic case, (2.12.9) achieves a similar end to the procedure described in Remark 9 of Sec. 2.7. \\subsection*{Exercise 3.} Consider the one-dimensional model problem discussed previously. Obtain exact expressions for $f=\\left\\{f_{a}^{e}\\right\\}, a=1,2$, for the following cases (ignore $g$ and $h$ contributions):\\\\ i. $\\boldsymbol{\\ell}=$ constant.\\\\ ii. $\\ell=\\delta(x-\\bar{x})$, the delta function, where $x_{1}^{e} \\leq \\bar{x} \\leq x_{2}^{e}$. Specialize for the cases $\\bar{x}=x_{b}^{e}$ and $\\bar{x}=\\left(x_{1}^{e}+x_{2}^{e}\\right) / 2$. Solution $$ \\begin{aligned} & \\text { i. } f_{a}^{e}=\\ell \\int_{x_{1}^{e}}^{x^ e_{2}} N_{a}(x) d x=\\frac{\\ell h^{e}}{2} \\underbrace{\\int_{-1}^{+1} N_{a}(\\xi) d \\xi}_{1} \\\\ & f^{e}=\\frac{\\ell h^{e}}{2}\\left\\{\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right\\} \\end{aligned} $$ ii. $f_{a}^{e}=\\int_{x_{1}^{e}}^{x_{2}^{e}} N_{a}(x) \\delta(x-\\bar{x}) d x=N_{a}(\\bar{x})$ For $x=x_{b}^{e}$, $$ \\begin{aligned} f_{a}^{e} & =N_{a}(\\bar{x})=N_{a}\\left(x_{b}^{e}\\right)=\\delta_{a b} \\quad \\text { (Kronecker delta) } \\\\ f^{e} & =\\left\\{\\begin{array}{l} \\delta_{1 b} \\\\ \\delta_{2 b} \\end{array}\\right\\} \\end{aligned} $$ For $\\bar{x}=\\left(x_{1}^{e}+x_{2}^{e}\\right) / 2$, $$ f_{a}^{e}=N_{a}(\\bar{x})=N_{a}\\left(\\frac{x_{1}^{e}+x_{2}^{e}}{2}\\right)=\\frac{1}{2} $$ Therefore, $$ f^{e}=\\frac{1}{2}\\left\\{\\begin{array}{l} 1 \\\\ 1 \\end{array}\\right\\} $$ \\subsection*{Exercise 4.} Consider the boundary-value problem for classical linear elastostatics discussed previously. In the linearized theory of small displacements superposed upon large, the stiffness term in the variational equation, $$ \\int_{\\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \\Omega $$ is replaced by $$ \\int_{\\Omega} w_{i, j} d_{i j k l} u_{k, l} d \\Omega $$ where $$ \\begin{aligned} d_{i j k l} & =c_{i j k l}+\\delta_{i k} \\sigma_{jl}^{0} \\\\ \\sigma_{j l}^{0} & =\\sigma_{lj}^{0} \\end{aligned} $$ and the $\\sigma_{j l}^{0}$ 's (i.e., initial stresses) are given functions of $\\boldsymbol{x} \\in \\boldsymbol{\\Omega}$. It follows from the symmetries of $c_{i j k l}$ and $\\sigma_{jl}^{0}$ that $$ d_{i j k l}=d_{k li j} $$ Assume $\\boldsymbol{n}_{s d}=2$. An index-free formulation of the stiffness term is given by $$ \\int_{\\Omega}\\left\\{\\begin{array}{c} w_{1,1} \\\\ w_{2,2} \\\\ w_{1,2}+w_{2,1} \\\\ w_{1,2}-w_{2,1} \\end{array}\\right\\}^{T} \\quad \\underbrace{D}_{4 \\times 4}\\left\\{\\begin{array}{c} u_{1,1} \\\\ u_{2,2} \\\\ u_{1,2}+u_{2,1} \\\\ u_{1,2}-u_{2,1} \\end{array}\\right\\} d \\Omega $$ which leads to the following definition of the element stiffness matrix: $$ k_{p q}^{e}=e_{i}^{T} \\int_{\\boldsymbol{\\Omega}^{e}} \\underbrace{B_{a}^{T}}_{2 \\times 4} \\underbrace{D}_{4 \\times 4} \\underbrace{B_{b}}_{4 \\times 2} d \\Omega e_{j} $$ Set up $B_{a}$ in terms of the shape function $N_{a}$. Define the components of $D$ in terms of the $d_{i j kl}$ 's. (The $\\sigma_{j l}^{0}$-contribution to the stiffness is sometimes called the initial-stress stiffiness matrix. It is important to account for it in the solution of many nonlinear problems.) \\subsection*{Exercise 5.}Let $\\boldsymbol{\\Omega}$ be a region in $\\mathbb{R}^{2}$ and let its boundary $\\Gamma=\\overline{\\Gamma_{1} \\cup \\Gamma_{2} \\cup \\Gamma_{3} \\cup \\Gamma_{4}}$ where $\\Gamma_{1}, \\ldots, \\Gamma_{4}$ are nonoverlapping subregions of $\\Gamma$. Let $n$ be the unit outward normal vector to $\\Gamma$ such that $\\boldsymbol{s}$ and $\\boldsymbol{n}$ form a right-hand rule basis; see Fig. 2.12.1. Consider the following boundary-value problem in classical linear elastostatics: Given $\\ell_i: \\Omega \\rightarrow \\mathbb{R} ; g_{i}: \\Gamma_{1} \\rightarrow \\mathbb{R} ; h_{i}: \\Gamma_{2} \\rightarrow \\mathbb{R} ; g_{n}$ and $h_{3}: \\Gamma_{3} \\rightarrow \\mathbb{R}$; and $g_{s}$ and $h_{n}: \\Gamma_{4} \\rightarrow \\mathbb{R}$; find $u_{i}: \\overline{\\boldsymbol{\\Omega}} \\rightarrow \\mathbf{R}$ such that $$ \\begin{aligned} & \\sigma_{ij, j}+f_{i}=0 \\quad \\text { in } \\Omega \\\\ & u_{i}=q_{i} \\quad \\text { on } \\Gamma_{1} \\\\ & \\sigma_{i j} n_{j}=h_{i} \\quad \\text { on } \\Gamma_{2} \\\\ & \\left.\\left.\\begin{array}{rl} u_{i} n_{i} & =g_{n} \\\\ \\sigma_{y} n_{j} s_{i} & =h_{s} \\end{array}\\right\\} \\quad \\text { on } \\Gamma_{3} \\quad \\text { ( } \\begin{array}{l} \\text { normal displacement } \\\\ \\text { tangential traction } \\end{array}\\right) \\\\ & \\left.\\begin{array}{rl} u_{i} s_{i} & =g_{s} \\\\ \\sigma_{i j} n_{j} n_{i} & =h_{n} \\end{array}\\right\\} \\quad \\text { on } \\Gamma_{4} \\quad\\binom{\\text { tangential displacement }}{\\text { normal traction }} \\end{aligned} $$ where $\\sigma_{ij}=c_{ijkl} u_{(k, l)}$. Establish a weak formulation for this problem in which all \" $g$-type\" boundary conditions are essential and all \"h-type\" boundary conditions are natural. State all requirements on the spaces $\\delta$ and $\\mathcal{V}$. Hint: $w=w_{n} n+w_{s} s$; i.e., $w_{i}=w_{n} n_{i}+w_{s} s_{i}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-49} Figure 2.12.1 \\subsection*{Exercise 6.} In practice, it is often useful to generalize the constitutive equation of classical elasticity to the form \\begin{equation*} \\sigma_{i j}=c_{i j k l}\\left(\\epsilon_{k l}-\\epsilon_{k l}^{0}\\right)+\\sigma_{i j}^{0} \\tag{2.12.11} \\end{equation*} where $\\epsilon_{i j}^{0}$ and $\\sigma_{i j}^{0}$ are the initial strain and initial stress, both given functions of $x$. The initial strain term may be used to represent thermal expansion effects by way of \\begin{equation*} \\epsilon_{k l}^{0}=-\\theta c_{k l} \\tag{2.12.12} \\end{equation*} where $\\theta$ is the temperature and the $c_{k i}$ 's are the thermal expansion coefficients (both given functions). Clearly, (2.12.11) will in no way change the stiffness matrix. However there will be additional contributions to $f_{p}^{e}$. Generalize the definition of $f_{p}^{e}$ to account for these additional terms.\\\\ \\textbf{Solution} We begin with the weak form $$ \\int_{\\Omega} w_{(i,j)} \\sigma_{i j} d \\Omega=\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega+\\sum_{i=1}^{n_{sd}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right) $$ Substituting (2.12.11) and (2.12.12) into the weak form leads to $$ \\begin{aligned} & \\int_{\\Omega} w_{(i,j)} c_{ijkl} \\epsilon_{k l} d \\Omega=\\int_{\\Omega} w_{i} \\ell_{i} d \\Omega+\\sum_{i=1}^{n_{sd}}\\left(\\int_{\\Gamma_{h_{i}}} w_{i} h_{i} d \\Gamma\\right) \\end{aligned} $$ $$ \\begin{aligned} & \\text { to right-hand side: } \\\\ & +\\int_{\\Omega} w_{(i, j)} c_{i jk l} \\epsilon_{k l}^{0} d \\Omega \\\\ & -\\int_{\\boldsymbol{\\Omega}} w_{(i,j)} \\sigma_{i j}^{0} d \\boldsymbol{\\Omega} \\end{aligned} $$ from which the additional terms in $f_{p}^{e}$ may be deduced: $$ f_{p}^{\\varepsilon}=\\cdots+e_{i}^{T} \\int_{\\boldsymbol{\\Omega}^{e}} B_{a}^{T} D \\theta c d \\Omega-e_{i}^{T} \\int_{\\Omega^{e}} B_{a}^{T} \\sigma^{0} d \\Omega $$ where $$ \\begin{aligned} c= & \\left\\{\\begin{array}{c} c_{11} \\\\ c_{22} \\\\ 2 c_{12} \\end{array}\\right\\} ; \\quad \\sigma^{0}=\\left\\{\\begin{array}{l} \\sigma_{11}^{0} \\\\ \\sigma_{22}^{0} \\\\ \\sigma_{12}^{0} \\end{array}\\right\\} ; \\ldots \\\\ & \\\\ & \\text { Without loss of generality, we } \\\\ & \\text { may assume symmetry, i.e., } c_{12}+c_{21}=2 c_{12} \\end{aligned} $$ \\subsection*{Exercise 7.} Consider the following boundary-value problem: $$ \\begin{aligned} u_{,xx}-p(p-1) x^{p-2} & =0, \\quad 0<x<1 \\\\ -u_{, x}(0) & =0 \\\\ u(1) & =1 \\end{aligned} $$ where $p$ is a given constant.\\\\ i. Obtain the exact solution to this problem for $\\boldsymbol{p}=5$. Sketch.\\\\ ii. State the weak formulation of the problem.\\\\ iii. State the Galerkin formulation.\\\\ iv. State the matrix formulation.\\\\ v. Solve the matrix problem assuming $p=5$ and using the piecewise linear finite element space for the following cases:\\\\ a. one element\\\\ b. two equal-length elements\\\\ vi. Compare the exact value of $u_{, x}(1)$ with the approximate values computed in part v. Explain why it is impossible for these results to compare favorably. \\subsection*{Exercise 8.} In heat conduction, it is often of interest to accurately calculate the boundary heat flux over a portion of the boundary where temperature is specified. Suppose we use the usual Galerkin finite element formulation to calculate the temperature. However, instead of calculating the heat flux in the usual way (i.e., by differentiating the temperature), we introduce a post-processing which derives from the following weak formulation:\\\\ Find $u \\in \\delta$ and $h \\in L_{2}\\left(\\Gamma_{g}\\right)$ such that for all $w \\in \\mathcal{V}$, $$ -\\int_{\\Omega} w_{,i}q_{i} d \\Omega=\\int_{\\Omega} w \\ell d \\Omega+\\int_{\\Gamma_{h}} w h d \\Gamma+\\int_{\\Gamma_{g}} w h d \\Gamma $$ where $h$ is the unknown heat flux on $\\Gamma_{g}$\\\\ (Note: In this formulation, it is not assumed that $w=0$ on $\\Gamma_{g}!$ )\\\\ i. Show, in addition to the usual differential equations and boundary conditions, that $$ h=-q_{i} n_{i} \\text { on } \\Gamma_{g} $$ arises naturally from the new weak formulation.\\\\ ii. State the Galerkin and matrix formulations corresponding to the new weak formulation assuming $h$ is approximated in the usual way, namely $$ h^{h}(x)=\\sum_{A \\in \\eta_{g}} N_{A}(x) h_{A} $$ (Hint: The equations governing the temperature are unchanged.)\\\\ iii. Specialize this formulation to the one-dimensional problem described in Exercise 7 and calculate the boundary flux at $x=1$ by the new procedure (cf. parts $v$ and vi of Exercise 7).\\\\ (Hint: The new method should produce exact results for these cases.)\\\\ iv. Develop a counterpart of the new formulation for elasticity. That is, introduce the $i$th component of traction as an independent unknown on $\\Gamma_{g_i}$ and carefully state the weak formulation.\\\\ v. Prove that the new method is exact for the one-dimensional model problem of Chapter 1.\n",
      "chunk word length: 18, chunk char length: 124, chunk = \\begin{enumerate} \\item J. E. Marsden and A. J. Tromba, Vector Calculus. San Francisco: W. H. Freeman, 1976. \\end{enumerate}\n",
      "chunk word length: 22, chunk char length: 173, chunk = \\begin{enumerate} \\setcounter{enumi}{1} \\item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N.J.: Prentice-Hall, 1973. \\end{enumerate}\n",
      "chunk word length: 99, chunk char length: 684, chunk = \\begin{enumerate} \\setcounter{enumi}{2} \\item G. Duvaut and J. L. Lions, Les Inquations en Mcanique et en Physique. Paris: Dunod, 1972. \\item G. Fichera, \"Existence Theorems in Elasticity,\" in Handbuch der Physik, Volume V1a/2, Mechanics of Solids II, ed. C. Truesdell. New York: Springer-Verlag, 1972. \\item M. Gurtin, \"The Linear Theory of Elasticity,\" in Handbuch der Physik, Volume V1a/2, Mechanics of Solids II, ed. C. Truesdell. New York: Springer-Verlag, 1972. \\item I. S. Sokolnikoff, Mathematical Theory of Elasticity (2nd ed.). New York: McGraw-Hill, 1956. \\item S. Timoshenko and J. N. Goodier, Theory of Elasticity (3rd ed.). New York: McGraw-Hill, 1969. \\end{enumerate}\n",
      "chunk word length: 35, chunk char length: 244, chunk = \\begin{enumerate} \\setcounter{enumi}{7} \\item N. E. Gibbs, W. G. Poole, Jr., and P. K. Stockmeyer, \"An Algorithm for Reducing the Bandwidth and Profile of a Sparse Matrix,\" SIAM Journal of Numerical Analysis, 13 (1976), 236-250. \\end{enumerate}\n",
      "max chunk length in words = 2562\n",
      "max chunk length in char = 18926\n",
      "saved\n",
      "Space size: (61, 1536)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I used fixed size chunks (512) with a 25% overlap.\n",
    "Make sure environment_sensitive is set to False for fixed size.\n",
    "\n",
    "We should embed all chapters to generate the embedding space. For the demo, I only included two chapters.\n",
    "please update the paths in latex_file_paths.\n",
    "\"\"\"\n",
    "\n",
    "# add all book chapters paths\n",
    "if production_mode == False:\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex'\n",
    "    ]\n",
    "elif production_mode == True:\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter5.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter6.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex'\n",
    "    ]\n",
    "\n",
    "tokens_per_chunk = 4096                         # was 512\n",
    "token_overlap = int(0.25 * tokens_per_chunk)    # 25% overlap\n",
    "environment_sensitive = False                   # If False, equations can split between two chunks, but chunk lengths remain fixed.\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    embedding_space_file_name = f'{main_dir}/hughes_latex_embedding_space_tpc{tokens_per_chunk}_o{token_overlap}.json'\n",
    "elif chunk_by_section == True:\n",
    "    embedding_space_file_name = f'{main_dir}/hughes_latex_embedding_space_by_sections_tpc{tokens_per_chunk}.json'\n",
    "    token_overlap = 0\n",
    "\n",
    "space = {}\n",
    "if not os.path.exists(embedding_space_file_name):\n",
    "    \n",
    "    chunks = process_latex_files(latex_file_paths, \n",
    "                                 tokens_per_chunk, \n",
    "                                 token_overlap, \n",
    "                                 environment_sensitive, \n",
    "                                 chunk_by_section = chunk_by_section)\n",
    "    \n",
    "    chunk_length = []\n",
    "    char_length = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        print(f\"chunk word length: {len(chunk.split(\" \"))}, chunk char length: {len(chunk)}, chunk = {chunk}\")\n",
    "        chunk_length.append(len(chunk.split(\" \")))\n",
    "        char_length.append(len(chunk))\n",
    "    print(f\"max chunk length in words = {np.max(chunk_length)}\")\n",
    "    print(f\"max chunk length in char = {np.max(char_length)}\")\n",
    "    #print(f\"chunk lengths = {chunk_length}\")\n",
    "\n",
    "    # using api\n",
    "    embedding_space = get_embeddings(client, chunks, model=embedding_model)\n",
    "    \n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'w') as json_file:\n",
    "        json.dump({'chunks': chunks, 'embedding_space': embedding_space}, json_file)\n",
    "\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'r') as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "\n",
    "    chunks = loaded_data['chunks']\n",
    "    embedding_space = np.array(loaded_data['embedding_space'])\n",
    "    print(\"loaded\")\n",
    "\n",
    "chunks = np.array(chunks)\n",
    "embedding_space = np.array(embedding_space)\n",
    "print(\"Space size:\", embedding_space.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Questions and Their Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk word length: 525, chunk char length: 3363, chunk = The main constituents of a finite element method for the solution of a boundary-value problem are\\\\ i. The variational or weak statement of the problem; and\\\\ ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple example.\\\\ Suppose we want to solve the following differential equation for $u$ : \\begin{equation*} u_{, x x}+f=0 \\tag{1.1.1} \\end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \\begin{equation*} f: [0,1] \\to \\mathbb{R} \\tag{1.1.2} \\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \\leq x \\leq 1$ ) and $\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\in[0,1], f(x) \\in \\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\textbf{\\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\[ \\boldsymbol{\\Omega}=] 0,1[ \\quad \\text { (open) } \\tag{1.1.3} \\] \\[ \\overline{\\boldsymbol{\\Omega}}=[0,1] \\quad \\text { (closed) } \\tag{1.1.4} \\] See Fig. 1.1.1. \\begin{figure}[h] \\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\vspace{0.5em} \\textbf{Figure 1.1.1} \\end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work.\n",
      "chunk word length: 299, chunk char length: 1996, chunk = A boundary-value problem for (1.1.1) involves imposing \\textbf{\\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \\begin{align} u(1) &= g \\tag{1.2.1} \\\\ -u_{, x}(0) &= h \\tag{1.2.2} \\end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \\textbf{\\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \\[ \\text{(S)} \\quad \\left\\{ \\parbox{0.8\\textwidth}{ \\text{Given } $f:\\overline{\\boldsymbol{\\Omega}} \\to \\mathbb{R}$ \\text{ and constants } $g$ \\text{ and } $h$, \\text{ find } $u:\\overline{\\boldsymbol{\\Omega}} \\to \\mathbb{R}$, \\text{ such that:} \\begin{align*} u_{,xx} + f &= 0 \\quad \\text{on } \\Omega \\\\ u(1) &= g \\\\ -u_{,x}(0) &= h \\end{align*} } \\right. \\] When we write $u_{, x x}+f=0$ on $\\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \\in \\Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \\begin{equation*} u(x)=g+(1-x) h+\\int_{x}^{1}\\left\\{\\int_{0}^{y} f(z) d z\\right\\} d y \\tag{1.2.3} \\end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section.\n",
      "chunk word length: 445, chunk char length: 3162, chunk = To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \\begin{equation*} \\int_{0}^{1}(u_{,x})^{2} d x<\\infty \\tag{1.3.1} \\end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\\boldsymbol{u} \\in \\boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \\in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \\begin{equation*} \\mathcal{S}=\\left\\{u \\mid u \\in H^{1}, u(1)=g\\right\\} \\quad \\text { (trial solutions) } \\tag{1.3.2} \\end{equation*} The fact that $\\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \\textbf{\\textit{weighting functions}}, or \\textbf{\\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\\mathcal{U}$ and defined by\\\\ \\begin{equation*} \\mathcal{U} = \\{w \\mid w \\in H^{1}, w(1)=0\\} \\quad \\text{(weighting functions)} \\tag{1.3.3} \\end{equation*} It simplifies matters somewhat to continue to think of $f: \\Omega \\rightarrow \\mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \\[ (\\mathcal{W}) \\quad \\left\\{ \\parbox{0.8\\textwidth}{ \\text{Given } $f,$q and $h$,\\text{ as before. Find } $u \\in \\mathfrak{f}$, \\text{ such that for all} \\mbox{$w \\in \\mathcal{U}$} \\begin{align*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.3.4} \\end{align*} } \\right. \\] Formulations of this type are often called \\textbf{\\textit{virtual work}}, or \\textbf{\\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \\textbf{\\textit{virtual displacements}}. Equation (1.3.4) is called the \\textbf{\\textit{variational equation}}, or (especially in mechanics) the \\textbf{\\textit{equation of virtual work.}} The solution of $(W)$ is called the \\textbf{\\textit{weak}}, or \\textbf{\\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider.\n",
      "chunk word length: 571, chunk char length: 3826, chunk = Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. \"Proofs\" of this kind are sometimes euphemistically referred to as \"formal proofs.\" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall \"prove\" the following. \\subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \\subsection*{Formal Proof} \\bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \\begin{equation*} 0=-\\int_{0}^{1} w(u_{, x x}+f) d x \\tag{1.4.1} \\end{equation*} for any $w \\in \\mathcal{U}$. Integrating (1.4.1) by parts results in \\begin{equation*} 0=\\int_{0}^{1} w_{, x} u_{, x} d x-\\int_{0}^{1} w f d x-\\left.w u_{, x}\\right|_{0} ^{1} \\tag{1.4.2} \\end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \\begin{equation*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.4.3} \\end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \\in \\mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\\\ \\\\ b. Now $u$ is assumed to be a weak solution. Thus $u \\in \\mathcal{S}$; consequently $u(1)=g$, and $$ \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h $$ for all $w \\in \\mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \\begin{equation*} 0=\\int_{0}^{1} w\\left(u_{, x x}+f\\right) d x+w(0)\\left[u_{, x}(0)+h\\right]. \\tag{1.4.4} \\end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\\\ i. $u_{, x x}+f=0$ on $\\Omega$; and\\\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\\boldsymbol{w}$ in (1.4.4) by \\begin{equation*} w=\\phi\\left(u_{, x x}+f\\right) \\tag{1.4.5} \\end{equation*} where $\\phi$ is smooth; $\\phi(x)>0$ for all $x \\in \\Omega=] 0,1[$; and $\\phi(0)=\\phi(1)=0$. For example, we can take $\\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \\in \\mathcal{U}$, so (1.4.5) defines a\\\\ \\begin{figure}[h] \\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \\vspace{0.5em} \\textbf{Figure 1.4.1} \\end{figure} \\footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\\\ \\begin{equation*} 0=\\int_{0}^{1} \\phi \\underbrace{\\left(u_{, x x}+f\\right)^{2}}_{\\geq 0} d x+0 \\tag{1.4.6} \\end{equation*} Since $\\phi>0$ on $\\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \\begin{equation*} 0=w(0)\\left[u_{, x}(0)+h\\right] \\tag{1.4.7} \\end{equation*} That $w \\in \\mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \\neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition.\n",
      "chunk word length: 468, chunk char length: 3228, chunk = \\begin{enumerate} \\item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \\textbf{\\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \\textbf{\\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \\item The method used to prove part (b) of the proposition goes under the name of the \\textbf{\\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \\end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \\begin{align*} a(w, u) & =\\int_{0}^{1} w_{, x} u_{, x} d x \\tag{1.4.8}\\\\ (w, f) & =\\int_{0}^{1} w f d x \\tag{1.4.9} \\end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \\begin{equation*} a(w, u)=(w, f)+w(0) h \\tag{1.4.10} \\end{equation*} Here, $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ are examples of \\textbf{\\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \\begin{align*} a(u, v) & =a(v, u) \\tag{1.4.11}\\\\ (u, v) & =(v, u) \\tag{1.4.12} \\end{align*} Bilinearity means linearity in each \"slot\"; for example, \\begin{align*} a\\left(c_{1} u+c_{2} v, w\\right) & =c_{1} a(u, w)+c_{2} a(v, w) \\tag{1.4.13}\\\\ \\left(c_{1} u+c_{2} v, w\\right) & =c_{1}(u, w)+c_{2}(v, w) \\tag{1.4.14} \\end{align*} \\\\ Exercise 1. Use the definitions of $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ to verify the properties of symmetry and bilinearity.\\\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|| 5/5 [01:07<00:00, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are generated\n",
      "Questions are embedded\n",
      "saved ../data/hughes_latex_Q_then_A_use_context/hughes_ch1_Qs_n40_by_sections_tpc1536.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "For generating questions, we want larger chunks with a bit of overlap.\n",
    "The following values are just for this demo, so please adjust them as needed.\n",
    "\n",
    "I only ran Chapter One.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chapter = chpt_for_quest_answ\n",
    "latex_file_path = f'../data/FEM_Hughes_LaTeX_Textbook/chapter{chapter}.tex'\n",
    "\n",
    "max_questions = 40                             # max number of questions per chunk\n",
    "\n",
    "tokens_per_chunk = 1536                       \n",
    "token_overlap = int(0.2 * tokens_per_chunk)   # 10% overlap\n",
    "environment_sensitive = True                  # If True, equations won't be split between chunks, which may result in chunks larger than the specified tokens_per_chunk\n",
    "                           \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_file_name = f\"{main_dir}/hughes_ch{chapter}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_file_name = f\"{main_dir}/hughes_ch{chapter}_Qs_n{max_questions}_by_sections_tpc{tokens_per_chunk}.json\"  \n",
    "    token_overlap = 0 \n",
    "\n",
    "if not os.path.exists(questions_file_name):\n",
    "    question_chunks = process_latex_files(latex_file_path, tokens_per_chunk, token_overlap, environment_sensitive, chunk_by_section=chunk_by_section)\n",
    "    \n",
    "    if production_mode == False:\n",
    "        question_chunks = question_chunks[1:6] # for testing small batch\n",
    "    \n",
    "    for question in question_chunks:\n",
    "        print(f\"chunk word length: {len(question.split(\" \"))}, chunk char length: {len(question)}, chunk = {question}\")\n",
    "\n",
    "    questions = {}  # main data\n",
    "\n",
    "    # we should save generation info we used\n",
    "    questions['info'] = {\n",
    "        'tokens_per_chunk': tokens_per_chunk,\n",
    "        'token_overlap': token_overlap,\n",
    "        'environment_sensitive': environment_sensitive,\n",
    "        'max_questions': max_questions\n",
    "    }\n",
    "\n",
    "    ## step 1: generate questions\n",
    "    questions['data'] = []\n",
    "    for i in tqdm(range(len(question_chunks)), desc=\"Generating Questions\"):\n",
    "        # q_for_chunk = gen_questions(client, question_chunks[i], max_questions, model=llm_model)\n",
    "        q_for_chunk = gen_questions_s(client, question_chunks[i], max_questions, model=llm_model)   # Using the new function\n",
    "        questions['data'].append({'chunk': question_chunks[i],'questions': q_for_chunk})\n",
    "    print('Questions are generated')\n",
    "\n",
    "    ## step 2: embedding all questions at once\n",
    "    all_questions = []\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            all_questions.append(sub_item['question'])\n",
    "    # using api\n",
    "    embeddings = get_embeddings(client, all_questions, model=embedding_model) \n",
    "    # add them to data:\n",
    "    k = 0\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            sub_item['embedding'] = embeddings[k]\n",
    "            k +=1\n",
    "    print('Questions are embedded')\n",
    "    \n",
    "\n",
    "    with open(questions_file_name, 'w') as json_file:\n",
    "        json.dump(questions, json_file, indent=4)\n",
    "    print('saved', questions_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_file_name, 'r') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Retrieval and Generating Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k context added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions: 100%|| 5/5 [09:28<00:00, 113.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are answered\n",
      "saved ../data/hughes_latex_Q_then_A_use_context/hughes_ch1_QAs_n40_topk10_by_sections.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "Since we answer each question separately, this process is slow.\n",
    "We might want to consider using the batch API for this.\n",
    "\"\"\"\n",
    "\n",
    "top_k = 10   # number of retrieved closest contexts         \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_answers_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}_topk{top_k}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_answers_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"   \n",
    "\n",
    "if not os.path.exists(questions_answers_file_name):\n",
    "\n",
    "    questions_answers = questions.copy()\n",
    "\n",
    "    # step 1) finding top_k context from the book embedding and adding them to each question\n",
    "    for item in questions_answers['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            ind = fixed_knn_retrieval(sub_item['embedding'], embedding_space, top_k)\n",
    "            context = ''\n",
    "            for i, chunk in enumerate(chunks[ind]):\n",
    "                context += f'\\n\\n Additional context {i}: {chunk}' \n",
    "            sub_item['context'] = context\n",
    "    print('top_k context added')\n",
    "\n",
    "    # step 2) generating answers (slow)  (should we try batch API?)\n",
    "    for item in tqdm(questions_answers['data'], desc=\"Answering Questions\"):\n",
    "        question_chunk = item['chunk']\n",
    "        for sub_item in item['questions']:\n",
    "            question = sub_item['question']\n",
    "            context = question_chunk + sub_item['context']\n",
    "            sub_item['answer'] = gen_answer(client, question, context)\n",
    "    print('Questions are answered')\n",
    "    \n",
    "    with open(questions_answers_file_name, 'w') as json_file:\n",
    "        json.dump(questions_answers, json_file, indent=4)\n",
    "    print('saved', questions_answers_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_answers_file_name, 'r') as json_file:\n",
    "        questions_answers = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_answers_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_chunk</th>\n",
       "      <th>context</th>\n",
       "      <th>coverage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The main constituents of a finite element meth...</td>\n",
       "      <td>\\n\\n Additional context 0: The main constituen...</td>\n",
       "      <td>90</td>\n",
       "      <td>What are the main constituents of the finite e...</td>\n",
       "      <td>The main constituents of the finite element me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The main constituents of a finite element meth...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>80</td>\n",
       "      <td>Explain the significance of the variational or...</td>\n",
       "      <td>The variational or weak statement of a boundar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The main constituents of a finite element meth...</td>\n",
       "      <td>\\n\\n Additional context 0: A boundary-value pr...</td>\n",
       "      <td>75</td>\n",
       "      <td>Given the differential equation $u_{, xx} + f ...</td>\n",
       "      <td>The function $f$ being described as smooth imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The main constituents of a finite element meth...</td>\n",
       "      <td>\\n\\n Additional context 0: The main constituen...</td>\n",
       "      <td>70</td>\n",
       "      <td>Define the terms 'domain' and 'range' in the c...</td>\n",
       "      <td>In the context of the function $f: [0,1] \\to \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The main constituents of a finite element meth...</td>\n",
       "      <td>\\n\\n Additional context 0: The main constituen...</td>\n",
       "      <td>65</td>\n",
       "      <td>What is the difference between the open interv...</td>\n",
       "      <td>The open interval $]0, 1[$ consists of all poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>\\begin{enumerate} \\item The boundary condition...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>75</td>\n",
       "      <td>Explain the concept of trial solutions in the ...</td>\n",
       "      <td>In the context of essential boundary condition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>\\begin{enumerate} \\item The boundary condition...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>70</td>\n",
       "      <td>What are the mathematical features captured by...</td>\n",
       "      <td>The mathematical features captured by the nota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>\\begin{enumerate} \\item The boundary condition...</td>\n",
       "      <td>\\n\\n Additional context 0: A boundary-value pr...</td>\n",
       "      <td>65</td>\n",
       "      <td>How does the integration of boundary condition...</td>\n",
       "      <td>Answer: \"NOT ENOUGH INFO.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>\\begin{enumerate} \\item The boundary condition...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>70</td>\n",
       "      <td>Discuss the role of the constants $c_1$ and $c...</td>\n",
       "      <td>The constants $c_1$ and $c_2$ in the bilineari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>\\begin{enumerate} \\item The boundary condition...</td>\n",
       "      <td>\\n\\n Additional context 0: \\begin{enumerate} \\...</td>\n",
       "      <td>65</td>\n",
       "      <td>What challenges might arise when applying fini...</td>\n",
       "      <td>The challenges that might arise when applying ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_chunk  \\\n",
       "0    The main constituents of a finite element meth...   \n",
       "1    The main constituents of a finite element meth...   \n",
       "2    The main constituents of a finite element meth...   \n",
       "3    The main constituents of a finite element meth...   \n",
       "4    The main constituents of a finite element meth...   \n",
       "..                                                 ...   \n",
       "112  \\begin{enumerate} \\item The boundary condition...   \n",
       "113  \\begin{enumerate} \\item The boundary condition...   \n",
       "114  \\begin{enumerate} \\item The boundary condition...   \n",
       "115  \\begin{enumerate} \\item The boundary condition...   \n",
       "116  \\begin{enumerate} \\item The boundary condition...   \n",
       "\n",
       "                                               context  coverage  \\\n",
       "0    \\n\\n Additional context 0: The main constituen...        90   \n",
       "1    \\n\\n Additional context 0: \\begin{enumerate} \\...        80   \n",
       "2    \\n\\n Additional context 0: A boundary-value pr...        75   \n",
       "3    \\n\\n Additional context 0: The main constituen...        70   \n",
       "4    \\n\\n Additional context 0: The main constituen...        65   \n",
       "..                                                 ...       ...   \n",
       "112  \\n\\n Additional context 0: \\begin{enumerate} \\...        75   \n",
       "113  \\n\\n Additional context 0: \\begin{enumerate} \\...        70   \n",
       "114  \\n\\n Additional context 0: A boundary-value pr...        65   \n",
       "115  \\n\\n Additional context 0: \\begin{enumerate} \\...        70   \n",
       "116  \\n\\n Additional context 0: \\begin{enumerate} \\...        65   \n",
       "\n",
       "                                              question  \\\n",
       "0    What are the main constituents of the finite e...   \n",
       "1    Explain the significance of the variational or...   \n",
       "2    Given the differential equation $u_{, xx} + f ...   \n",
       "3    Define the terms 'domain' and 'range' in the c...   \n",
       "4    What is the difference between the open interv...   \n",
       "..                                                 ...   \n",
       "112  Explain the concept of trial solutions in the ...   \n",
       "113  What are the mathematical features captured by...   \n",
       "114  How does the integration of boundary condition...   \n",
       "115  Discuss the role of the constants $c_1$ and $c...   \n",
       "116  What challenges might arise when applying fini...   \n",
       "\n",
       "                                                answer  \n",
       "0    The main constituents of the finite element me...  \n",
       "1    The variational or weak statement of a boundar...  \n",
       "2    The function $f$ being described as smooth imp...  \n",
       "3    In the context of the function $f: [0,1] \\to \\...  \n",
       "4    The open interval $]0, 1[$ consists of all poi...  \n",
       "..                                                 ...  \n",
       "112  In the context of essential boundary condition...  \n",
       "113  The mathematical features captured by the nota...  \n",
       "114                         Answer: \"NOT ENOUGH INFO.\"  \n",
       "115  The constants $c_1$ and $c_2$ in the bilineari...  \n",
       "116  The challenges that might arise when applying ...  \n",
       "\n",
       "[117 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I think it's better to work with JSON/DataFrame in the code, but for reviewing QAs, CSV is easier to work with\n",
    "\"\"\"\n",
    "\n",
    "csv_file_name = f\"{main_dir}/hughes_ch{chapter}_QAs_n{max_questions}.csv\"   \n",
    "# ----------------------------------\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in questions_answers['data']:\n",
    "    question_chunk = item['chunk']\n",
    "    for sub_item in item['questions']:\n",
    "        new_item = {}\n",
    "        new_item['question_chunk'] = question_chunk\n",
    "        for k,v in sub_item.items():\n",
    "            if k == 'embedding':\n",
    "                continue\n",
    "            new_item[k] = v\n",
    "        data.append(new_item)\n",
    "\n",
    "# data[0]\n",
    "df = pd.DataFrame(data)[['question_chunk','context','coverage','question','answer']]\n",
    "df.to_csv(csv_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=160):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "Explain the concept of continuity, differentiability, and smoothness in relation to finite element methods, and why these concepts are crucial for accurate\n",
      "solutions.\n",
      "A:\n",
      "Continuity, differentiability, and smoothness are fundamental concepts in the context of finite element methods (FEM) as they pertain to the properties of\n",
      "functions used in the formulation and solution of boundary-value problems.  1. **Continuity**: A function is said to be continuous if small changes in the input\n",
      "result in small changes in the output. In FEM, continuity is essential to ensure that the approximate solutions do not exhibit abrupt changes, which could lead\n",
      "to inaccuracies in the numerical solution.  2. **Differentiability**: A function is differentiable if it has a derivative at each point in its domain. This\n",
      "means that the function has a defined slope at every point, which is crucial for understanding how the function behaves and changes. In FEM, differentiability\n",
      "is important because many physical problems involve derivatives, such as stress and strain in materials.  3. **Smoothness**: A function is described as smooth\n",
      "if it is continuously differentiable to a certain degree. For instance, a function that is $C^k$ is $k$-times continuously differentiable. Smoothness is\n",
      "particularly important in FEM because it ensures that the functions used to approximate solutions do not have discontinuities or sharp corners, which can\n",
      "complicate the analysis and lead to numerical errors.  These concepts are crucial for accurate solutions in FEM for several reasons: - **Avoiding Technical\n",
      "Difficulties**: Smooth functions help avoid issues related to discontinuities that can complicate the mathematical analysis and numerical implementation. -\n",
      "**Ensuring Convergence**: The properties of continuity and differentiability are often required for the convergence of numerical methods. If the functions used\n",
      "in the finite element formulation are not smooth, the numerical solution may not converge to the true solution. - **Physical Relevance**: Many physical\n",
      "phenomena modeled by FEM, such as the behavior of materials under load, inherently assume smoothness in the displacement fields. Discontinuities in these fields\n",
      "can lead to unrealistic predictions.  In summary, continuity, differentiability, and smoothness are essential for ensuring that the finite element methods yield\n",
      "accurate and reliable solutions to boundary-value problems, as they directly influence the mathematical properties of the functions involved in the analysis.\n",
      "\n",
      "Chunk used for Q generation:\n",
      "The main constituents of a finite element method for the solution of a boundary-value problem are\\\\ i. The variational or weak statement of the problem; and\\\\\n",
      "ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple\n",
      "example.\\\\ Suppose we want to solve the following differential equation for $u$ : \\begin{equation*} u_{, x x}+f=0 \\tag{1.1.1} \\end{equation*} where a comma\n",
      "stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We\n",
      "write \\begin{equation*} f: [0,1] \\to \\mathbb{R} \\tag{1.1.2} \\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0\n",
      "\\leq x \\leq 1$ ) and $\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will\n",
      "use the notation $\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\in[0,1], f(x) \\in \\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and\n",
      "$\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we\n",
      "sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we\n",
      "do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to\n",
      "consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element\n",
      "work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and\n",
      "physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation\n",
      "(1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical\n",
      "parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to\n",
      "simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end\n",
      "points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\textbf{\\textit{open and closed unit intervals,}} respectively. To\n",
      "simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\[ \\boldsymbol{\\Omega}=]\n",
      "0,1[ \\quad \\text { (open) } \\tag{1.1.3} \\] \\[ \\overline{\\boldsymbol{\\Omega}}=[0,1] \\quad \\text { (closed) } \\tag{1.1.4} \\] See Fig. 1.1.1. \\begin{figure}[h]\n",
      "\\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\vspace{0.5em} \\textbf{Figure 1.1.1} \\end{figure} At this point,\n",
      "considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is\n",
      "necessary for good finite element work.\n",
      "\n",
      "Retrieved context:\n",
      "\n",
      "\n",
      " 0: The main constituents of a finite element method for the solution of a boundary-value problem are\\\\ i. The variational or weak statement of the problem;\n",
      "and\\\\ ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple\n",
      "example.\\\\ Suppose we want to solve the following differential equation for $u$ : \\begin{equation*} u_{, x x}+f=0 \\tag{1.1.1} \\end{equation*} where a comma\n",
      "stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We\n",
      "write \\begin{equation*} f: [0,1] \\to \\mathbb{R} \\tag{1.1.2} \\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0\n",
      "\\leq x \\leq 1$ ) and $\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will\n",
      "use the notation $\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\in[0,1], f(x) \\in \\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and\n",
      "$\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we\n",
      "sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we\n",
      "do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to\n",
      "consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element\n",
      "work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and\n",
      "physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation\n",
      "(1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical\n",
      "parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to\n",
      "simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end\n",
      "points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\textbf{\\textit{open and closed unit intervals,}} respectively. To\n",
      "simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\[ \\boldsymbol{\\Omega}=]\n",
      "0,1[ \\quad \\text { (open) } \\tag{1.1.3} \\] \\[ \\overline{\\boldsymbol{\\Omega}}=[0,1] \\quad \\text { (closed) } \\tag{1.1.4} \\] See Fig. 1.1.1. \\begin{figure}[h]\n",
      "\\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\vspace{0.5em} \\textbf{Figure 1.1.1} \\end{figure} At this point,\n",
      "considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is\n",
      "necessary for good finite element work.\n",
      "\n",
      " 1: \\subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies\n",
      "and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was,\n",
      "admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is\n",
      "an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of\n",
      "serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round\n",
      "out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published\n",
      "papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a\n",
      "grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \\Omega \\rightarrow\n",
      "\\mathbb{R}$ (recall $\\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\\Omega)$, if its derivatives of order $j$,\n",
      "where $0 \\leq j \\leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\\infty}$. function is one that possesses\n",
      "a continuous derivative of any order (i.e., $j=0,1, \\ldots, \\infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and\n",
      "bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \\in \\Omega$ ). \\subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x,\n",
      "x^{2}$, etc.) are $C_{b}^{\\infty}$. \\subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\\Omega$, as are all its derivatives; hence it is\n",
      "$C^{\\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \\in \\Omega$; see Fig. 1.I.1). Consequently this\n",
      "function is not of class $C_{b}^{k}$ for any $k \\geq 0$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A\n",
      "continuous function that is not bounded. \\subsection*{Example 3} The function \\[ f(x)= \\begin{cases}x, & x \\leq \\frac{1}{2} \\tag{1.I.1}\\\\ 1 / 2, &\n",
      "x>\\frac{1}{2}\\end{cases} \\] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \\geq\n",
      "1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \\[ f(x)=\\left\\{\\begin{array}{cc} \\frac{x^{2}}{2}, & x \\leq\n",
      "\\frac{1}{2} \\tag{1.I.2}\\\\ \\frac{\\left(x-\\frac{1}{4}\\right)}{2}, & x>\\frac{1}{2} \\end{array}\\right. \\] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may\n",
      "wish to verify this.) There is no universally accepted definition of what is meant by a \"smooth\" function. However, it is generally taken to mean that at least\n",
      "one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\\infty$. The $C^{k}$ and $C_{b}^{k}$ functions\n",
      "employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\\overline{\\boldsymbol{\\Omega}}=[0,1]$, instead of\n",
      "$\\boldsymbol{\\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real\n",
      "numbers and are not allowed to be $\\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light.\n",
      "However, in some situations the differences between $C^{k}(\\Omega)$ and $C_{b}^{k}(\\Omega)$ must be kept in mind. Generally, finite element functions are smooth\n",
      "on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them\n",
      "as locally smooth but globally \"rough.\" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics\n",
      "employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a \"generalized derivative,\" as was used in\n",
      "solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step\n",
      "function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the\n",
      "Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized\n",
      "functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by\n",
      "what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and\n",
      "consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability\n",
      "properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of\n",
      "functions defined as follows: \\begin{equation*} H^{k}=H^{k}(\\Omega)=\\left\\{w \\mid w \\in L_{2} ; w_{, x} \\in L_{2} ; \\ldots ; \\underbrace{w_{x \\ldots\n",
      "x}}_{\\text{k times}} \\in L_{2}\\right\\} \\tag{1.I.3} \\end{equation*} where \\begin{equation*} L_{2}=L_{2}(\\Omega)=\\left\\{w \\mid \\int_{0}^{1} w^{2} d\n",
      "x<\\infty\\right\\} \\tag{1.I.4} \\end{equation*} In words, the Sobolev space of degree $k$, denoted by $\\boldsymbol{H}^{\\boldsymbol{k}}$, consists of functions that\n",
      "possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From\n",
      "(1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \\subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems.\n",
      "The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In\n",
      "particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension,\n",
      "$H^{k+1} \\subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required\n",
      "$H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are\n",
      "$C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain \"singularities\" are precluded by square-integrability. For\n",
      "example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture\n",
      "mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious\n",
      "training in \"functional analysis.\" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and\n",
      "references therein.\n",
      "\n",
      " 2: \\begin{enumerate} \\item From the corollary we see that the derivatives are second-order accurate at the midpoints. \\end{enumerate} \\footnotetext{${ }^{3}\n",
      "\\mathrm{~A}$ function $f(x)$ is said to be $O\\left(x^{k}\\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \\rightarrow$ a constant as $x \\rightarrow 0$. For\n",
      "example, $f(x)=x^{k}$ is $O\\left(x^{k}\\right)$, as is $f(x)=\\sum_{j=k}^{k+1} x^{j}, l \\geq 0$. But neither is $O\\left(x^{k+1}\\right)$. (Verify.) } 2. If the\n",
      "exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is\n",
      "exact at the midpoints. This is the case when $f(x)=p=$ constant.\\\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The\n",
      "midpoints of linear \"elements\" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within\n",
      "elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \\ldots, n$ ). Consider the standard finite difference \"stencil\" for $u_{, x\n",
      "x}+\\phi=0$ at a typical internal node, namely, \\begin{equation*} \\frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \\tag{1.10.18} \\end{equation*} Assuming \\& varies\n",
      "in piecewise linear fashion and so can be expanded as \\begin{equation*} f=\\sum_{A=1}^{n+1} f_{A} N_{A} \\tag{1.10.19} \\end{equation*} where the $f_{A}$ 's are\n",
      "the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be\n",
      "capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 ,\n",
      "accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested\n",
      "reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\\\ i.\n",
      "It is exact at the nodes.\\\\ ii. There exists at least one point in each element at which the derivative is exact.\\\\ iii. The derivative is second-order accurate\n",
      "at the midpoints of the elements.\n",
      "\n",
      " 3: The examples of the preceding section employed definitions of $\\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear\n",
      "finite element space. To define the general case in which $\\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping\n",
      "subintervals. The typical subinterval is denoted by $\\left[x_{A}, x_{A+1}\\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \\ldots, n$. We also require $x_{1}=0$ and\n",
      "$x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes\n",
      "referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The\n",
      "mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\\max h_{A}, A=1,2, \\ldots, n$ ). The smaller $h$, the more\n",
      "\"refined\" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical\n",
      "internal node (i.e., $2 \\leq A \\leq n$ ) \\[ N_{A}(x)=\\left\\{\\begin{array}{cl} \\frac{\\left(x-x_{A-1}\\right)}{h_{A-1}}, & x_{A-1} \\leq x \\leq x_{A} \\tag{1.8.1}\\\\\n",
      "\\frac{\\left(x_{A+1}-x\\right)}{h_{A}}, & x_{A} \\leq x \\leq x_{A+1} \\\\ 0, & \\text { elsewhere } \\end{array}\\right. \\] whereas for the boundary nodes we have\n",
      "\\begin{align*} & N_{1}(x)=\\frac{x_{2}-x}{h_{1}}, \\quad x_{1} \\leq x \\leq x_{2} \\tag{1.8.2}\\\\ & N_{n+1}(x)=\\frac{x-x_{n}}{h_{n}}, \\quad x_{n} \\leq x \\leq x_{n+1}\n",
      "\\tag{1.8.3} \\end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as \"hat,\" \"chapeau,\" and \"roof\"\n",
      "functions. Note that $N_{A}\\left(x_{B}\\right)=\\delta_{A B}$, where\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure\n",
      "1.8.1 Basis functions for the piecewise linear finite element space.\\\\ $\\delta_{A B}$ is the Kronecker delta (i.e., $\\delta_{A B}=1$ if $A=B$, whereas\n",
      "$\\delta_{A B}=0$ if $A \\neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the\n",
      "subintervals that contain $x_{A}$. A typical member $w^{h} \\in \\mathcal{U}^{h}$ has the form $\\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note\n",
      "that $w^{\\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of\n",
      "$w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.)\n",
      "Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly,\n",
      "$w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\\boldsymbol{n}$.\\\\ \\includegraphics[max width=\\textwidth,\n",
      "center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\\boldsymbol{h}} \\in \\mathcal{U}^{\\boldsymbol{k}}$.\\\\ Typical members of\n",
      "$f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element\n",
      "functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-\n",
      "dimensional model problem: \\begin{equation*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.8.4} \\end{equation*} where $w \\in \\mathcal{U}$\n",
      "and $u \\in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \\ldots, n)$, but may suffer slope discontinuities across\n",
      "element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of\n",
      "the functions, show that: \\begin{align*} 0= & \\sum_{A=1}^{n} \\int_{x_{A}}^{x_{A}+1} w\\left(u_{, x x}+f\\right) d x+w(0)\\left[u_{, x}\\left(0^{+}\\right)+h\\right]\n",
      "\\\\ & +\\sum_{A=2}^{n} w\\left(x_{A}\\right)\\left[u_{, x}\\left(x_{A}^{+}\\right)-u_{, x}\\left(x_{A}^{-}\\right)\\right] \\tag{1.8.5} \\end{align*} Arguing as in Sec.\n",
      "1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\\\ i. $u_{, x x}(x)+f(x)=0$, where $\\left.x \\in\\right] x_{A}, x_{A+1}[$ and $A=1,2,\n",
      "\\ldots, n$,\\\\ ii. $-u_{, x}\\left(0^{+}\\right)=h$; and\\\\ iii. $u_{, x}\\left(x_{A}^{-}\\right)=u_{, x}\\left(x_{A}^{+}\\right)$, where $A=2,3, \\ldots, n$. Observe\n",
      "that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted\n",
      "with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over\n",
      "element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of\n",
      "(i)-(iii) is obtained.\n",
      "\n",
      " 4: This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-\n",
      "Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval\n",
      "(i.e., $\\Omega=] 0,1[, \\bar{\\Omega}=[0,1]$ ).\\\\ \\[ \\text{(S)} \\quad \\left\\{ \\begin{minipage}{0.8\\textwidth} \\text {Given } $f: \\Omega \\rightarrow \\mathbb{R}$\n",
      "\\text { and constants } $M$ \\text { and } $f$, \\text { find } $u: \\bar{\\Omega} \\rightarrow \\mathbb{R}$ \\text { such that:} \\begin{align*} E I u_{,xxxx} = f\n",
      "\\quad \\text {on } \\Omega \\quad \\text {(transverse equilibrium)}\\\\ u(1) &= 0 & \\text{(zero transverse displacement)} \\\\ u_{x}(1) &= 0 & \\text{(zero slope)} \\\\ E\n",
      "I u_{,xx}(0) &= M & \\text{(prescribed moment)} \\\\ E I u_{,xxx}(0) &= Q & \\text{(prescribed shear)} \\end{align*} \\end{minipage} \\right. \\] where $E$ is Young's\n",
      "modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\\\ The setup is shown in Fig. 1.16.1.\\\\ \\includegraphics[max\n",
      "width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\\\ Let $\\mathfrak{f}=\\mathcal{U}=\\left\\{w \\mid w \\in H^{2}(\\Omega),\n",
      "w(1)=w_{x}(1)=0\\right\\}^{7}$. Then a corresponding weak form of the problem is:\\\\ \\[ \\text{(W)} \\quad \\left\\{ \\begin{minipage}{0.8\\textwidth} \\text{Given} $f,\n",
      "M$, \\text{, and} $Q$, find $u \\in \\mathfrak{f}$ \\text{such that for all} $w \\in \\mathcal{U}$\\\\ \\begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \\end{align*}\n",
      "\\end{minipage} \\right. \\] where $$ \\begin{aligned} a(w, u) & =\\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\\\ (w, f) & =\\int_{0}^{1} w f d x \\end{aligned} $$\n",
      "\\footnotetext{${ }^{7} w \\in H^{2}(\\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\\left.f_{0}^{1}\\left(w_{, x x}\\right)^{2} d\n",
      "x<\\infty\\right)$. }The collection of functions, $\\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the\n",
      "kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \\in \\mathcal{U}$ is continuously differentiable. For\n",
      "reasonable l, these problems possess unique solutions. Let $\\mathfrak{d}^{h}=\\mathcal{U}^{h}$ be a finite-dimensional approximation of $\\mathfrak{f}$. In\n",
      "particular, we assume $w^{h} \\in \\mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\\\ (G)\n",
      "$\\left\\{\\begin{array}{c}\\text { Given } f, M, \\text { and } Q, \\text { find } u^{h} \\in \\delta^{h} \\text { such that for all } w^{h} \\in \\mathcal{U}^{h} \\\\\n",
      "a\\left(w^{h}, u^{h}\\right)=\\left(w^{h}, f\\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\\end{array}\\right.$\\\\ a. Assuming all functions are smooth and bounded, show that\n",
      "the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\\\ b. Assume $0=x_{1}<x_{2}<\\cdots<x_{n+1}=1$ and\n",
      "$\\mathcal{U}^{h}=\\left\\{w^{h} \\mid w^{h} \\in C^{1}(\\bar{\\Omega})\\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\\left[x_{A}, x_{A+1}\\right]$\n",
      "is a cubic polynomial (i.e., consists of a linear combination of $\\left.\\left.1, x, x^{2}, x^{3}\\right)\\right\\}^{8}$. This is a space of piecewise cubic Hermite\n",
      "shape functions. Observe that $w^{h} \\in \\mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$\n",
      "and $x_{2}$ in place of $x_{\\mathrm{A}}$ and $x_{\\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x)\n",
      "w^{h}\\left(x_{1}\\right)+N_{3}(x) w^{h}\\left(x_{2}\\right)+N_{2}(x) w_{. x}^{h}\\left(x_{1}\\right)+N_{4}(x) w_{, x}^{h}\\left(x_{2}\\right) $$ where $$\n",
      "\\begin{aligned} & N_{1}(x)=\\frac{-\\left(x-x_{2}\\right)^{2}\\left[-h+2\\left(x_{1}-x\\right)\\right]}{h^{3}} \\\\ &\n",
      "N_{2}(x)=\\frac{\\left(x-x_{1}\\right)^{\\prime}\\left(x-x_{2}\\right)^{2}}{h^{2}} \\\\ &\n",
      "N_{3}(x)=\\frac{\\left(x-x_{1}\\right)^{2}\\left[h+2\\left(x_{2}-x\\right)\\right]}{h^{3}} \\\\ & N_{4}(x)=\\frac{\\left(x-x_{1}\\right)^{2}\\left(x-x_{2}\\right)}{h^{2}}\n",
      "\\end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\\prime}$ s are constants. Determine them by requiring the following\n",
      "four conditions hold: $$ \\begin{aligned} w^{h}\\left(x_{1}\\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\\\ w^{h}\\left(x_{2}\\right) & =c_{1}+c_{2}\n",
      "x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\\\ w_{,x}^{h}\\left(x_{1}\\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\\\ w_{,x}^{h}\\left(x_{2}\\right) & =c_{2}+2 c_{3}\n",
      "x_{2}+3 c_{4} x_{2}^{2} \\end{aligned} $$ \\footnote{${ }^{8}$ The notation $w^{k} \\in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element\n",
      "functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal\n",
      "displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam\n",
      "bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\\\ c. Locate the optimal curvature points in the sense of\n",
      "Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\\xi$-element coordinate system\n",
      "introduced in Sec. 1.12 (recall $\\left.\\xi=\\left(2 x-x_{A}-x_{A+1}\\right) / h_{A}\\right)$, the location of the Barlow curvature points may be expressed as $\\xi=\n",
      "\\pm 1 / \\sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\\\ d. What is the rate of convergence of curvature at\n",
      "these points? (Ans. $O\\left(h^{3}\\right)$ ).\\\\ e. If the segment of the beam $\\left[x_{A}, x_{A+1}\\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the\n",
      "exact solution), which points are optimal?\\\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element\n",
      "equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\\\ g.\n",
      "Prove that $$ \\begin{gathered} u^{h}\\left(x_{A}\\right)=u\\left(x_{A}\\right) \\\\ u_{, x}^{h}\\left(x_{A}\\right)=u_{, x}\\left(x_{A}\\right) \\end{gathered} $$ where\n",
      "$x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole,\n",
      "$\\delta_{x}\\left(x-x_{A}\\right)$, which is the generalized derivative of the delta function.\\\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$\n",
      "constant.\\\\ i. Why do we require that the functions in $\\mathcal{U}^{h}$ have continuous first derivatives?\\\\ j. Calculate the $4 \\times 4$ element stiffness\n",
      "matrix, $$ k_{p q}^{e}=\\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \\quad 1 \\leq p, q \\leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\\mathrm{e}}$.\\\\ k.\n",
      "(See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \\in \\mathcal{U}$ and $u \\in \\mathfrak{f}$ are smooth on element interiors (i.e., on $]\n",
      "x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-\n",
      "cubic Hermite functions.) Show that $$ \\begin{aligned} 0= & \\sum_{A=1}^{n} \\int_{x_{A}}^{x_{A}+1} w\\left(E I u_{, x x x x}-f\\right) d x \\\\ & -w_{, x}(0)\\left(E\n",
      "I u_{, x x}\\left(0^{+}\\right)-M\\right) \\\\ & +w(0)\\left(E I u_{, x x x}\\left(0^{+}\\right)-Q\\right) \\\\ & -\\sum_{A=2}^{n} w_{, x}\\left(x_{A}\\right) E I\\left(u_{, x\n",
      "x}\\left(x_{A}^{+}\\right)-u_{, x x}\\left(x_{A}^{-}\\right)\\right) \\\\ & +\\sum_{A=2}^{n} w\\left(x_{A}\\right) E I\\left(u_{, x x x}\\left(x_{A}^{+}\\right)-u_{, x x\n",
      "x}\\left(x_{A}^{-}\\right)\\right) \\end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\\\ i. EI $u_{, x x x x}(x)=f(x)$, where\n",
      "$\\left.x \\in\\right] x_{A}, x_{A+1}[$ and $A=1,2, \\ldots, n$\\\\ ii. $E I u_{, x x}\\left(0^{+}\\right)=M$\\\\ iii. EI $u_{, x x x}\\left(0^{+}\\right)=Q$\\\\ iv. $E I\n",
      "u_{, x x}\\left(x_{A}^{+}\\right)=E I u_{, x x}\\left(x_{A}^{-}\\right)$, where $A=2,3, \\ldots, n$\\\\ v. $E I u_{, x x x}\\left(x_{A}^{+}\\right)=E I u_{, x x\n",
      "x}\\left(x_{A}^{-}\\right)$, where $A=2,3, \\ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are\n",
      "continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which\n",
      "are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v).\n",
      "\n",
      " 5: \\begin{enumerate} \\item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see\n",
      "that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as\n",
      "\\textbf{\\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary\n",
      "conditions of this type are called \\textbf{\\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary\n",
      "conditions is extremely important in more complicated situations which we will consider later on. \\item The method used to prove part (b) of the proposition\n",
      "goes under the name of the \\textbf{\\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables\n",
      "us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional\n",
      "problems, it is essential to have a thorough understanding of these procedures. \\end{enumerate} Now we see that to obtain approximate solutions to the original\n",
      "boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the\n",
      "latter. Roughly speaking, the basic idea is to approximate $f$ and $\\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and\n",
      "$\\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go\n",
      "about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \\begin{align*} a(w, u)\n",
      "& =\\int_{0}^{1} w_{, x} u_{, x} d x \\tag{1.4.8}\\\\ (w, f) & =\\int_{0}^{1} w f d x \\tag{1.4.9} \\end{align*} In terms of (1.4.8) and (1.4.9), the variational\n",
      "equation takes the form \\begin{equation*} a(w, u)=(w, f)+w(0) h \\tag{1.4.10} \\end{equation*} Here, $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ are examples of\n",
      "\\textbf{\\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the\n",
      "symmetry property is \\begin{align*} a(u, v) & =a(v, u) \\tag{1.4.11}\\\\ (u, v) & =(v, u) \\tag{1.4.12} \\end{align*} Bilinearity means linearity in each \"slot\"; for\n",
      "example, \\begin{align*} a\\left(c_{1} u+c_{2} v, w\\right) & =c_{1} a(u, w)+c_{2} a(v, w) \\tag{1.4.13}\\\\ \\left(c_{1} u+c_{2} v, w\\right) & =c_{1}(u, w)+c_{2}(v,\n",
      "w) \\tag{1.4.14} \\end{align*} \\\\ Exercise 1. Use the definitions of $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot)$ to verify the properties of symmetry and\n",
      "bilinearity.\\\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical\n",
      "understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus\n",
      "ideas developed and results obtained are seen at once to have very broad applicability.\n",
      "\n",
      " 6: We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject\n",
      "is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for\n",
      "the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step\n",
      "in developing the method is to construct finite-dimensional approximations of $f$ and $\\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and\n",
      "$\\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\\mathcal{U}^{h}$ with a \\textit{\\textbf{mesh}}, or\n",
      "\\textit{\\textbf{discretization}}, of the domain $\\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and\n",
      "$\\mathcal{U}^{h}$ as being subsets of $f$ and $\\mathcal{U}$, respectively. This is written as \\begin{align*} f^{h} \\subset f & \\text { (i.e., if } \\left.u^{h}\n",
      "\\in f^{h}, \\text { then } u^{h} \\in f\\right) \\tag{1.5.1}\\\\ \\mathcal{U}^{h} \\subset \\mathcal{U} & \\text { (i.e., if } w^{h} \\in \\mathcal{U}^{h}, \\text { then }\n",
      "w^{h} \\in\\mathcal{U}) \\tag{1.5.2} \\end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are\n",
      "(respectively) that if $u^{h} \\in f^{h}$ and $w^{h} \\in \\mathcal{U}^{h}$, then \\begin{align*} & u^{h}(1)=q \\tag{1.5.3}\\\\ & w^{h}(1)=0 \\tag{1.5.4} \\end{align*}\n",
      "The collections, $f, \\mathcal{U}, d^{h}$, and $\\mathcal{U}^{h}$, are often referred to as \\textit{\\textbf{function space}}. The terminology space in mathematics\n",
      "usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\\mathcal{U}$, then $c_{1}\n",
      "v+c_{2} w$ is also in $\\mathcal{U}$. Both $\\mathcal{U}$ and $\\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is\n",
      "clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2}\n",
      "\\notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to\n",
      "$f$ and $f^{h}$.\n",
      "\n",
      " 7: So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in\n",
      "question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and\n",
      "\"local support\" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis\n",
      "functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical\n",
      "properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view\n",
      "called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite\n",
      "element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall\n",
      "attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following\n",
      "quantities. Linear finfte element (global description) \\begin{center} \\begin{tabular}{lll} $(g 1)$ & Domain: & $\\left[x_{A}, x_{A+1}\\right]$ \\\\ $(g 2)$ & Nodes:\n",
      "& $\\left\\{x_{A}, x_{A+1}\\right\\}$ \\\\ $(g 3)$ & Degrees of freedom: & $\\left\\{d_{A}, d_{A+1}\\right\\}$ \\\\ $(g 4)$ & Shape functions: & $\\left\\{N_{A},\n",
      "N_{A+1}\\right\\}$ \\\\ $(g 5)$ & Interpolation function: & \\\\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \\in\\left[x_{A}, x_{A+1}\\right]$ & \\\\ \\end{tabular}\n",
      "\\end{center} (Recall $d_{A}=u^{h}\\left(x_{A}\\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally\n",
      "defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape\n",
      "functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a\n",
      "typical element may be standardized. These are given as follows: Linear finite element (local description)\\\\ (l1) Domain: $\\left[\\xi_{1}, \\xi_{2}\\right]$\n",
      "\\footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin\n",
      "methods), we would also have to specify a set of weighting functions, say\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37}\n",
      "} (l2) Nodes: $\\left\\{\\xi_{1}, \\xi_{2}\\right\\}$\\\\ (l3) Degrees of freedom: $\\left\\{d_{1}, d_{2}\\right\\}$\\\\ (I4) Shape functions: $\\left\\{N_{1}, N_{2}\\right\\}$\\\\\n",
      "(15) Interpolation function: $$ u^{h}(\\xi)=N_{1}(\\xi) d_{1}+N_{2}(\\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\\\ We\n",
      "shall relate the domains of the global and local descriptions by an \"affine\" transformation $\\xi:\\left[x_{A}, x_{A+1}\\right] \\rightarrow\\left[\\xi_{1},\n",
      "\\xi_{2}\\right]$, such that $\\xi\\left(x_{A}\\right)=\\xi_{1}$ and $\\xi\\left(x_{A+1}\\right)=\\xi_{2}$. It is standard practice to take $\\xi_{1}=-1$ and $\\xi_{2}=+1$.\n",
      "Thus $\\xi$ may be represented by the expression \\begin{equation*} \\xi(x)=c_{1}+c_{2} x \\tag{1.12.1} \\end{equation*} where $c_{1}$ and $c_{2}$ are constants\n",
      "which are determined by \\[ \\left.\\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \\tag{1.12.2}\\\\ 1 & =c_{1}+x_{A+1} c_{2} \\end{array}\\right\\} \\] Solving this system\n",
      "yields \\begin{equation*} \\xi(x)=\\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \\tag{1.12.3} \\end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\\xi$ is obtained by\n",
      "solving for $x$ : \\begin{equation*} x(\\xi)=\\frac{h_{A} \\xi+x_{A}+x_{A+1}}{2} \\tag{1.12.4} \\end{equation*} In (1.12.1), $\\xi$ is a mapping and $x$ is a point,\n",
      "whereas in (1.12.4), $x$ is a mapping and $\\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \\ldots$ pertain to the\n",
      "local numbering system. The subscripts $A, B, C, \\ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will\n",
      "frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion\n",
      "as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity\n",
      "in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\\xi)=N_{A}\\left(x^{e}(\\xi)\\right)$, where $x^{e}:\\left[\\xi_{1},\n",
      "\\xi_{2}\\right] \\rightarrow$ $\\left[x_{1}^{e}, x_{2}^{e}\\right]=\\left[x_{A}, x_{A+1}\\right]$, etc.). In terms of $\\xi$, the shape functions in the local\n",
      "description take on a standard form \\begin{equation*} N_{a}(\\xi)=\\frac{1}{2}\\left(1+\\xi_{a} \\xi\\right), \\quad a=1,2 \\tag{1.12.5} \\end{equation*} Note also that\n",
      "(1.12.4) may be written in terms of (1.12.5): \\begin{equation*} x^{e}(\\xi)=\\sum_{a=1}^{2} N_{a}(\\xi) x_{a}^{e} \\tag{1.12.6} \\end{equation*} This has the same\n",
      "form as the interpolation function (cf. 15).\\\\ For future reference, we note the following results: \\begin{gather*} N_{a,\n",
      "\\xi}=\\frac{\\xi_{a}}{2}=\\frac{(-1)^{a}}{2} \\tag{1.12.7}\\\\ x_{, \\xi}^{e}=\\frac{h^{e}}{2} \\tag{1.12.8} \\end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and\n",
      "\\begin{equation*} \\xi_{, x}^{e}=\\left(x_{, \\xi}^{e}\\right)^{-1}=\\frac{2}{h^{e}} \\tag{1.12.9} \\end{equation*} The local and global descriptions of the eth\n",
      "element are depicted in Fig. 1.12.1.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global\n",
      "descriptions of the $e$ th element.\n",
      "\n",
      " 8: Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \\begin{equation*} \\frac{\\partial}{\\partial \\xi} f(x(\\xi))=f_{, x}(x(\\xi))\n",
      "x_{, \\xi}(\\xi) \\tag{1.15.2} \\end{equation*} Proofs of these results may be found in [11].\\\\ The computation of $k^{e}$ proceeds as follows: $$ \\begin{aligned}\n",
      "k_{a b}^{e} & =\\int_{\\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \\quad \\text { (by definition) } \\\\ & =\\int_{-1}^{+1} N_{a, x}(x(\\xi)) N_{b, x}(x(\\xi)) x_{,\n",
      "\\xi}(\\xi) d \\xi \\end{aligned} $$ (Change of variables, where $x(\\xi)$ is defined by (1.12.6)) $$ =\\int_{-1}^{+1} N_{a, \\xi}(\\xi) N_{b, \\xi}(\\xi)\\left(x_{,\n",
      "\\xi}(\\xi)\\right)^{-1} d \\xi $$ (Chain rule; $\\left.N_{a, \\xi}(\\xi)=(\\partial / \\partial \\xi) N_{a}(x(\\xi))=N_{a, x}(x(\\xi)) x_{, \\xi}(\\xi)\\right)$ $$\n",
      "=(-1)^{a+b} / h^{e} \\quad(\\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \\[ k^{e}=\\frac{1}{h^{e}}\\left[\\begin{array}{rr} 1 & -1 \\tag{1.15.3}\\\\ -1 & 1\n",
      "\\end{array}\\right] \\] Observe that $N_{a, \\xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\\xi)$. We shall see that this\n",
      "is generally true, and hence these computations may be done once and for all. The derivatives $\\boldsymbol{x}_{, \\xi}$ and $\\boldsymbol{\\xi}_{, x}$ do depend on\n",
      "the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases.\n",
      "Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to\n",
      "reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might\n",
      "replace $f$ by its linear interpolate over each element, namely, \\begin{equation*} f^{h}=\\sum_{a=1}^{2} f_{a} N_{a} \\tag{1.15.4} \\end{equation*} where\n",
      "$f_{a}=f\\left(x\\left(\\xi_{a}\\right)\\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This\n",
      "represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear \"loading\" of the element.) Now\n",
      "standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the\n",
      "explicit calculation of an element force vector: \\begin{align*} \\int_{\\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\\int_{-1}^{+1} N_{a}(x(\\xi)) f^{h}(x(\\xi)) x_{,\n",
      "\\xi}(\\xi) d \\xi \\quad \\text { (change of variables) } \\\\ & =\\frac{h^{e}}{2} \\sum_{b=1}^{2} \\int_{-1}^{+1} N_{a}(\\xi) N_{b}(\\xi) d \\xi f_{b} \\quad \\text { (by\n",
      "(1.12.8)) } \\tag{1.15.5} \\end{align*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \\end{center} Figure 1.15.1\n",
      "Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\\left(\\int_{-1}^{+1} N_{a} N_{b} d \\xi=\\left(1+\\delta_{a\n",
      "b}\\right) / 3\\right)$ yields \\[ \\begin{array}{rlr} \\mathfrak{f}^{e} & =\\frac{h^{e}}{6}\\left[\\begin{array}{cc} 2 & 1 \\\\ 1 & 2\n",
      "\\end{array}\\right]\\left\\{\\begin{array}{l} f_{1} \\\\ f_{2} \\end{array}\\right\\} & \\begin{array}{c} \\text { (+ boundary terms } \\\\ \\text { cf. (1.13.12)) }\n",
      "\\end{array} \\\\ & =\\frac{h^{e}}{6}\\left\\{\\begin{array}{l} 2 f_{1} + f_{2} \\\\ f_{1} + 2 f_{2} \\end{array}\\right\\} & \\text { (+ boundary terms) } \\tag{1.15.6}\n",
      "\\end{array} \\] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\\left(h^{2}\\right)$ errors in the data;\n",
      "in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces\n",
      "at worst $O\\left(h^{2}\\right)$ errors in $u^{\\boldsymbol{h}}$ and $u_{, x}^{\\boldsymbol{h}}$. The following exercise indicates that there may be better ways to\n",
      "approximate given data. \\subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ).\n",
      "Determine a piecewise linear approximation-not necessarily continuous-to $\\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The\n",
      "analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \\left.u_{,xx}-\\lambda u+f=0\n",
      "\\quad \\text { on } \\Omega=\\right] 0,1[ $$ where $\\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions\n",
      "as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \\int_{\\Omega}\\left(w_{\\cdot x} u_{, x}+w\n",
      "\\lambda u\\right) d x=\\int_{\\Omega} w f d x+w(0) h $$ where $u \\in \\mathfrak{f}, w \\in \\mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w,\n",
      "\\lambda u)=(w, f)+w(0) h $$ i. Let $\\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \\begin{array}{r} a\\left(w^{h},\n",
      "v^{h}\\right)+\\square= \\\\ \\left(w^{h}, f\\right)+w^{h}(0) h-a\\left(w^{h}, gS^{h}\\right) \\\\ -\\square \\end{array} $$ ii. Define $K_{A B}=a\\left(N_{A},\n",
      "N_{B}\\right)+\\square$\\\\ and $$ k_{a b}^{e}=a\\left(N_{a}, N_{b}\\right)^{e}+\\square $$ iii. Determine $\\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\\left[k_{a\n",
      "b}^{e}\\right]=[\\square] $$ iv. Show that $\\boldsymbol{K}$ is symmetric.\\\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary\n",
      "condition $w^{h}(1)=0$ ? Why?\\\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\\lambda g+\\delta_{y}=0 $$ and can be written as $$ g(x)=\n",
      "\\begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \\leq x \\leq y \\\\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \\leq x \\leq 1\\end{cases} $$ where $p=\\lambda^{1 / 2}$ and the\n",
      "$c$ 's are determined from the following four boundary and continuity conditions: $$ \\begin{aligned} g(1) & =0 \\\\ g_{, x}(0) & =0 \\\\ g\\left(y^{+}\\right) &\n",
      "=g\\left(y^{-}\\right) \\\\ g_{, x}\\left(y^{+}\\right) & =g_{. x}\\left(y^{-}\\right)-1 \\end{aligned} $$ Why is the piecewise linear finite element space incapable of\n",
      "attaining nodally exact solutions in this case?\\\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e}\n",
      "N_{1}(x)+d_{2}^{e} N_{2}(x), \\quad x \\in \\Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$\n",
      "d_{a}^{e}=u^{h}\\left(x_{a}^{e}\\right), \\quad a=1,2 $$ What is the attribute which this choice of functions attains?\n",
      "\n",
      " 9: \\subsection*{2.1 INTRODUCTORY REMARKS} It makes no sense to attempt to \"solve\" a boundary-value problem without a precise knowledge of what the problem is.\n",
      "The truth of this statement seems self-evident. Unfortunately, attempts are often made to solve vaguely defined problems, creating considerable confusion and,\n",
      "sometimes, totally erroneous results. In this chapter we present precise statements of multidimensional boundary-value problems in classical linear heat\n",
      "conduction and elastostatics. The presentation is similar in many respects to that for the one-dimensional model problem of Chapter 1. In particular, we discuss\n",
      "strong and weak forms, their equivalence, corresponding Galerkin formulations, the definitions of element arrays, and pertinent data processing concepts. In\n",
      "multidimensions, the data processing ideas necessarily become more involved. The reader is urged to study them carefully as they are necessary in order to\n",
      "understand the computer implementation of finite element techniques. \\subsection*{2.2 PRELIMINARIES} Let $n_{s d}(=2$ or 3 ) denote the number of space\n",
      "dimensions of the problem under consideration. Let $\\Omega \\subset \\mathbb{R}^{n_{sd}}$ be an open set\\footnote{For our purposes, it is sufficient to think of\n",
      "an open set as one without its boundary. }. We shall employ the following alternative representations for $\\boldsymbol{x}$ and $\\boldsymbol{n}$ : with piecewise\n",
      "smooth boundary $\\Gamma$. A general point in $\\mathbb{R}^{n_{sd}}$ is denoted by $\\boldsymbol{x}$. We will identify the point $\\boldsymbol{x}$ with its position\n",
      "vector emanating from the origin of ${R}^{n_{sd}}$. The unit outward normal vector to $\\Gamma$ is denoted by $\\boldsymbol{n}$. \\begin{align*} & \\left(n_{s\n",
      "d}=2\\right): \\quad x=\\left\\{x_{i}\\right\\}=\\left\\{\\begin{array}{l} x_{1} \\\\ x_{2} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} x \\\\ y \\end{array}\\right\\} \\quad\n",
      "n=\\left\\{n_{i}\\right\\}=\\left\\{\\begin{array}{l} n_{1} \\\\ n_{2} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} n_{x} \\\\ n_{y} \\end{array}\\right\\} \\tag{2.2.1}\\\\ &\n",
      "\\left(n_{s d}=3\\right): \\quad x=\\left\\{x_{i}\\right\\}=\\left\\{\\begin{array}{l} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} x \\\\ y \\\\ z\n",
      "\\end{array}\\right\\} \\quad n=\\left\\{n_{i}\\right\\}=\\left\\{\\begin{array}{l} n_{1} \\\\ n_{2} \\\\ n_{3} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} n_{x} \\\\ n_{y} \\\\\n",
      "n_{z} \\end{array}\\right\\} \\tag{2.2.2} \\end{align*} where $x_{i}$ and $n_{i}, 1 \\leq i \\leq n_{s d}$, are the Cartesian components of $\\boldsymbol{x}$ and\n",
      "$\\boldsymbol{n}$, respectively; see Figure 2.2.1. Unless otherwise specified we shall work in terms of Cartesian components of vectors and tensors.\\\\\n",
      "\\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-02} Figure 2.2.1\\\\ We assume that $\\Gamma$ admits the decomposition\n",
      "\\begin{equation*} \\Gamma=\\overline{\\Gamma_{g} \\cup \\Gamma_{h}} \\tag{2.2.3} \\end{equation*} where \\begin{equation*} \\Gamma_{g} \\cap \\Gamma_{h}=\\varnothing\n",
      "\\tag{2.2.4} \\end{equation*} and $\\Gamma_{g}$ and $\\Gamma_{h}$ are open sets in $\\Gamma$. The notations are defined as follows: $\\cup$ is the set union symbol.\n",
      "Thus $\\Gamma_{g} \\cup \\Gamma_{h}$ means the set of all points $x$ contained in either $\\Gamma_{g}$ or $\\Gamma_{h}$. Also, $\\cap$ is the set intersection symbol.\n",
      "Thus $\\Gamma_{g} \\cap \\Gamma_{h}$ means the set of all points contained in both $\\Gamma_{g}$ and $\\Gamma_{h}$. The empty set is denoted by $\\varnothing$. Thus\n",
      "(2.2.4) means that there is no point $x$ contained in both $\\Gamma_{g}$ and $\\Gamma_{h}$ (i.e., $\\Gamma_{g}$ and $\\Gamma_{h}$ do not intersect or overlap). A\n",
      "bar above a set means set closure, i.e., the union of the set with its boundary. Thus \\begin{equation*} \\bar{\\Omega}={\\Omega} \\cup \\Gamma \\tag{2.2.5}\n",
      "\\end{equation*} To understand the meaning of $\\overline{\\Gamma_{g} \\cup \\Gamma_{h}}$, we must define the boundaries of $\\Gamma_{g}$ and $\\Gamma_{h}$ in\n",
      "$\\Gamma$. We shall do this with the aid of an example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9  # try different QAs\n",
    "\n",
    "print('Q:')\n",
    "print_wrapped(df.iloc[i,:]['question'])\n",
    "print('A:')\n",
    "print_wrapped(df.iloc[i,:]['answer'])\n",
    "print('\\nChunk used for Q generation:')\n",
    "print_wrapped(df.iloc[i,:]['question_chunk'])\n",
    "print('\\nRetrieved context:')\n",
    "for item in df.iloc[i,:]['context'].split('Additional context'):\n",
    "    print_wrapped(item)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

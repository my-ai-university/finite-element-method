{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading functions from the scripts\n",
    "\"\"\"\n",
    "Mostafa:\n",
    "I used the new structured output for question generation.\n",
    "It's a beta version, but it works on my end (10/23/2024).\n",
    "https://platform.openai.com/docs/guides/structured-outputs/structured-outputs\n",
    "\n",
    "For answer generation, I had some issues, so I used the standard API.\"\n",
    "\n",
    "Please upgrade before running this notebook: pip install --upgrade openai\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Get parent directory of the notebook \n",
    "sys.path.append(parent_dir)  #  to the Python path\n",
    "\n",
    "from scripts.chunking import process_latex_files\n",
    "from scripts.embedding import get_embeddings, fixed_knn_retrieval\n",
    "from scripts.prompts import gen_questions, gen_questions_s, gen_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting API and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I suggest using 'gpt-4o' for production runs, but it is more expensive.\n",
    "For embeddings, I recommend 'text-embedding-3-large.' We only need to run it once, but it also costs more.\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "# https://openai.com/index/new-embedding-models-and-api-updates/\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "\"\"\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "production_mode = True\n",
    "chunk_by_section = True\n",
    "chpt_for_quest_answ = 108\n",
    "author_for_quest_answ = \"garikipati\"\n",
    "\n",
    "if production_mode == False:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "elif production_mode == True:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "\n",
    "embedding_model = f\"text-embedding-3-{embedding_size}\"  # NOTE: this must be the same for all embeddings. \n",
    "author_for_quest_answ = author_for_quest_answ.lower()\n",
    "\n",
    "# Setting path for root data folder\n",
    "main_dir = f'../data/{author_for_quest_answ}_latex_Q_then_A_use_context'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Context Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding space filename: ../data/garikipati_latex_Q_then_A_use_context/garikipati_latex_embedding_space_by_sections_tpc4096_large.json\n",
      "loaded\n",
      "Space size: (221, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I used fixed size chunks (512) with a 25% overlap.\n",
    "Make sure environment_sensitive is set to False for fixed size.\n",
    "\n",
    "We should embed all chapters to generate the embedding space. For the demo, I only included two chapters.\n",
    "please update the paths in latex_file_paths.\n",
    "\"\"\"\n",
    "\n",
    "# add all book chapters paths\n",
    "\n",
    "if author_for_quest_answ.lower() == \"hughes\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "    ]\n",
    "\n",
    "elif author_for_quest_answ == \"garikipati\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter101.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter102.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter103.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter104.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter105.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter106.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter107.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter108.tex',\n",
    "    ]\n",
    "\n",
    "tokens_per_chunk = 4096                         # was 512\n",
    "token_overlap = int(0.25 * tokens_per_chunk)    # 25% overlap\n",
    "environment_sensitive = False                   # If False, equations can split between two chunks, but chunk lengths remain fixed.\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_tpc{tokens_per_chunk}_o{token_overlap}_{embedding_size}.json'\n",
    "elif chunk_by_section == True:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_by_sections_tpc{tokens_per_chunk}_{embedding_size}.json'\n",
    "    token_overlap = 0\n",
    "print(f\"embedding space filename: {embedding_space_file_name}\")\n",
    "\n",
    "space = {}\n",
    "if not os.path.exists(embedding_space_file_name):\n",
    "    \n",
    "    chunks = process_latex_files(latex_file_paths, \n",
    "                                 tokens_per_chunk, \n",
    "                                 token_overlap, \n",
    "                                 environment_sensitive, \n",
    "                                 chunk_by_section = chunk_by_section)\n",
    "    \n",
    "    chunk_length = []\n",
    "    char_length = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        print(f\"chunk word length: {len(chunk.split(\" \"))}, chunk char length: {len(chunk)}, chunk = {chunk}\")\n",
    "        chunk_length.append(len(chunk.split(\" \")))\n",
    "        char_length.append(len(chunk))\n",
    "    print(f\"max chunk length in words = {np.max(chunk_length)}\")\n",
    "    print(f\"max chunk length in char = {np.max(char_length)}\")\n",
    "    #print(f\"chunk lengths = {chunk_length}\")\n",
    "\n",
    "    # using api\n",
    "    embedding_space = get_embeddings(client, chunks, model=embedding_model)\n",
    "    \n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'w') as json_file:\n",
    "        json.dump({'embedding_model': embedding_model, 'chunks': chunks, 'embedding_space': embedding_space}, json_file)\n",
    "\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'r') as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "\n",
    "    chunks = loaded_data['chunks']\n",
    "    embedding_space = np.array(loaded_data['embedding_space'])\n",
    "    print(\"loaded\")\n",
    "\n",
    "chunks = np.array(chunks)\n",
    "embedding_space = np.array(embedding_space)\n",
    "print(\"Space size:\", embedding_space.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Questions and Their Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk word length: 1401, chunk char length: 7542, chunk = Okay. So let's get on. So what, what we're aiming to do in this segment is get some sense of the stability of the equations that we need to look at. Now if we want to look at stability we have to first understand we must first understand the stability of the time exact case, all right? Because that is the, the sort of behavior, the sort of response we are aspiring towards for our system, right? For our algorithmic system. Okay? So, in terms of stability, let's first understand the time exact case. Right, now, we've derived the single degree of freedom, modal equations, for a partic, for an arbitrary mode L. Okay? Now, everything we do holds for every mode, right? The, the, the, our, our analysis holds for any mode, because we're really working for an arbitrary mode. With that in mind, I can afford, I believe, to drop the explicit, use of the modal index, L. Okay? All right, so I'm going to drop that, right? So, from now on, the time for, for the time exact case, since we're working a single degree of freedom, for the time exact case and for the time discrete case, when we are working with single degree of freedom modal equations, I'm just going to write them as d dot scalar plus lambda d equals 0. Okay? But one more thing, I've got rid of one index L, but I want to bring another one. The one I want to bring back is the, I'm going to use h, right? And it's not an index, it's a superscript. Why am I bringing h back here? Remember h is our old friend the element, size. Right? It denotes the element size. The fact of spacial discretization. Why am I bringing it back? Right. It's because our n and k matrices depend upon our discretization, our spatial discretization. Right? So the ei, eigenvalues we're working with here are truly the spatially discretized eigen, the, the eigenvalues corresponding to the spatially discretized system. Okay? So lemme, lemme just state that here. Lambda h is the eigenvalue of a mode, or corresponding to a mode, that was, that is obtained after spatial discretization. Okay, so the factor spacial discretization, which is which shows itself up, which, which shows up in the finite element size, is indeed reflected in lambda h. Okay? Alternately, because these are partial differential equations, one could do a fully continuous analysis of them, and then one would have a eigenvalue corresponding to modes, but those would not be discretized modes, okay? Those modes or those eigen those eigenmodes would be actually eigenfunctions, not eigenvectors. Okay? So there would be a, so there's a difference. We're really working with the eigenvalues of the, of the spatially discretized system here, and that will show up, it's, it's for that reason that I'm bringing back our memories of h here. Okay? All right, so the time exact case is this, plus of course boundary condition. Sorry, the initial condition, right? So this in fact, d(0) equals d not, right? On the previous slide we've used the modal index L, but we've just decided to drop it here. Okay? Without, without risk of confusion, because everything we do holds for every mode. All right, so what is the stability of the system? How do we, how do we know what the stability of the system is? This equation is one of the simpler ODEs you're likely to encounter. Okay? We can directly write down the exact solution. All right? So the exact solution is d as a function of t equals d sub 0 exponent of minus lambda h times t. Okay? It's easy enough to check that that is indeed the exact solution. If you plug it into your, equations, you will find, and into our ODE, you will find that it satisfies the ODE, and it also, respects the initial condition. Okay? Just set T equal to 0 on the right hand side, since exponent or minus 0 is 1. You get back d at 0 equals d not. Okay? So this is the exact solution. All right, now, what about lambda h? What do we know about lambda H? It's an eigenvalue of the system, right? What is lambda h? Lambda h turns out to be greater than or equal to 0. Okay? All right? Why is lambda h greater than or equal to 0? We're not going to prove it, but do you know what properties give us this? It is the fact that M is positive definite. Right? And K is usually positive definite, but in the most general case, if we, if we want to also allow for insulation along certain directions, if you're talking of heat conduction or the possibility that there is no transport along certain directions if you're talking of mass diffusion, then k is a positive semi definite Okay, earlier on we'd used the fact that K can pretty much be taken to be positive definite, unless you really want to have insulation. We'd use that fact to make the observation that arriving that, that we are going to get eigenvectors that, that are linearly independent. Okay? All right. So, we have this, we have this sort of situation. All right, if that is the case since lambda h is greater than or equal to 0, what can we say about d? Say tn plus 1, right? And we know what this means. It just means that we're evaluating the solution at time n plus 1. What can we say about d at tn plus 1 relative to d at tn? Right? And, and here I'm using the fact that because of the nature of our time discretization and our choice of the progression of time instance, tn plus 1 is greater than tn. Right? So, let me just recall that also. We are, of course, using here the fact that we are progressing in time, so tn plus 1 is greater than or equal to tn, right? It's usually, it's, it's greater than tn. We nev, we never use it equal to tn, because that would mean we have a 0 times. Okay? All right, so given this, what should I use in this blank here? I've left a big blank spot between d and tn plus one and d and tn. What relational operator do I use? Right. Because the exponent of a negative argument is less than one, right? What we see is that this is a decaying function. Right, it's monotonically decreasing. Okay? Or another way at looking at it is, that d at t n plus one divided by d at tn is lesser than or equal to one, provided of course d at tn is not 0. Okay, but, nevertheless it's monotonically decreasing. All right, so we have monotonically decreasing, sorry, time dependent coefficient for our mode. Okay? All right. And this essential says that the nature of our heat conduction equation or our, or nature of our, mass diffusion equation, the kind we are looking at here, is for the solution to tend to be K. Okay? There is no tendency for the solution to tend to increase, provided we have set the forcing equal to 0. All right? And it is on in order to expose this characteristic of the equations we are working with that we are considering the homogeneous case. Because clearly, you could be supplying heat to increase the to, to, to raise the temperature, or you could be, you could have a local supply of mass, or, or, or, again, influx of heat or mass in order to push up the temperature or the mass concentration at, at any point. And therefore, these modes could be increasing in a problem that is in homogeneous. Okay? But we wanted to get to this fundamental characteristic of the equation, so we just assume the homo, we, we are considering the homogeneous case. All right, so why are we doing this? The reason we are doing this is because we want to understand what is the exact behavior that our algorithmic equations should aim to represent? Okay? All right. So, with this in hand, let's ask, ask ourselves what, what the same sort of analysis tells us for our time discrete equation. And this is\n",
      "chunk word length: 748, chunk char length: 3959, chunk = really quite easy, because the way we've written out the time discrete equation, it is, it is in algebraic form. It just gives us dn plus 1 multiplied by co, by some factor minus dn multiplied by some other factor, okay? So the time discrete equation, if you go back and look at it in your notes, and I have it right up here in my slides, so let me just flash it up again, it is in the middle of this slide, right? Marked out as time discrete equation, time discrete case. Okay? So I'm going to rewrite it using the same notation that we are now following, which is to drop the modal index L, and instead for lambda, bring back the superscript h. Okay? So the time discrete case we'll use in this notation is the following. It is d times 1 plus alpha delta t lambda h equals, I'm just moving it to, to the right hand side, equals dn 1 minus 1 minus alpha delta t, lambda h. Okay? Sorry, and here I have dn plus 1. Okay? All right, now, let's try to find that same ratio that we'd found in the case of the time exact problem. All right? So, the equivalent ratio to form here is dn plus 1 and d divided by dn. Okay? And this as we see is equal to 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Now, in the context of our time discrete algorithmic problem, we tend to call this a ratio, we tend to denote it as A. Okay? And A, very obviously is picked because we really want, because this is really an amplification factor. Right? Inasmuch as it is obtained as a ratio of dn plus 1 at, over dn, it essentially tells us how is our time discrete solution getting magnified from one time step to the other. All right? Or getting amplified from one time step to the other. Okay? And you note that A depends upon, if you want to think of it in that way, it depends upon alpha, it depends upon lambda t, and it depends upon lambda h. Right? Okay? So, what that says is that, yes, our integration algorithm matters, right, whether we're choosing one type of the oiler fam, one member of the oiler family or the other. Our time step matters for stability, that's not, that should not be a big surprise. But interestingly as well, the spatial discretization we've used does matter. Okay? And this is why I took pains to point out that the eigenvalue we're working with for any mode, is really a discretized eigenvalue in the sense that it, it reflects the spatial discretization. All right? And that too does affect our amplification factor. All right? When we, well let's just do one more thing. What do we mean now by a stable problem? Right? We wanted to reflect what we saw for the time exact case. Okay? So, for stability the requirement we want to have, right, is the following; we want to say that the magnitude of A should be lesser than or equal to 1. Okay? All right. Now, you may wonder why we are going with the magnitude. All right, what would happen, and why not just say, a has to be less than one? Well, what you're going to see, is that, this is a, after all, we're doing approximations here, right? We're constructing the, the reason we're the getting districtizations is we want to approximate the time dependent behavior. Well, one, result of that, approximation and of that districtization is that it possible for our solution to sometimes go negative here. Okay? So that is something that we will have to deal with. Right? And that's why we recognize that A could be, could, could be negative, dn plus 1 over dn could be a negative ratio. Solution can change signs, we're just recognizing that the algorithm may do that and therefore we are restricting ourselves further by saying that the magnitude of A has to be lesser than or equal to 1. Okay, we are not guaranteed, positive solutions always for our time discrete problem. Okay, so when we come back, we're going to end the segment here, when we return, we are going to, apply this, this condition to our, time discrete, problem and see what it tells us.\n",
      "chunk word length: 1477, chunk char length: 7619, chunk = All right. What we're going to do in this segment is complete our stability analysis of our Time-Discrete, Single Degree of Freedom, ODE. All right? Or a single degree of freedom equation. Okay. So let's recall where, where we got. We got as far as the amplification factor. Okay. And that amplification factor gives us dn plus 1 divided by dn equals A, this is the amplification factor. And it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? All right. Now what we want to consider is the fact that what we're going to use in our analysis is the fact that alpha lies in the, in the closed interval, zero to one, right? Delta t, which is our time step is greater than zero. Okay? And lambda h, we've agreed is greater than or equal to zero. Okay? All right. Now the stability condition. Or the stability criteria is that the magnitude of A is lesser than or equal to 1. Okay? All right. So this is what guarantees decaying response, right? From one time step to another. Okay? By the way, the fact that we arrive at this condition is actually the, is what we call a linear stability condition. Okay? And the linear aspect of it comes from the fact that we have indeed a linear problem here. And for linear problems, one can state the stability requirement by just saying that well, if your solution does not grow from one time step to the other, you have a, you have a you have a stable problem. Okay? When we go to non-linear problems, we can't quite use that. Because the physics of non-linear problems can actually lead to solutions growing, right? Over certain regimes, right? And that, that is the right physics for those sorts of problems. So let's get back to this. So, if this is what we have let's see how we can, we can extend this. What this implies, of course, for us directly is that minus 1 should be lesser than or equal to A should be lesser than or equal to 1. Okay? So, A has to lie in that interval. All right? Okay. So let's look at this. So what that means then is that minus 1 is lesser than or equal to 1 minus 1 minus alpha delta t, lambda h divided by 1 plus alpha delta t lambda h is lesser than or equal to 1. Okay? Now because of the conditions I put up here, right? We are guaranteed that our denominator is positive, right? So that says, if we are now what, what this lets us do is safely multiply through by that by the denominator and we get here. Minus 1 plus alpha delta t lambda h is lesser than or equal to 1 minus 1 minus alpha delta t lambda h is lesser than or equal to 1 plus alpha delta t lambda h. Okay? All right. Okay. So to move on, let's just go to the next slide and let's look at the right-hand side of that inequality. Okay? So consider in form, right? 1 minus 1 minus alpha delta t lambda h is lesser than equal to 1 plus alpha delta t lambda h. Okay? We can consider this, we see that these two cancel out. Okay? And we manipulate things and what we see here is that we get alpha plus 1 minus alpha delta t lambda h has to be greater than or equal to 0. Okay? And we do this by simply moving what remains on the left-hand side of the inequality on the first line over to the right-hand side. Okay? Now those cancel out, all right? And we are left with sorry. There's a delta t here. What we are left with here is the delta t lambda h is greater than or equal to zero. Okay? That's what the right-hand equalities says. All right. But this is always satisfied. Okay? All right? Let's look now at what happens. And, and in fact, this is always satisfied regardless of alpha. We satisfied for, for all alpha, all right? Because alpha doesn't even show up in this final form. All right. Now let's consider the left-hand side inequality. Okay? And that left-hand side inequality is that minus 1, sorry, minus 1 plus alpha delta t lambda h Is lesser than or equal to 1 minus 1 minus alpha delta t lambda h. Okay? All right. So now let's work with this a little. When we work with this a little, let's, let's, let me rewrite this by moving This term over to the right-hand side and, and, and rewrite them. So when we do that, we get 1 minus 1 minus alpha delta t lambda h plus 1 minus alpha delta t lambda h is greater than or equal to 0. Okay. That is the condition which must hold and this condition you note involves alpha, delta t and lambda h. Okay? Continuing to work with it, this implies that two plus 2 minus 1 all right. Minus two alpha, hm. What's going wrong here? Alex, you may need to just give me a pause here. I've got a two here. That over back, okay. Alex, I'm going to backup, I made a mistake here. Go back to the beginning of this last equation. Okay. I note that I need a plus sign there. Okay. Put that plus sign and go ahead and now, I get 2 plus 2 alpha, right? One alpha coming from here and another alpha coming from here. 2 alpha minus 1 times delta t lambda h is greater than or equal to 0. Okay? That is the condition we need. Let's see what else we can do with this now. I could write this out as 2 has to be greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? All right. All right. Now let us look at whether this can, under what conditions this may be satisfied. Okay?. Now let's save this and move on to the next slide, okay? Let's consider the following cases, okay? So case one. All right? And for case one, let us suppose that alpha is greater than or equal to one-half. Okay? What, when, when that happens, what that says, what that means is that 1 minus 2 alpha, delta t lambda h is always what? Right? It's always lesser than or equal to zero, okay? Which implies that yes, if alpha is greater than or equal to half, 2 is always greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? Right? This thing always holds, holds unconditionally. Right? And by unconditionally, what we mean is that if alpha is greater than or equal to half, our stability condition holds for all delta t, right? For all delta t greater than zero, right. Okay. So we have a condition, we have a, a situation of unconditional stability of our method. Okay? So this is unconditional stability if alpha is greater than or equal to half. Okay. So, in particular, observe that alpha equals half, which is the Crank Nicholson method or the midpoint jewel as I sometimes call it, as a lot of other people call it. Right? Or even alpha equals 1, which is backwards Euler. Right? These are both unconditionally stable. You can take a time step as big as you like and you're okay. Okay? A solution may be terrible, it may be very inaccurate, but you're still stable. The solution will not blow up, right? The amplification factor will remain, the magnitude of the amplification factor will remain bound by, by one. Okay? Case two is the interesting one. Okay? Case two is that alpha belongs to zero comma half not including the half, right? Okay. All right. Which is basically zero is lesser than or equal to alpha is less than a half, okay? This is the other case. All right? So now what happens is that we have a situation of that's okay. So, so in this case, we have a, we must still have all right. So we, we are trying to see whether 2 is greater than or equal to 1 minus 2 alpha delta t lambda h. Right. This condition needs to be satisfied for alpha, such that zero is less than or equal to alpha is less than half. Okay? All right. This is what we're trying to look at, okay? Now observe that since alpha lies in this range that I've written on the right this parenthesis is always what? Is it all, what can you say about the quantity in that grid emphasis, in those parenthesis? Right? It's always\n",
      "chunk word length: 778, chunk char length: 3981, chunk = positive. Okay? So now we get a condition, where we see that on, on, as far as delta t is concerned, delta t must be lesser or equal to 2 divided by 1 minus 2 alpha. We're able to divide through by 1 minus 2 alpha, because it is indeed greater than 0. Okay, so it is a positive quantity. 1 minus 2 alpha is positive. But then we see the delta t has to be lesser than or equal to 2 over 2, 1 minus 2 alpha, times lambda h. Okay? So this is a situation of conditional stability. Okay? So the algorithm is stable, right? It can be stable. But it depends upon the size of delta t, okay, conditional stability, which means conditional upon delta T. Okay? So delta T has to be bounded by something. Delta T cannot be too big. Sort of makes sense, right? You have an algorithm that is not entirely stable, that is not always stable. In order for it to stable well, take very small delta t's, right, take very small times 6. Ok, so, an example of this is of course alpha equals 0. Right, which is forward Euler. Okay. All right, now, to end this segment let us consider what else can happen here with delta t. Remember that lambda h, because of the nature of our Eigenvalue problem, lambda H if you ask yourself, what is the order of lambda h? Okay? And where does lambda h appear? Order of lambda h depends upon the Eigenvalue problem, on the Eigenvalue problem. K psi, where psi is the mode shape corresponding to this particular mode. K psi equals lambda h, n psi. Okay? Now, what this suggests, is that lambda h is of the order of, the order of lambda h is of the sort of terms that are like M inverse K. All right? Right. So it's with the order of the norm of terms in M, inverse K. All right. Now, I want us to think about how do M and K depend on the element size? Right, because these after all are obtained by the spatial discretization. Okay? So, recall M is of the order integral over omega e, and then we have the sum over e, sorry, we have the assembly over e and all that. We have the assembly over e, integral over omega e of terms of this type, right? We have, we have shape functions, right? So this is really of this is really like M A B in a particular element. Okay, so I can actually get rid of this, right. I'm just doing this element twice, sorry, I'm changing things around a lot. Okay, element twice, this is what we have, right, integral over omega E, N, A, N, B, D V. Okay, K AB from an element is of order, integral over omega e NA,X NB,X right? These are derivatives. Right? So there are derivatives involved in KAB and yes. KAB may also have a so this is xi, xj, K, kappa here may have a kappa ij. Okay? Dv, all right, and yes the mass matrix may also have a specific heat there. Right, or row could be one if you are doing diffusion. Okay? What is, what is the order of these sorts of terms? These sorts of terms, because their shape function derivatives, are of order one over h. Okay? As a result, what we're seeing is that MAB is of order may be 1, right. But KAB is of order h to the minus 2, okay. As a result, the order of lambda h, okay, is h to the minus 2, okay? In terms of the element size. Now what does that say for delta t? Delta t has to be lesser than or equal to 2 over 1 minus 2 alpha lambda h. Well, this lambda h is like h to the minus 2. Okay? As h gets smaller, our restriction on delta t gets more and more stringent. Okay? So the remark that I will make here is that restriction On delta t, gets more stringent. As h tends to 0. Essentially as we refine our finite element mesh, our spatial discretization, we see that it has an effect even on our temporal discretization if we're working with a conditionally stable method. Right? For the time integration, okay. So, this is how the finite element discretization also makes its presence felt in the time discretization when one is working with a semi discrete method and happens upon a conditionally stable problem, conditionally stable method. All right. We'll end the segment here.\n",
      "chunk word length: 1434, chunk char length: 7577, chunk = Welcome back. We are moving steadily ahead with our analysis of our Euler family of algorithms, or time integration algorithms for our parabolic PDE. What we've managed to accomplish in the last segment was an understanding of stability properties. And I'm going to start this next segment with very quickly summarizing that those results and moving on from there. Okay. So, the result that we had was for stability. Okay? What we found was that for the Euler family, right? If I look at it, look at the stability results in terms of this parameter alpha. Right? What we find is that for alpha greater than or equal to half, we have algorithms that are unconditionally stable And what this means, is that it does not matter how big our time step is, okay? So unconditionally stable, also meaning any delta t greater than 0. Okay? Any of them will give us stability. Right? For the other cases when we have, alpha lying between, alpha less than one half, and remember alpha has to lie between 0 and 1, right? So, it can't get smaller than 0. So, if alpha is less than a half, what we have are methods that are conditionally stable. All right, and the condition that we obtain is one on delta t. We find in particular that delta t must be a lesser than or equal to 2 over 1 minus 2 alpha, times lambda h. All right? Now, I want to say something more about this result. Observe that for lambda h, which is the the discrete eigenvalue corresponding to the particular mode that we are looking at, right? Lambda h can take on different values for different modes. So when we are looking at the full matrix vector problem, what we need to ensure is that this conditional stability is satisfied for all modes. Okay? So this result must hold for all modes. Right? Consequently, what we want to look at, what we want to impose is this condition for the maximum eigenvalue. All right? This implies that really the kind of condition we must work with for our matrix vector problem, is that delta t is lesser than or equal to 2 over 1 minus 2 alpha lambda h max. Okay? The maximum I can value over all modes. Right? And that is the, something we obtain from an eigenvalue decomposition of the problem, which also we have looked at. The final result we noted, is that lambda h and therefore lambda h max also, varies as the elements size to the minus 2. Okay? All right? And what this implies then, what this means for our methods, is that the spatial discretization does in deed fact affect our, time integration, right? In particular, it, it affects our choice of time stamp. Okay? What this translates to then is that delta t is or, or delta d max really, if one is following this conditional branch of the algorithms, of, of the stability of the algorithms. It means the delta D max,uh, goes as, h square. Okay? Consequently as we refine the mesh, the spatial mesh, we are constrained to using smaller and smaller maximum time steps. All right? Okay, so this really a summary of the type of our stability results. What I would like to do to move on is revisit our amplification factor, okay? So let us recall, let's recall the amplification factor. All right. And that is something we denoted as A. And if I recall correctly, A is equal to 1 minus 1 minus alpha delta t lambda h divided by one plus alpha delta t lambda h. All right? This is the amplification factor. And we recall also how it applies to our algorithmic problem. We see what we have is that dn plus 1 is equal to A dn, okay? Now, what I want to do is look at the effect that this amplification factor has on the behavior of high order modes. Okay? So, we are going to use this fact to look at the behavior of high order modes. Okay? And for our implementation, what this means is, first of all what do high order modes mean? What is a mode of a high order? What characterizes the order of mode? It's not simply the number, right? It's not simply what number we ascribed to it, the m or the l that we've been using, right? We know that m equals 1 to ndf and the, where mappears is in labeling the mode, right? Psi m. Right? Just m having a large number does not make it a high order mode, that is simply an arbitrary choice we have made of labeling modes. Right? So what does make a mode high order? It is the value of lambda h. Okay? Essentially large values of lambda h are the higher eigenvalues, and in any linear system those are the higher order modes, right? For the system, it turns out that those are the modes that have higher order spatial frequencies. Okay? So so it's lambda h, but really we can we, once we have lambda h and we have a what we know about lambda h, a once we know we have a time step, this really implies lambda h delta t. Right? And it's doesn't need convenient to work in terms of lambda h delta t, because this is how it shows up in our problems, right? Okay? All right, all right, so let's look at what effect we had on our amplification factor. And to do that, I am going to plot up on the vertical axis, the amplification factor. And on the horizontal axis, I'm going to write, I'm going to plot this quantity, m, lambda h delta t. Okay? So the idea is that we are looking at the effect of higher order modes and, and, but, but the way we are plotting it up and the way we're, we're studying it we could get to the same behavior either by looking at lambda h large or just the effect of having a very large delta t. Okay? All right, so here I'm going to plot like I said I'm going to plot the amplification factor. Right? The formula for which we have on the previous slide. All right, it's, it's useful by the way, also to look at what the amplification factor is for our exact equation. Okay? And I'm going to write that up here for the exact equation, for the, for the exacting, the time exact equation. All right, the time exact single degree of freedom model equation we know that d of t equals d at 0 exponent of minus lambda ht. Okay? So effectively we have here a sort of time-continuous amplification factor. All right, we can just look at this quantity as being the amplification factor, right? And then if we apply again to the idea that we want to look at how it varies between one times step in the other, all right, from tn to tn plus 1, what you will see is that the amplification factor for the exact problem is essentially exponent of minus lambda h delta t. All right, the idea is that you could take this exact equation, simply write it out as a mapping from dn to dn plus 1, okay? And then you will see that the amplification factor you get is exactly the exponent of minus lambda h delta t, right? So, and, and, so that's just an exponentially decaying function, right? So let's write that one out first. We see that the amplification factor can have a maximum value of 1, all right as written here, right? The exact amplification factor. And I'm going to try to draw a an exponentially decaying function, and hopefully I can draw it reasonably smooth. Okay? So, this is what I'm going to call A exact. Okay? And in parentheses here I'm going write out the limit that A exact tends to as lambda h delta t tends to infinity. Okay? Clearly, as lambda h delta t tends to infinity, exact goes to 0. Right? Okay, now we're going to return to our actual amplification factor, right, our algorithmic amplification factor. And look at what value it takes in the limit for the different members of our, family, okay? So, since I have all this room here, I'm going to make use of it and I'm going to write the actual amplification factor here. And recall that it is 1 minus 1\n",
      "chunk word length: 744, chunk char length: 3858, chunk = minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Because we want to look at it in the limit as lambda h delta t tends to infinity, I'm going to simply divide through by that quantity. Okay? So I get 1 over lambda h delta t minus 1 minus alpha divided by 1 over lambda h delta t plus alpha. All right? Okay. Now, so I'm going to look then at for various values of alpha, I'm going to look at what happens as limit lambda h delta t tends to infinity. Okay? Okay? Let's start at the top of the range of alpha. Say alpha equals 1. All right? For alpha equals 1, when lambda h delta t tends to infinity, if you look at that limit, you basically get 0. Okay, all right? So this function right so let me plot this out now. Let me use here's black. All right, so if it is as something like this. I should admit here that I'm not paying very careful attention to how it behaves relative to the, relative to the exact exponential at the left end of the limit. Right, I'm, I'm not paying much attention to that, so, but you can check that, okay? So, don't worry about the behavior on the left. What I'm interested in is the behavior for high order modes, right, which is 4 lambda h delta t tending to infinity, right? So what we have here, is that A, and instead of saying exact, I'm going to say, I'm going to write as a subscript here the, the value of alpha. Okay? So a1, and of course I'm using the colors, so a1 as lambda h delta t tends to infinity, which is the, the amplification factor now for the backward Euler algorithm, right, was alpha equals 1. This thing also goes to 0. All right? Let's change colors here again, go back to red. What I'm going to do is write them for all the others, okay? Let's write, and I'm going to write them only for, for the ones that we really care about here. Let me write alpha equals half, right? Which is the Crank Nicolson method. What we see is that for alpha equals half, substituting that value of half in there, we see that the amplification factor goes to minus 1. Okay? And for alpha equals 0, which is the fold Euler method, we have what do we have? We see that that limit is actually, the limit doesn't exist. Right? Because as we we simply set alpha equal to 0 that traction becomes unbounded, right? But let me allow me to write a unreal number, an unreal number, basically it's unbound, so it goes to infinity. Okay, that's not strictly to say that infinity's not a limit, right? But anyhow, you know what I mean. All right, so this is the situation we have. Let's let's look at what these things appeared like. So if that is 1, then this is minus 1. Okay? And okay, I'm going to show you what Crank Nicolson looks like on this, in green. Crank Nicolson essentially tends in the limit to minus 1. Okay? So a half, or, or the midpoint rule, as I used to call it, a half tends to minus 1. Okay? And finally for our backward Euler algorithm, what we see is that we actually get sorry for our forward Euler, we've already plotted our backward Euler. For forward Euler, what we see is that it is a it's a straight line. Okay, it's a straight line, and with, with a negative slope it falls to right, it leaves the plot. Okay, so this is a for 0, the forward euler algorithm, which is infinity. Right? It's actually it goes to actually minus infinity. Right? Okay? So that's how they behave, and you can plot up some of the others also in between here. So, what does this mean? If we stare at this plot, what you see is that when it comes to the high order modes, it is backward Euler, which has the behavior of the exact equation. All right? Alternately, we may state this as saying that backward Euler tends to dissipate with high order modes. This is numerical dissipation, right, in addition to the, to the physical dissipation that exists with the heat conduction or the master fusion.\n",
      "chunk word length: 1486, chunk char length: 7628, chunk = Right, so, remarks. Backward Euler. Right, which is alpha equals 1 damps out, well, I don't like the term damps, I'm going to say dissipates. Dissipates high order modes. Okay, and this is what we call a numerical dissipation. Okay, and because as I observed this has the same high-order behavior as the exact equation, it is often preferred. Okay? Numerical dissipation, right? And, let me also say here, it is similar to time exact equation. Right. The second note here is that there really isn't much to say about forward Euler by the way. It should come as no surprise to us as th, that that the amplification factor for forward Euler tends to minus infinity as lambda h delta t gets large. We've already seen some evidence of it. And in what way have we seen evidence of it? We've seen it in the fact that if delta t gets too large we know that the, that the forward Euler method is no longer stable, right? And that is also reflected in, in the, in the amplification factor going to minus, going to, getting unbounded in this case, minus infinity, as delta t gets too large, right?. Remember lambda h delta t can be can get large if either lambda h or delta t get large. If you have large times that, we know that there is a tendency for forward Euler to blow up, so to speak. Right. And we see that in the amplification factor, tending to minus infinity. Okay, so let me state that though. Forward Euler. Forward Euler, which is alpha equals zero. Has unbounded. Lambda has unbounded A. Okay, all of this, by the way, is in the context of high order modes, right. So I should say up here, this is all for high order modes. Okay, so everything I'm writing here applies to high order modes, okay? All right, so, this should come as no surprise to us, because we know that forward Euler has this tendency to, to sort of lose it, okay? Remark three, is that the surprising thing is that the midpoint rule. Or it may be a surprise. That the midpoint rule, which is alpha equals one half. Right has, has as we've seen A tending to mi, to -1, okay. What does this mean? What this means is that dn plus 1 for the high order modes, right if this is a high order mode I'm writing out, right? Kay, if dn plus 1 is the model coefficient of of a high order mode. What we're seeing is that dn plus 1 tends, is equal to minus dn. This leads to oscillatory behavior in the high order modes. And it's not uncommon to see solutions for from, from the mid-point rule which with respect to time, all right, if we're plotting certain modes with respect to time, what we will see is that if this is t1, t2, t3 and so on. Right? What we may very well see is that you get you get essentially oscillatory behavior. Okay? Something that's done is to simply form a time average, okay. And to take a time average you basically go damp out, well you don't truly damp out, but you, eliminate the effect of these oscillations in your post processed solution. Right, time average the, time average the solution, okay, to get around this. All right, this is important to know about the behavior of these methods in the high frequency limit, right, for high order modes. Okay. So we've looked at stability, we've understood high order, and the behavior of high order modes. What we are going to do next is essentially prepare ourselves for for talking about convergence. Alright? And the way we do that is by first looking at the notion of consistency. Okay? So, we look now at the idea of consistency. Alright. I order to get to consistency, let's go back to our discretized equation. Okay? And that equation, as you may recall, is the following. It is dn plus 1 times 1 plus alpha, well, no, let me back up a little here. Let me write it in fact in the following manner. Let me write it all the way back as dn plus 1 minus dn over delta t plus lambda h d plus alpha equals. Now, up to recently, up to a few seconds ago we were looking at the homogeneous problem, right? Which we got to by turning off the high order modes, sorry by turning off the forcing. We're now bringing back the forcing. Okay? And I'm going to write this as F at N plus alpha, on the right hand side. Okay? If you wonder what FN plus alpha is, FN plus alpha, right, for this particular mode, is got by looking at F sub n plus alpha, the whole vector, right? And essentially, right, dotting it with whichever mode we are looking at. Okay? If we we're looking at mode L we would dot it with the Lth mode, okay. But since we've agreed that we're going to drop the explicit mention of the model number, right, we will just write it as FN plus alpha here. Okay, alright, now, let's multiply through by delta T and see what we get. We get dn plus 1 minus dn plus lambda h Delta t. I'm going to expand out dn plus alpha, as alpha dn plus 1, plus 1 minus alpha dn equals delta tFn plus alpha, alright. Working with this a little more, what we get is dn plus 1 times 1 plus alpha delta t lambda H minus one minus, one minus alpha delta t lambda h, dn minus delta tFn plus alpha, equals zero. As a final step I'm going to divide through by one plus alpha delta t lambda h to give me dn plus 1 minus. Now when I divide this quantity by this I get back my amplification factor. So I'm going to jump a step and simply write that as A dn minus delta t divided by 1 plus alpha delta t lambda h, Fn plus alpha equals 0. Okay? Right, and I'm going to take the final step of calling this quantity here l at n plus one. Okay? Or I think I'll call it m, doesn't really matter, let me just call it l at n. Okay. All right? So this is how I want to write out my time discrete model equation, right, including the forcing. Now, the notion of consistency is the following. What would happen if you were to take the time exact modal equation, and plug it in here. Okay, so now let me now write that as d at tn plus 1 is the time exact mode corresponding. To lambda h. Okay? The particular value of lambda h gives us a certain mode, and that's it. Okay. Now the question we ask is what would happen if we were to take our time exact model solution and plug it back in here. All right? So what we are saying is that if we do that we get d at t and plus 1 minus A d at tn, right? This is also th, the time exact model value at time tn minus minus Ln. Okay, because the last term Ln does not depend upon on the, upon the modal values. Okay? Now, this equation is equal to zero. Correct, right? The left hand side on this equation is equal to zero. My question is, if we were to not plug in our exact solution here, right, as we, as I've done. Will the right hand, will what I've written here on the left hand side be equal to zero or not? In general, it's not equal to zero, okay? In general, however, one can write it out as delta t times some quantity tau. Okay? Tau which in general depends upon the time. Okay, right, the reason we are writing it out as delta T times tau is that, because we recognize that having started from this form of the equation, we've actually multiplied it, multiplied the ODE through by delta T. So we expect that whatever the right hand side is, it already has factor of delta T sitting in there. Right, so we choose to write it in this fashion. Okay, now. This is the general form that we would get for the exact equation. All right. It turns out that we can now make the following identification. Okay. What we say is that. Sorry. See, let me back up to the previous slide. Let me just say one more thing here. If we can show that our exact solution satisfies an equation of this type, okay, we had what we call a consistent method. Hour is no what no let me back up a little\n",
      "chunk word length: 404, chunk char length: 2067, chunk = more its not quite a consistent method, this is a consistency condition. Okay? This is a consistency condition where we have a consistent method if we can show further. That tau, which I've written as depending upon the, the actual time. If you can show that tau is lesser than or equal to sum constant c times delta t, to some power K, okay? Where K is greater than zero. Okay? If we can show this, then we have a consistent method. Here's why. If we can show that this holds, one can say that as in, in the limit, as delta T tends to zero. Right? What we will find is that d at tn plus 1, minus Ad at tn, minus Ln, which is equal to delta t c delta t to the power of k, right? This thing also tends to zero. Right? This entire thing also tends to zero. Okay? All right, if k is greater than zero. Alright, and therefore what this suggests is that, yes the equation we're working with, the finite difference equation we're working with in time, is one that when we plug in the exact solution, though it does not immediately satisfy the exact solution, at least it does satisfy the limit of, of, vanishing time scales. Okay, so in the limit delta t tends to 0, the time discrete equation. Admits. The exact solution. Right, and which I'm sure you will agree is a useful property for a method. Right, for a numerical method. Okay? I should mention, of course, that in that, in that result t of tao N eq, is lesser than or equal to c delta t to the power k. C is a constant, and K is what we call, k is the order of accuracy. Okay? And it turns out that k equals 2 if alpha equals half, right. The midpoint rule. And it's equal to 1 otherwise. Okay. All right? So, the midpoint rule, as you may expect, because it does look at the, it does apply the algorithm between n and n plus 1. Gives you higher accuracy then any of the other methods. Right, so backward Euler and forward Euler are, are, are down here. And any other method also has Euler factor of c 1. Okay? Only the midpoint rule is second order accurate. Okay, this would be a great place to end this segment.\n",
      "chunk word length: 1551, chunk char length: 7691, chunk = So, we'll proceed. What did in the last segment was look at high, the behavior of high order modes and also introduce this notion of consistency of the finite difference approach to time discretization. I ought to mention that proof of that, those results for consistency, and order of accuracy are obtained by essentially looking at the time exact solution, carrying out Taylor series expansions, and finite Taylor series expansions, and then, and then manipulating them. It's a, it's a fairly straightforward exercise but somewhat tedious, okay? So, in this segment we are going to essentially move to the end, end game for this particular topic. We are going to look at convergence. Okay, so we are going to look at convergence. Of. Time discrete. Solution. All right. In order to talk of convergence, as you may recall from our early treatment of error analysis for the finite element method itself, we need first to identify what our error is? Okay. So we define the error, and as we've done before in the context of the finite element solution itself we will define the error as the time, as the discrete solution, right? But in this case, we look at the discrete solution at n plus 1, minus the time exact solution at n plus 1. Okay? All right. Now this is what we define as e at n plus 1. Okay? Now, e n plus 1 is of course, is just another vector in the same space. Right? Right, just as d and t are sorry, your d n plus 1 n d at tn plus 1 are vectors and this rndf space. But what that means is that because we have a basis our orthonormal basis, we can expand e as well in that basis, right? So we can essentially carry out the modal decomposition. Of e at n plus 1. Right. And that model of decomposition essentially is that e at n plus 1 equals sum over m, e m for the mode at n plus 1, right, those are our scalar modal coefficients times psi m. All right, just as for any other vector. Okay, now how do we pose the question of convergence, right? We say that something has converged, right? Or we say our solution has converged, have converged. Convergence Right, is the requirement that as limit as the limit of n plus 1, tending to infinity, right, as we take more and more steps, right. What we expect to see is that e n plus 1, dotted with M e n plus 1 equals 0. Right, that limit is 0. Okay. Can you tell me why it is okay to, or can you think, why it is okay to put M inside there? Normally one would say that well, the error has to vanish, right? But why is it okay to put M in there? It's because M is positive definite. Right? All right, so let me just state this here. Since. M is positive definite, all right. What we see is that e n plus 1 dotted with M e n plus 1, right, equals 0, if and only if e n plus 1 itself equals 0. Okay? Only if, it's only when that vector itself is equal to 0 that, that quadratic product is equal to 0, since M is positive definite. And so, and this is why it means that well, yes, if, if this particular limit is tending to 0, it has to be that e itself, e n plus 1 itself is tending to 0. Okay, so this is why it is okay to consider this, as our convergence criterion. All right. But a more importantly, we are not going to work in this form. Right, we are not going to work, with e dot with M e. We're going to work in terms of modal coefficients, because our entire analysis has been in terms of modal coefficients, right. But it's okay, because I'm going show that for this quantity tending to 0. Right. This quantity that I put a brace on, okay, that quantity tends to zero, only modal coefficients themselves go to zero. Okay. So let me, I've told you what I'm going to show you. But in order to write it out, let me say the following, but not the following. Okay, e n plus 1, dotted with M, e n plus 1 equals sum over m, e m, n plus 1 psi m, dotted with M sum over l, e l n plus 1. Psi. Right, and we're going to put parentheses here. Okay. Each of those sums is for one of those e's right, in the quadratic product. All right? Okay. Well, we know how this works now. This is equal to, let me see how I want to write it, right. It is the sum over m and l, right, of, e n, n plus one, psi m dot M psi l. All right, and for no particular reason I'm going to put parentheses around that, and here I have e l n plus 1. All right? However, we know that this is delta m l. Because of the order normality of the psis with respect to m. Right? Well, if that is the case, we know that, this product that we started out with, e, n plus one, dotted with m e, n plus one, equals sum over m, e m n plus one. E m n plus one. All right, essentially what has happened there is the chronicle delta, has been used to, to turn that l index into an m index, right? So this is basically just the Euclidean norm if, with respect to the psi basis, right, so this is sum over m, e m n plus one, the whole square. Okay, so now it's clear that if this quantity is standing to zero it means that the modal coefficients, that sum itself has to tend to zero. Right, which means the modal coefficients themselves have to go to zero. Okay? So what this implies, finally, is that, limit, n plus one, tends to infinity. E, n plus one, dotted with M e, n plus one, equals, essentially limit as n plus one tends to zero, sorry, tends to infinity. Of the sum. Okay? All right? So clearly, if this, has to be equal to zero, if this limit is equal to zero, it just means that, each, every single modal coefficient, tends to zero. Okay? All right, so limit n plus 1 tends to infinity. E n plus 1. Dotted with M e n plus 1, equals zero. If and only if, each E m n plus 1, equals zero in the limit. All right? So, it means that it's okay to just look at what happens with the modal coefficients. Okay? All right, so. Study convergence. Of modal coefficients, right, E n plus one. Right and now dropping the explicit, mention of the modal, index. Right I've just dropped n. Okay? All right, well that's what we're going to do. Okay, so, how do we set it up? We set it up by going back and writing out our time-discrete, discrete equation, our modal time-discrete equation in the form that we set up in order to pose the question of consistency. All right, so here is what I say. Consider. The time discrete equation in the following form. D n plus 1 minus A d n minus L n equals zero, okay? And consider the, the exact equation also forced into this form. We know, however, that the exact equation does not cooperate, and. Give us a zero right hand side, instead we get delta t, tau at t n. Okay, right, what I'm going to do now is subtract the second equation from the first, okay, so I'm going to change signs. Okay, and now when I add those two together, I get on the left hand side, right, d n plus one, minus d t n plus one, is essentially our e, at n plus one. Right? Okay? Right? It is the, error in the corresponding modal coefficient, right? And here I get minus A e at n. All right, the Lns cancel out, and I'm left with minus delta t tau at t n. I'm going to rewrite this as an expression that will allow me to write out a recursive formula for the error. Error at n plus 1 equals A times error at e minus delta t. Tao at tn, okay? I'm going to use recursion now, and write from here e at n equals A e at n minus 1 minus delta t tau t n minus 1, okay. What that implies is that on substituting e as written here in that expression, right? I get e at n plus 1 equals A square e at n minus 1, minus A to the power 0 delta t, tau at t n minus 1, okay. Right. Minus A to the power 1 delta t tau tn minus 1, and I just realized that here it is just tn. All right, I can take yet another step, okay. What, if I were to now rewrite an equation for e n minus 1. All\n",
      "chunk word length: 247, chunk char length: 1135, chunk = right, I would get A e n minus 2, minus delta t tau at t n minus 2, okay? And making the substitution, right? You see how this is going, right? What I would get is that e, sorry. E at n plus 1 equals A cube e n minus 2, minus A to the 0 delta t tau tn, minus A to the 1 delta t tau tn minus 1, minus A squared, right. Minus A squared delta t tau at tn minus 2, okay. Now one can go on with this, and I'm sure you've all done it at one stage or the other with similar expressions. When you go all the way back to, the zeroth step, right, so the initial time, we get e at n plus 1 equals A, to the power n plus 1. Okay, right. A to the power n plus 1, e at 0, okay. Minus, sum i going from 0 to n, A to the power i, delta t, tau at tn minus i, right? You can check that this is what the recursive formula reduces to, okay? All right, let's stare at this. What is e at 0? What is the error at time t equal to 0? It's equal to 0, right? Because we've made sure that our initial condition is actually obtained from the time exact solution, right? I mean, at the initial time there is no error, right, because we fixed the initial condition.\n",
      "chunk word length: 1499, chunk char length: 7641, chunk = Okay, so what this gives us then is that. This tells us then that e at n plus 1 equals minus sum i going from 0 to n A to the power delta t tao t at n minus i. Okay. All right. Let's work with this. And we're going to work now, by invoking some inequality. Okay. In particular what we can say, first of all, is that the first con, the first step that we will take is not an inequality. It is to say that the magnitude of e n plus 1, right, is equal to essentially magnitude of sum i going from 0 to n, A to the power i delta t tao t n minus i, okay? Right, we're taking the magnitude of that sum, the absolute value of that sum. Now, is where things get really interesting. We, now's where the inequalities come up, okay? And, and remember when we write the inequalities, I'm going to start out by writing this. And you recall when we looked at the, our error analysis for the finite element method I made the point that when I write such an inequality, what I mean is that the previous right-hand side, this one is bounded from above by what I'm about to write now as the new right-hand side, okay? So, what I get is that, that is lesser than or equal to i going from 0 to n the magnitude of the, the absolute value of each one of those terms, and that's A to the power i, A to the power of i delta t tau add t n minus i. Okay? All right the absolute value of a sum is bounded from above by the sum of the absolute values. And this result is a very standard step in analysis. It is called the triangle inequality. Okay? But there is more. We can say further that that, that our most recent right-hand side itself is bounded from above by this expression. Okay? Essentially what we're seeing here that is that the that any absolute value, which is a product, right? So the product, the absolute value of a product is boundary from above by the product of the absolute values. Right, and this result is called the Cauchy–Schwarz inequality. Okay? But now we have more. We have stability, right? If our method is stable, what we are able to say is that sum i going from 0, to n of 1 times magnitude of delta t times tau tn minus i, bounds from above our previous right-hand side. Okay? Essentially, what I've done is replace, A i with 1, right. And why, why am I able to do that? It's a property of the methods we are looking at. What property are we applying here? It's stability, right? Because we know that the magnitude of A has to be lesser than or equal to 1. The absolute value of A has to be less than or equal to one for stability, therefore, A to the power i has an absolute value also lesser than or equal to 1. Okay, so if you re-substitute that with one, we get a bounding from above of what we had as our previous right-hand side, okay. But now the story goes on, now we know that delta t is a positive quantity, right. So what we can do here is, we know that delta t's a positive quantity and, furthermore, we know that for tau, the condition of consistency allowed us to say something about it. Okay, so in one step I'm going to do this. I'm going to pull the delta t out. All right. And I have inside my sum. C delta t to the power of k. Okay. Why am I able to say this? It's because we said that we have consistency, okay? All right, we have consistency. Now one could introduce another step inside here, which would be to first say well, if all the tau's over all the n minus i's are take maximum of them. Okay and then I get to this point, right. So let me say that one can see inside of here, when they first, if one wanted to be really careful about this then I guess one should be really careful. So one would say, that first of all you would have a previous step inside here, which is to say that the, that this right-hand side, right? Using that, one could say that sorry, the entire right-hand side, not just that. One could say that this entire right-hand side, first of all would be bounded from above by sum i equals 0 to n delta t times the maximum over all i, t n minus i. Right. One could take that step, right? Which is to say, that well since each of the tau, you know you have a different value of tau for each time step. Let's consider the maximum over all i, okay. But then we know that that maximum has to be bounded from above by C delta t to the power of k, because we have consistency. Right, so then we get to this step that I have here. Right. Okay, all right. Okay now well, what do we get here? We have delta t times sorry, the sum here goes from 0 to n. Sorry, right. Okay, we have delta t times a sum of n plus 1 steps, because i is going from 0 to n, of that quantity c delta t times c delta t to the power of k. 'Kay, so we can very well say that it is just like multiplying delta t essentially n plus 1 times, right. So this thing is lesser than or equal to t to the power n plus 1. Sorry, t at n plus 1, not t to the power n plus 1, but t at n plus 1 times c plus delta t, to the power of k. Okay. All right, just summing over those n plus 1 steps. Okay, right right. Now and now from here, we see however, we see that as delta t tends to 0, right, because k is greater than 0, right? Then as delta d turns to 0, we see that the righ- hand side also turns to zero, okay? All right, but. Limit delta t tends to 0 of c delta t to the power k is equal to 0 for k, greater than 0. Right, and where do we get this? Why are we allowed to see a case greater than zero? Once again it is the result of consistency. Okay, so what we see finally, is that the absolute value of our modal coefficient of the error is bounded from about by, essentially by 0. Right. Which means it, it, it still tends at 0, right, and the limit as delta t tends to 0, right. What we are seeing is that, that quantity is lesser than or equal to 0. Okay, so indeed we have conversions. Right, and that's the end of our proof. Okay? Make a quick remark here, which is that what we've seen here is a demonstration of the use of consistency. And stability. Implies convergence. Okay, which is a very standard it's actually a tier in numerical analysis. Okay it's called the Lax theorem. Okay? All right we're actually done with this entire topic of methods for parabolic problems. And what and, and the approach we've taken if you just to summarize is to carry out a standard spacial discretization using the finite element method. But to the time discretization using finite difference methods, and since here we're looking at first order of problems. We looked at the Euler family. We analyzed its stability understood the behavior of its higher order modes looked at consistency, and looked at how stability and consistency give us convergence. Let me make one more remark now about about the be, about the use of the different algorithms, okay. In particular, let me do this, okay. Let me say, let me say that here I have alpha, okay, and let's look at alpha equals 0, alpha equals one-half, and alpha equals 1. Okay, and just for connection to other things. Let's give this, this algorithms their names, right? So this is forward Euler. Right, this is the mid-point rule, or the Crank Nicolson method. And this is backward Euler. Okay let's look here at the stability. Let's look at order of accuracy. And let's draw a straight line. Okay better, and let's look finally at high order modes, right. Okay? Stability of forward Euler is conditional. Right, and we've seen the cons, the stability condition. Midpoint rule and backward Euler are unconditionally stable. Okay? Order of accuracy, forward Euler has order of accuracy 1, midpoint Euler has order of accuracy 2, backward Euler has order of accuracy 1. For high order\n",
      "chunk word length: 154, chunk char length: 835, chunk = modes, limit as lambda h delta t tends to infinity. Limit of A in the tends to infinity not 0. Okay forward Euler is let me just see it tends to minus infinity right, because nothing can be equal to minus infinity, right. Midpoint rule. Limit lender each delta t, tends to infinity is equal to minus 1. Okay? Oscillatory behavior. For backward Euler we see that limit lender h delta t tends to infinity. Oh, sorry, limit of A here. Okay, here too, limit of A equals 0. Okay, so it damps out high order modes, dissipates them away. So this is the, broadly speaking, the behavior of the three main are the three most commonly used members of this family. Depending upon the problem you choose your method, okay. All right, we're done with this segment and with this unit here when we return we will take up the problem of Elastodynamics.\n",
      "chunk word length: 1382, chunk char length: 7524, chunk = We're going to start a new unit, and a new equation now, we're going to go on to hyperbolic equations. We've looked at elliptic and parabolic equations, we now move on to hyperbolic equations. The canonical example here is elastodynamics, in whatever dimensions you wish. We're going to look at it in three dimensions, because we've left 1D far behind now. Okay, so, title of this topic of this unit is methods for hyperbolic. Linear of course. Pdes in vector unknowns. All right? And the example is linear, r i, linear elastodynamics. In 3D. Okay? So the setting is the following. I don't have my lego vectors today for my bases but I'll use my fingers okay. So that's my basis and this is the body of interest. When we considered linearized steady state elasticity. We were interested, of course, in how this ball would deform or how this continuum potato would deform. We didn't, however, consider the so-called dynamic effects, or the, or the, the effects that lead to wave propagation, all right? We couldn't, for instance, for that reason, also study the problem of this ball being actually tossed through space, right, and tumbling through space, and deforming perhaps at the same time. Right. We couldn't look at the time evolution of that problem. Right. Because we were looking at the steady state problem. So now we take away restriction, and look at the full blown elastodynamics problem. Okay. So Here we have the, the setting is essentially the same, as far as our pictures are concerned here, right. So we have our figures are concerned here, e1, e2, e3. We have a body of interest, right. This is omega. As we've done before, we have a decomposition of the domain into Dirichlet, into a Dirichlet subset for that particular component of the displacement field, right? And the corresponding Neumann subset. Okay? And this holds for i equals 1, 2, 3, right. X would be a point here, right. Which will be described by its position vector. Right, everything that we've seen from before holds, right? The decomposition of the, of the boundary into. The Dirichlet. Sorry, I got the union in the wrong position. It's the union of this Dirichlet boundary and the Neumann boundary. And, of course, those are disjoined, we know that. This is the empty set, and so on, right, we have all of this. This of course holds for I equals 1, 2, 3, right, three dimensions. Okay. I'm going to straightaway put down the strong form of the problem, right. The strong form of the problem is the following. Now, given data, ugi, t bar i. F i, right? In addition, we need some more data now. We need also other functions which I'm going to denote as u i 0, okay, and v i 0. Okay, we are going to use them for initial conditions. Alright so given all of this, and of course, the constitutive relation sigma ij equals C ij kl epsilon kl. We also know that we have the kinematics, right? Epsilon kl equals one half partial of uk with respect to xl, plus partial of ul, with respect to xk. Right? We have all of that stuff. Right, the only new things that you're seeing here are these two. Functions which I'm telling you right now we've been used in initial conditions. There, there, there is one more. We do need another coefficient, which I'm going to denote again here as rho, rho here is just the mass. Sorry, the mass density. Okay? Let me get rid of these arrows from here so that it's not confusing to think that they're pointing up from density. Those are the initial conditions. All right, we have all of this. The problem that we are trying to find is the fall, is to find u i okay, now it's a function of position and time. Okay. Right, and remember that i runs over one two three. Right, so everything that I've written on the first line, the first five functions is down here. Right. These functions are all just components of vectors. Right. Okay, so we want to find u i such that. Such that the following holds. Rho, second derivative of u i, with respect to time equals sigma ig comma j, plus fi, okay. In omega cross 0, comma T. Just as we did for the time dependent parabolic problem, right. We say that our pde must hold over the spatial domain and the time interval, 0 to T. Okay? Let me leave this here and then go on to the next slide to write out boundary conditions. Boundary conditions are no different. Right? For boundary conditions we have u i, at some position x and time t equals this given function ug for component i at positions x and t, and time t. Right. Now note that we're allowing here that Dirichlet data to vary with time. Okay this just allows us to have time dependent Dirichlet boundary conditions. Just as as we had for our time dependent heat conduction or time dependent mass diffusion. Right? We allowed the possibility that the Dirichlet conditions varied with time. Okay right at any point x belonging to a point in the Dirichlet boundary, right, for that particular displacement component. Our Neumann boundary condition or our traction boundary condition also is as before. Okay. Functional position and time for a point x belonging to the corresponding Neumann boundary. And note here that I'm continuing to use t bar for the traction function where as the t here is for time. Okay so it was, it was an anticipation of this final clash of notations that I have been using t bar for the traction. Okay, initial conditions. Our elastodynamics equation our pde for elastodynamics is a second order pde in time. And therefore, how many initial conditions do we need? Two, right? So we have u i at some position x but at time t equals 0, equals u, how do I write it, u i 0. Okay, which could be a function of position. We're allowing, we, we of course need to allow the possibility that well not just the possibility. We have to allow for initial conditions so, to be defined at every point. Right. So for every point x we have an initial condition of the displacement. Right. What it basically says what is the initial configuration of the body. Okay? So, this holds of, of for all x in omega. It is second order, so we need two initial conditions. Next initial condition is for u i dot x comma 0 equals the specified distribution of velocities. Okay, what this means is that at the initial condition we are saying that not only do we start out knowing where every point on this body is, right, that is the first of those initial conditions. What we are saying we must also know what the initial velocities are. Okay. That's the second initial condition. All right, this is it. This is our strong form. Okay. I'm going to straight away go ahead and write out the weak form. Right? The weak form. Of course, this is going to be the infinite dimensional weak form, but we know that going from there to the finite dimensional weak form is not such a big thing. The weak form. Okay? Given all the data that I have just put out there, right? I'm not going to repeat the data, okay? The weak form is find. U i belonging to S, okay. Where for our purposes here S consists of all u i such that u i equals u g i on. Okay. Find this such that for all w i, belonging to V where V consists of all weighting functions. Remember, w i is our weighting, our, our weighting functions. v belongs to w i such that w i equals 0 on. That Dirichlet boundary. Okay. Such that for all w i belonging to V, the following cond, integral condition holds. Now, integral over omega, w i rho, second derivative, second time derivative of u i. Plus integral over omega, w i comma j, sigma ij, dV, plus sorry, is equal to\n",
      "chunk word length: 225, chunk char length: 1236, chunk = integral over omega w i fi dV. Plus as before, the sum over spacial dimensions 1, 2, 3 now. Integral over the corresponding Neumann boundary, w i t bar i dS. Okay, that's it. Now, if you stare hard at this weak form, you should observe that it is obtained by just adding one term to our weak form for the steady state elasticity problem. Right? And that extra term is just this one. Observe furthermore that this term requires no integration by parts. Right, it's literally obtained by looking at the left hand side of our strong form, which is right here. Okay, look at the left hand side of the equation at the bottom. Multiply that by Wi, the weighting function. Integrate of the domain, right? We know that the rest of the stuff on the right hand side is what attracts integration by parts, right? Especially the divergence of sigma, the first term on the right-hand side in the strong form. Well, I know directly how the weak form arises, nothing new here, right? Just add in that, this, extra term on the left hand side. All right, we'll end the segment here. When we return we will simply write out the weak form, sorry, the finite dimensional weak form and go directly into the finite element of matrix vector recreations. Good.\n",
      "chunk word length: 976, chunk char length: 5207, chunk = Welcome back. We are now on to, developing methods for, hyperbolic PDE's, in, three dimensions in vector unknowns. And specifically the the example problem we're working with problem we're working with is that of linear elastodynamics in 3D. What we've accomplished for this problem is a statement of the strong form and of the weak form. We'll pick up from there and write out the finite dimensional weak form and plunge directly into matrix factor equations now. So, we are doing, like I said, the problem of hyperbolic PD is, in vector unknowns. And in three dimensions. Okay? Of course, this is, these are all linear, right? So we're doing linear hyperbolic PDEs and so on. Okay? And also the canonical problem we're talking of here is linear, the last two dynamics in 3D. Okay? I'll direct you write out the weak form, but the finite dimensional weak form, which you know, really requires very little, extra specification over the infinite dimensional weak form. Also, I will, spare us the details writing out of the data here, right? We're very familiar with all of that now. All right. So, the task we have here is to find U i sup h. And remember the h now indicates that we're talking of a finite dimensional problem. Find u i sup h belongs to the space S h which is a subset of the larger space S. And in, and particularly we are thinking of s, h as consisting of all functions u, I, sup h. And we expect these to belong to h 1 over the domain. Such that u, I h, equals u i given on the corresponding Dirichlet boundary. All right? The, the part of the Dirichlet boundary that corresponds to that particular spatial dimension, denoted by i, all right. We have this. Such that. For all whi belonging to vh. Which is a subset of we where we H consists of waiting functions WI sup H, also H1. Search that. WI Sup h vanishes on that corresponding Dirichlet boundary. Okay, so find u i sup h such that for all w i sup h belonging to v h, the following condition holds, right? The usual integral equation, except now that we have the one extra term, right? The term that's second order in time. Okay, so we have integral over omega, w i sup h rho, second time derivative of u, integrated over the volume, plus integral over omega, w h i comma j, sigma h i j DV equals integral over omega. WH sup I, sorry, sub I. FI, DB, plus sum over spatial dimensions. One to three in this case, or n, s, d in general. Integral over the corresponding boundary. W, h, I, t bar, I D, s. Okay? That is our finite dimensional weak form. All right? And I didn't state that anywhere. So, let me just do this. All right? So, this is r finite dimensional Weak form. Okay? All right. Now we know how things play out from here. Everything works out just as before, right? We're going to use the same basis functions we're doing 3D so you can think of trilinear hexahedral as being the simplest of those elements into which we decompose the domain. Everything works out just the same, okay? Also note that each of these terms is going to give us our, our standard sort of contribution that we know so well. Right, when we account for the fact that sigma H satisfies the constitutive relation, right, which makes it. Truly linearized elasticity. Right? What we get from this term is a very standard one that we've worked with at least once in great detail. Right? So we recall that this term gives rise to c transpose Kd. All right, and these two terms here give us. C transpose F. Okay, also, now. When you recognize that. The second time derivative and the second time derivatives are in need of DT squared there. All right. When you recognize that this term on the left hand term, essentially enrolls the waiting function W multiplying the second time derivative of the trial solution, right? And you work through things just as we work things out in the case of, the parabolic problem in 3D. What you will see is that this gives rise to, what sort of matrix? We have coming from 'w' and we're going to have a 'd' coming from to u right? In particular, we get a d dot d double dot because we have two time derivatives on u. What is a matrix that goes between them? It's one we've encountered. It's the mass matrix. Okay. So essentially, this is the form that the matrix vector equations take in the case of linear elastodynamics. There's just one detail I want to point out here, which is something to do with the construction of M, because of the fact that our vector d has at each node three scalar degrees of freedom. There is one little detail about the construction of M, which I'm going to show you right now, okay? So let me just say here that we're going to look at one little thing to do with the form of M. For this problem, okay. And in order to understand that, what we need to do is consider the, element integral corresponding to that term. Okay, so we consider integral over omega. W, h, I, row second time derivative of u, I, u, I, h sorry there should be an h there and an h here. D, v okay, we recall this is simply a sum over the elements. Integral over omega E, WHI row second time derivative of. Right. Remember this ok. So. Okay, we we're going to work with just that element in a group to clarify things.\n",
      "chunk word length: 1450, chunk char length: 7590, chunk = Okay, we'll do it here. So, the element integral. Okay, integral over omega e. W h i, rho, second time derivative. Right, this one. I'm going to go straight away into, writing it out, with our basis functions and summing over basis functions and all that sort of thing. Okay? So, we know that this is integral over omega e. Actually let me skip some more steps. Okay, because we're now such experts at this that we don't really need to write every single thing. Okay, let's do it this way. Fine, sum over a comma b. C a e i. Okay? And you recall that c a e i is each of those is the i'th degree of freedom at the a'th node of element e. Okay? This. Integral over omega e. N a, let's have the rho there. N a n b d v. And outside of this integral. Come the degrees of freedom that, are used to, construct the representation for the trial solution, right? So that will turn out to be d b element e degree of freedom i, okay? And what is implied here is a sum over our, spatial dimensions i. Okay? Now, the form that I have in parentheses on the right hand side. Okay. The everything including the integral, is what we identified previously to be the a b component of the consistent mass matrix. Okay, same thing. The same thing happens here, except for the fact that there's one thing we need account for. Which is that there is a sum over i here, okay. So let me just mention this here. Recall sum on i, okay. And what that does is lets us write this thing out as sum over a comma b, c a e i, integral over omega e. Rho n a n b d v, but now what I'm going to do is, I wrote that d v too close, I'm going to slip in here a Kronecker delta, okay? So allow me to do this and I will show you why I'm doing this. Okay? Since I've slipped in the Kronecker delta and given it indices i j. I'm going to turn the d b e i from the previous line, this one. Into d b e. J. Okay. All right. As a result, now when we put this all together what you see is that what you observe is that though the contributions to the, from the integrand are the same as we had for the, for the linear parabolic. Problem with the scalar unknown, right. What we've done here to counter the fact that we have vector unknowns, is to include this Kronecker delta right there, okay? All right, now when we put everything together, we get the following. Okay. We're going, we're now, in what I'm about to write, I'm going to abandon the explicit, writing of the sum over, over a and b. Okay? And in fact, let me give myself more room here. Just before I go ahead and write that I know that I've forgotten the second time derivatives on the d. To account for the fact that we have a second time derivative right there. Okay? Now everything's fine. Okay. So the way I'm going to write this by dropping the explicit sum over a and b is the following. I get, c one e right? Transpose okay? C two e transpose. So on up to c number of nodes in the element, e transposed. Okay, this one for instance, c two e transpose, is simply c two e one. C two e two. C two e three. Okay? All right. All of that now multiplying some big matrix which makes up our mask matrix. Okay? And out here we get our vectors, or vector of d one e double dot d to e. Double dot all the way down to d n n e. Element e. Double dot. Okay? And like I did here for c two e let me just point out that this is the little vector d two, e one, double dot, d two, e two, double dot, d two, e three. Double dot. Okay? All right, now. Note that for each combination of nodes from the c vector and the d vector, okay? We get a little matrix, okay? And that little matrix is obtained by writing out here we have here let me show you what we get here. Okay. We have here a little matrix which is row n a. N b d v over the element and let me actually make it n one n one to show you the very first combination okay? But this matrix, that integral itself, multiplies what you may call the three by three identity matrix. Okay. That's because it's this matrix that I've just written, that little sub matrix that I've just written. Which multiplies, which, sort of intercedes in the multiplication between c one e transpose and d one e double dot. Okay, this continues until we get another such matrix. Right, each of these, each of these blocks is a little matrix, right. Each of these is an integral over omega e. Of, you know, n something, n something else, integral over d v. Okay? All right? All the way down. So we have how many such blocks? All right? We have n n e such blocks in the column wise direction and n n e such blocks in the row wise direction. Okay. All right. All right. And what's important to note is that each of these little matrices is itself an identity matrix, right? And this is what our Kronecker delta does for us. Okay? And so the general term in here would be something like integral over omega e rho n a, n b, d v one zero zero zero one zero zero zero one. All right, you'd get other such block matrices everywhere. Okay. All right. And this matrix that I've written out here is our element mass matrix. Okay, in consistent form, okay, the consistent mass matrix. All right so this is an important thing to note. This is slightly different from what we saw for the linear parabolic p d, where each degree of freedom was, was, is essentially a single scalar. Unknown. Right? And, and in that case, this, the, this identity matrix essentially collapsed to the scalar one. Okay? The fact that we are doing, dealing with vector unknowns here, just, has, has essentially expanded that scalar one into a, into an identity matrix. Right? Three by three identity, that's all. Other than that, everything is the same. Okay it's probably useful for us to ask, now what are the dimensions of m, e? Okay, what will they be? Right the dimensions of m, e are number of nodes in the element, times number of spacial dimensions, squared, right? Those are the dimensions of m e. Right. Okay. That's it. This is our element mass matrix. The only thing that's different with, with regard to what we've seen before. Assembly works just as before. Assemble over degrees of freedom, common degrees of freedom across elements. Belonging to the same global degree of freedom. Implies that those corresponding matrix contributions from every. From, from, from the neighboring elements add up. Okay? All right. So let me just state that here. Assembly proceeds as before. Over global degrees of freedom. Okay? What we are left with, finally at end of the process, right, after we, we, we say that. Well this has to, our, our weak form has to hold for all c belonging to the appropriate, Euclidean space, right. What we get at the end of it, is the following set of equations, right. We get m d double dot plus k d. Equals f. Okay? When we were not doing elastodynamics, but essentially elastostatics, this first term was missing. Okay? It's just shown up now, right. Everything else is the same. We know that boundary conditions are buried inside here, right, including time dependent Dirichlet boundary conditions. That also works just as we saw earlier, right, in the case of the time dependent parabolic problem, okay. If we have Dirichlet conditions, they are time dependent. Well we know exactly what they are at every time, at, at any time, right they will go to the right hand side. What we would have here in addition are initial conditions, right. D at zero. Equals the, the vector of, well it essentially equals the vector form by taking every Dirichlet condition. At degrees of freedom lying, corresponding Dirichlet boundary. And, putting them all in a vector d\n",
      "chunk word length: 237, chunk char length: 1364, chunk = nought, okay? We did this for the parabolic problem. We need another boundary condition here, sorry, another initial condition, because we are second order in time. That's d dot at time e t equals zero, which we will denote as v nought. Okay? All right? And v nought, you remember, is simply, it's simply constructed of, u dot. Sorry, it's actually I think just the vectors, the value of the function v nought, right? At points x a, right? For every degree of freedom, a, that lies, sorry, for every single degree of freedom a. Right, for the initial condition has to be specified over the entire vector D. Okay? This is essentially it. Right, just as we specified initial conditions for our parabolic problem, we have two initial conditions now for this problem. All right? Okay, so that's really all we need to know about how the matrix, the matrix equations are obtained. The process is exactly the same. We get another mass matrix. There are some detailing mass matrix, it's at, at, for corresponding to every combination of degrees of freedom, it's slightly blown up. We have that little three by three identity matrix sitting there. And we have two initial conditions, right? And we know how to specify those initial conditions. So, so really, that's what we need to know. We'll stop this segment here. When we return, we will talk about time discretization.\n",
      "chunk word length: 1404, chunk char length: 7545, chunk = Welcome back. What we've accomplished in the previous segment is a explication hopefully of the matrix vector form of the problem for lin, of linear elastodynamics in 3D. Now we proceed with the time discretization and understanding a little bit about the about the methods that are used to solve this particular problem, which is also a ODE now. Okay, so let's start with the matrix vector problem. Okay. What we derived at the end of the last segment was the following. Md double dot plus Kd equals F, right. This is the second orderal d in time and we have initial conditions. D at time T equals 0 is d naught, and d dot at time T equals 0 is that vector, V naught. Now, we can proceed from here, but it's useful to include one extra element in here. And that extra element is a sort of throwback to the times when to the times really even before finite element methods became very popular in structural mechanics when it was common to write out matrix equations of this sort for for structures, right? People would, would, the notion of using nodes and degrees of freedom had, had already been established, especially in the context of structures like crosses and frames and so on. And in that setting of structural mechanics, it was common to include in addition to the mass and stiffness matrix that we see here, matrices that we see here, a damping matrix, okay? So and, and here's how this was done, okay? So including the effect of structural damping. Okay. The way this will be done will be to include a damping matrix of C. Again, I'm just following the standard notation that tended to be followed in this business. So, we have yet another matrix. All right, so we have C which would often be modeled using what is called Rayleigh damping. And this was done in a very sort of simple, very empirical manner by taking some constant, a, multiplied with the matrix M plus some other constant b, multiplying the matrix K. Just like that. Okay? All right? Empirical, but it was found to work. And, there are other reasons why this works. We don't need to go into those reasons. This is the model for what is called Rayleigh damping. Okay. Where a and b are constants. All right? And that was it. There was no attempt to try and derive these this new damping matrix from any more fundamental partial differential equation. That can be done but, but, but it was not necessarily done in, in, in writing this out, which is just explicitly written out in this form. Okay. And then the form, the damping would be, would be included in, was the following. So now we would get the equations of, elastodynamics. With structural damping. Right. And that equation would turn out, would be Md double dot plus Cd dot plus Kd equals F, all right. Where the idea of damping was, that this is some sort viscous damping essentially that was being modeled, and you see the effect of viscosity. If you're familiar with that sort of physical phenomenon and the fact that you have a single time derivative on this extra term that's been introduced to the problem. Okay, that's just like a first, that is indeed a first time derivative, a single time derivative on the d vector. Right, plus boundary, sorry, plus initial conditions as usual. Okay? This is essentially the model. Now, from here, one can go on and write the time discretized form, just as before, right. From here, what we do is for time discretization. All right, we do exactly what we did before, which is to say that our interval, zero to T we write as the union of all these time intervals, of t0 to t1, so on all the way up to t sub N minus 1 to tN. Right? It's the union of all these intervals, right, of each of these subintervals where t naught, in the way I've set up the time interval, t naught would be 0. Tn would be capital T. Okay? We have everything just as we knew from before. All right? And then we say again that d At n is the time, discrete. Approximation. Write approx for short. The time discreet approximation to d at t n. Okay, just as we did for the parabolic problem. Okay, and then we get our time-discretized, our time-discrete matrix vector equation. Okay. Let me go back, just for a second to have you look at it. Right, we have it here. So, you see M d double dot plus C d dot plus K d. Now d double dot, because d of course displacement, d double dot is essentially the acceleration. Right, likewise d dot is indeed the velocity vector, right, at the global degrees of freedom. And d is the displacement factor. With this is mind, the time-discrete matrix-vector equation is often written as M at n multiplying a n plus one, where a n plus one now is the acceleration, right. The approximation to the acceleration at time t, n plus one. Plus C v at n plus one, v being the velocity, plus K d n plus one equals F at n plus one. Okay? With initial conditions now being that d not and v not are known. Okay? All right. Now, the family of algorithms that is commonly used to solve this equation, this time-discrete form of the equation is what is called the Newmark family. Newmark family of, algorithms. For second order ODEs, right? They're second order because they're second order in time. Okay? The way this family works is the following. For, now, we need to have some parameters for this family. Just as for the Euler family of algorithms for first order equations, we have our parameter alpha. Here, because these are second order ODEs, we need two parameters, it turns out, and one of those parameters I'm going to denote as gamma. And gamma belongs to the closed interval zero comma one, just like alpha did. And the other parameter I'm going to write as twice of beta, where twice of beta now belongs to the interval zero to one. Alternately beta belongs to the interval zero to half. Right? The closed interval zero to half. All right? Now, with this in place, here's how the Newmark family works. It says that d at n plus 1 equals d at n plus delta t times v at n plus delta t squared over two, times, 1 minus 2 beta, multiplying a n. That's the approximation at the acceleration at time tn, plus 2 beta, multiplying a n. That's the approximation to, to the acceleration at time t n plus one. All right? And then because it's a second order algorithm, we need something for v n plus one as well. V n plus one is equal to v n plus delta t times one minus gamma at a n, plus gamma at a n plus one. Okay? Those equations together with our time discrete matrix factor equations written above here, and of course initial conditions, right? Initial conditions here are just that d not and v not are known, okay? This summarizes our, family of algorithms for linear elastodynamics, okay? Now, let's talk about solution techniques, 'kay? The solution technique, that I'm going to talk about, I'm going to talk about a single approach, not two approaches as we did for the parabolic problem. The method I am going to talk about is what's called the a method, the a being for acceleration. Okay, and here is how it works, we again define predictors and correctors. Right? We say that d n plus 1, tilde, equals d n plus delta t v n plus delta t squared over two. One minus two beta a n. Okay? That's the predictor for d. The predictor for v is v n plus delta t. I don't need anything in the denominator, it's just delta t. Times 1 minus gamma a n. All right? And just as we did before for the parabolic problem, we've looked at the update formulas for d and v and simply extracted out those parts of the formulas that come from everything known at time t n. Okay, so\n",
      "chunk word length: 593, chunk char length: 2978, chunk = these are our predictors. The correctors are. Right, those are predictors, and the correctors are the following, right. The predi, the correctors are obtained by simply writing d n plus 1 equals predictor Right, plus corrector. Now the corrector for d n plus 1 is delta t square beta a n plus 1. And the corrector for v is. This one, right? These are the correctors. Right, and what I've done now is write the corrector step for both of them. The a-method essentially, just like the, the methods in the case of the parabolic problems, the a-method is obtained by substituting these corrector steps in the original equation. Okay? So what we get is on substituting these corrector steps. Right, on substituting we get the following. M times delta t square beta plus C delta t gamma plus sorry. I'm getting ahead of myself a little, so let, let me just rewrite this line. I'm trying to skip a couple of steps, and I realized I was already making errors, so I'll just back up a little. Okay, so when, when we substitute these correctors, what we get is the following. We get M a n plus 1 plus C multiplying, C multiplies v n plus 1. So we get the predictor plus corrector. And for K, K multiplies d n plus 1. So again, we get predictor plus Corrector. Okay, this is the entire left hand side. All of this equals F at n plus 1. Now this is the so-called a-method, and by using that, those predictors and correctors, what we've done is to rewrite the equation entirely in terms of a at n plus 1 and predictors for d and v. All right? So this essentially then lets us rewrite this as M plus C delta t gamma plus K delta t square beta, all of this multiplying a at n plus 1. Equals F at n plus 1. And then the terms multiplying the predictors are just moved over to the right-hand side. And why can we do this? Right, it's because the predictors are known, right? They depend only upon the solution at n, which we always assume we know when we construct these time-stepping algorithms. So we get here, right, we get minus C d n plus 1 tilde minus K d, sorry. Sorry, it's C v n plus 1 tilde. C v n plus 1 tilde plus K d n plus 1 tilde. All right? That's our method. Okay, we can now go ahead and invert this, solve for a, once we have a at n plus 1, using our corrector steps we get back d n plus 1 and v n plus 1. Okay? The only thing we need in order to sort of start up this algorithm is a at 0. Okay? In order to get A at 0. Just use the equation at 0. All right, and by equation here I mean the time-discrete equation. And that is M a 0 equals F at 0 minus C times v at 0. This works because v 0 is known, right? It is just the initial condition. Minus K d at 0. d 0 is also known. All right? v 0 is known, d 0 is known. Okay? Okay. So here we have it. That is our standard solution approach for this problem. We can end this segment here. When we return, we will start our analysis, and that analysis also is going to be based upon our approach of modal decompositions. All right?\n",
      "chunk word length: 970, chunk char length: 5176, chunk = welcome back in the last segment we set up the time discretized equation for linear elastic dynamics and also looked at a canonical solution of technique for it right what I call the a method we get into analysis now as for the parabolic problem the analysis is based upon a suitably chosen eigenvalue problem and and the way we carry out this analysis is to use I can value problem to construct a decomposition of our OTE in two modes ok so our analysis is least on the eigenvalue problem space in the following eigenvalue problem on the good omega square m side equals case I alright where Omega square u we recognized to be the natural frequencies for each army goes the natural frequency at natural frequency of oscillation right in the context of our problem because each Omega would be a natural frequency of oscillation okay with this problem in as a basis and then as before proceeding to construct em or target can proceeding to consider I can functions are right conductors side that are M orthogonal right so the size r m or talking I convectors ok when we do this right we and we proceed just as we did for the for the parabolic problem right so we're right by doing this we can now do things like saying that any vector like the d vector can be constructed through a sum over l.d soup l side and right where each side L is an eigenvector again these vectors are mr Turner ok should probably site here right and say that l equals just as we saw before 12 total number of degrees of freedom in the problem ok we do this foodie and of course we can do this for every other up for other vectors that show up in the problem right we can do this will be an A and so on okay are we take this approach and what we see is that we get when we take this approach we get a reduction to ndf single degree of freedom model problems right or moral equations all right now people to go back and write this full-time exacto de réduction 2nds single degree of freedom more equations and we just continue here see of good time exact OD alright and the form of those equations is the falling right to the time exactly remember because it's pretty much the way we did for the parabolic problems right remember we wrote out the reduction for the for the time exactly d and then extended that to go time to strategize rudy is over the time exactly we would get dl double dot plus 2cl army girl each l arm d l dot plus Omega each square d l equals 0 if we consider the homogeneous case right remember the homogeneous cases when we have zero right hand side now r omega each l is just reminding us that that we have natural frequencies but those natural frequencies because they depend upon our matrices which are obtained by spatial discretization write those natural frequencies also reflect the effect of spatial discretization ok so these are what we will call the finite-dimensional or species specially discretized natural frequencies ok just as for the parabolic problem we considered lambda each sub L right which is simply the which simply was the effect of spatial discretization upon the eigenvalues it's something him right so that's America each l \\& 4 c sub L which actually properly should better be written as a see each subnet ok there's an H and agile because that also does reflect the effect of spatial discretization okay see each sub L is simply our remember the constants we use to define really damping it's those constants divided by the corresponding natural frequencies ok and this is what is called modal damping ratio ok all right now the way we proceed with our analysis is the following because we have a second-order de we are we rewrite our second order o de using a technique that's very well established in ordinary differential equations we write it as 2 first-order Cody's right so we rewrite the second-order OTE as to first order Cody's alright and in order to do this we say they're right we are now looking for the solution of a vector it was just the two vector where y is d and d dot ok alright it's not difficult to rewrite that single low second-order only in terms of this right arm and in fact what we will also do is that as we recall from the case of the parabolic problem ok for what we get for the time discretize problem is the falling maybe get a mobile be composition of the time discretized problem also write in model form ok and that problem is the following weekend II and +1 Modell plus 2c each sub L Omega H sub L the n plus 1 mode and plus Omega each arm and square d and +1 right where each of these is a moral coefficient the elmora coefficient of the corresponding vector ok this equals 0 is the homogeneous problem ok alright right and now the relations between the model coefficients between the DN plus 1 VN plus 1 and n plus 1 will be satisfied with those those model coefficient satisfy the same conditions that we obtained from numark family right for the full vectors okay right so we get new mark family equations relate dn1 at al v sorry it's not the vector right it's the mode v + + 1 @ l and he and plus 1 at ok alright those are the equations that involve the coefficient though are the parameters gamma and Vito\n",
      "chunk word length: 1230, chunk char length: 6722, chunk = Okay? And then, when we do this reduction of a second order ODE to two first order ODEs, for the time discretized problem, okay, we get the following form. We get the following form. We get y at n plus 1, okay, equals A, which is now our amplification matrix, y at n plus L at n. Okay? All right, and let me tell you just once more, that Y at n plus 1, is d at n plus 1, v at n plus 1. All right? This is of course dn, vn. And A here is a 2 by 2 amplification matrix. Okay? It plays the same role as our scalar amplification factor for our parabolic problem, okay? And the definition of A, as well as the definition of this two vector Ln, reflects the reflects the Newmark algorithms right, with the gammas and betas and everything. Okay? We can work out all these details, but it's just tedious detail, which we are not going to truly use. Okay? All right, so then this is our time discretized equation, now written in, in the form of two first order ODEs instead of one second order ODE. Okay? I'm going to give you a summary of the stability results right now. Okay, stability. Okay? So, if 2 beta is greater than or equal to gamma is greater than or equal to half, we have unconditional stability. Okay? If on the other hand, gamma being greater than or equal to half, beta lies between 0 and gamma over 2. These conditions together give us conditional stability. Okay? Now remember, we are talking about a single degree of freedom modal equation, okay? Where, because of the fact that we have a second order ODE, what we are solving for are dn plus 1 and vn plus 1, right, both of those modal coefficients. The conditional stability holds when omega h, right, which is the frequency, corresponding to that particular mode, right? Remember, as before we're suppressing the modes, okay? Actually, even back here, we are already suppressing. Mode, number, index, all right, L. Okay? So that's going on here as well. So when, even though I've just written omega h here, it's really omega hL for each L, right? We need to consider this for each L, okay. So the stability condition also requires that omega h delta t, should be lesser than or equal to a quantity that I'm going to denote as omega critical, okay? Where omega critical. Equals the following. Ch, and again, this is Ch sub l really, but for every mode, okay? C h times gamma minus half, plus gamma over 2 minus beta, plus c h gamma minus half, sorry, c h squared gamma minus half, the whole square. All of that to the power one half. The whole thing divided by gamma over 2 minus beta, okay? That's the critical frequency, okay? What we see here is that there is the effect of damping. Okay? And, what we also observe is that the effect of damping, right? And, and we have damping when c h is, greater than 0. The effect of damping is to increase the critical frequency, okay? So we have the undamped critical frequency. Let's say omega critical u for undamped, okay? This is got by setting c h equal to 0, okay? And it is just gamma over 2 minus beta to the power minus half, okay? Right? I just want to point out that this undamped critical frequency is a lower bound to omega critical, all right? So what we're seeing is that the undamped critical frequency is lesser than or equal to the actual critical frequency, when you have some damping, okay? All right. Okay? So what we see as well is that omega h delta t, which needs to be less than the, than the critical frequency, is, it satisfies this sort of a condition. Okay? What I mean by saying this is that actually, let, sorry, let me not write this line. This is, is sort of attempt to state a condition. Let me not write that, that equation, it, it can be misinterpreted. Instead let me say this. The undamped critical frequency is a more stringent condition. On omega h delta t, right? It's really a condition on delta t. So what we are saying is that instead of saying that it has to be less than the actual critical frequency, right? Instead of using the condition that I have at the bottom of this slide, right? If instead of that, we were to say that, well, omega h delta t has to be less than this quantity, okay? Then we are actually imposing a more stringent condition upon our algorithm, our time integration algorithm, okay? Okay, with this in hand, I'm just going to list out properties of some sort of canonical, almost classical, members of this Newmark family, okay? And, I'm going to do this part in a table, where I'm going to list the method here. I will say what type it is. And by type I mean is it implicit or explicit? I will list here beta, gamma, let me see, what else do I need to list here, right? I will list here the critical frequency for stability for the undamped case. And finally, I will also write here the order of accuracy. Okay? So, the methods we are going to consider are the following, the first one we will consider is the, what is sometimes called the Trapezoidal Rule. We consider four methods, okay. I'll write them out first, trapezoidal rule, think linear acceleration. We have the average acceleration. These are all names of methods. And finally we have the central difference method. And, when I say central difference and trapezoidal method, trapezoidal rule, note that they, they will not be the same as what you may be familiar with from first-order ODEs. Simply because we are using terminology here that has been established for second-order ODEs, okay? All right, all of these methods are implicit except for the, except for the central difference method. Okay? Now, stability. They're all for, they all use gamma equals half, okay? Now, the trapezoidal rule uses beta equals one quarter. And because this combination of beta and gamma makes it unconditionally stable, there is no question of what the critical frequency is for stability, right? It's unconditionally stable, all right? So there's nothing to say there. Linear acceleration uses one-sixth. And what happens here is that the the critical frequency is 2 root 3, okay? Average acceleration uses beta equals 112, gamma equals half. And if I remember the undamped critical frequency is square root of 6. The central difference method finally uses beta equals 0. And the undamped, actu, critical frequency here is 2, okay? For order of accuracy, all of these are second order. Okay? One caveat though is that the explicit you get a truly explicit algorithm only for M and C being diagonal. All right? It's just a summary of some of members of the family. As you can imagine, because we are talking of a, of integration algorithms of second order ODEs, the numbers of this family are, are, are a few more, right? It's a fairly large family. Okay, we can afford to stop this segment here.\n",
      "chunk word length: 1462, chunk char length: 7604, chunk = All right. What we saw on the previous segment was a fairly quick statement of the stability conditions for the Newmark family, right? And we saw the explicit results for some of them. We didn't derive any of them. What I'd like to do in this segment is outline the, the sort of analysis that leads to those, to those conclusions. We won't get into a completely detailed step-by-step derivation of the results. Unlike what we did for the parabolic problem, okay? But we'll sketch out the, the approach. Okay, so the stability analysis here, as in the case of the parabolic problem. Is based upon examining a particular object. Can you recall what object we examined in the case of the parabolic problem? We examined the amplification factor. Here, too, we have an amplification factor, but it is a matrix. Okay. So, the stability analysis is based on, an eigenvalue analysis of the amplification matrix, all right? Okay. And, the way we proceed with this is to, if I remember that the amplification matrix is what we denoted as A. Okay and you also recall that the amplification matrix is what showed up in this formulation of the problem as here, y n plus 1 equals Ayn plus Ln when we consider the full n homogeneous problem, okay? So what we're talking about is analyzing this, and all our detail of the Newmark family is sitting inside there, right? The particular values of gamma and beta we've chosen, and so on, okay. Here is the condition, okay? We define what is called the spectral radius. The spectral radius of A, right? And we denote that as we've used rho quite a bit, so let me use something else here. Let me just say r, okay, spectral radius A, r sub r function, okay, okay? This is defined as the maximum over i, okay? The maximum over i of lambda i of A, where those lambdas are essentially the eigenvalues of this two by two matrix. All right, and because it's a two by two matrix, of course, i just runs over one and two, okay? That's probably not even worth using an index there. Okay, so let's say that max i equals 1,2 right? Essentially it's the maximum eigenvalue, okay? Not just the maximum eigenvalue, but actually it is the magnitude of it, okay? Where we have accounting for the fact that the matrix A may not always be symmetric, and therefore, it could have complex eigenvalues, right? Accounting for that, we write our spectral radius as being defined as the max i equals 1, 2. Now, that magnitude that I wrote up there is properly the square root of the product of lambda i and its complex conjugate, which is going to be denoted as lambda i of A bar, okay? Where that bar implies the complex conjugate. Of lambda i A, okay? That is our spectral radius. Now the condition for stability requires that. All right, it requires that r, the spectral radius, should be lesser than or equal to 1. Okay, the spectral radius is defined there, should be lesser than or equal to 1. I'm going to say a little more about this condition. We have r can be lesser than or equal to 1, if lambda 1 and lambda 2 are distinct. Okay, it turns out, that if lambda 1 and lambda 2 are distinct the condition that we get that r can be lesser than or equal to 1 involves the fact that the eigenvectors of A. Are linearly independent. Okay, on the other hand r has to be strictly less than 1, right? Not lesser than or equal to 1, it has to be strictly less than 1, if lambda 1 equals lambda 2, right? We have repeated roots, okay? And in this case it turns out that the eigenvectors. Of A, are linearly dependent. Okay? I'm going to do very quick demonstration of why this is the case, okay? So, let's look at the two cases, okay. Let's first look at what happens if they are linearly independent eigenvectors. Okay. If they are linearly independent eigenvectors, then let's look at what happens for the homogeneous problem as we go from one time step to the other, okay? Essentially, what we see is that yn plus 1 equals A yn, right? But then yn is equal to A yn minus 1, and so on, right, yn minus 1 equals, tatatata, right? Goes on. So what we are seeing here is that with every step is getting multiplied by itself, right? If you just make these substitutions in here, we see that, right? Okay, so what we are seeing is that, after a certain number of steps, we're seeing that yn plus 1 equals A to the power n plus 1 times y0. All right? So, what we need to worry about is what is happening with A. All right, as it gets multiplied by itself, what are the powers of A? If we have linearly independent eigenvectors, one can show that A, okay, can be written as some matrix P, times a two by two matrix, which is lambda 1, 0, 0, lambda 2. P inverse, okay? As a result, we get from this, we get A to the power n equals P, lambda 1 to the power n, 0, 0, lambda 2 to the power n, P inverse, okay. So now you note that even if lambda 1 were equal to 1, all right this sort of a form stays well bounded, okay? All right, okay. This sort of a form stays bounded, right? So the amplification that is applied to the initial condition to get say, the nth time step solution does not get unbounded, okay? Things are different if you do have, if we have linearly dependent eigenvectors. In this case, the best we can do is write A as, let's say, some other matrix Q, two by two matrix, lambda 1, 1, lambda 2, Q inverse. Okay? This is the case of the linearly dependent eigenvectors, okay? Now, if you go through the process now, and calculate a to the power n. >> Okay, what you get is a form where you get Q, lambda 1 to the power n. You get lambda 2 to the power n, and all is looking good except for the fact that here you get n lambda, well lambda 1 is equal to lambda 2 here, okay. So it doesn't really matter. Let me just do this. In the case of linearly dependent eigenvectors, you have lambda 1 equal to lambda 2. So, you get n times lambda 1 to the power n minus 1, Q inverse. And now, do you see a problem as n gets large? Okay, what you note is that if you look at what happens here? And you have lambda 1 equal to 1. Okay? You see that the off diagonal term becomes n, okay? Right. In that case this term becomes n. Right? And then as you go to higher, and hard, higher, as you go, as you advance in time steps you have this sort of gradual sort of tendency towards unboundedness. Okay, so what happens in this case is that the off diagonal term. Diverges, as n. Okay? All right. Further analysis of this problem is based upon essentially solving for lambdas, right? So. Solving for the lambdas, right? The equation that we need to solve for the lambdas is the following. Lambda square, remember this is just the characteristic equation that you use to solve for the eigenvalues of a matrix, all right? So we will write it as lambda squared minus 2, A1, lambda plus A2 equals 0, okay. And here A1 equals one-half trace of A, and A2 equals the determinant of A. Okay? All right? With this form, it's just a simple quadratic equation, right? So we get lambda 1, lambda 2, are equal to what is it? A1 plus or minus square root of A1 squared minus A2. I believe, let me just look at that. Yeah, okay, so these are lambda 1 and lambda 2. All right. Now, here is the, sort of stability condition again written in terms of A1 and A2. Okay, because of course stability depends upon the values of lambda, but then since we have lambda 1, lambda 2 given by these conditions for A1 and A2, we can write it out in terms of A1 and A2. Okay? The conditions that we get are the following. We get minus A2 plus 1, divided by 2 is lesser than or equal to A1 is lesser than or equal to A2 plus 1 divided by 2 if\n",
      "chunk word length: 232, chunk char length: 1137, chunk = the magnitude of A2 is less than 1. Okay? All right. And. Okay. And otherwise, we get minus 1 is less than A1 is less than 1 if the magnitude of A2, sorry, it's not just the magnitude of A2. It's A2 itself. Here too, it's just A2. If A2 is equal to 1. Okay, one can plot this thing up in this result up in an A1, A2 space, and here's what we see. If this is A1, and here we're plotting up A2. Okay? Let me see. I think, I'm going to mark some critical points here. These are the points 1, 1, minus 1, 1, and down here, I have the point 0 minus 1. Okay, in this, we have between 1, 1 and 0, 1, that line, and here we have that segment, okay? The first condition, this one okay, holds everywhere except for that line. Right, because that dashed line is A2 equals 1. Okay, so this sort of inverted triangle that I've drawn is the acceptable region. Okay for stability, if A2 is less than 1. Okay, if A2 equals 1, right, then what it says, is that on that line, it's got to eliminate, it's got to be, you've got to leave out those two points. Okay? That is the region of stability that we have here. Okay. Okay, we can end this segment here.\n",
      "chunk word length: 1382, chunk char length: 7525, chunk = All right. In the previous segment what we looked at was, again, a sketch of the way we would approach the stability analysis for our modal equations for linear elastodynamics, all right. What I'd like to do here is actually take a step that we also took in the case of the parabolic problem which was to go from stability. Our understanding of stability to also very quickly cover high, high order models, okay? So, we've derived stability conditions on A. All right? Our amplification matrix. And in particular, what we said was that if A1 equals one-half of the trace of A. And A2 equals determinant of A. What we found was that the conditions are based upon lambda 1, lambda 2 equals A1. Plus or minus square root of A 1 squared minus A2. Okay? This is how we determine lambda 1 and lambda 2 from our characteristic equation for this matrix. And then we impose conditions at lambda 1 and lambda 2 can be, need to be lesser than or equal to 1. We've also understood when they have to be strictly less than 1. All right. So these are the stability requirements. Right, so we also said from here r, which is the maximum of the square root of lambda i and lambda i bar. Right? Where the lambda i bar refers to the complex conjugate. Right? Okay, and then we say, finally, r has to be lesser than or equal to 1, right? And we've understood when it can, when it needs to be strictly less than 1. All right, so this is how we go about our stability analysis. Now you recall that when we looked at the parabolic problem, we looked at the, the amplification factor, and also used it to tell us something more about the high order modes. Okay. And we saw that the different members in that case of the Oiler family did different things to high order modes. Okay. It emerges that in the case of this problem, also the amplification factor plays a role. And in order to damp out high order modes, okay. In order to damp our high order modes, high order modes. Are decaying right? If if our eigenvalues become purely real, right? Sorry it's the other way, sorry. High-order modes are non-decaying, sorry, are non-decaying, if lambda 1 and lambda 2 are purely real. Okay? And therefore in order to damp out the higher-order modes, what we need is that the discriminant of this relation, right, should be negative. Right? So what we need is that A1 square minus A2, is less than 0 for damping or digging, of high order modes. Okay, so this is really a sort of parabolic condition and if we go back and now plot out this parabolic condition on our, A1 A2 space that we introduced at the end of the last segment, here is what we'd see. That is 1, 1. This point here is minus 1, 1. And this point here is 0,1. Okay. What we did do last time was observe that our stability condition gives us this sort of triangular region. Okay. In addition to this, it emerges that our condition for damping out of higher order modes requires that there is a parabola which takes on this shape. Okay. So we need to remain within that parabola, not outside of that parabola. So, this is the region in which we get damping of high water modes and okay according to that condition. Right, so it is in that region that we get, damping of high order modes. Okay. Let me see if I can get back to my original color here. Okay, the effect this has is the following. You know, the stability condition for the condition for, for, for stability independent of time step series, right? So, so really the unconditional stability, okay, condition is the following. It is that beta is greater than or equal to. Gamma over two. Okay? So this is unconditional. Okay? It doesn't quite make sense to say stability condition and then fit an unconditional stability condition, so let me say stability. Requirement here. Stability requirement. All right, all right, so this is the unconditional stability requirement. It turns out, however that we need, beta to be, not only greater than gamma over two. But, this requirement of damping out of higher order modes, requires that beta should be greater than or equal to gamma plus half the whole squared to damp out higher order modes. Okay. And one way in which this manifests itself is in the effect on, the spectral radius. Here's what happens, right. So the effect of, of this requirement on the spectral radius is the following. Okay, turns out that if you plot out the spectral radius, here it's r, and here we look at, delta t times, 2 pi, omega h, okay, and this is just like we plotted in our, previous, study of the parabolic problem. We plotted up the amplification factor on the hori, on the vertical axis, and on the horizontal axis we plotted up lambda t times lamb, sorry, delta t times lambda h, which was the eigenvalue of that problem, all right? In this case, you're plotting up delta t times two pi omega h, which is effectively like delta t divided by the time needed of oscillation or something, okay? So for r, if this is the value 1. Right? What tends to happen is that as delta t over two pi omega h increases, the entity times two pi omega h increases, if you have values of gamma or, or beta that are, you know, we satisfy the unconditional stability requirement but they have a, they're not as big enough as to satisfy the second requirement, right, of being greater than gamma plus half the whole squared. Okay, here's the sort of thing that happens with the spectral radius. And along this point is where you get, real, fully real eigenvalues. Okay? So you get real eigenvalues around here. All right, and this is for, gamma by two lesser than or equal to beta, lesser than or equal to gamma plus half of the whole squared. Okay? If on the other hand you have, beta exceeding gamma plus half the whole squared, then you tend to get, a behavior which looks like this. Okay? This is for, beta greater than gamma plus half the whole squared. Okay, and this is what leads to damping of high-order modes. Okay. So, that is what we need to know about, how high-order modes depend how, how our analysis of high-order modes follows from our stability analysis. Perhaps the last thing I want to say about this particular problem is how we go about getting to the, to a notion of conversions. Okay. And before we end, and, you recall that stability and consistency are the two requirements that lead to convergence. Okay? Now, in order to understand consistency of this problem, we go back to looking at this form of the, evolution equation for the time this could rise, modal equations, okay? A is our amplification matrix. Remember y is our two vector which has the displacement of the velocity in it. Right? And Ln is what we get from the forcing. Okay, so now we're going back to the inhomogeneous problem here. Okay, again, this is exactly the way we set things up for the parabolic problem. All right. Where Ln was the effect of our putting back the forcing. Right? We, we turned off the forcing when we studied stability. We turn it back on when we study consistency. Okay. So, this is the time-discrete problem. Okay, the time-discrete modal equation. Now, for consistency, we say that well, if instead of the time-discrete, solution, we were to look at the time exact solution, and just plug it into the equation we've written above here, right? We would get y of t n plus 1 equals A, our amplification matrix, times y and tn, right? Y of tn plus one and y of tn are the corresponding exact solutions. Okay? We get a plus Ln, right? But in general, the exact, the time exact solution does not satisfy\n",
      "chunk word length: 547, chunk char length: 2776, chunk = our finite difference equation. Right? So we get, in addition, a term delta t times now a vector tau depending upon tn. Its a vector tau because of course y is a two vector, right? So tau also has to be two vector. Okay? So this is the time-exact, model equation. Okay? Now, consistency requires. That tau be written as again, a, a, a 2 vector, c. You can think of it as c being two constants, c1, c2, okay. c times delta t to the power k. 'Kay, where, or we could write this as tau1, tau2. Equals both functions of t n. All right? They are equal to c1, c2, times delta t to the power of k, where k is greater than 0, and c1, c2 are constants. Okay? Right? And, when I listed the various some of the members of this family, I said that, that the, their order of accuracy was 2. Okay? That order of accuracy is given to us by k. Right? So, when a couple of segments ago, I listed a, several members of this family and said that they all had order of accuracy 2, what it means is that in this consistency condition, the order of accuracy, k, is equal to 2 for all of them. Okay? Now, when we use consistency and stability to get a convergence, right, the so called Lax Theorem, right, which is that consistency and stability give us convergence of our time integration algorithms for this problem, right? That convergence condition is dependent upon writing out our error. E again is a 2 vector, right, because we're looking at the error in the displacement and the velocity, right, for every mode, okay? So, what we get is that e at n plus 1 equals A to the power of n plus 1, times the error at, at time t equals 0, all right. And, the error at time equals 0 is also a 2 vector because it has the error in the displacement and the error in the velocity. Okay? Plus sorry, it's minus sum i going from, from 0 to n. Right, you get i going from 0 to n, delta t, A to the power i, okay? The tau vector, which is the 2 vector I wrote just above here, at tn. Okay, this is the condition that we need to evaluate, we need to work with for convergence, and, everything works out just as we saw for the parabolic problem. Right? It's just that we have to deal with a matrix vector equation and then matrix vector inequalities here, but things essentially work out. Using what we know about stability and what we have written out for consistency, we can prove that limit of en plus 1 equals 0 as delta t tends to 0. Right, its limit as delta t tends to 0 of en plus 1, right, is equal to 0, all right? You get a 0 back to there, right? So you get the error in the displacement and the velocity tending to 0. Well, that's what we need to know as far as the analysis of this problem is concerned. And, that also concludes our rather quick study of methods for linear elastodynamics in 3D.\n",
      "chunk word length: 288, chunk char length: 1694, chunk = There were a couple of errors in both work in this segment. The first error appears on this slide where I talked about how for damping of high-order modes, we need to go a step beyond merely, looking at unconditional stability. The inequalities that we are talking about in this case, appeared here, and the error is in this very last, inequality. I wrote it as just gamma plus one-half the whole square. It should be gamma plus one-half the whole square, divided by 4, okay? That gives us the correct condition to be satisfied for damping of high-order modes. The next error appeared two slides later. There we go. And it appeared in the way I labeled that equation. I called it as it appears there, the time-exact model equation. That is not quite correct. The equation I've written out there is obtained by substituting the time-exact, or the time-continuous solution into the time-discreet model equation, okay? When we do that, we find out, we discover that the time-exact solution does not actually satisfy the time-discreet model equation. Instead, we're left with this extra term here, right? And it's on the base of this term that we talk about consistency and accuracy of the method. So, properly this equation should be labeled like it should be labeled just what I called it, which is the time-exact solution, substituted into the model equation, okay? So, let me call it that time-exact solution. S-O-L-N, short for solution. Okay? Substituted. In time-discreet. Model equation. Okay? When we do that, we get this extra term. Like I, reiterated, a minute ago, and that is the basis for our analysis of consistency, and accuracy of the method. With that things workout consistently.\n",
      "chunk word length: 1377, chunk char length: 7447, chunk = So if you've got as far as this particular video, I expect that you have also taken the time to watch many of those that preceded it and in the process have learned something about the finite element method. You may also have tried the quizzes and hopefully some of the programming assignments as well. If you've done a good proportion of all of those, you are actually pretty well prepared to go on and do other things with the finite element method, and also branch out to maybe learning about other topics which could then use the finite element method. Recognizing, of course, that what we've done so far is essentially meant to be an introduction at about the graduate level in any university. What I'd like to do with this last Lecture is point you to some resources out there, some of which you may already have known and maybe you know about all of these resources. But I'll just point you to some of them as things that you could go on to do having done all this work to learn the finite element method. So I'll start with this slide and this information in front of me. You see, I've talked about telling you a little bit about open source finite element codes and also about some related course material that's out there. As far as the open source finite element methods are concerned, you see these two links in front of you, The first is for Deal.II which of course many of you have probably tried out already, maybe for the assignments or maybe even on your own. And the other is for a different collection of softwares which I will also tell you about. So here we are on the Deal.II website and like I said many of you may already have seen it and tested things out on it before. Deal.II is essentially a collection of, as it says, open source code and libraries, which, with the background you have gathered, will allow you to go on and gradually build up more and more of your own repertoire in using finite element methods. You can go to this website yourself and browse around it. But let me tell you how you could very quickly start to be even more effective than you have been so far. If you click here on dev, you can see the drop down list and you go to tutorials. You come to this page, which actually sets you up to look at a number of examples with theory, the mathematics, and even code that will allow you to gradually go to more and more problems. There are a number of ways in which you can look at it and the way I like best is to click on the list here and you see there's an extensive list of examples of problems solved with the finite element method running all the way up to 50, 50 plus problems. So you could start out on with any of these, maybe you could even dive in if you think you've already accumulated some expertise and can afford to go on. So this is something that I would really encourage you to consider. And of course, you have been using Deal.II in a somewhat reduced framework as we were teaching you this introduction to finite element methods. Let's go back then and look at the other example of a software that I also have up for you, the FEniCS Project. So we click there, and it takes you to that website. The FEniCS Project is also a collection of free softwares, as you see right here and they explain what they mean by free software, and their philosophy and so forth. In this case you could go down and go along and look at applications. You'll see there's an extensive list here of different types of applications that you could use from this collection. If you scroll down, you see some things that you would begin to recognize, you see there is a solid mechanics library. There's something here for time integration, which you also know a little bit about. Is another kind of equation. There is this micromagnetics and so on, right. There's also software there about going massively parallel, and really many things. And the nice thing about Deal.II and FEniCS is the fact that they are open source, they are free, and they are also actually very advanced in terms of combining computational science with modern computer science. So I'd really encourage you to consider these two sites more closely. So let's go back then, to my slide here. So those are two of the open source softwares that I would really encourage you to consider. There are others out there of course, and by no means in mind am I saying that the others should not be considered. These are things that we use in our research group. Moving on then to courses. This course that you have been taking, the Finite Element Method for Problems in Physics, is going to continue in an on-demand format after it ends with this first iteration. And what that means is that will be available for you to start working with at any time that you like. You can start as you like and finish within a certain amount of time after that. You don't need to wait for a particular time when the course is going to be offered to start. These lectures are also available on YouTube, and in order to get to that let me do something slightly different. What I've brought you to is a page of a page on Open Michigan as it says, but let me show you how to get there. Well you can see how to get there, you just go to open.umich.edu, and that will bring you to a landing page where the University of Michigan has provided a whole host of open educational resources. Here, if you then go to the search bar and you type in Introduction to Finite Element Methods, I'd looked for it earlier. Do a search, shows you where the material is available. You click on Materials there and you see some of the lectures that you're already familiar with. If you click on the YouTube icon there, it will bring you to the whole series of lectures that you've been using on YouTube, right. So these lectures are already there, they actually were there even before our MOOC began, and I know that some of you had discovered these lectures, okay. So this resource is available. Also, with Open Michigan, we have provided a different series of lectures and this is a series called Lectures on Continuum Physics, okay? Search for that and there we go. Lectures on Continuum Physics. There we go. All right, so this is another series that we had recorded and provided, also about the same time that finite element lectures were provided. You click on Materials, and again you'll see all kinds of resources there. You see the video lectures. There are also assignments, which are available as PDFs. If you click on the YouTube link, likewise you go to this series on YouTube. Okay? Now, this series of lectures is also going to be shortly provided on a MOOC platform, okay, as we see here at the bottom of the slide. And this launch of lectures on continuum physics as a MOOC is expected to happen sometime during 2016. We're not certain exactly when, and this really depends on how quickly we can get all the pieces together. But the video lectures are already available, it's just packaging it as a MOOC that is going to take a little more time. So that's it really. I wanted to provide a brief indication of where you could go from these lectures that you've been following. And of course, there are many many other resources already available on the web, and we would encourage you to try as many of them as possible. That's it for now. And stay tuned. We will be back either with more in infinite elements or definitely, pretty soon, more on continuum physics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|██████████| 27/27 [05:17<00:00, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are generated\n",
      "Questions are embedded\n",
      "saved ../data/garikipati_latex_Q_then_A_use_context/garikipati_ch108_Qs_n40_by_sections_tpc1536.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "For generating questions, we want larger chunks with a bit of overlap.\n",
    "The following values are just for this demo, so please adjust them as needed.\n",
    "\n",
    "I only ran Chapter One.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chapter = chpt_for_quest_answ\n",
    "\n",
    "if author_for_quest_answ == \"hughes\":\n",
    "    latex_file_path = f'../data/FEM_Hughes_LaTeX_Textbook/chapter{chapter}.tex'\n",
    "elif author_for_quest_answ == \"garikipati\":\n",
    "    latex_file_path = f'../data/FEM_Garikipati_lectures/chapter{chapter}.tex'\n",
    "\n",
    "max_questions = 40                             # max number of questions per chunk\n",
    "\n",
    "tokens_per_chunk = 1536                       \n",
    "token_overlap = int(0.2 * tokens_per_chunk)   # 10% overlap\n",
    "environment_sensitive = True                  # If True, equations won't be split between chunks, which may result in chunks larger than the specified tokens_per_chunk\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def embed_all_q(questions):\n",
    "    all_questions = []\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            all_questions.append(sub_item['question'])\n",
    "    # using api\n",
    "    embeddings = get_embeddings(client, all_questions, model = embedding_model) \n",
    "    # add them to data:\n",
    "    k = 0\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            sub_item['embedding'] = embeddings[k]\n",
    "            k +=1\n",
    "    print('Questions are embedded')\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chapter}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chapter}_Qs_n{max_questions}_by_sections_tpc{tokens_per_chunk}.json\"  \n",
    "    token_overlap = 0 \n",
    "\n",
    "if not os.path.exists(questions_file_name):\n",
    "    question_chunks = process_latex_files(latex_file_path, tokens_per_chunk, token_overlap, environment_sensitive, chunk_by_section=chunk_by_section)\n",
    "    \n",
    "    if production_mode == False:\n",
    "        question_chunks = question_chunks[0:7] # for testing small batch\n",
    "    \n",
    "    for question in question_chunks:\n",
    "        print(f\"chunk word length: {len(question.split(\" \"))}, chunk char length: {len(question)}, chunk = {question}\")\n",
    "\n",
    "    questions = {}  # main data\n",
    "\n",
    "    # we should save generation info we used\n",
    "    questions['info'] = {\n",
    "        'tokens_per_chunk': tokens_per_chunk,\n",
    "        'token_overlap': token_overlap,\n",
    "        'environment_sensitive': environment_sensitive,\n",
    "        'max_questions': max_questions,\n",
    "        'embedding_model': embedding_model,\n",
    "        'llm_model_questions': llm_model_questions,\n",
    "        'llm_model_answers': llm_model_answers\n",
    "    }\n",
    "\n",
    "    ## step 1: generate questions\n",
    "    questions['data'] = []\n",
    "    for i in tqdm(range(len(question_chunks)), desc=\"Generating Questions\"):\n",
    "        # q_for_chunk = gen_questions(client, question_chunks[i], max_questions, model=llm_model_questions)\n",
    "        q_for_chunk = gen_questions_s(client, question_chunks[i], max_questions, model=llm_model_questions)   # Using the new function\n",
    "        questions['data'].append({'chunk': question_chunks[i],'questions': q_for_chunk})\n",
    "    print('Questions are generated')\n",
    "\n",
    "    ## step 2: embedding all questions at once\n",
    "    questions = embed_all_q(questions)\n",
    "    \n",
    "\n",
    "    with open(questions_file_name, 'w') as json_file:\n",
    "        json.dump(questions, json_file, indent=4)\n",
    "    print('saved', questions_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_file_name, 'r') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_file_name)\n",
    "\n",
    "if questions['info']['embedding_model'] != embedding_model:\n",
    "    print(\"embedding model mismatch. re-embedding questions\")\n",
    "    questions = embed_all_q(questions)\n",
    "    questions['info']['embedding_model'] = embedding_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Retrieval and Generating Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../data/garikipati_latex_Q_then_A_use_context/garikipati_ch108_QAs_n40_topk10_by_sections.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "Since we answer each question separately, this process is slow.\n",
    "We might want to consider using the batch API for this.\n",
    "\"\"\"\n",
    "\n",
    "top_k = 10   # number of retrieved closest contexts         \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chapter}_QAs_n{max_questions}_topk{top_k}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "elif chunk_by_section == True:\n",
    "    questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chapter}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"   \n",
    "\n",
    "if not os.path.exists(questions_answers_file_name):\n",
    "\n",
    "    questions_answers = questions.copy()\n",
    "\n",
    "    # step 1) finding top_k context from the book embedding and adding them to each question\n",
    "    for item in questions_answers['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            ind = fixed_knn_retrieval(sub_item['embedding'], embedding_space, top_k)\n",
    "            context = ''\n",
    "            for i, chunk in enumerate(chunks[ind]):\n",
    "                context += f'\\n\\n Additional context {i}: {chunk}' \n",
    "            sub_item['context'] = context\n",
    "    print('top_k context added')\n",
    "\n",
    "    # step 2) generating answers (slow)  (should we try batch API?)\n",
    "    for item in tqdm(questions_answers['data'], desc=\"Answering Questions\"):\n",
    "        question_chunk = item['chunk']\n",
    "        for sub_item in item['questions']:\n",
    "            question = sub_item['question']\n",
    "            context = question_chunk + sub_item['context']\n",
    "            sub_item['answer'] = gen_answer(client, question, context, model = llm_model_answers)\n",
    "    print('Questions are answered')\n",
    "    \n",
    "    with open(questions_answers_file_name, 'w') as json_file:\n",
    "        json.dump(questions_answers, json_file, indent=4)\n",
    "    print('saved', questions_answers_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_answers_file_name, 'r') as json_file:\n",
    "        questions_answers = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_answers_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_chunk</th>\n",
       "      <th>context</th>\n",
       "      <th>coverage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay. So let's get on. So what, what we're aim...</td>\n",
       "      <td>\\n\\n Additional context 0: All right. What we'...</td>\n",
       "      <td>80</td>\n",
       "      <td>What is the significance of understanding the ...</td>\n",
       "      <td>Understanding the stability of the time exact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay. So let's get on. So what, what we're aim...</td>\n",
       "      <td>\\n\\n Additional context 0: All right. What we'...</td>\n",
       "      <td>85</td>\n",
       "      <td>Explain the role of spatial discretization in ...</td>\n",
       "      <td>Spatial discretization plays a crucial role in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay. So let's get on. So what, what we're aim...</td>\n",
       "      <td>\\n\\n Additional context 0: The Galerkin method...</td>\n",
       "      <td>90</td>\n",
       "      <td>How does the eigenvalue $\\\\lambda^h$ relate to...</td>\n",
       "      <td>In the context of finite element analysis, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Okay. So let's get on. So what, what we're aim...</td>\n",
       "      <td>\\n\\n Additional context 0: All right we are re...</td>\n",
       "      <td>75</td>\n",
       "      <td>Describe the exact solution for the single deg...</td>\n",
       "      <td>The exact solution for the single degree of fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay. So let's get on. So what, what we're aim...</td>\n",
       "      <td>\\n\\n Additional context 0: All right. What we'...</td>\n",
       "      <td>70</td>\n",
       "      <td>Why is it important to consider the homogeneou...</td>\n",
       "      <td>The importance of considering the homogeneous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>So if you've got as far as this particular vid...</td>\n",
       "      <td>\\n\\n Additional context 0: So if you've got as...</td>\n",
       "      <td>85</td>\n",
       "      <td>What are some of the key features of the Deal....</td>\n",
       "      <td>Deal.II and FEniCS are both open-source platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>So if you've got as far as this particular vid...</td>\n",
       "      <td>\\n\\n Additional context 0: So if you've got as...</td>\n",
       "      <td>70</td>\n",
       "      <td>How do the resources provided by Open Michigan...</td>\n",
       "      <td>The resources provided by Open Michigan and si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>So if you've got as far as this particular vid...</td>\n",
       "      <td>\\n\\n Additional context 0: The main constituen...</td>\n",
       "      <td>65</td>\n",
       "      <td>What is the significance of having access to a...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>So if you've got as far as this particular vid...</td>\n",
       "      <td>\\n\\n Additional context 0: So if you've got as...</td>\n",
       "      <td>80</td>\n",
       "      <td>How can the integration of computational scien...</td>\n",
       "      <td>Answer: The integration of computational scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>So if you've got as far as this particular vid...</td>\n",
       "      <td>\\n\\n Additional context 0: So if you've got as...</td>\n",
       "      <td>75</td>\n",
       "      <td>What are the potential benefits of exploring d...</td>\n",
       "      <td>Answer: Exploring different types of applicati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_chunk  \\\n",
       "0    Okay. So let's get on. So what, what we're aim...   \n",
       "1    Okay. So let's get on. So what, what we're aim...   \n",
       "2    Okay. So let's get on. So what, what we're aim...   \n",
       "3    Okay. So let's get on. So what, what we're aim...   \n",
       "4    Okay. So let's get on. So what, what we're aim...   \n",
       "..                                                 ...   \n",
       "379  So if you've got as far as this particular vid...   \n",
       "380  So if you've got as far as this particular vid...   \n",
       "381  So if you've got as far as this particular vid...   \n",
       "382  So if you've got as far as this particular vid...   \n",
       "383  So if you've got as far as this particular vid...   \n",
       "\n",
       "                                               context  coverage  \\\n",
       "0    \\n\\n Additional context 0: All right. What we'...        80   \n",
       "1    \\n\\n Additional context 0: All right. What we'...        85   \n",
       "2    \\n\\n Additional context 0: The Galerkin method...        90   \n",
       "3    \\n\\n Additional context 0: All right we are re...        75   \n",
       "4    \\n\\n Additional context 0: All right. What we'...        70   \n",
       "..                                                 ...       ...   \n",
       "379  \\n\\n Additional context 0: So if you've got as...        85   \n",
       "380  \\n\\n Additional context 0: So if you've got as...        70   \n",
       "381  \\n\\n Additional context 0: The main constituen...        65   \n",
       "382  \\n\\n Additional context 0: So if you've got as...        80   \n",
       "383  \\n\\n Additional context 0: So if you've got as...        75   \n",
       "\n",
       "                                              question  \\\n",
       "0    What is the significance of understanding the ...   \n",
       "1    Explain the role of spatial discretization in ...   \n",
       "2    How does the eigenvalue $\\\\lambda^h$ relate to...   \n",
       "3    Describe the exact solution for the single deg...   \n",
       "4    Why is it important to consider the homogeneou...   \n",
       "..                                                 ...   \n",
       "379  What are some of the key features of the Deal....   \n",
       "380  How do the resources provided by Open Michigan...   \n",
       "381  What is the significance of having access to a...   \n",
       "382  How can the integration of computational scien...   \n",
       "383  What are the potential benefits of exploring d...   \n",
       "\n",
       "                                                answer  \n",
       "0    Understanding the stability of the time exact ...  \n",
       "1    Spatial discretization plays a crucial role in...  \n",
       "2    In the context of finite element analysis, the...  \n",
       "3    The exact solution for the single degree of fr...  \n",
       "4    The importance of considering the homogeneous ...  \n",
       "..                                                 ...  \n",
       "379  Deal.II and FEniCS are both open-source platfo...  \n",
       "380  The resources provided by Open Michigan and si...  \n",
       "381                           Answer: NOT ENOUGH INFO.  \n",
       "382  Answer: The integration of computational scien...  \n",
       "383  Answer: Exploring different types of applicati...  \n",
       "\n",
       "[384 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I think it's better to work with JSON/DataFrame in the code, but for reviewing QAs, CSV is easier to work with\n",
    "\"\"\"\n",
    "\n",
    "csv_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chapter}_QAs_n{max_questions}.csv\"   \n",
    "# ----------------------------------\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in questions_answers['data']:\n",
    "    question_chunk = item['chunk']\n",
    "    for sub_item in item['questions']:\n",
    "        new_item = {}\n",
    "        new_item['question_chunk'] = question_chunk\n",
    "        for k,v in sub_item.items():\n",
    "            if k == 'embedding':\n",
    "                continue\n",
    "            new_item[k] = v\n",
    "        data.append(new_item)\n",
    "\n",
    "# data[0]\n",
    "df = pd.DataFrame(data)[['question_chunk','context','coverage','question','answer']]\n",
    "df.to_csv(csv_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=160):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "Explain the significance of assuming a homogeneous case in the stability analysis of finite element equations.\n",
      "A:\n",
      "The significance of assuming a homogeneous case in the stability analysis of finite element equations is to expose the fundamental characteristic of the\n",
      "equations being analyzed. By considering the homogeneous case, where external forces or sources (such as heat supply or mass influx) are set to zero, the\n",
      "analysis focuses on the intrinsic behavior of the system. This allows for a clearer understanding of the natural tendency of the solution, which, in the context\n",
      "provided, is to decay or remain stable over time without external influences. This fundamental characteristic is crucial for determining what the exact behavior\n",
      "of the algorithmic equations should aim to represent, ensuring that the numerical methods used in the finite element analysis accurately capture the inherent\n",
      "stability properties of the system.\n",
      "\n",
      "Chunk used for Q generation:\n",
      "Okay. So let's get on. So what, what we're aiming to do in this segment is get some sense of the stability of the equations that we need to look at. Now if we\n",
      "want to look at stability we have to first understand we must first understand the stability of the time exact case, all right? Because that is the, the sort of\n",
      "behavior, the sort of response we are aspiring towards for our system, right? For our algorithmic system. Okay? So, in terms of stability, let's first\n",
      "understand the time exact case. Right, now, we've derived the single degree of freedom, modal equations, for a partic, for an arbitrary mode L. Okay? Now,\n",
      "everything we do holds for every mode, right? The, the, the, our, our analysis holds for any mode, because we're really working for an arbitrary mode. With that\n",
      "in mind, I can afford, I believe, to drop the explicit, use of the modal index, L. Okay? All right, so I'm going to drop that, right? So, from now on, the time\n",
      "for, for the time exact case, since we're working a single degree of freedom, for the time exact case and for the time discrete case, when we are working with\n",
      "single degree of freedom modal equations, I'm just going to write them as d dot scalar plus lambda d equals 0. Okay? But one more thing, I've got rid of one\n",
      "index L, but I want to bring another one. The one I want to bring back is the, I'm going to use h, right? And it's not an index, it's a superscript. Why am I\n",
      "bringing h back here? Remember h is our old friend the element, size. Right? It denotes the element size. The fact of spacial discretization. Why am I bringing\n",
      "it back? Right. It's because our n and k matrices depend upon our discretization, our spatial discretization. Right? So the ei, eigenvalues we're working with\n",
      "here are truly the spatially discretized eigen, the, the eigenvalues corresponding to the spatially discretized system. Okay? So lemme, lemme just state that\n",
      "here. Lambda h is the eigenvalue of a mode, or corresponding to a mode, that was, that is obtained after spatial discretization. Okay, so the factor spacial\n",
      "discretization, which is which shows itself up, which, which shows up in the finite element size, is indeed reflected in lambda h. Okay? Alternately, because\n",
      "these are partial differential equations, one could do a fully continuous analysis of them, and then one would have a eigenvalue corresponding to modes, but\n",
      "those would not be discretized modes, okay? Those modes or those eigen those eigenmodes would be actually eigenfunctions, not eigenvectors. Okay? So there would\n",
      "be a, so there's a difference. We're really working with the eigenvalues of the, of the spatially discretized system here, and that will show up, it's, it's for\n",
      "that reason that I'm bringing back our memories of h here. Okay? All right, so the time exact case is this, plus of course boundary condition. Sorry, the\n",
      "initial condition, right? So this in fact, d(0) equals d not, right? On the previous slide we've used the modal index L, but we've just decided to drop it here.\n",
      "Okay? Without, without risk of confusion, because everything we do holds for every mode. All right, so what is the stability of the system? How do we, how do we\n",
      "know what the stability of the system is? This equation is one of the simpler ODEs you're likely to encounter. Okay? We can directly write down the exact\n",
      "solution. All right? So the exact solution is d as a function of t equals d sub 0 exponent of minus lambda h times t. Okay? It's easy enough to check that that\n",
      "is indeed the exact solution. If you plug it into your, equations, you will find, and into our ODE, you will find that it satisfies the ODE, and it also,\n",
      "respects the initial condition. Okay? Just set T equal to 0 on the right hand side, since exponent or minus 0 is 1. You get back d at 0 equals d not. Okay? So\n",
      "this is the exact solution. All right, now, what about lambda h? What do we know about lambda H? It's an eigenvalue of the system, right? What is lambda h?\n",
      "Lambda h turns out to be greater than or equal to 0. Okay? All right? Why is lambda h greater than or equal to 0? We're not going to prove it, but do you know\n",
      "what properties give us this? It is the fact that M is positive definite. Right? And K is usually positive definite, but in the most general case, if we, if we\n",
      "want to also allow for insulation along certain directions, if you're talking of heat conduction or the possibility that there is no transport along certain\n",
      "directions if you're talking of mass diffusion, then k is a positive semi definite Okay, earlier on we'd used the fact that K can pretty much be taken to be\n",
      "positive definite, unless you really want to have insulation. We'd use that fact to make the observation that arriving that, that we are going to get\n",
      "eigenvectors that, that are linearly independent. Okay? All right. So, we have this, we have this sort of situation. All right, if that is the case since lambda\n",
      "h is greater than or equal to 0, what can we say about d? Say tn plus 1, right? And we know what this means. It just means that we're evaluating the solution at\n",
      "time n plus 1. What can we say about d at tn plus 1 relative to d at tn? Right? And, and here I'm using the fact that because of the nature of our time\n",
      "discretization and our choice of the progression of time instance, tn plus 1 is greater than tn. Right? So, let me just recall that also. We are, of course,\n",
      "using here the fact that we are progressing in time, so tn plus 1 is greater than or equal to tn, right? It's usually, it's, it's greater than tn. We nev, we\n",
      "never use it equal to tn, because that would mean we have a 0 times. Okay? All right, so given this, what should I use in this blank here? I've left a big blank\n",
      "spot between d and tn plus one and d and tn. What relational operator do I use? Right. Because the exponent of a negative argument is less than one, right? What\n",
      "we see is that this is a decaying function. Right, it's monotonically decreasing. Okay? Or another way at looking at it is, that d at t n plus one divided by d\n",
      "at tn is lesser than or equal to one, provided of course d at tn is not 0. Okay, but, nevertheless it's monotonically decreasing. All right, so we have\n",
      "monotonically decreasing, sorry, time dependent coefficient for our mode. Okay? All right. And this essential says that the nature of our heat conduction\n",
      "equation or our, or nature of our, mass diffusion equation, the kind we are looking at here, is for the solution to tend to be K. Okay? There is no tendency for\n",
      "the solution to tend to increase, provided we have set the forcing equal to 0. All right? And it is on in order to expose this characteristic of the equations\n",
      "we are working with that we are considering the homogeneous case. Because clearly, you could be supplying heat to increase the to, to, to raise the temperature,\n",
      "or you could be, you could have a local supply of mass, or, or, or, again, influx of heat or mass in order to push up the temperature or the mass concentration\n",
      "at, at any point. And therefore, these modes could be increasing in a problem that is in homogeneous. Okay? But we wanted to get to this fundamental\n",
      "characteristic of the equation, so we just assume the homo, we, we are considering the homogeneous case. All right, so why are we doing this? The reason we are\n",
      "doing this is because we want to understand what is the exact behavior that our algorithmic equations should aim to represent? Okay? All right. So, with this in\n",
      "hand, let's ask, ask ourselves what, what the same sort of analysis tells us for our time discrete equation. And this is\n",
      "\n",
      "Retrieved context:\n",
      "\n",
      "\n",
      " 0: All right. What we're going to do in this segment is complete our stability analysis of our Time-Discrete, Single Degree of Freedom, ODE. All right? Or a\n",
      "single degree of freedom equation. Okay. So let's recall where, where we got. We got as far as the amplification factor. Okay. And that amplification factor\n",
      "gives us dn plus 1 divided by dn equals A, this is the amplification factor. And it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t\n",
      "lambda h. Okay? All right. Now what we want to consider is the fact that what we're going to use in our analysis is the fact that alpha lies in the, in the\n",
      "closed interval, zero to one, right? Delta t, which is our time step is greater than zero. Okay? And lambda h, we've agreed is greater than or equal to zero.\n",
      "Okay? All right. Now the stability condition. Or the stability criteria is that the magnitude of A is lesser than or equal to 1. Okay? All right. So this is\n",
      "what guarantees decaying response, right? From one time step to another. Okay? By the way, the fact that we arrive at this condition is actually the, is what we\n",
      "call a linear stability condition. Okay? And the linear aspect of it comes from the fact that we have indeed a linear problem here. And for linear problems, one\n",
      "can state the stability requirement by just saying that well, if your solution does not grow from one time step to the other, you have a, you have a you have a\n",
      "stable problem. Okay? When we go to non-linear problems, we can't quite use that. Because the physics of non-linear problems can actually lead to solutions\n",
      "growing, right? Over certain regimes, right? And that, that is the right physics for those sorts of problems. So let's get back to this. So, if this is what we\n",
      "have let's see how we can, we can extend this. What this implies, of course, for us directly is that minus 1 should be lesser than or equal to A should be\n",
      "lesser than or equal to 1. Okay? So, A has to lie in that interval. All right? Okay. So let's look at this. So what that means then is that minus 1 is lesser\n",
      "than or equal to 1 minus 1 minus alpha delta t, lambda h divided by 1 plus alpha delta t lambda h is lesser than or equal to 1. Okay? Now because of the\n",
      "conditions I put up here, right? We are guaranteed that our denominator is positive, right? So that says, if we are now what, what this lets us do is safely\n",
      "multiply through by that by the denominator and we get here. Minus 1 plus alpha delta t lambda h is lesser than or equal to 1 minus 1 minus alpha delta t lambda\n",
      "h is lesser than or equal to 1 plus alpha delta t lambda h. Okay? All right. Okay. So to move on, let's just go to the next slide and let's look at the right-\n",
      "hand side of that inequality. Okay? So consider in form, right? 1 minus 1 minus alpha delta t lambda h is lesser than equal to 1 plus alpha delta t lambda h.\n",
      "Okay? We can consider this, we see that these two cancel out. Okay? And we manipulate things and what we see here is that we get alpha plus 1 minus alpha delta\n",
      "t lambda h has to be greater than or equal to 0. Okay? And we do this by simply moving what remains on the left-hand side of the inequality on the first line\n",
      "over to the right-hand side. Okay? Now those cancel out, all right? And we are left with sorry. There's a delta t here. What we are left with here is the delta\n",
      "t lambda h is greater than or equal to zero. Okay? That's what the right-hand equalities says. All right. But this is always satisfied. Okay? All right? Let's\n",
      "look now at what happens. And, and in fact, this is always satisfied regardless of alpha. We satisfied for, for all alpha, all right? Because alpha doesn't even\n",
      "show up in this final form. All right. Now let's consider the left-hand side inequality. Okay? And that left-hand side inequality is that minus 1, sorry, minus\n",
      "1 plus alpha delta t lambda h Is lesser than or equal to 1 minus 1 minus alpha delta t lambda h. Okay? All right. So now let's work with this a little. When we\n",
      "work with this a little, let's, let's, let me rewrite this by moving This term over to the right-hand side and, and, and rewrite them. So when we do that, we\n",
      "get 1 minus 1 minus alpha delta t lambda h plus 1 minus alpha delta t lambda h is greater than or equal to 0. Okay. That is the condition which must hold and\n",
      "this condition you note involves alpha, delta t and lambda h. Okay? Continuing to work with it, this implies that two plus 2 minus 1 all right. Minus two alpha,\n",
      "hm. What's going wrong here? Alex, you may need to just give me a pause here. I've got a two here. That over back, okay. Alex, I'm going to backup, I made a\n",
      "mistake here. Go back to the beginning of this last equation. Okay. I note that I need a plus sign there. Okay. Put that plus sign and go ahead and now, I get 2\n",
      "plus 2 alpha, right? One alpha coming from here and another alpha coming from here. 2 alpha minus 1 times delta t lambda h is greater than or equal to 0. Okay?\n",
      "That is the condition we need. Let's see what else we can do with this now. I could write this out as 2 has to be greater than or equal to 1 minus 2 alpha delta\n",
      "t lambda h. Okay? All right. All right. Now let us look at whether this can, under what conditions this may be satisfied. Okay?. Now let's save this and move on\n",
      "to the next slide, okay? Let's consider the following cases, okay? So case one. All right? And for case one, let us suppose that alpha is greater than or equal\n",
      "to one-half. Okay? What, when, when that happens, what that says, what that means is that 1 minus 2 alpha, delta t lambda h is always what? Right? It's always\n",
      "lesser than or equal to zero, okay? Which implies that yes, if alpha is greater than or equal to half, 2 is always greater than or equal to 1 minus 2 alpha\n",
      "delta t lambda h. Okay? Right? This thing always holds, holds unconditionally. Right? And by unconditionally, what we mean is that if alpha is greater than or\n",
      "equal to half, our stability condition holds for all delta t, right? For all delta t greater than zero, right. Okay. So we have a condition, we have a, a\n",
      "situation of unconditional stability of our method. Okay? So this is unconditional stability if alpha is greater than or equal to half. Okay. So, in particular,\n",
      "observe that alpha equals half, which is the Crank Nicholson method or the midpoint jewel as I sometimes call it, as a lot of other people call it. Right? Or\n",
      "even alpha equals 1, which is backwards Euler. Right? These are both unconditionally stable. You can take a time step as big as you like and you're okay. Okay?\n",
      "A solution may be terrible, it may be very inaccurate, but you're still stable. The solution will not blow up, right? The amplification factor will remain, the\n",
      "magnitude of the amplification factor will remain bound by, by one. Okay? Case two is the interesting one. Okay? Case two is that alpha belongs to zero comma\n",
      "half not including the half, right? Okay. All right. Which is basically zero is lesser than or equal to alpha is less than a half, okay? This is the other case.\n",
      "All right? So now what happens is that we have a situation of that's okay. So, so in this case, we have a, we must still have all right. So we, we are trying to\n",
      "see whether 2 is greater than or equal to 1 minus 2 alpha delta t lambda h. Right. This condition needs to be satisfied for alpha, such that zero is less than\n",
      "or equal to alpha is less than half. Okay? All right. This is what we're trying to look at, okay? Now observe that since alpha lies in this range that I've\n",
      "written on the right this parenthesis is always what? Is it all, what can you say about the quantity in that grid emphasis, in those parenthesis? Right? It's\n",
      "always positive. Okay? So now we get a condition, where we see that on, on, as far as delta t is concerned, delta t must be lesser or equal to 2 divided by 1\n",
      "minus 2 alpha. We're able to divide through by 1 minus 2 alpha, because it is indeed greater than 0. Okay, so it is a positive quantity. 1 minus 2 alpha is\n",
      "positive. But then we see the delta t has to be lesser than or equal to 2 over 2, 1 minus 2 alpha, times lambda h. Okay? So this is a situation of conditional\n",
      "stability. Okay? So the algorithm is stable, right? It can be stable. But it depends upon the size of delta t, okay, conditional stability, which means\n",
      "conditional upon delta T. Okay? So delta T has to be bounded by something. Delta T cannot be too big. Sort of makes sense, right? You have an algorithm that is\n",
      "not entirely stable, that is not always stable. In order for it to stable well, take very small delta t's, right, take very small times 6. Ok, so, an example of\n",
      "this is of course alpha equals 0. Right, which is forward Euler. Okay. All right, now, to end this segment let us consider what else can happen here with delta\n",
      "t. Remember that lambda h, because of the nature of our Eigenvalue problem, lambda H if you ask yourself, what is the order of lambda h? Okay? And where does\n",
      "lambda h appear? Order of lambda h depends upon the Eigenvalue problem, on the Eigenvalue problem. K psi, where psi is the mode shape corresponding to this\n",
      "particular mode. K psi equals lambda h, n psi. Okay? Now, what this suggests, is that lambda h is of the order of, the order of lambda h is of the sort of terms\n",
      "that are like M inverse K. All right? Right. So it's with the order of the norm of terms in M, inverse K. All right. Now, I want us to think about how do M and\n",
      "K depend on the element size? Right, because these after all are obtained by the spatial discretization. Okay? So, recall M is of the order integral over omega\n",
      "e, and then we have the sum over e, sorry, we have the assembly over e and all that. We have the assembly over e, integral over omega e of terms of this type,\n",
      "right? We have, we have shape functions, right? So this is really of this is really like M A B in a particular element. Okay, so I can actually get rid of this,\n",
      "right. I'm just doing this element twice, sorry, I'm changing things around a lot. Okay, element twice, this is what we have, right, integral over omega E, N,\n",
      "A, N, B, D V. Okay, K AB from an element is of order, integral over omega e NA,X NB,X right? These are derivatives. Right? So there are derivatives involved in\n",
      "KAB and yes. KAB may also have a so this is xi, xj, K, kappa here may have a kappa ij. Okay? Dv, all right, and yes the mass matrix may also have a specific\n",
      "heat there. Right, or row could be one if you are doing diffusion. Okay? What is, what is the order of these sorts of terms? These sorts of terms, because their\n",
      "shape function derivatives, are of order one over h. Okay? As a result, what we're seeing is that MAB is of order may be 1, right. But KAB is of order h to the\n",
      "minus 2, okay. As a result, the order of lambda h, okay, is h to the minus 2, okay? In terms of the element size. Now what does that say for delta t? Delta t\n",
      "has to be lesser than or equal to 2 over 1 minus 2 alpha lambda h. Well, this lambda h is like h to the minus 2. Okay? As h gets smaller, our restriction on\n",
      "delta t gets more and more stringent. Okay? So the remark that I will make here is that restriction On delta t, gets more stringent. As h tends to 0.\n",
      "Essentially as we refine our finite element mesh, our spatial discretization, we see that it has an effect even on our temporal discretization if we're working\n",
      "with a conditionally stable method. Right? For the time integration, okay. So, this is how the finite element discretization also makes its presence felt in the\n",
      "time discretization when one is working with a semi discrete method and happens upon a conditionally stable problem, conditionally stable method. All right.\n",
      "We'll end the segment here.\n",
      "\n",
      " 1: The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of\n",
      "$\\mathcal{U}^{h}$. Let $\\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \\bar{\\Omega} \\rightarrow \\mathbb{R}$, where\n",
      "$A=1,2, \\ldots, n$. By this we mean that if $w^{h} \\in \\mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \\ldots, n$, such that \\begin{align*} w^{h} &\n",
      "=\\sum_{A=1}^{n} c_{A} N_{A} \\\\ & =c_{1} N_{1}+c_{2} N_{2}+\\cdots+c_{n} N_{n} \\tag{1.6.1} \\end{align*} The $N_{A}$ 's are referred to as shape, basis, or\n",
      "interpolation functions. We require that each $N_{A}$ satisfies \\begin{equation*} N_{A}(1)=0, \\quad A=1,2, \\ldots, n \\tag{1.6.2} \\end{equation*} from which it\n",
      "follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\\delta^{h}$ we need to\n",
      "specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \\bar{\\Omega} \\rightarrow \\mathbb{R}$, which has the property \\begin{equation*}\n",
      "N_{n+1}(1)=1 \\tag{1.6.3} \\end{equation*} (Note $N_{n+1} \\notin \\mathcal{U}^{h}$. ) Then $g^{h}$ is given by \\begin{equation*} g^{h}=g N_{n+1} \\tag{1.6.4}\n",
      "\\end{equation*} and thus \\begin{equation*} g^{h}(1)=g \\tag{1.6.5} \\end{equation*} With these definitions, a typical $u^{h} \\in f^{h}$ may be written as\n",
      "\\begin{align*} u^{h} & =v^{h}+g^{h} \\\\ & =\\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \\tag{1.6.6} \\end{align*} where the $d_{A}$ 's are constants and from which it is\n",
      "apparent that $u^{h}(1)=g$.\\\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \\begin{align*} a\\left(\\sum_{A=1}^{n} c_{A} N_{A},\n",
      "\\sum_{B=1}^{n} d_{B} N_{B}\\right)=\\left(\\sum_{A=1}^{n} c_{A} N_{A}, f\\right)+ & {\\left[\\sum_{A=1}^{n} c_{A} N_{A}(0)\\right] h } \\\\ & -a\\left(\\sum_{A=1}^{n}\n",
      "c_{A} N_{A}, g N_{n+1}\\right) \\tag{1.6.7} \\end{align*} By using the bilinearity of $a(\\cdot, \\cdot)$ and $(\\cdot, \\cdot),(1.6 .7)$ becomes \\begin{equation*}\n",
      "0=\\sum_{A=1}^{n} c_{A} G_{A} \\tag{1.6.8} \\end{equation*} where \\begin{equation*} G_{A}=\\sum_{B=1}^{n} a\\left(N_{A}, N_{B}\\right) d_{B}-\\left(N_{A},\n",
      "f\\right)-N_{A}(0) h+a\\left(N_{A}, N_{n+1}\\right) q \\tag{1.6.9} \\end{equation*} Now the Galerkin equation is to hold for all $w^{h} \\in \\mathcal{U}^{h}$. By\n",
      "(1.6.1), this means for all $c_{A}$'s, $A=1,2, \\ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \\ldots,\n",
      "n$, must be identically zero, i.e., from (1.6.9) \\begin{equation*} \\sum_{B=1}^{n} a\\left(N_{A}, N_{B}\\right) d_{B}=\\left(N_{A}, f\\right)+N_{A}(0)\n",
      "h-a\\left(N_{A}, N_{n+1}\\right) g \\tag{1.6.10} \\end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a\n",
      "system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \\begin{align*} K_{A B} & =a\\left(N_{A}, N_{B}\\right)\n",
      "\\tag{1.6.11}\\\\ F_{A} & =\\left(N_{A}, f\\right)+N_{A}(0) h-a\\left(N_{A}, N_{n+1}\\right) g \\tag{1.6.12} \\end{align*} Then (1.6.10) becomes \\begin{equation*}\n",
      "\\sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \\quad A=1,2, \\ldots, n \\tag{1.6.13} \\end{equation*} Further simplicity is gained by adopting a matrix notation. Let\n",
      "\\begin{align*} & \\boldsymbol{K}=\\left[K_{A B}\\right]=\\left[\\begin{array}{cccc} K_{11} & \\boldsymbol{K}_{12} & \\cdots & K_{1 n} \\\\ \\boldsymbol{K}_{21} & K_{22} &\n",
      "\\cdots & K_{2 n} \\\\ \\vdots & \\vdots & & \\vdots \\\\ K_{n 1} & K_{n 2} & \\cdots & K_{n n} \\end{array}\\right] \\tag{1.6.14}\\\\ &\n",
      "\\boldsymbol{F}=\\left\\{F_{A}\\right\\}=\\left\\{\\begin{array}{c} F_{1} \\\\ F_{2} \\\\ \\vdots \\\\ F_{n} \\end{array}\\right\\} \\tag{1.6.15} \\end{align*} and \\[\n",
      "d=\\left\\{d_{B}\\right\\}=\\left\\{\\begin{array}{c} d_{1} \\tag{1.6.16}\\\\ d_{2} \\\\ \\vdots \\\\ d_{n} \\end{array}\\right\\} \\] Now (1.6.13) may be written as\n",
      "\\begin{equation*} \\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F} \\tag{1.6.17} \\end{equation*} The following terminologies are frequently applied, especially when\n",
      "the problem under consideration pertains to a mechanical system: $$ \\begin{aligned} \\boldsymbol{K} & =\\text { stiffness matrix } \\\\ \\boldsymbol{F} & =\\text {\n",
      "force vector } \\\\ \\boldsymbol{d} & =\\text { displacement vector } \\end{aligned} $$ A variety of physical interpretations are of course possible. At this point,\n",
      "we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\\\ (M) $\\left\\{\\begin{array}{c}\\text { Given the coefficient matrix } \\boldsymbol{K} \\text {\n",
      "and vector } \\boldsymbol{F}, \\text { find } \\boldsymbol{d} \\text { such that } \\\\ \\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}\\end{array}\\right.$ The solution\n",
      "of $(M)$ is, of course, just $d=K^{-1} \\boldsymbol{F}$ (assuming the inverse of $\\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ )\n",
      "may be obtained at any point $x \\in \\bar{\\Omega}$ by employing (1.6.6), viz., \\begin{equation*} u^{h}(x)=\\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \\tag{1.6.18}\n",
      "\\end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of\n",
      "$(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The\n",
      "quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \\subsection*{Remarks} \\begin{enumerate} \\item The matrix $K$ is\n",
      "symmetric. This follows from the symmetry of $a(\\cdot, \\cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and\n",
      "trial solutions): \\end{enumerate} \\begin{align*} K_{A B} & =a\\left(N_{A}, N_{B}\\right) \\\\ & =a\\left(N_{B}, N_{A}\\right) \\\\ & =K_{B A} \\tag{1.6.19} \\end{align*}\n",
      "In matrix notation \\begin{equation*} K=K^{\\boldsymbol{T}} \\tag{1.6.20} \\end{equation*} where the superscript $\\boldsymbol{T}$ denotes transpose. The symmetry of\n",
      "$\\boldsymbol{K}$ has important computational consequences.\\\\ \\\\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of\n",
      "the process one must go through in developing a finite element method for any given problem: \\begin{equation*} (S) \\Leftrightarrow(W) \\approx(G)\n",
      "\\Leftrightarrow(M) \\tag{1.6.21} \\end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated\n",
      "situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain\n",
      "$\\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\\\ 3. It is sometimes\n",
      "convenient to write \\begin{equation*} u^{h}(x)=\\sum_{A=1}^{n+1} N_{A}(x) d_{A} \\tag{1.6.22} \\end{equation*} where $d_{n+1}=g$.\n",
      "\n",
      " 2: Let $\\delta^{h}$ and $\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\delta$ and $\\mathcal{V}$, respectively. We assume all members of\n",
      "$\\mathcal{V}^{h}$ vanish, or vanish approximately, on $\\Gamma_{g}$ and that each member of $\\delta^{h}$ admits the representation \\begin{equation*}\n",
      "u^{h}=v^{h}+g^{h} \\tag{2.4.1} \\end{equation*} where $v^{h} \\in \\mathcal{V}^{h}$ and $g^{h}$ results in satisfaction, or at least approximate satisfaction, of\n",
      "the boundary condition $u=g$ on $\\Gamma_{g}$. The Galerkin formulation is given as follows:\\\\ (G) $\\left\\{\\begin{array}{c}\\text { Given } \\ell, g \\text {, and }\n",
      "h\\left[\\text { as in (W)], find } u^{h}=v^{h}+y^{h} \\in \\delta^{h} \\text { such that for all }\\right. \\\\ w^{h} \\in \\mathcal{V}^{h}(\\mathrm{cf} . \\text { Sec.\n",
      "1.5): } \\\\ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell \\right)+\\left(w^{h}, h\\right)_{\\Gamma}-a\\left(w^{h}, g^{h}\\right) \\\\ \\text { (2.4.2)\n",
      "}\\end{array}\\right.$ We now view our domain as \"discretized\" into element domains $\\Omega^{\\text {e }}$, $1 \\leq e \\leq n_{e l}$. In two dimensions the element\n",
      "domains might be simply triangles and quadrilaterals; see Fig. 2.4.1. Nodal points may exist anywhere on the domain but most frequently appear at the element\n",
      "vertices and interelement boundaries and less often in the interiors.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-09}\n",
      "Figure 2.4.1\\\\ In Sec. 1.9 the global nodal ordering and ordering of equations in the matrix system coincided. In multidimensional applications this would prove\n",
      "to be an inconvenient restriction with regard to data preparation. In what follows, a more flexible scheme is described. Let $\\boldsymbol{\\eta}=\\left\\{1,2,\n",
      "\\ldots, n_{\\text {np}}\\right\\}$, the set of global node numbers where $n_{\\text {np}}$ is the number of nodal points. By the terminology $g$-node we shall mean\n",
      "a node, $A$, at which it is prescribed that $\\boldsymbol{u}^{\\boldsymbol{h}}=\\boldsymbol{g}$. Let $\\eta_{g} \\subset \\eta$ be the set of \" $g$-nodes.\" The\n",
      "complement of $\\eta_{g}$ in $\\eta$, denoted by $\\eta-\\eta_{g}$, is the set of nodes at which $\\boldsymbol{u}^{\\boldsymbol{h}}$ is to be determined. The number\n",
      "of nodes in $\\boldsymbol{\\eta}-\\boldsymbol{\\eta}_{\\boldsymbol{g}}$ equals $n_{\\text {eq }}$, the number of equations. A typical member of $\\mathcal{V}^{h}$ is\n",
      "assumed to have the form \\begin{equation*} w^{h}(x)=\\sum_{A \\in \\eta-\\eta_{g}} N_{A}(x) c_{A} \\tag{2.4.3} \\end{equation*} where $N_{A}$ is the shape function\n",
      "associated with node number $A$ and $c_{A}$ is a constant. We assume throughout that $w^{h}=0$ if and only if $c_{A}=0$ for each $A \\in \\eta-\\eta_{g}$. Likewise\n",
      "\\begin{equation*} v^{h}(x)=\\sum_{A \\in \\eta-\\eta_{g}} N_{A}(x) d_{A} \\tag{2.4.4} \\end{equation*} where $d_{A}$ is the unknown at node $A$ (i.e., temperature)\n",
      "and \\begin{equation*} g^{h}(x)=\\sum_{A \\in \\eta_{g}} N_{A}(x) g_{A}, \\quad g_{A}=g\\left(x_{A}\\right) \\tag{2.4.5} \\end{equation*} From (2.4.5), we see that\n",
      "$g^{h}$ has been defined to be the nodal interpolate of $g$ by way of the shape functions. \\footnote{This is not the only possibility, nor the best from the\n",
      "standpoint of accuracy. However, in practice it is generally the most convenient. } Consequently, $g^{h}$ will be, generally, only an approximation of $g$. See\n",
      "Fig. 2.4.2. Additional sources of error are (1) the use of approximations $\\ell^{h}$ and $h^{h}$ in place of $\\ell$ and $h$, respectively; and (2) domain\n",
      "approximations in which the element boundaries do not exactly coincide with $\\Gamma$. Analyses of these approximations are presented in Strang and Fix [2].\\\\\n",
      "\\includegraphics[max width=\\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-10} Figure 2.4.2 Piecewise linear approximation of boundary data (schematic).\\\\\n",
      "Substituting (2.4.3)-(2.4.5) into (2.4.2) and arguing as in Sec. 1.6, results in \\[ \\begin{array}{r} \\sum_{B \\in \\eta-\\eta_{g}} a\\left(N_{A}, N_{B}\\right)\n",
      "d_{B}=\\left(N_{A}, \\ell\\right)+\\left(N_{A}, h\\right)_{\\Gamma}-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right) g_{B} \\tag{2.4.6}\\\\ A \\in \\eta-\\eta_{g}\n",
      "\\end{array} \\] To define the global stiffness matrix and force vector, we need to first specify the global ordering of equations. For this purpose we introduce\n",
      "the ID array, sometimes called the destination array, which assigns to node $A$ the corresponding global equation number, viz., \\[ \\text{ID}(A) = \\begin{cases}\n",
      "\\overbrace{P}^{\\text{Global equation number}} & \\text{if } A \\in \\eta - \\eta_{g}, \\\\ 0 & \\text{if } A \\in \\eta_{g} \\end{cases} \\] where $1 \\leq P \\leq n_{e q}$.\n",
      "The dimension of ID is $n_{n p}$. As may be seen from (2.4.7), nodes at which $g$ is prescribed are assigned \"equation number\" zero. An example of the setup of\n",
      "ID and other important data-processing arrays is presented in Sec. 2.6. The matrix equivalent of (2.4.6) is given as follows: \\begin{gather*} K d=F\n",
      "\\tag{2.4.8}\\\\ \\boldsymbol{K}=\\left[K_{P Q}\\right], \\quad d=\\left\\{d_{Q}\\right\\}, \\quad F=\\left\\{F_{P}\\right\\}, \\quad 1 \\leq P, Q \\leq n_{e q} \\tag{2.4.9}\\\\ K_{P\n",
      "Q}=a\\left(N_{A}, N_{B}\\right), \\quad P=\\operatorname{ID}(A), \\quad Q=\\operatorname{ID}(B) \\tag{2.4.10}\\\\ F_{P}=\\left(N_{A}, \\ell\\right)+\\left(N_{A},\n",
      "h\\right)_{\\Gamma}-\\sum_{B \\in \\eta_{g}} a\\left(N_{A}, N_{B}\\right) g_{B} \\tag{2.4.11} \\end{gather*} The main properties of $\\boldsymbol{K}$ are established in\n",
      "the following theorem. \\\\ \\\\ \\textbf{Theorem} \\begin{enumerate} \\item $K$ is symmetric. \\item $K$ is positive definite. \\end{enumerate} \\textbf{Proof}\n",
      "\\begin{enumerate} \\item The symmetry of $\\boldsymbol{K}$ follows directly from the symmetry of $a(\\cdot, \\cdot)$, viz., \\end{enumerate} $$ \\begin{aligned} K_{P\n",
      "Q} & =a\\left(N_{A}, N_{B}\\right) & & \\text { (by definition) } \\\\ & =a\\left(N_{B}, N_{A}\\right) & & \\text { (symmetry of } a(\\cdot, \\cdot)) \\\\ & =K_{Q P} & &\n",
      "\\text { (by definition) } \\end{aligned} $$ \\begin{enumerate} \\setcounter{enumi}{1} \\item (Recall that we must show (i) $c^{T} K c \\geq 0$ and (ii) $c^{T} K c=0$\n",
      "implies $c=0$.) \\end{enumerate} To each $n_{e q}$-vector $c=\\left\\{c_{p}\\right\\}$, we may associate a member $w^{h} \\in \\mathcal{V}^{h}$ by the expression\n",
      "$\\boldsymbol{w}^{h}=\\Sigma_{A \\in \\eta-\\eta_{g}} N_{A} \\bar{c}_{A}$, where $\\bar{c}_{A}=c_{P}, P=\\operatorname{ID}(A)$.\\\\ i. $$ \\begin{array}{rlrl}\n",
      "\\boldsymbol{c}^{T} K c & =\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{PQ} c_{Q} & \\\\ & =\\sum_{A, B \\in \\eta - \\eta_g} \\bar{c}_{A} a\\left(N_{A}, N_{B}\\right) \\bar{c}_{B} &\n",
      "\\\\ & =a\\left(\\sum_{A \\in \\eta-\\eta_{g}} N_{A} \\bar{c}_{A}, \\sum_{B \\in \\eta-\\eta_{g}} N_{B} \\bar{c}_{B}\\right) & & \\text { (bilinearity of } a(\\cdot, \\cdot)) \\\\\n",
      "& =a\\left(w^{h}, w^{h}\\right) & & \\text { (definition of } \\left.w^{h}\\right) \\\\ & =\\int_{\\Omega} \\underbrace{w_{,i}^{h} k_{ij} w_{,j}^{h}}_{\\geq 0} d \\Omega &\n",
      "& \\text { (positive-definiteness of conductivities) } \\\\ & \\geq 0 & & \\end{array} $$ ii. Assume $\\boldsymbol{c}^{\\boldsymbol{T}} \\boldsymbol{K c}=\\mathbf{0}$.\n",
      "By the proof of part (i), $$ \\int_{\\Omega} \\underbrace{w_{, i}^{h} \\kappa_{i j} w_{, j}^{h}}_{\\geq 0} d \\Omega=0 $$ and thus it follows that $$ w_{, i}^{h}\n",
      "\\kappa_{i j} w_{, j}^{h}=0 $$ By the positive-definiteness hypothesis on the conductivities, this requires $w_{, i}^{h}=0$ and so $w^{h}$ must be constant.\n",
      "However $w^{h}=0$ on $\\Gamma_{g}$ (which is not empty) and so $w^{h}$ must be zero throughout $\\Omega$. By the definition of $\\boldsymbol{w}^{\\boldsymbol{h}}$,\n",
      "it follows that each $c_{P}=0$; that is $c=0$, which was to be proved. \\\\ \\\\ \\\\ \\\\ \\\\ \\textbf{Remarks} \\begin{enumerate} \\item Observe that it is the positive-\n",
      "definiteness hypothesis on the constitutive coefficients (i.e., $\\kappa_{ij}$ 's) and the boundary condition incorporated in the definition of\n",
      "$\\mathcal{V}^{\\boldsymbol{h}}$ which together result in the positive-definiteness of $\\boldsymbol{K}$ and thus ensure its invertibility. \\item The explicit\n",
      "structure of the shape functions, which will be delineated in Chapter 3, will also result in $\\boldsymbol{K}$ being banded. \\end{enumerate}\n",
      "\\subsection*{Exercise 1.} (This exercise is a multidimensional analog of the one contained in Sec. 1.8.) Let $$ \\Gamma_{\\text{int\n",
      "}}=\\left(\\bigcup_{e=1}^{n_{el}} \\Gamma^{e}\\right)-\\Gamma \\quad \\text { (interior element boundaries) } $$ One side of $\\Gamma_{\\text{int}}$ is (arbitrarily)\n",
      "designated to be the \" + side\" and the other is the \" - side.\" Let $\\boldsymbol{n}^{+}$and $\\boldsymbol{n}^{-}$be unit normals to $\\Gamma_{\\text {int }}$ which\n",
      "point in the minus and plus directions, respectively. Clearly $n^{+}=-n^{-}$. Let $q_{i}^{+}$and $q_{i}^{-}$denote the values of $q_{i}$ obtained by approaching\n",
      "$x \\in \\Gamma_{\\mathrm{int}}$ from + and - sides, respectively. The \"jump\" in $q_{n}=q_{i} n_{i}$ at $x$ is defined to be $$ \\begin{aligned} {\\left[q_{n}\\right]\n",
      "} & =\\left(q_{i}^{+}-q_{i}^{-}\\right) n_{i}^{+} \\\\ & =q_{i}^{+} n_{i}^{+}+q_{i}^{-} n_{i}^{-} \\end{aligned} $$ As may be easily verified, the jump is invariant\n",
      "with respect to reversing the + and - designations. Consider the weak formulation (i.e., (2.3.6)) and assume $w$ and $u$ are smooth on the element interiors but\n",
      "may experience discontinuities in gradient across element boundaries. (Functions of this type contain the standard $C^{0}$ finite element interpolations; see\n",
      "Chapter 3.) Show that $$ 0=\\sum_{e=1}^{n_{e l}} \\int_{\\Omega^{e}} w\\left(q_{i, i}-\\ell\\right) d \\Omega-\\int_{\\Gamma_{h}} w\\left(q_{n}+h\\right) d\n",
      "\\Gamma-\\int_{\\Gamma_{int}} w\\left[q_{n}\\right] d \\Gamma $$ from which the Euler-Lagrange conditions may be readily deduced:\\\\ i. $q_{i, i}= \\ell$ in\n",
      "$\\bigcup_{e=1}^{n_{el}} \\Omega^{e}$\\\\ ii. $-q_{n}=h$ on $\\Gamma_{h}$\\\\ iii. $\\left[q_{n}\\right]=0$ on $\\Gamma_{\\text {int }}$ As may be seen, (i) is the heat\n",
      "equation on the element interiors and (iii) is a continuity condition across element boundaries on the heat flux. Contrast the present results with those\n",
      "obtained assuming $w$ and $u$ are globally smooth. The Galerkin finite element formulation obtains an approximate solution to (i) through (iii).\n",
      "\n",
      " 3: It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in\n",
      "the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase\n",
      "the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme\n",
      "is applicable to systems of equations\\\\ $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example,\n",
      "symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \\subsection*{Gauss Elimination} \\begin{itemize} \\item\n",
      "Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \\item Solve the second equation for $d_{2}$ and eliminate $d_{2}$\n",
      "from the remaining $n-2$ equations. \\item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \\item Solve the $n$-th\n",
      "equation for $d_{n}$. \\end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example,\n",
      "suppose we began with a system of four equations as follows: $$ \\left[\\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\\\ K_{21} & K_{22} & K_{23} & K_{24}\n",
      "\\\\ K_{31} & K_{32} & K_{33} & K_{34} \\\\ K_{41} & K_{42} & K_{43} & K_{44} \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4}\n",
      "\\end{array}\\right\\}=\\left\\{\\begin{array}{l} F_{1} \\\\ F_{2} \\\\ F_{3} \\\\ F_{4} \\end{array}\\right\\} $$ The augmented matrix corresponding to this system is \\[\n",
      "\\left[ \\begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\\\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\\\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\\\\n",
      "K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{K}} & \\underbrace{\\phantom{F_1}}_{F}\n",
      "\\end{array} \\right] \\] After the forward reduction, the augmented matrix becomes\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K‘_{14} & F'_1 \\\\ 0 &\n",
      "1 & K'_{23} & K'_{24} & F'_2 \\\\ 0 & 0 & 1 & K'_{34} & F'_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\,\n",
      "K_{14}}}_{U}} & \\underbrace{\\phantom{F_1}}_{F'} \\end{array} \\tag{1.11.1} \\right] \\] corresponding to the upper triangular system $\\boldsymbol{U}\n",
      "\\boldsymbol{d}=\\boldsymbol{F}^{\\prime} \\cdot{ }^{4}$ It is a simply verified fact that if $\\boldsymbol{K}$ is banded, then $\\boldsymbol{U}$ will be also.\n",
      "Employing the reduced augmented matrix, proceed as follows: \\begin{itemize} \\item Eliminate $d_{n}$ from equations $n-1, n-2, \\ldots, 1$.\\\\ \\footnotetext{${\n",
      "}^{4} \\text{Primes will be used to denote intermediate quantities throughout this section}.$} \\item Eliminate $d_{n-1}$ from equations $n-2, n-3, \\ldots, 1$.\n",
      "\\item Eliminate $d_{2}$ from the first equation. \\end{itemize} This procedure is called back substitution. For example, in the example just given, after back\n",
      "substitution we obtain\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\\\ 0 & 1 & 0 & 0 & d_2 \\\\ 0 & 0 & 1 & 0 & d_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\\n",
      "\\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{I}} & \\underbrace{\\phantom{F_1}}_{d} \\end{array} \\tag{1.11.2} \\right] \\]\n",
      "corresponding to the identity $1 \\boldsymbol{d}=\\boldsymbol{d}$. The solution winds up in the last column. \\subsection*{Hand-Calculation Algorthm} In a hand\n",
      "calculation, Gauss elimination can be performed on the augmented matrix as follows. \\subsection*{Forward reduction} \\begin{itemize} \\item Divide row 1 by\n",
      "$K_{11}$. \\item Subtract $K_{21} \\times$ row 1 from row 2. \\item Subtract $K_{31} \\times$ row 1 from row 3. \\item Subtract $K_{n 1} \\times$ row 1 from row $n$.\n",
      "\\end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \\left[\\begin{array}{llll|l} 1 &\n",
      "\\boldsymbol{K}_{12}^{\\prime} & \\boldsymbol{K}_{3}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & \\boldsymbol{K}_{22}^{\\prime\n",
      "\\prime} & \\boldsymbol{K}_{23}^{\\prime \\prime} & \\boldsymbol{K}_{24}^{\\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime} \\\\ 0 & \\boldsymbol{K}_{32}^{\\prime} &\n",
      "\\boldsymbol{K}_{33}^{\\prime 3} & \\boldsymbol{K}_{34}^{\\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime} \\\\ \\mathbf{0} & \\boldsymbol{K}_{42}^{\\prime} &\n",
      "\\boldsymbol{K}_{43}^{3} & \\boldsymbol{K}_{44}^{\\prime \\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime} \\end{array}\\right] $$ Note that if $\\boldsymbol{K}_{\\mathbf{A\n",
      "1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \\begin{itemize} \\item Divide row 2 by $K_{22}^{\\prime \\prime}$. \\item\n",
      "Subtract $K_{32}^{\\prime \\prime} \\times$ row 2 from row 3. \\item Subtract $K_{42}^{n} \\times$ row 2 from row 4. \\item Subtract $K_{n 2}^{\\prime \\prime} \\times$\n",
      "row 2 from row $n$. \\end{itemize} The result for the example will look like\\\\ $\\left[\\begin{array}{cccc|c}1 & \\boldsymbol{K}_{12}^{\\prime} &\n",
      "\\boldsymbol{K}_{13}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & 1 & \\boldsymbol{K}_{23}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{24}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{33}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{34}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{43}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{44}^{\\prime \\prime\\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime \\prime} \\\\ & & & & \\end{array}\\right]$ Note that only the submatrix enclosed in\n",
      "dashed lines is affected in this procedure.\\\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \\subsection*{Back\n",
      "substitution} \\begin{itemize} \\item Subtract $K_{n-1, n}^{\\prime} \\times$ row $n$ from row $n-1$. \\item Subtract $K_{n-2, n}^{\\prime} \\times$ row $n$ from row\n",
      "$n-2$.\\\\ \\vdots \\item Subtract $K_{1, n}^{\\prime} \\times$ row $n$ from row 1 \\end{itemize} After these steps the augmented matrix, for this example, will look\n",
      "like $$ \\left[\\begin{array}{cccc|c} 1 & \\bar{K}_{12}^{\\prime} & \\bar{K}_{3}^{\\prime} & 0 & F_{1}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 1 & K_{23}^{\\prime} & 0 &\n",
      "F_{2}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 0 & 1 & 0 & d_{3} \\\\ 0 & 0 & 0 & 1 & d_{4} \\end{array}\\right] $$ Note that the submatrix enclosed in dashed lines is\n",
      "unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\\prime}$ is\n",
      "altered. Now clear the second-to-last column in the coefficient matrix: \\begin{itemize} \\item Subtract $K_{n-2, n-1}^{\\prime} \\times$ row $n-1$ from row $n-2$.\n",
      "\\item Subtract $K_{n-3, n-1}^{\\prime} \\times$ row $n-1$ from row $n-3$.\\\\ \\vdots \\item Subtract $K_{1 . n-1}^{\\prime} \\times$ row $n-1$ from row 1.\n",
      "\\end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\\boldsymbol{F}$ ). Repeat as above until\n",
      "columns $\\boldsymbol{n}-2, n-3, \\ldots, 2$ are cleared. The result is (1.11.2). \\subsection*{Remarks} \\begin{enumerate} \\item In passing we note that the above\n",
      "procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss\n",
      "elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the\n",
      "procedure. However, it is felt that the given scheme is the clearest for hand calculations. \\item The numerical example with which we close this section\n",
      "illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain\n",
      "zero throughout the calculations). The reader is urged to perform the calculations. \\end{enumerate} \\subsection*{Example of Gauss ellmination} $$\n",
      "\\left[\\begin{array}{rrrr} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\\n",
      "d_{3} \\\\ d_{4} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right\\} $$ \\subsection*{Augmented matrix} $$ \\left[\\begin{array}{rrrr|r}\n",
      "1 & -1 & 0 & 0 & 1 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right] $$ Forward reduction $$ \\begin{aligned} &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\end{aligned} $$\n",
      "\\subsection*{Back substitution} $$ \\begin{aligned} & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1\n",
      "& 1 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]}\n",
      "\\\\ & {\\left[\\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\\\ \\begin{array}{l}\n",
      "\\left\\{ \\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4} \\end{array} \\right\\} = \\left\\{ \\begin{array}{l} 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{array} \\right\\} \\end{array}\n",
      "\\end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \\begin{aligned} u_{, x x}(x)+f(x) & =0 \\quad x \\in] 0,1[\n",
      "\\\\ u(1) & =g \\\\ -u_{, x}(0) & =h \\end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\\\ a. Employing the linear finite element space with\n",
      "equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\\left(h=\\right.$ mesh parameter $\\left.=\\frac{1}{4}\\right)$. Recall that in\n",
      "Sec. 1.7 this was carried out for $n=1$ and $n=2\\left(h=1\\right.$ and $h=\\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss\n",
      "elimination to solve $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your\n",
      "answers since they must be exact at the nodes.\\\\ b. Let $r e_{, x}=\\left|u_{, x}^{h}-u_{. x}\\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{,\n",
      "x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\\\ c. Employing the data for $h=1, \\frac{1}{2}$, and\n",
      "$\\frac{1}{4}$, plot $\\ln r e_{, x}$ versus $\\ln h$.\\\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following\n",
      "questions:\\\\ i. What is the significance of the slope of the graph in part (c)?\\\\ ii. What is the significance of the $y$-intercept?\n",
      "\n",
      " 4: The main constituents of a finite element method for the solution of a boundary-value problem are\\\\ i. The variational or weak statement of the problem;\n",
      "and\\\\ ii. The approximate solution of the variational equations through the use of \"finite element functions.\" To clarify concepts we shall begin with a simple\n",
      "example.\\\\ Suppose we want to solve the following differential equation for $u$ : \\begin{equation*} u_{, x x}+f=0 \\tag{1.1.1} \\end{equation*} where a comma\n",
      "stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We\n",
      "write \\begin{equation*} f: [0,1] \\to \\mathbb{R} \\tag{1.1.2} \\end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0\n",
      "\\leq x \\leq 1$ ) and $\\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will\n",
      "use the notation $\\in$ to mean \"in\" or \"a member of.\" Thus for each $x \\in[0,1], f(x) \\in \\mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and\n",
      "$\\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we\n",
      "sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we\n",
      "do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to\n",
      "consult Appendix 1.I, \"An Elementary Discussion of Continuity, Differentiability and Smoothness,\" for further remarks on this important aspect of finite element\n",
      "work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and\n",
      "physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation\n",
      "(1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical\n",
      "parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to\n",
      "simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end\n",
      "points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \\textbf{\\textit{open and closed unit intervals,}} respectively. To\n",
      "simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \\[ \\boldsymbol{\\Omega}=]\n",
      "0,1[ \\quad \\text { (open) } \\tag{1.1.3} \\] \\[ \\overline{\\boldsymbol{\\Omega}}=[0,1] \\quad \\text { (closed) } \\tag{1.1.4} \\] See Fig. 1.1.1. \\begin{figure}[h]\n",
      "\\centering \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \\vspace{0.5em} \\textbf{Figure 1.1.1} \\end{figure} At this point,\n",
      "considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is\n",
      "necessary for good finite element work.\n",
      "\n",
      " 5: The examples of the preceding section employed definitions of $\\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear\n",
      "finite element space. To define the general case in which $\\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping\n",
      "subintervals. The typical subinterval is denoted by $\\left[x_{A}, x_{A+1}\\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \\ldots, n$. We also require $x_{1}=0$ and\n",
      "$x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes\n",
      "referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The\n",
      "mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\\max h_{A}, A=1,2, \\ldots, n$ ). The smaller $h$, the more\n",
      "\"refined\" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical\n",
      "internal node (i.e., $2 \\leq A \\leq n$ ) \\[ N_{A}(x)=\\left\\{\\begin{array}{cl} \\frac{\\left(x-x_{A-1}\\right)}{h_{A-1}}, & x_{A-1} \\leq x \\leq x_{A} \\tag{1.8.1}\\\\\n",
      "\\frac{\\left(x_{A+1}-x\\right)}{h_{A}}, & x_{A} \\leq x \\leq x_{A+1} \\\\ 0, & \\text { elsewhere } \\end{array}\\right. \\] whereas for the boundary nodes we have\n",
      "\\begin{align*} & N_{1}(x)=\\frac{x_{2}-x}{h_{1}}, \\quad x_{1} \\leq x \\leq x_{2} \\tag{1.8.2}\\\\ & N_{n+1}(x)=\\frac{x-x_{n}}{h_{n}}, \\quad x_{n} \\leq x \\leq x_{n+1}\n",
      "\\tag{1.8.3} \\end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as \"hat,\" \"chapeau,\" and \"roof\"\n",
      "functions. Note that $N_{A}\\left(x_{B}\\right)=\\delta_{A B}$, where\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure\n",
      "1.8.1 Basis functions for the piecewise linear finite element space.\\\\ $\\delta_{A B}$ is the Kronecker delta (i.e., $\\delta_{A B}=1$ if $A=B$, whereas\n",
      "$\\delta_{A B}=0$ if $A \\neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the\n",
      "subintervals that contain $x_{A}$. A typical member $w^{h} \\in \\mathcal{U}^{h}$ has the form $\\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note\n",
      "that $w^{\\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of\n",
      "$w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.)\n",
      "Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly,\n",
      "$w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\\boldsymbol{n}$.\\\\ \\includegraphics[max width=\\textwidth,\n",
      "center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\\boldsymbol{h}} \\in \\mathcal{U}^{\\boldsymbol{k}}$.\\\\ Typical members of\n",
      "$f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element\n",
      "functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-\n",
      "dimensional model problem: \\begin{equation*} \\int_{0}^{1} w_{, x} u_{, x} d x=\\int_{0}^{1} w f d x+w(0) h \\tag{1.8.4} \\end{equation*} where $w \\in \\mathcal{U}$\n",
      "and $u \\in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \\ldots, n)$, but may suffer slope discontinuities across\n",
      "element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of\n",
      "the functions, show that: \\begin{align*} 0= & \\sum_{A=1}^{n} \\int_{x_{A}}^{x_{A}+1} w\\left(u_{, x x}+f\\right) d x+w(0)\\left[u_{, x}\\left(0^{+}\\right)+h\\right]\n",
      "\\\\ & +\\sum_{A=2}^{n} w\\left(x_{A}\\right)\\left[u_{, x}\\left(x_{A}^{+}\\right)-u_{, x}\\left(x_{A}^{-}\\right)\\right] \\tag{1.8.5} \\end{align*} Arguing as in Sec.\n",
      "1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\\\ i. $u_{, x x}(x)+f(x)=0$, where $\\left.x \\in\\right] x_{A}, x_{A+1}[$ and $A=1,2,\n",
      "\\ldots, n$,\\\\ ii. $-u_{, x}\\left(0^{+}\\right)=h$; and\\\\ iii. $u_{, x}\\left(x_{A}^{-}\\right)=u_{, x}\\left(x_{A}^{+}\\right)$, where $A=2,3, \\ldots, n$. Observe\n",
      "that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted\n",
      "with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over\n",
      "element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of\n",
      "(i)-(iii) is obtained.\n",
      "\n",
      " 6: So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in\n",
      "question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and\n",
      "\"local support\" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis\n",
      "functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical\n",
      "properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view\n",
      "called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite\n",
      "element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall\n",
      "attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following\n",
      "quantities. Linear finfte element (global description) \\begin{center} \\begin{tabular}{lll} $(g 1)$ & Domain: & $\\left[x_{A}, x_{A+1}\\right]$ \\\\ $(g 2)$ & Nodes:\n",
      "& $\\left\\{x_{A}, x_{A+1}\\right\\}$ \\\\ $(g 3)$ & Degrees of freedom: & $\\left\\{d_{A}, d_{A+1}\\right\\}$ \\\\ $(g 4)$ & Shape functions: & $\\left\\{N_{A},\n",
      "N_{A+1}\\right\\}$ \\\\ $(g 5)$ & Interpolation function: & \\\\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \\in\\left[x_{A}, x_{A+1}\\right]$ & \\\\ \\end{tabular}\n",
      "\\end{center} (Recall $d_{A}=u^{h}\\left(x_{A}\\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally\n",
      "defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape\n",
      "functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a\n",
      "typical element may be standardized. These are given as follows: Linear finite element (local description)\\\\ (l1) Domain: $\\left[\\xi_{1}, \\xi_{2}\\right]$\n",
      "\\footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin\n",
      "methods), we would also have to specify a set of weighting functions, say\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37}\n",
      "} (l2) Nodes: $\\left\\{\\xi_{1}, \\xi_{2}\\right\\}$\\\\ (l3) Degrees of freedom: $\\left\\{d_{1}, d_{2}\\right\\}$\\\\ (I4) Shape functions: $\\left\\{N_{1}, N_{2}\\right\\}$\\\\\n",
      "(15) Interpolation function: $$ u^{h}(\\xi)=N_{1}(\\xi) d_{1}+N_{2}(\\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\\\ We\n",
      "shall relate the domains of the global and local descriptions by an \"affine\" transformation $\\xi:\\left[x_{A}, x_{A+1}\\right] \\rightarrow\\left[\\xi_{1},\n",
      "\\xi_{2}\\right]$, such that $\\xi\\left(x_{A}\\right)=\\xi_{1}$ and $\\xi\\left(x_{A+1}\\right)=\\xi_{2}$. It is standard practice to take $\\xi_{1}=-1$ and $\\xi_{2}=+1$.\n",
      "Thus $\\xi$ may be represented by the expression \\begin{equation*} \\xi(x)=c_{1}+c_{2} x \\tag{1.12.1} \\end{equation*} where $c_{1}$ and $c_{2}$ are constants\n",
      "which are determined by \\[ \\left.\\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \\tag{1.12.2}\\\\ 1 & =c_{1}+x_{A+1} c_{2} \\end{array}\\right\\} \\] Solving this system\n",
      "yields \\begin{equation*} \\xi(x)=\\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \\tag{1.12.3} \\end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\\xi$ is obtained by\n",
      "solving for $x$ : \\begin{equation*} x(\\xi)=\\frac{h_{A} \\xi+x_{A}+x_{A+1}}{2} \\tag{1.12.4} \\end{equation*} In (1.12.1), $\\xi$ is a mapping and $x$ is a point,\n",
      "whereas in (1.12.4), $x$ is a mapping and $\\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \\ldots$ pertain to the\n",
      "local numbering system. The subscripts $A, B, C, \\ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will\n",
      "frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion\n",
      "as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity\n",
      "in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\\xi)=N_{A}\\left(x^{e}(\\xi)\\right)$, where $x^{e}:\\left[\\xi_{1},\n",
      "\\xi_{2}\\right] \\rightarrow$ $\\left[x_{1}^{e}, x_{2}^{e}\\right]=\\left[x_{A}, x_{A+1}\\right]$, etc.). In terms of $\\xi$, the shape functions in the local\n",
      "description take on a standard form \\begin{equation*} N_{a}(\\xi)=\\frac{1}{2}\\left(1+\\xi_{a} \\xi\\right), \\quad a=1,2 \\tag{1.12.5} \\end{equation*} Note also that\n",
      "(1.12.4) may be written in terms of (1.12.5): \\begin{equation*} x^{e}(\\xi)=\\sum_{a=1}^{2} N_{a}(\\xi) x_{a}^{e} \\tag{1.12.6} \\end{equation*} This has the same\n",
      "form as the interpolation function (cf. 15).\\\\ For future reference, we note the following results: \\begin{gather*} N_{a,\n",
      "\\xi}=\\frac{\\xi_{a}}{2}=\\frac{(-1)^{a}}{2} \\tag{1.12.7}\\\\ x_{, \\xi}^{e}=\\frac{h^{e}}{2} \\tag{1.12.8} \\end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and\n",
      "\\begin{equation*} \\xi_{, x}^{e}=\\left(x_{, \\xi}^{e}\\right)^{-1}=\\frac{2}{h^{e}} \\tag{1.12.9} \\end{equation*} The local and global descriptions of the eth\n",
      "element are depicted in Fig. 1.12.1.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global\n",
      "descriptions of the $e$ th element.\n",
      "\n",
      " 7: To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e\n",
      "l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \\leq e \\leq n_{e l}$.\\\\ \\includegraphics[max width=\\textwidth,\n",
      "center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\\\ Now recall the definitions of the (global) stiffness matrix and force vector\\\\ \\\\ \\[ K =\n",
      "\\underbrace{\\left[ K_{AB} \\right]}_{n \\times n}, \\quad F = \\underbrace{\\left\\{ F_A \\right\\}}_{n \\times 1} \\tag{1.13.1} \\] where \\begin{gather*} K_{A\n",
      "B}=a\\left(N_{A}, N_{B}\\right)=\\int_{0}^{1} N_{A, x} N_{B, x} d x \\tag{1.13.2}\\\\ F_{A}=\\left(N_{A}, f\\right)+\\delta_{A 1} h-a\\left(N_{A}, N_{n+1}\\right) g \\\\\n",
      "=\\int_{0}^{1} N_{A} f d x+\\delta_{A 1} h-\\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \\tag{1.13.3} \\end{gather*} ( $\\operatorname{In}(1.13 .3)$ we have assumed\n",
      "$N_{A}\\left(x_{1}\\right)=\\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over\n",
      "the element domains. Thus \\[ \\begin{array}{ll} \\boldsymbol{K}=\\sum_{e=1}^{n_{e l}} \\boldsymbol{K}^{e}, & \\boldsymbol{K}^{e}=\\left[K_{A B}^{e}\\right] \\\\\n",
      "\\boldsymbol{F}=\\sum_{e=1}^{n_{e l}} \\boldsymbol{F}^{e}, & \\boldsymbol{F}^{e}=\\left\\{F_{\\hat{A}}^{e}\\right\\} \\tag{1.13.5} \\end{array} \\] where \\begin{align*}\n",
      "K_{A B}^{e} & =a\\left(N_{A}, N_{B}\\right)^{e}=\\int_{\\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \\tag{1.13.6}\\\\ F_{A}^{e} & =\\left(N_{A}, f\\right)^{e}+\\delta_{e 1}\n",
      "\\delta_{A 1} h-a\\left(N_{A}, N_{n+1}\\right)^{e} g \\\\ & =\\int_{\\Omega^{e}} N_{A} f d x+\\delta_{e 1} \\delta_{A 1} h-\\int_{\\Omega^{e}} N_{A, x} N_{n+1, x} d x g\n",
      "\\tag{1.13.7} \\end{align*} and $\\Omega^{e}=\\left[x_{1}^{e}, x_{2}^{e}\\right]$, the domain of the eth element.\\\\ The important observation to make is that\n",
      "$\\boldsymbol{K}$ and $\\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this\n",
      "procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \\begin{equation*} K_{A B}^{e}=0, \\quad \\text\n",
      "{ if } A \\neq e \\text { or } e+1 \\text { or } B \\neq e \\text { or } e+1 \\tag{1.13.8} \\end{equation*} and \\begin{equation*} F_{A}^{e}=0, \\quad \\text { if } A\n",
      "\\neq e \\text { or } e+1 \\tag{1.13.9} \\end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add\n",
      "in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$\n",
      "and element force vector $f^{e}$ as follows: \\begin{align*} & k^{e}=\\underbrace{\\left[k_{a b}^{e}\\right]}_{2 \\times 2}, \\quad\n",
      "f^{e}=\\underbrace{\\left\\{f_{a}^{e}\\right\\}}_{2 \\times 1} \\tag{1.13.10}\\\\ & k_{a b}^{e}=a\\left(N_{a}, N_{b}\\right)^{e}=\\int_{\\Omega^{e}} N_{a, x} N_{b, x} d x\n",
      "\\tag{1.13.11}\\\\ & f_{a}^{e}=\\int_{\\Omega^{e}} N_{a} f d x+\\left\\{\\begin{array}{cl} \\delta_{a 1} h & e=1 \\\\ 0 & e=2,3, \\ldots, n_{e l}-1 \\\\ -k_{a 2 }^{e} g &\n",
      "e=n_{e l} \\end{array}\\right. \\tag{1.13.12} \\end{align*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \\end{center}\n",
      "Here $\\boldsymbol{k}^{e}$ and $\\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\\boldsymbol{K}^{e}$ and $\\boldsymbol{F}^{e}$ are\n",
      "defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ \"go\" in $\\boldsymbol{K}$ and $\\boldsymbol{F}$,\n",
      "respectively, requires keeping additional information. This is discussed in the following section.\n",
      "\n",
      " 8: Let $\\delta^{h}$ and $\\mathcal{V}^{h}$ be finite-dimensional approximations to $\\delta$ and $\\mathcal{V}$, respectively. We assume members $w^{h} \\in\n",
      "\\mathcal{V}^{h}$ result in satisfaction, or approximate satisfaction, of the boundary condition $w_{i}=0$ on $\\Gamma_{g_i}$, and members of $\\delta^{h}$ admit\n",
      "the decomposition \\begin{equation*} u^{h}=v^{h}+g^{h} \\tag{2.8.1} \\end{equation*} where $\\boldsymbol{g}^{h} \\in \\mathcal{V}^{h}$ and $\\boldsymbol{g}^{h}$\n",
      "results in satisfaction, or approximate satisfaction, of the boundary condition $u_{i}=g_{i}$ on $\\Gamma_{g_i}$. The Galerkin formulation of our problem is\n",
      "given as follows:\\\\ (G) $\\left\\{\\begin{array}{c}\\text { Given } \\ell, g, \\text { and } h \\text { (as in }(W)) \\text {, find } u^{h}=v^{h}+{g}^{h} \\in \\delta^{h}\n",
      "\\text { such that for all } w^{h} \\in \\mathbb{V}^{h} \\\\ a\\left(w^{h}, v^{h}\\right)=\\left(w^{h}, \\ell\\right)+\\left(w^{h},\n",
      "h\\right)_{\\Gamma}-a\\left(w^{h},g^{h}\\right) \\quad \\text { (2.8.2) }\\end{array}\\right.$ To define the global stiffness matrix and force vector for elasticity, it\n",
      "is necessary to introduce the ID array. This entails a generalization of the definition given in Sec. 2.6, since in the present case there will be more than 1\n",
      "degree of freedom per node. For elasticity there are $n_{s d}$ degrees of freedom per node, but in order to include in our\\\\ definition cases such as heat\n",
      "conduction, we shall take the fully general situation in which it is assumed that there are $n_{\\text {dof}}$ degrees of freedom per node.\\footnote{In general,\n",
      "this is taken to mean the maximum number of degrees of freedom per node in the global model. It is possible in practice to have elements with fewer degrees of\n",
      "freedom per node contributing to the model.} In this case\\\\ \\[ \\text{ID}(\\underbrace{i}_{\\text{Degree of freedom number}}, \\underbrace{A}_{\\text{Global node\n",
      "number}}) = \\left\\{ \\begin{aligned} &\\underbrace{P}_{\\text{Global equation number}} &\\quad \\text{if } A \\in \\eta - \\eta_{g_i} \\\\ &0 &\\quad \\text{if } A \\in\n",
      "\\eta_{g_i} \\end{aligned} \\right. \\] where $1 \\leq i \\leq n_{\\text {dof }}$. Thus ID has dimensions $n_{\\mathrm{dof}} \\times n_{n p}$. If $n_{\\mathrm{dof}}=1$,\n",
      "we reduce to the case considered previously in Sec. 2.6 (i.e., $\\operatorname{ID}(i, A)=\\operatorname{ID}(A)$ ). Recall that $\\boldsymbol{\\eta}=\\left\\{1,2,\n",
      "\\ldots, n_{n p}\\right\\}$ denotes the set of global node numbers. Let $\\boldsymbol{\\eta}_{\\boldsymbol{g}_{i}} \\subset \\boldsymbol{\\eta}$ be the set of nodes at\n",
      "which $u_{i}^{h}=\\boldsymbol{g}_{i}$ and let $\\boldsymbol{\\eta}-\\boldsymbol{\\eta}_{\\boldsymbol{g}_{i}}$ be the complement of $\\eta_{g i}$. For each node in\n",
      "$\\eta-\\eta_{g i}$, the nodal value of $u_{i}^{h}$ is to be determined. The explicit representations of $v_{i}^{h}$ and $g_{i}^{h}$, in terms of the shape\n",
      "functions and nodal values are \\begin{align*} & v_{i}^{h}=\\sum_{A \\in \\eta-\\eta_{g_{i}}} N_{A} d_{\\underbrace{i}_{\\text{Degree of freedom number}}\n",
      "\\underbrace{A}_{\\text{Global node number}}} \\quad \\text { (no sum on } i \\text { ) } \\tag{2.8.4}\\\\ & g_{i}^{h}=\\sum_{A \\in \\eta_{g_{i}}} N_{A} g_{iA} \\quad\n",
      "\\text { (no sum on } i \\text { ) } \\end{align*} Let $e_{i}$ denote the $i$th Euclidean basis vector for $\\mathbb{R}^{n_{sd}} ; \\boldsymbol{e}_{i}$ has a 1 in\n",
      "slot $i$ and zeros elsewhere. For example \\[ \\left(n_{s d}=2\\right) \\quad e_{1}=\\left\\{\\begin{array}{l} 1 \\tag{2.8.6}\\\\ 0 \\end{array}\\right\\}, \\quad\n",
      "e_{2}=\\left\\{\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right\\} \\] \\[ \\left(n_{s d}=3\\right) \\quad e_{1}=\\left\\{\\begin{array}{l} 1 \\tag{2.8.7}\\\\ 0 \\\\ 0\n",
      "\\end{array}\\right\\}, \\quad e_{2}=\\left\\{\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\end{array}\\right\\} \\quad e_{3}=\\left\\{\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\end{array}\\right\\} \\]\n",
      "The vector versions of (2.8.4) and (2.8.5) may be defined with the aid of $e_{i}$, viz., \\begin{align*} & v^{h}=v_{i}^{h} e_{i} \\tag{2.8.8}\\\\ & g^{h}=g_{i}^{h}\n",
      "e_{i} \\tag{2.8.9} \\end{align*} Likewise, a typical member $w^{h} \\in \\mathcal{V}^{h}$ has the representation \\begin{equation*} w^{h}=w_{i}^{h} e_{i}, \\quad\n",
      "w_{i}^{h}=\\sum_{A \\in \\eta-\\eta_{g_{i}}} N_{A} c_{i A} \\quad \\text {(no sum on i)}\\tag{2.8.10} \\end{equation*} Substituting (2.8.4), (2.8.5), and\n",
      "(2.8.8)-(2.8.10) into (2.8.2) and arguing along the lines of Sec. 1.6 results in (verify!) \\begin{align*} & \\sum_{j=1}^{n_{\\text {dof }}}\\left(\\sum_{B \\in\n",
      "\\eta-\\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) d_{j B}\\right)=\\left(N_{A} e_{i}, \\ell\\right)+\\left(N_{A} e_{i}, h\\right)_{\\Gamma} \\\\ &\n",
      "\\quad-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{j B}\\right), \\quad A \\in \\eta-\\eta_{g_i}, \\quad 1 \\leq i \\leq\n",
      "n_{s d} \\tag{2.8.11\\footnotemark} \\end{align*} \\footnotetext{For correct interpretation of the meaning of these equations, the sum on $j$ should be taken first.\n",
      "For example, in two dimensions \\[ \\begin{aligned} \\sum_{j=1}^{2}\\left(\\sum_{B \\in \\eta-\\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) d_{jB}\\right)= &\n",
      "\\sum_{B \\in \\eta-\\eta_{g_1}} a\\left(N_{A} e_{i}, N_{B} e_{1}\\right) d_{1 B} \\\\ & +\\sum_{B \\in \\eta-\\eta_{g_2}} a\\left(N_{A} e_{i}, N_{B} e_{2}\\right) d_{2 B}\n",
      "\\end{aligned} \\] and \\[ \\begin{aligned} \\sum_{j=1}^{2}\\left(\\sum_{B \\in n_{g_j}}\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{jB}\\right)= & \\sum_{B \\in \\eta_{g_1}}\n",
      "a\\left(N_{A} e_{i}, N_{B} e_{1}\\right) g_{1 B} \\\\ & +\\sum_{B \\in n_{g_2}} a\\left(N_{A} e_{i}, N_{B} e_{2}\\right) g_{2 B} \\end{aligned} \\] } This is equivalent\n",
      "to the matrix equation\\\\ \\begin{align*} & K d=\\boldsymbol{F} \\tag{2.8.12}\\\\ & \\text { where } \\\\ & K=\\left[K_{P Q}\\right] \\tag{2.8.13}\\\\ &\n",
      "d=\\left\\{d_{Q}\\right\\} \\tag{2.8.14}\\\\ & F=\\left\\{F_{P}\\right\\} \\tag{2.8.15}\\\\ & K_{P Q}=a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) \\tag{2.8.16}\\\\ &\n",
      "F_{P}=\\left(N_{A} e_{i}, \\ell\\right)+\\left(N_{A} e_{i}, h\\right)_{\\Gamma}-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B}\n",
      "e_{j}\\right) g_{jB}\\right) \\tag{2.8.17}\\\\ & \\text { in which } \\\\ & P=\\operatorname{ID}(i, A), \\quad Q=\\operatorname{ID}(j, B) \\end{align*} Equation (2.8.16)\n",
      "may be written in more explicit form by using (2.7.26) and noting that (see (2.7.21) and (2.7.22)): \\begin{equation*} \\epsilon\\left(N_{A} e_{i}\\right)=B_{A}\n",
      "e_{i} \\tag{2.8.19} \\end{equation*} where \\begin{align*} & \\left(n_{s d}=2\\right) \\quad B_{A}=\\left[\\begin{array}{cc} N_{A, 1} & 0 \\\\ 0 & N_{A, 2} \\\\ N_{A, 2} &\n",
      "N_{A, 1} \\end{array}\\right] \\tag{2.8.20}\\\\ & \\left(n_{s d}=3\\right) \\quad B_{A}=\\left[\\begin{array}{ccc} N_{A, 1} & 0 & 0 \\\\ 0 & N_{A, 2} & 0 \\\\ 0 & 0 & N_{A,\n",
      "3} \\\\ 0 & N_{A, 3} & N_{A, 2} \\\\ N_{A, 3} & 0 & N_{A, 1} \\\\ N_{A, 2} & N_{A, 1} & 0 \\end{array}\\right] \\tag{2.8.21} \\end{align*} \\subsection*{Exercise 1.}\n",
      "Verify (2.8.19)-(2.8.21). With these, (2.8.16) becomes\\\\ \\[ K_{PQ} = e_i^T \\int_\\Omega B_A^T D B_B \\, d\\Omega \\, e_j \\] \\begin{itemize} \\item $PQ$: Global\n",
      "equation numbers \\item $i, j$: Degree of freedom numbers \\item $A, B$: Global node numbers \\end{itemize} and the indices are related by (2.8.18). Equation\n",
      "(2.8.17) is also amenable to explication. Note that, by (2.7.18) \\begin{equation*} \\left(N_{A} e_{i}, \\ell\\right)=\\int_{\\Omega} N_{A} \\ell_{i} d \\Omega\n",
      "\\tag{2.8.23} \\end{equation*} and likewise by (2.7.19) \\begin{equation*} \\left(N_{A} e_{i}, h\\right)_{\\Gamma}=\\int_{\\Gamma_{h_{i}}} N_{A} h_{i} d \\Gamma \\quad\n",
      "\\text { (no sum) } \\tag{2.8.24} \\end{equation*} Thus (2.8.17) may be written as \\begin{equation*} F_{P}=\\int_{\\Omega} N_{A} \\ell_{i} d\n",
      "\\Omega+\\int_{\\Gamma_{h_i}} N_{A} h_{i} d \\Gamma-\\sum_{j=1}^{n_{dof}}\\left(\\sum_{B \\in \\eta_{g_j}} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) g_{j B}\\right)\n",
      "\\tag{2.8.25} \\end{equation*} Now that we have defined $\\boldsymbol{K}$, we can establish its fundamental properties. We shall need the following preliminary\n",
      "results. Let $n_{s d}=2$ or 3 and let $w: \\Omega \\rightarrow \\mathbb{R}^{n_{s d}}$. If $w_{(i, j)}=0$ (\"zero strains\"), then $\\boldsymbol{w}$ admits the\n",
      "representations: \\begin{align*} & \\left(n_{s d}=2\\right) \\quad w(x)=\\overbrace{c}^{\\text{Translation}}+\\overbrace{c_3\\left(x_{1} e_{2}-x_{2}\n",
      "e_{1}\\right)}^{\\text{Rotation}} \\tag{2.8.26}\\\\ & \\left(n_{s d}=3\\right) \\quad w(x)=\\underbrace{c_{1}}_{\\text{Translation}}+\\underbrace{c_{2} \\times\n",
      "x}_{\\text{Rotation}}\\tag{2.8.27} \\end{align*} where \\[ c=\\left\\{\\begin{array}{l} c_{1} \\tag{2.8.28}\\\\ c_{2} \\end{array}\\right\\} \\quad\n",
      "c_{1}=\\left\\{\\begin{array}{l} c_{11} \\\\ c_{12} \\\\ c_{13} \\end{array}\\right\\} \\quad c_{2}=\\left\\{\\begin{array}{l} c_{21} \\\\ c_{22} \\\\ c_{23} \\end{array}\\right\\}\n",
      "\\] and $c_{3}$ are constants; and $\\times$ denotes the vector cross product. Equations (2.8.26) and (2.8.27) define infinitesimal rigid-body motions.\n",
      "\\subsection*{Assumption $\\mathbf{R}$} We assume that the homogeneous boundary conditions incorporated into the definition of $\\mathcal{V}^{h}$ preclude\n",
      "nontrivial infinitesimal rigid-body motions. In other words, we assume that if $w^{h} \\in \\mathcal{V}^{h}$ is a rigid-body motion, then $w^{h}$ is identically\n",
      "zero. \\subsection*{Theorem} \\begin{enumerate} \\item $K$ is symmetric. \\item If Assumption $R$ holds, then $K$ is also positive definite. \\end{enumerate} Proof\n",
      "of $1$. Symmetry. We may note that symmetry of $\\boldsymbol{K}$ follows from (2.8.16) and the symmetry of $a(\\cdot, \\cdot)$. However, we shall provide an\n",
      "alternative proof in terms of (2.8.22).\\\\ $$ \\begin{aligned} K_{P Q} & =e_{i}^{T} \\int_{\\Omega} B_{A}^{T} D B_{B} d \\Omega e_{j} \\\\ & =e_{j}^{T} \\int_{\\Omega}\n",
      "B_{B}^{T} D^{T} B_{A} d \\Omega e_{i} \\\\ & =e_{j}^{T} \\int_{\\Omega} B_{B}^{T} D B_{A} d \\Omega e_{i} \\quad \\text {(symmetry of D)} \\\\ & =K_{Q P} \\end{aligned} $$\n",
      "\\subsection*{Remark} Note that the symmetry of $\\boldsymbol{K}$ followed from the symmetry of $\\boldsymbol{D}$, which was a consequence of the major symmetry of\n",
      "the $c_{i j k l}$ 's (see (2.7.3)). Proof of 2. Positive definite (Recall from Sec. 1.9 that we must show (i) $\\boldsymbol{c}^{T} \\boldsymbol{K} c \\geq 0$ and\n",
      "(ii) $\\boldsymbol{c}^{T} \\boldsymbol{K} \\boldsymbol{c}=0$ implies $\\boldsymbol{c}=0$.) Let $w_{i}^{h}=\\Sigma_{A \\in \\eta_{-\\eta_{g_i}}} N_{A} c_{iA}$ be a\n",
      "member of $\\mathcal{V}_{i}^{h}$. Then $c_{P}=c_{iA}$, where $P=$ $\\operatorname{ID}(i, A), 1 \\leq P \\leq n_{e q}$, defines the components of an $n_{e q}$-vector\n",
      "$c$.\\\\ i. $$ \\begin{aligned} & \\boldsymbol{c}^{T} K c=\\sum_{P, Q=1}^{n_{eq}} c_{P} K_{P Q} c_{Q} \\\\ & =\\sum_{i, j=1}^{n_{dof}}\\left(\\sum_{\\substack{A \\in\n",
      "\\eta-\\eta_{g_i} \\\\ B \\in \\eta-\\eta_{g_j}}} c_{i A} a\\left(N_{A} e_{i}, N_{B} e_{j}\\right) c_{j B}\\right) \\quad \\text { (definition of } K_{P Q} \\text { ) } \\\\ &\n",
      "=a\\left(\\sum_{i=1}^{n_{dof}}\\left(\\sum_{A \\in \\eta-\\eta_{g_i}} c_{iA} N_{A} e_{i}\\right), \\sum_{j=1}^{n_ {dof}}\\left(\\sum_{B \\in \\eta-\\eta_{g_j}} c_{j B} N_{B}\n",
      "e_{j}\\right)\\right) \\quad \\text { (bilinearity of } a(\\cdot, \\cdot)) \\\\ & =a\\left(w^{h}, w^{h}\\right) \\quad \\text { (definition of } w^{h} \\text { ) } \\\\ &\n",
      "=\\int_{\\Omega} \\underbrace{w_{(i, j)}^{h} c_{ijkl} w_{(k, l)}^{h} d \\Omega}_{\\geq 0} \\quad \\text { (by (2.7.5) and (2.7.17)) } \\\\ & \\geq 0 \\end{aligned} $$ ii.\n",
      "Assume $\\boldsymbol{c}^{\\boldsymbol{T}} \\boldsymbol{K} \\boldsymbol{c}=\\mathbf{0}$. By the proof of part ( $\\left.\\mathbf{(}\\right)$, we deduce that $$\n",
      "w_{(i,j)}^{h} c_{i j k l} w_{(k, l)}^{h}=0 $$ From (2.7.6), this means that $w_{(i, j)}^{h}=0$, and so $w^{h}$ is an infinitesimal rigid motion. By Assumption\n",
      "$R, \\boldsymbol{w}^{\\boldsymbol{h}}=\\mathbf{0}$, from which it follows that $c_{p}=0$; hence $\\boldsymbol{c}=\\mathbf{0}$. \\subsection*{Remark} Positive\n",
      "definiteness of $\\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary\n",
      "conditions being incorporated into $\\mathcal{V}^{h}$.\n",
      "\n",
      " 9: All right. What we saw on the previous segment was a fairly quick statement of the stability conditions for the Newmark family, right? And we saw the\n",
      "explicit results for some of them. We didn't derive any of them. What I'd like to do in this segment is outline the, the sort of analysis that leads to those,\n",
      "to those conclusions. We won't get into a completely detailed step-by-step derivation of the results. Unlike what we did for the parabolic problem, okay? But\n",
      "we'll sketch out the, the approach. Okay, so the stability analysis here, as in the case of the parabolic problem. Is based upon examining a particular object.\n",
      "Can you recall what object we examined in the case of the parabolic problem? We examined the amplification factor. Here, too, we have an amplification factor,\n",
      "but it is a matrix. Okay. So, the stability analysis is based on, an eigenvalue analysis of the amplification matrix, all right? Okay. And, the way we proceed\n",
      "with this is to, if I remember that the amplification matrix is what we denoted as A. Okay and you also recall that the amplification matrix is what showed up\n",
      "in this formulation of the problem as here, y n plus 1 equals Ayn plus Ln when we consider the full n homogeneous problem, okay? So what we're talking about is\n",
      "analyzing this, and all our detail of the Newmark family is sitting inside there, right? The particular values of gamma and beta we've chosen, and so on, okay.\n",
      "Here is the condition, okay? We define what is called the spectral radius. The spectral radius of A, right? And we denote that as we've used rho quite a bit, so\n",
      "let me use something else here. Let me just say r, okay, spectral radius A, r sub r function, okay, okay? This is defined as the maximum over i, okay? The\n",
      "maximum over i of lambda i of A, where those lambdas are essentially the eigenvalues of this two by two matrix. All right, and because it's a two by two matrix,\n",
      "of course, i just runs over one and two, okay? That's probably not even worth using an index there. Okay, so let's say that max i equals 1,2 right? Essentially\n",
      "it's the maximum eigenvalue, okay? Not just the maximum eigenvalue, but actually it is the magnitude of it, okay? Where we have accounting for the fact that the\n",
      "matrix A may not always be symmetric, and therefore, it could have complex eigenvalues, right? Accounting for that, we write our spectral radius as being\n",
      "defined as the max i equals 1, 2. Now, that magnitude that I wrote up there is properly the square root of the product of lambda i and its complex conjugate,\n",
      "which is going to be denoted as lambda i of A bar, okay? Where that bar implies the complex conjugate. Of lambda i A, okay? That is our spectral radius. Now the\n",
      "condition for stability requires that. All right, it requires that r, the spectral radius, should be lesser than or equal to 1. Okay, the spectral radius is\n",
      "defined there, should be lesser than or equal to 1. I'm going to say a little more about this condition. We have r can be lesser than or equal to 1, if lambda 1\n",
      "and lambda 2 are distinct. Okay, it turns out, that if lambda 1 and lambda 2 are distinct the condition that we get that r can be lesser than or equal to 1\n",
      "involves the fact that the eigenvectors of A. Are linearly independent. Okay, on the other hand r has to be strictly less than 1, right? Not lesser than or\n",
      "equal to 1, it has to be strictly less than 1, if lambda 1 equals lambda 2, right? We have repeated roots, okay? And in this case it turns out that the\n",
      "eigenvectors. Of A, are linearly dependent. Okay? I'm going to do very quick demonstration of why this is the case, okay? So, let's look at the two cases, okay.\n",
      "Let's first look at what happens if they are linearly independent eigenvectors. Okay. If they are linearly independent eigenvectors, then let's look at what\n",
      "happens for the homogeneous problem as we go from one time step to the other, okay? Essentially, what we see is that yn plus 1 equals A yn, right? But then yn\n",
      "is equal to A yn minus 1, and so on, right, yn minus 1 equals, tatatata, right? Goes on. So what we are seeing here is that with every step is getting\n",
      "multiplied by itself, right? If you just make these substitutions in here, we see that, right? Okay, so what we are seeing is that, after a certain number of\n",
      "steps, we're seeing that yn plus 1 equals A to the power n plus 1 times y0. All right? So, what we need to worry about is what is happening with A. All right,\n",
      "as it gets multiplied by itself, what are the powers of A? If we have linearly independent eigenvectors, one can show that A, okay, can be written as some\n",
      "matrix P, times a two by two matrix, which is lambda 1, 0, 0, lambda 2. P inverse, okay? As a result, we get from this, we get A to the power n equals P, lambda\n",
      "1 to the power n, 0, 0, lambda 2 to the power n, P inverse, okay. So now you note that even if lambda 1 were equal to 1, all right this sort of a form stays\n",
      "well bounded, okay? All right, okay. This sort of a form stays bounded, right? So the amplification that is applied to the initial condition to get say, the nth\n",
      "time step solution does not get unbounded, okay? Things are different if you do have, if we have linearly dependent eigenvectors. In this case, the best we can\n",
      "do is write A as, let's say, some other matrix Q, two by two matrix, lambda 1, 1, lambda 2, Q inverse. Okay? This is the case of the linearly dependent\n",
      "eigenvectors, okay? Now, if you go through the process now, and calculate a to the power n. >> Okay, what you get is a form where you get Q, lambda 1 to the\n",
      "power n. You get lambda 2 to the power n, and all is looking good except for the fact that here you get n lambda, well lambda 1 is equal to lambda 2 here, okay.\n",
      "So it doesn't really matter. Let me just do this. In the case of linearly dependent eigenvectors, you have lambda 1 equal to lambda 2. So, you get n times\n",
      "lambda 1 to the power n minus 1, Q inverse. And now, do you see a problem as n gets large? Okay, what you note is that if you look at what happens here? And you\n",
      "have lambda 1 equal to 1. Okay? You see that the off diagonal term becomes n, okay? Right. In that case this term becomes n. Right? And then as you go to\n",
      "higher, and hard, higher, as you go, as you advance in time steps you have this sort of gradual sort of tendency towards unboundedness. Okay, so what happens in\n",
      "this case is that the off diagonal term. Diverges, as n. Okay? All right. Further analysis of this problem is based upon essentially solving for lambdas, right?\n",
      "So. Solving for the lambdas, right? The equation that we need to solve for the lambdas is the following. Lambda square, remember this is just the characteristic\n",
      "equation that you use to solve for the eigenvalues of a matrix, all right? So we will write it as lambda squared minus 2, A1, lambda plus A2 equals 0, okay. And\n",
      "here A1 equals one-half trace of A, and A2 equals the determinant of A. Okay? All right? With this form, it's just a simple quadratic equation, right? So we get\n",
      "lambda 1, lambda 2, are equal to what is it? A1 plus or minus square root of A1 squared minus A2. I believe, let me just look at that. Yeah, okay, so these are\n",
      "lambda 1 and lambda 2. All right. Now, here is the, sort of stability condition again written in terms of A1 and A2. Okay, because of course stability depends\n",
      "upon the values of lambda, but then since we have lambda 1, lambda 2 given by these conditions for A1 and A2, we can write it out in terms of A1 and A2. Okay?\n",
      "The conditions that we get are the following. We get minus A2 plus 1, divided by 2 is lesser than or equal to A1 is lesser than or equal to A2 plus 1 divided by\n",
      "2 if the magnitude of A2 is less than 1. Okay? All right. And. Okay. And otherwise, we get minus 1 is less than A1 is less than 1 if the magnitude of A2, sorry,\n",
      "it's not just the magnitude of A2. It's A2 itself. Here too, it's just A2. If A2 is equal to 1. Okay, one can plot this thing up in this result up in an A1, A2\n",
      "space, and here's what we see. If this is A1, and here we're plotting up A2. Okay? Let me see. I think, I'm going to mark some critical points here. These are\n",
      "the points 1, 1, minus 1, 1, and down here, I have the point 0 minus 1. Okay, in this, we have between 1, 1 and 0, 1, that line, and here we have that segment,\n",
      "okay? The first condition, this one okay, holds everywhere except for that line. Right, because that dashed line is A2 equals 1. Okay, so this sort of inverted\n",
      "triangle that I've drawn is the acceptable region. Okay for stability, if A2 is less than 1. Okay, if A2 equals 1, right, then what it says, is that on that\n",
      "line, it's got to eliminate, it's got to be, you've got to leave out those two points. Okay? That is the region of stability that we have here. Okay. Okay, we\n",
      "can end this segment here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9  # try different QAs\n",
    "\n",
    "print('Q:')\n",
    "print_wrapped(df.iloc[i,:]['question'])\n",
    "print('A:')\n",
    "print_wrapped(df.iloc[i,:]['answer'])\n",
    "print('\\nChunk used for Q generation:')\n",
    "print_wrapped(df.iloc[i,:]['question_chunk'])\n",
    "print('\\nRetrieved context:')\n",
    "for item in df.iloc[i,:]['context'].split('Additional context'):\n",
    "    print_wrapped(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

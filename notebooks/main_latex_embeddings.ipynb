{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading functions from the scripts\n",
    "\"\"\"\n",
    "Mostafa:\n",
    "I used the new structured output for question generation.\n",
    "It's a beta version, but it works on my end (10/23/2024).\n",
    "https://platform.openai.com/docs/guides/structured-outputs/structured-outputs\n",
    "\n",
    "For answer generation, I had some issues, so I used the standard API.\"\n",
    "\n",
    "Please upgrade before running this notebook: pip install --upgrade openai\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))  # Get parent directory of the notebook \n",
    "sys.path.append(parent_dir)  #  to the Python path\n",
    "\n",
    "from scripts.chunking import process_latex_files\n",
    "from scripts.embedding import get_embeddings, fixed_knn_retrieval\n",
    "from scripts.prompts import gen_questions, gen_questions_s, gen_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting API and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "production_mode = True\n",
    "chunk_by_section = True\n",
    "chpt_for_quest_answ = \"2023-spring\"           # str, chapter number or student csv filename\n",
    "author_for_quest_answ = \"students\"            # hughes, garikipati, or students  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I suggest using 'gpt-4o' for production runs, but it is more expensive.\n",
    "For embeddings, I recommend 'text-embedding-3-large.' We only need to run it once, but it also costs more.\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "# https://openai.com/index/new-embedding-models-and-api-updates/\n",
    "# https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
    "\"\"\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "if production_mode == False:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "elif production_mode == True:\n",
    "    llm_model_questions = \"gpt-4o\"\n",
    "    llm_model_answers = llm_model_questions     # option to run different model\n",
    "    embedding_size = \"large\"                    # small or large\n",
    "\n",
    "embedding_model = f\"text-embedding-3-{embedding_size}\"  # NOTE: this must be the same for all embeddings. \n",
    "author_for_quest_answ = author_for_quest_answ.lower()\n",
    "\n",
    "# Setting path for root data folder\n",
    "main_dir = f'../data/{author_for_quest_answ}_latex_Q_then_A_use_context'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Context Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding space filename: ../data/students_latex_Q_then_A_use_context/students_latex_embedding_space_by_sections_tpc4096_large.json\n",
      "loaded\n",
      "Space size: (221, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I used fixed size chunks (512) with a 25% overlap.\n",
    "Make sure environment_sensitive is set to False for fixed size.\n",
    "\n",
    "We should embed all chapters to generate the embedding space. For the demo, I only included two chapters.\n",
    "please update the paths in latex_file_paths.\n",
    "\"\"\"\n",
    "\n",
    "# add all book chapters paths\n",
    "\n",
    "if author_for_quest_answ.lower() == \"hughes\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "    ]\n",
    "\n",
    "elif author_for_quest_answ == \"garikipati\" or \"students\":\n",
    "    latex_file_paths = [ \n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter1.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter2.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter3.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter4.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter7.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter8.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter9.tex',\n",
    "        '../data/FEM_Hughes_LaTeX_Textbook/chapter10.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter101.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter102.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter103.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter104.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter105.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter106.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter107.tex',\n",
    "        '../data/FEM_Garikipati_lectures/chapter108.tex',\n",
    "    ]\n",
    "\n",
    "tokens_per_chunk = 4096                         # was 512\n",
    "token_overlap = int(0.25 * tokens_per_chunk)    # 25% overlap\n",
    "environment_sensitive = False                   # If False, equations can split between two chunks, but chunk lengths remain fixed.\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if chunk_by_section == False:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_tpc{tokens_per_chunk}_o{token_overlap}_{embedding_size}.json'\n",
    "elif chunk_by_section == True:\n",
    "    embedding_space_file_name = f'{main_dir}/{author_for_quest_answ}_latex_embedding_space_by_sections_tpc{tokens_per_chunk}_{embedding_size}.json'\n",
    "    token_overlap = 0\n",
    "print(f\"embedding space filename: {embedding_space_file_name}\")\n",
    "\n",
    "space = {}\n",
    "if not os.path.exists(embedding_space_file_name):\n",
    "    \n",
    "    chunks = process_latex_files(latex_file_paths, \n",
    "                                 tokens_per_chunk, \n",
    "                                 token_overlap, \n",
    "                                 environment_sensitive, \n",
    "                                 chunk_by_section = chunk_by_section)\n",
    "    \n",
    "    chunk_length = []\n",
    "    char_length = []\n",
    "    print(chunks)\n",
    "    for chunk in chunks:\n",
    "        print(f\"chunk word length: {len(chunk.split(\" \"))}, chunk char length: {len(chunk)}, chunk = {chunk}\")\n",
    "        chunk_length.append(len(chunk.split(\" \")))\n",
    "        char_length.append(len(chunk))\n",
    "    print(f\"max chunk length in words = {np.max(chunk_length)}\")\n",
    "    print(f\"max chunk length in char = {np.max(char_length)}\")\n",
    "    #print(f\"chunk lengths = {chunk_length}\")\n",
    "\n",
    "    # using api\n",
    "    embedding_space = get_embeddings(client, chunks, model=embedding_model)\n",
    "    \n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'w') as json_file:\n",
    "        json.dump({'embedding_model': embedding_model, 'chunks': chunks, 'embedding_space': embedding_space}, json_file)\n",
    "\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    # save\n",
    "    with open(embedding_space_file_name, 'r') as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "\n",
    "    chunks = loaded_data['chunks']\n",
    "    embedding_space = np.array(loaded_data['embedding_space'])\n",
    "    print(\"loaded\")\n",
    "\n",
    "chunks = np.array(chunks)\n",
    "embedding_space = np.array(embedding_space)\n",
    "print(\"Space size:\", embedding_space.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Questions and Their Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Questions: 100%|██████████| 20/20 [00:00<00:00, 265462.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are generated\n",
      "Questions are embedded\n",
      "saved ../data/students_latex_Q_then_A_use_context/students_2023-spring_Qs_n40_tpc1536_o307.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "For generating questions, we want larger chunks with a bit of overlap.\n",
    "The following values are just for this demo, so please adjust them as needed.\n",
    "\n",
    "I only ran Chapter One.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "if author_for_quest_answ == \"hughes\":\n",
    "    latex_file_path = f'../data/FEM_Hughes_LaTeX_Textbook/chapter{chpt_for_quest_answ}.tex'\n",
    "elif author_for_quest_answ == \"garikipati\":\n",
    "    latex_file_path = f'../data/FEM_Garikipati_lectures/chapter{chpt_for_quest_answ}.tex'\n",
    "\n",
    "max_questions = 40                             # max number of questions per chunk\n",
    "\n",
    "tokens_per_chunk = 1536                       \n",
    "token_overlap = int(0.2 * tokens_per_chunk)   # 10% overlap\n",
    "environment_sensitive = True                  # If True, equations won't be split between chunks, which may result in chunks larger than the specified tokens_per_chunk\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def embed_all_q(questions):\n",
    "    all_questions = []\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            all_questions.append(sub_item['question'])\n",
    "    # using api\n",
    "    embeddings = get_embeddings(client, all_questions, model = embedding_model) \n",
    "    # add them to data:\n",
    "    k = 0\n",
    "    for item in questions['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            sub_item['embedding'] = embeddings[k]\n",
    "            k +=1\n",
    "    print('Questions are embedded')\n",
    "    return questions\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if author_for_quest_answ == \"hughes\" or author_for_quest_answ == \"garikipati\":\n",
    "    if chunk_by_section == False:\n",
    "        questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "    elif chunk_by_section == True:\n",
    "        questions_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_Qs_n{max_questions}_by_sections_tpc{tokens_per_chunk}.json\"  \n",
    "        token_overlap = 0 \n",
    "elif author_for_quest_answ == \"students\":\n",
    "    questions_file_name = f\"{main_dir}/{author_for_quest_answ}_{chpt_for_quest_answ}_Qs_n{max_questions}_tpc{tokens_per_chunk}_o{token_overlap}.json\"\n",
    "    token_overlap = 0\n",
    "\n",
    "if not os.path.exists(questions_file_name):\n",
    "    questions = {}  # main data\n",
    "\n",
    "    # we should save generation info we used\n",
    "    questions['info'] = {\n",
    "        'tokens_per_chunk': tokens_per_chunk,\n",
    "        'token_overlap': token_overlap,\n",
    "        'environment_sensitive': environment_sensitive,\n",
    "        'max_questions': max_questions,\n",
    "        'embedding_model': embedding_model,\n",
    "        'llm_model_questions': llm_model_questions,\n",
    "        'llm_model_answers': llm_model_answers\n",
    "    }\n",
    "\n",
    "    if author_for_quest_answ == \"garikipati\" or author_for_quest_answ == \"hughes\":\n",
    "        question_chunks = process_latex_files(latex_file_path, tokens_per_chunk, token_overlap, environment_sensitive, chunk_by_section=chunk_by_section)\n",
    "    \n",
    "        if production_mode == False:\n",
    "            question_chunks = question_chunks[0:1] # for testing small batch\n",
    "        \n",
    "        for question in question_chunks:\n",
    "            print(f\"chunk word length: {len(question.split(\" \"))}, chunk char length: {len(question)}, chunk = {question}\")\n",
    "\n",
    "    \n",
    "    elif author_for_quest_answ == 'students':\n",
    "        question_path = f'../data/FEM_Student_questions/{chpt_for_quest_answ}.csv'\n",
    "        df_students = pd.read_csv(question_path, index_col=False)\n",
    "        if production_mode == False:\n",
    "            df_students = df_students.iloc[0:1]\n",
    "        question_chunks = df_students['Directory'].to_list()\n",
    "        question_chunks = [f\"{chpt_for_quest_answ}-{str(i)}\" for i in question_chunks]\n",
    "        q_for_chunk = df_students['Questions'].to_list()\n",
    "        q_for_chunk = [literal_eval(i) for i in q_for_chunk]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"author for question/answer not supported\")\n",
    "\n",
    "    ## step 1: generate questions\n",
    "    questions['data'] = []\n",
    "    for i in tqdm(range(len(question_chunks)), desc=\"Generating Questions\"):\n",
    "        # q_for_chunk = gen_questions(client, question_chunks[i], max_questions, model=llm_model_questions)\n",
    "        # return list of dictionaries w/ 'question' and 'coverage' keys\n",
    "        if author_for_quest_answ == \"garikipati\" or author_for_quest_answ == \"hughes\":\n",
    "            q_for_chunk = gen_questions_s(client, question_chunks[i], max_questions, model=llm_model_questions)   # Using the new function\n",
    "            questions['data'].append({'chunk': question_chunks[i],'questions': q_for_chunk})\n",
    "        elif author_for_quest_answ == \"students\":\n",
    "            questions['data'].append({'chunk': (question_chunks[i]),'questions': q_for_chunk[i]})\n",
    "    print('Questions are generated')\n",
    "\n",
    "    ## step 2: embedding all questions at once\n",
    "    questions = embed_all_q(questions)\n",
    "    \n",
    "\n",
    "    with open(questions_file_name, 'w') as json_file:\n",
    "        json.dump(questions, json_file, indent=4)\n",
    "    print('saved', questions_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_file_name, 'r') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_file_name)\n",
    "\n",
    "if questions['info']['embedding_model'] != embedding_model:\n",
    "    print(\"embedding model mismatch. re-embedding questions\")\n",
    "    questions = embed_all_q(questions)\n",
    "    questions['info']['embedding_model'] = embedding_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Retrieval and Generating Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k context added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering Questions: 100%|██████████| 20/20 [18:01<00:00, 54.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions are answered\n",
      "saved ../data/students_latex_Q_then_A_use_context/students_2023-spring_QAs_n40_topk10_by_sections.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "Since we answer each question separately, this process is slow.\n",
    "We might want to consider using the batch API for this.\n",
    "\"\"\"\n",
    "\n",
    "top_k = 10   # number of retrieved closest contexts         \n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "if author_for_quest_answ == \"hughes\" or author_for_quest_answ == \"garikipati\":\n",
    "    if chunk_by_section == False:\n",
    "        questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_tpc{tokens_per_chunk}_o{token_overlap}.json\"   \n",
    "    elif chunk_by_section == True:\n",
    "        questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"   \n",
    "elif author_for_quest_answ == \"students\":\n",
    "    questions_answers_file_name = f\"{main_dir}/{author_for_quest_answ}_{chpt_for_quest_answ}_QAs_n{max_questions}_topk{top_k}_by_sections.json\"\n",
    "\n",
    "if not os.path.exists(questions_answers_file_name):\n",
    "\n",
    "    questions_answers = questions.copy()\n",
    "\n",
    "    # step 1) finding top_k context from the book embedding and adding them to each question\n",
    "    for item in questions_answers['data']:\n",
    "        for sub_item in item['questions']:\n",
    "            ind = fixed_knn_retrieval(sub_item['embedding'], embedding_space, top_k)\n",
    "            context = ''\n",
    "            for i, chunk in enumerate(chunks[ind]):\n",
    "                context += f'\\n\\n Additional context {i}: {chunk}' \n",
    "            sub_item['context'] = context\n",
    "    print('top_k context added')\n",
    "\n",
    "    # step 2) generating answers (slow)  (should we try batch API?)\n",
    "    for item in tqdm(questions_answers['data'], desc=\"Answering Questions\"):\n",
    "        question_chunk = item['chunk']\n",
    "        for sub_item in item['questions']:\n",
    "            question = sub_item['question']\n",
    "            context = question_chunk + sub_item['context']\n",
    "            sub_item['answer'] = gen_answer(client, question, context, model = llm_model_answers)\n",
    "    print('Questions are answered')\n",
    "    \n",
    "    with open(questions_answers_file_name, 'w') as json_file:\n",
    "        json.dump(questions_answers, json_file, indent=4)\n",
    "    print('saved', questions_answers_file_name)\n",
    "\n",
    "else:\n",
    "    with open(questions_answers_file_name, 'r') as json_file:\n",
    "        questions_answers = json.load(json_file)\n",
    "\n",
    "    print('loaded', questions_answers_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_chunk</th>\n",
       "      <th>context</th>\n",
       "      <th>coverage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-spring-230327</td>\n",
       "      <td>\\n\\n Additional context 0: All right. So, we'l...</td>\n",
       "      <td>100</td>\n",
       "      <td>Why is \\(\\underline{\\text{det}(A)}\\) when it i...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-spring-230327</td>\n",
       "      <td>\\n\\n Additional context 0: \\tau\\left(t_{n-1}\\r...</td>\n",
       "      <td>100</td>\n",
       "      <td>How can we get the K (order of accuracy)?</td>\n",
       "      <td>Answer: To determine the order of accuracy $k$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-spring-230327</td>\n",
       "      <td>\\n\\n Additional context 0: Right, so, remarks....</td>\n",
       "      <td>100</td>\n",
       "      <td>In module 11, why is \\(K=2\\) for the midpoint ...</td>\n",
       "      <td>Answer: The order of accuracy \\(K\\) is determi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-spring-230327</td>\n",
       "      <td>\\n\\n Additional context 0: All right. What we'...</td>\n",
       "      <td>100</td>\n",
       "      <td>Why do we introduce the damping term? Is it an...</td>\n",
       "      <td>Answer: The damping term is introduced to remo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-spring-230327</td>\n",
       "      <td>\\n\\n Additional context 0: \\lambda_{1,2}(A)=A_...</td>\n",
       "      <td>100</td>\n",
       "      <td>How does damping increase the critical frequen...</td>\n",
       "      <td>Answer: In the context of the Newmark method, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2023-spring-230118</td>\n",
       "      <td>\\n\\n Additional context 0: To develop the elem...</td>\n",
       "      <td>100</td>\n",
       "      <td>- If the EA and fA terms in the stiffness matr...</td>\n",
       "      <td>If the $EA$ and $fA$ terms in the stiffness ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2023-spring-230118</td>\n",
       "      <td>\\n\\n Additional context 0: Welcome back. At th...</td>\n",
       "      <td>100</td>\n",
       "      <td>- How does the stiffness matrix change if our ...</td>\n",
       "      <td>The stiffness matrix changes when using non-li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2023-spring-230118</td>\n",
       "      <td>\\n\\n Additional context 0: With all of that, w...</td>\n",
       "      <td>100</td>\n",
       "      <td>- If we wanted to write the Finite Element Ass...</td>\n",
       "      <td>If we wanted to write the Finite Element Assem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2023-spring-230118</td>\n",
       "      <td>\\n\\n Additional context 0: Okay, so, so this i...</td>\n",
       "      <td>100</td>\n",
       "      <td>Will global degrees of freedom (DoF) be added ...</td>\n",
       "      <td>Answer: NOT ENOUGH INFO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2023-spring-230118</td>\n",
       "      <td>\\n\\n Additional context 0: Great, welcome back...</td>\n",
       "      <td>100</td>\n",
       "      <td>Why do we write the first element in the matri...</td>\n",
       "      <td>The first element in the matrix-vector weak fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         question_chunk                                            context  \\\n",
       "0    2023-spring-230327  \\n\\n Additional context 0: All right. So, we'l...   \n",
       "1    2023-spring-230327  \\n\\n Additional context 0: \\tau\\left(t_{n-1}\\r...   \n",
       "2    2023-spring-230327  \\n\\n Additional context 0: Right, so, remarks....   \n",
       "3    2023-spring-230327  \\n\\n Additional context 0: All right. What we'...   \n",
       "4    2023-spring-230327  \\n\\n Additional context 0: \\lambda_{1,2}(A)=A_...   \n",
       "..                  ...                                                ...   \n",
       "385  2023-spring-230118  \\n\\n Additional context 0: To develop the elem...   \n",
       "386  2023-spring-230118  \\n\\n Additional context 0: Welcome back. At th...   \n",
       "387  2023-spring-230118  \\n\\n Additional context 0: With all of that, w...   \n",
       "388  2023-spring-230118  \\n\\n Additional context 0: Okay, so, so this i...   \n",
       "389  2023-spring-230118  \\n\\n Additional context 0: Great, welcome back...   \n",
       "\n",
       "     coverage                                           question  \\\n",
       "0         100  Why is \\(\\underline{\\text{det}(A)}\\) when it i...   \n",
       "1         100          How can we get the K (order of accuracy)?   \n",
       "2         100  In module 11, why is \\(K=2\\) for the midpoint ...   \n",
       "3         100  Why do we introduce the damping term? Is it an...   \n",
       "4         100  How does damping increase the critical frequen...   \n",
       "..        ...                                                ...   \n",
       "385       100  - If the EA and fA terms in the stiffness matr...   \n",
       "386       100  - How does the stiffness matrix change if our ...   \n",
       "387       100  - If we wanted to write the Finite Element Ass...   \n",
       "388       100  Will global degrees of freedom (DoF) be added ...   \n",
       "389       100  Why do we write the first element in the matri...   \n",
       "\n",
       "                                                answer  \n",
       "0                             Answer: NOT ENOUGH INFO.  \n",
       "1    Answer: To determine the order of accuracy $k$...  \n",
       "2    Answer: The order of accuracy \\(K\\) is determi...  \n",
       "3    Answer: The damping term is introduced to remo...  \n",
       "4    Answer: In the context of the Newmark method, ...  \n",
       "..                                                 ...  \n",
       "385  If the $EA$ and $fA$ terms in the stiffness ma...  \n",
       "386  The stiffness matrix changes when using non-li...  \n",
       "387  If we wanted to write the Finite Element Assem...  \n",
       "388                           Answer: NOT ENOUGH INFO.  \n",
       "389  The first element in the matrix-vector weak fo...  \n",
       "\n",
       "[390 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mostafa: \n",
    "I think it's better to work with JSON/DataFrame in the code, but for reviewing QAs, CSV is easier to work with\n",
    "\"\"\"\n",
    "\n",
    "csv_file_name = f\"{main_dir}/{author_for_quest_answ}_ch{chpt_for_quest_answ}_QAs_n{max_questions}.csv\"   \n",
    "# ----------------------------------\n",
    "\n",
    "data = []\n",
    "\n",
    "for item in questions_answers['data']:\n",
    "    question_chunk = item['chunk']\n",
    "    for sub_item in item['questions']:\n",
    "        new_item = {}\n",
    "        new_item['question_chunk'] = question_chunk\n",
    "        for k,v in sub_item.items():\n",
    "            if k == 'embedding':\n",
    "                continue\n",
    "            new_item[k] = v\n",
    "        data.append(new_item)\n",
    "\n",
    "# data[0]\n",
    "df = pd.DataFrame(data)[['question_chunk','context','coverage','question','answer']]\n",
    "df.to_csv(csv_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=160):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "How can we get the K (order of accuracy)?\n",
      "A:\n",
      "Answer: To determine the order of accuracy $k$, we need to consider the local truncation error $\\tau(t_n)$ of the algorithm. According to the context, the order\n",
      "of accuracy $k$ is defined by the condition $|\\tau(t)| \\leq c \\Delta t^{k}$ for all $t \\in [0, T]$, where $c$ is a constant independent of $\\Delta t$, and $k >\n",
      "0$.   For the generalized trapezoidal methods, the order of accuracy $k$ is 1 for all $\\alpha \\in [0,1]$, except when $\\alpha = \\frac{1}{2}$, in which case $k =\n",
      "2$. This is because the local truncation error $\\tau(t_n)$ can be expanded using Taylor series, and the terms of the expansion determine the order of accuracy.\n",
      "The trapezoidal rule ($\\alpha = \\frac{1}{2}$) is the only member of the family of methods that is second-order accurate.\n",
      "\n",
      "Chunk used for Q generation:\n",
      "2023-spring-230327\n",
      "\n",
      "Retrieved context:\n",
      "\n",
      "\n",
      " 0: \\tau\\left(t_{n-1}\\right)-\\Delta t \\cdot \\tau\\left(t_{n}\\right) $$ and so on. The final result is $$ e\\left(t_{n+1}\\right)=A^{n+1} e(0)-\\Delta t\n",
      "\\sum_{i=0}^{n} A^{i} \\tau\\left(t_{n-i}\\right) $$ The first term on the right vanishes since $e(0)=d_{0}-d(0)=0$. Taking absolute values evaluated at $t_{n}$\n",
      "instead of $t_{n+1}$ and using some elementary facts: $$ \\begin{aligned} & \\left|e\\left(t_{n}\\right)\\right|=\\Delta t\\left|\\sum_{i=0}^{n-1} A^{i}\n",
      "\\tau\\left(t_{n-1-i}\\right)\\right| \\\\ & \\leq \\Delta t \\sum_{i=0}^{n-1}|A|^{i}\\left|\\tau\\left(t_{n-1-i}\\right)\\right| \\\\ & \\leq \\Delta t\n",
      "\\sum_{i=0}^{n-1}\\left|\\tau\\left(t_{n-1-i}\\right)\\right| \\quad \\text { (stability) } \\\\ & \\leq t_{n} \\max |\\tau(t)| \\quad t \\in[0, T] \\\\ & \\leq t_{n} c \\Delta\n",
      "t^{k} \\quad \\text { (consistency) } \\end{aligned} $$ Therefore $e\\left(t_{n}\\right) \\rightarrow 0$ as $\\Delta t \\rightarrow 0$, and furthermore the rate of\n",
      "convergence is $k$ (i.e., $\\left.e\\left(t_{n}\\right)=O\\left(\\Delta t^{k}\\right)\\right)$ \\subsubsection*{Remarks} \\begin{enumerate} \\item This result establishes\n",
      "the convergence of the generalized trapezoidal methods. The maximal rate of convergence is 2 and is attained by the trapezoidal rule. \\item This theorem is a\n",
      "particular example of perhaps the most celebrated theorem in numerical analysis, the Lax equivalence theorem, which may be stated as \"consistency plus stability\n",
      "is necessary and sufficient for convergence.\" \\end{enumerate} Exercise 1. Suppose that in (8.2.32) $F_{n+\\infty}$ is replaced by an approximation,\n",
      "$F_{n+\\alpha}^{\\Delta t}$, which\\\\ satisfies $\\left|F_{n+\\alpha}-F_{n+\\alpha}^{A_{t}}\\right| \\leq$ constant $\\cdot \\Delta t^{q}$. What condition must $q$\n",
      "satisfy to retain $k$ thorder accuracy? \\subsection*{8.2.5 Additional Exercises} Exercise 2. Consider the following one-parameter ( $\\alpha$ ) family of\n",
      "predictor-corrector algorithms: $$ \\begin{aligned} & M v_{n+1}+K \\widetilde{d}_{n+1}=F_{n+1} \\\\ & \\widetilde{d}_{n+1}=d_{n}+(1-\\alpha) \\Delta t v_{n} \\quad\n",
      "\\text { (predictor) } \\\\ & d_{n+1}=\\tilde{d}_{n+1}+\\alpha \\Delta t v_{n+1} \\quad \\text { (corrector) } \\end{aligned} $$ Assume $\\alpha \\in[0,1]$\\\\ i. Determine\n",
      "an expression for the amplification factor $A$ and $\\operatorname{plot} A$ versus $\\lambda^{h} \\Delta t$.\\\\ ii. Determine under what circumstances the\n",
      "algorithms are stable.\\\\ iii. Obtain an expression for the local truncation error.\\\\ iv. Determine the rate of convergence.\\\\ v. If $\\boldsymbol{M}$ is\n",
      "diagonal, is the algorithm implicit or explicit? Answers: The modal equations are: \\begin{align*} v_{n+1}+\\lambda^{h} \\tilde{d}_{n+1} & =F_{n+1} \\tag{a}\\\\\n",
      "\\tilde{d}_{n+1} & =d_{n}+(1-\\alpha) \\Delta t v_{n} \\tag{b}\\\\ d_{n+1} & =\\tilde{d}_{n+1}+\\alpha \\Delta t v_{n+1} \\tag{c} \\end{align*} (a) and (c) imply (\n",
      "$\\left.1-\\alpha \\Delta t \\lambda^{h}\\right) v_{n+1}+\\lambda^{h} d_{n+1}=F_{n+1}$ Likewise at $n: \\quad\\left(1-\\alpha \\Delta t \\lambda^{h}\\right)\n",
      "v_{n}+\\lambda^{h} d_{n}=F_{n}$\\\\ (b) and (c) imply $d_{n+1}=d_{n}+(1-\\alpha) \\Delta t v_{n}+\\alpha \\Delta t v_{n+1}$ Multiply (f) by ( $\\left.1-\\alpha \\Delta t\n",
      "\\lambda^{h}\\right)$ :\\\\ $\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) d_{n+1}= \\left(1-\\alpha \\Delta t \\lambda^{h}\\right) d_{n}+\\Delta t(1-\\alpha)\n",
      "\\overbrace{\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) v_{n}}^{(e)} + \\alpha \\Delta t \\underbrace{\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) v_{n+1}}_{d}$\\\\\n",
      "$\\left(1-\\alpha \\cancel{\\Delta t \\lambda^{h}}\\right) d_{n+1} = \\left(1-\\cancel{\\alpha \\Delta t \\lambda^{h}} \\right) d_{n}+\\Delta\n",
      "t(1-\\cancel{\\alpha})\\left[F_{n}-\\cancel{\\lambda}^{h} d_{n}\\right]+\\alpha \\Delta t\\left[F_{n+1}-\\lambda^{h} \\cancel{d_{n+1}}\\right]$ $$ d_{n+1}=\\left(1-\\Delta t\n",
      "\\lambda^{h}\\right) d_{n}+\\Delta t \\underbrace{\\left[(1-\\alpha) F_{n}+\\alpha F_{n+1}\\right]}_{F_{n+\\alpha}} $$ i. $A=1-\\Delta t \\lambda^{'}$\\\\ ii. $|A|<1$\n",
      "implies $\\Delta t \\lambda^{h}<2$\\\\ iii. $\\Delta t \\tau\\left(t_{n}\\right)=d\\left(t_{n+1}\\right)-\\left(1-\\Delta t \\lambda^{h}\\right) d\\left(t_{n}\\right)-\\Delta t\n",
      "F_{n+\\alpha}$ $$ =\\Delta t \\underbrace{\\left(\\dot{d}\\left(t_{n}\\right)+\\lambda^{'} d\\left(t_{n}\\right)-F\\left(t_{n}\\right)\\right)}_{0}+\\Delta\n",
      "t^{2}\\left(\\frac{\\dot{d}\\left(t_{n}\\right)}{2}-\\alpha \\dot{F}\\left(t_{n}\\right)\\right)+O\\left(\\Delta t^{3}\\right) $$ Therefore $|\\tau(t)| \\leq c \\Delta t$\\\\ iv.\n",
      "$k=1$\\\\ v. Explicit Exercise 3. Consider the following fractional-step algorithm for the homogeneous model heat equation: \\[ \\begin{array}{r} \\frac{d_{n+1 /\n",
      "2}-d_{n}}{(\\Delta t / 2)}+\\lambda^{h} d_{n}=0 \\\\ \\frac{d_{n+1}-d_{n+1 / 2}}{(\\Delta t / 2)}+\\lambda^{h} d_{n+1}=0 \\tag{b} \\end{array} \\] i. Obtain an expression\n",
      "for the amplification factor.\\\\ ii. Determine the conditions under which the algorithm is stable.\\\\ iii. What is the rate of convergence of this algorithm with\n",
      "respect to solutions of $\\dot{d}+\\lambda^{h} d=0$ ? \\subsubsection*{Answers:} i. Solve (a) for $d_{n+1 / 2}$ and eliminate from (b) to arrive at $$ d_{n+1}=A\n",
      "d_{n} ; \\quad A=\\frac{1-\\Delta t \\lambda^{h} / 2}{1+\\Delta t \\lambda^{h} / 2} $$ Observe that $A$ is identical to the one obtained for $\\alpha=\\frac{1}{2}$\n",
      "(i.e., trapezoidal rule) in the generalized trapezoidal family.\\\\ ii. Unconditionally stable\\\\ iii. $k=2$ Exercise 4. Consider the homogeneous semidiscrete heat\n",
      "equation, $$ M \\dot{d}+K d=0 $$ and the following algorithm: $$ \\begin{aligned} d_{n+1} & =d_{n}+\\Delta t v_{n}+\\frac{\\Delta t^{2}}{2} a_{n} \\\\ M v_{n+1}+K\n",
      "d_{n+1} & =0 \\\\ M a_{n+1}+K v_{n+1} & =0 \\end{aligned} $$ i. Determine the amplification factor for this algorithm and plot it versus $\\lambda^{h} \\Delta t$.\n",
      "Hint: $$ \\psi_{m}^{T} M\\left(M^{-1} K\\right)^{2} \\psi_{m}=\\lambda_{m}^{2} $$ ii. Determine the stability condition.\\\\ iii. Assuming $M$ is diagonal, is this\n",
      "algorithm implicit or explicit? Justify your answer.\\\\ iv. Determine an expression for the local truncation error. Hint: Expand about $t_{n}$.\\\\ v. Based on the\n",
      "result of part (iv), determine the order of accuracy of this algorithm. \\subsubsection*{Answers:} i. $A=1-\\Delta t \\lambda^{h}+\\left(\\Delta t\n",
      "\\lambda^{h}\\right)^{2} / 2$\\\\ ii. $\\Delta t<2 / \\lambda_{n_{\\text {eq}}}^{h}$\\\\ iii. Explicit\\\\ iv. $\\tau\\left(t_{n}\\right)=\\Delta t^{2} \\ddot{d}(\\bar{t}) / 6$,\n",
      "where $t \\in\\left[t_{n}, t_{n+1}\\right]$\\\\ v. $k=2$ Exercise 5. Modal decomposition of the algorithm $$ \\begin{aligned} d_{n+1} & =d_{n}+\\frac{\\Delta\n",
      "t}{2}\\left(v_{n}+v_{n+1}\\right)+\\frac{\\Delta t^{2}}{12}\\left(a_{n}-a_{n+1}\\right) \\\\ M v_{n+1} & =-K d_{n+1}+F_{n+1} \\\\ M a_{n+1} & =-K v_{n+1}+\\dot{F}_{n+1}\n",
      "\\end{aligned} $$ leads to the difference equation \\begin{align*} \\left(1+\\frac{\\Delta t \\lambda^{h}}{2}+\\frac{\\left(\\Delta t \\lambda^{h}\\right)^{2}}{12}\\right)\n",
      "d_{n+1}= & \\left(1-\\frac{\\Delta t \\lambda^{h}}{2}+\\frac{\\left(\\Delta t \\lambda^{h}\\right)^{2}}{12}\\right) d_{n}+\\frac{\\Delta t}{2}\\left(F_{n}+F_{n+1}\\right) \\\\\n",
      "& -\\frac{\\Delta t^{2} \\lambda^{h}}{12}\\left(F_{n}-F_{n+1}\\right)+\\frac{\\Delta t^{2}}{12}\\left(\\dot{F}_{n}-\\dot{F}_{n+1}\\right) \\tag{a} \\end{align*} i. Plot the\n",
      "amplification factor $A$ versus $\\Delta t \\lambda^{h}$. Comment on the stability of the algorithm. [Answer: Unconditionally stable.]\\\\ ii. Determine the order\n",
      "of accuracy. [Answer: $k=4$. This one is a lot of work!] \\subsubsection*{Remark} The matrix version of (a), which would be implemented on the computer to obtain\n",
      "$d_{n+1}$, involves the product $\\boldsymbol{K} \\boldsymbol{M}^{-1} \\boldsymbol{K}$ in the coefficient matrix. This product has a profile structure involving\n",
      "approximately twice the number of terms as the original $K$ if $M$ is diagonal and is full if $M$ is consistent. Thus the algorithm involves considerably more\n",
      "storage and computations than does the generalized trapezoidal method, for example. It is also inconvenient to obtain the time derivatives of $\\boldsymbol{F}$\n",
      "required at each step. Exercise 6. The SDOF model problem under consideration consists of $$ \\begin{aligned}\n",
      "\\dot{d}+\\left(\\lambda^{h}+\\tilde{\\lambda}^{h}\\right) d & =F \\\\ d(0) & =d_{0} \\end{aligned} $$ Assume $$ \\lambda^{h}>0, \\quad \\tilde{\\lambda}^{h}>0 $$ Consider\n",
      "the following one-parameter family of algorithms: $$ \\begin{aligned} v_{n+1}+\\lambda^{h} d_{n+1}+\\tilde{\\lambda}^{h} \\widetilde{d}_{n+1} & =F_{n+1} \\\\\n",
      "\\widetilde{d}_{n+1} & =d_{n}+(1-\\alpha) \\Delta t v_{n} \\\\ d_{n+1} & =\\widetilde{d}_{n+1}+\\alpha \\Delta t v_{n+1} \\end{aligned} $$ Assume $\\boldsymbol{\\alpha}\n",
      "\\in[0,1]$.\\\\ i. Determine an expression for the amplification factor.\\\\ ii. Determine under what circumstances the algorithm is stable.\\\\ iii. Obtain an\n",
      "expression for the local truncation error.\\\\ iv. Determine the rate of convergence. \\subsubsection*{Answers:} i. $A=\\left[1-\\alpha \\Delta t\n",
      "\\tilde{\\lambda}^{h}-(1-\\alpha) \\Delta t\\left(\\lambda^{h}+\\tilde{\\lambda}^{h}\\right)\\right] /\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)$\\\\ ii. $\\Delta\n",
      "t\\left[\\widetilde{\\lambda}^{h}+\\lambda^{h}(1-2 \\alpha)\\right]<2$\\\\ iii. $\\tau\\left(t_{n}\\right)=\\Delta t\\left[\\left(\\alpha-\\frac{1}{2}\\right)\n",
      "\\ddot{d}\\left(t_{n+\\alpha}\\right)+\\alpha \\tilde{\\lambda}^{h} \\dot{d}\\left(t_{n+\\alpha}\\right)\\right]$\\\\ iv. $k=1$ Exercise 7. Consider the following one-\n",
      "parameter ( $\\alpha$ ) family of iterative, predictor-corrector algorithms: $$ \\begin{aligned} M v_{n+1}^{i+1}+K d_{n+1}^{'} & =F_{n+1} \\\\ d_{n+1}^{0} &\n",
      "=d_{n}+(1-\\alpha) \\Delta t v_{n} \\\\ d_{n+1}^{i+1} & =d_{n+1}^{0}+\\alpha \\Delta t v_{n+1}^{i+1} \\end{aligned} $$ Assume $\\alpha \\in[0,1]$ and $i=0,1, \\ldots, I$,\n",
      "where $I+1$ is the total number of iterations. Define $$ \\begin{aligned} d_{n+1} & =d_{n+1}^{l+1} \\\\ v_{n+1} & =v_{n+1}^{l+1} \\end{aligned} $$ (With $I=0$, this\n",
      "algorithm is the same as the one considered in Exercise 2.) Take the case $I=1$.\\\\ i. Determine an expression for the amplification factor.\\\\ ii. Determine\n",
      "under what circumstances the algorithms are stable.\\\\ iii. Obtain an expression for the local truncation error.\\\\ iv. Determine the rate of convergence as a\n",
      "function of $\\alpha$. Solution: \\begin{align*} M v_{n+1}^{1}+K d_{n+1}^{0} & =F_{n+1} \\tag{a}\\\\ M v_{n+1}^{2}+K d_{n+1}^{1} & =F_{n+1} \\tag{b}\\\\ d_{n+1}^{0} &\n",
      "=d_{n}+(1-\\alpha) \\Delta t v_{n} \\tag{c}\\\\ d_{n+1}^{1} & =d_{n+1}^{0}+\\alpha \\Delta t v_{n+1}^{1} \\tag{d}\\\\ d_{n+1}^{2} & =d_{n+1}^{0}+\\alpha \\Delta t\n",
      "v_{n+1}^{2} \\tag{e} \\end{align*} \\begin{equation*} \\text { (a) } \\Rightarrow v_{n+1}^{1}=M^{-1}\\left(F_{n+1}-K d_{n+1}^{0}\\right) \\tag{f} \\end{equation*} (d)\n",
      "and (f) $\\Rightarrow d_{n+1}^{1}=d_{n+1}^{0}+\\alpha \\Delta t M^{-1}\\left(F_{n+1}-K d_{n+1}^{0}\\right)$\\\\ (b) and (g) $\\Rightarrow M v_{n+1}^{2}+K\n",
      "d_{n+1}^{0}+\\alpha \\Delta t K M^{-1} F_{n+1}-\\alpha \\Delta t K M^{-1} K d_{n+1}^{0}=F_{n+1}$ That is, $$ M v_{n+1}^{2}+K\\left(I-\\alpha \\Delta t M^{-1} K\\right)\n",
      "d_{n+1}^{0}=\\left(I-\\alpha \\Delta t K M^{-1}\\right) F_{n+1} $$ Let $$ \\begin{aligned} & B=I-\\alpha \\Delta t M^{-1} K \\\\ & B=I-\\alpha \\Delta t K M^{-1}\n",
      "\\end{aligned} $$ Therefore, \\begin{equation*} M v_{n+1}^{2}+K B d_{n+1}^{0}=\\boldsymbol{\\tilde{B}} F_{n+1} \\tag{h} \\end{equation*} (e) and (h) $\\Rightarrow M\n",
      "v_{n+1}^{2}+\\boldsymbol{K B}\\left(d_{n+1}^{2}-\\alpha \\Delta t v_{n+1}^{2}\\right)=\\widetilde{B} F_{n+1}$ Therefore, $$ \\begin{aligned} (M-\\alpha \\Delta t K B)\n",
      "v_{n+1}^{2}+K B d_{n+1}^{2} & =\\widetilde{B} F_{n+1} \\\\ v_{n+1}^{2}=v_{n+1}, \\quad d_{n+1}^{2} & =d_{n+1} \\end{aligned} $$ Thus $$ \\begin{aligned} \\widetilde{M}\n",
      "v_{n+1}+\\widetilde{K} d_{n+1} & =\\widetilde{F}_{n+1} \\\\ d_{n+1} & =d_{n}+(1-\\alpha) \\Delta t v_{n}+\\alpha \\Delta t v_{n+1} \\end{aligned} $$ where $$\n",
      "\\widetilde{M}=M-\\alpha \\Delta t K B, \\quad \\widetilde{K}=K B, \\quad \\text { and } \\quad \\widetilde{F}_{n+1}=\\widetilde{B} F_{n+1} $$ Hence, the modal equation\n",
      "is $$ \\left(1-\\alpha \\Delta t \\lambda^{h} z\\right) v_{n+1}+\\lambda^{h} z d_{n+1}=z F_{n+1} $$ where $$ d_{n+1}=d_{n}+(1-\\alpha) \\Delta t v_{n}+\\alpha \\Delta t\n",
      "v_{n+1} $$ and $$ z=1-\\alpha \\Delta t \\lambda^{h} $$ Therefore $$ \\begin{aligned} \\left(1-\\alpha \\Delta t \\lambda^{h} z\\right) \\cdot \\frac{1}{\\alpha \\Delta\n",
      "t}\\left(d_{n+1}-d_{n}-(1-\\alpha) \\Delta t v_{n}\\right)+\\lambda^{h} z d_{n+1} & =z F_{n+1} \\\\ d_{n+1}-\\left(1-\\alpha \\Delta t \\lambda^{h} z\\right)\n",
      "d_{n}-\\left(1-\\alpha \\Delta t \\lambda^{h} z\\right)(1-\\alpha) \\Delta t v_{n} & =\\alpha \\Delta t z F_{n+1} \\end{aligned} $$ but $$ \\left(1-\\alpha \\Delta t\n",
      "\\lambda^{h} z\\right) v_{n}+\\lambda^{h} z d_{n}=z F_{n} $$ Thus $$ d_{n+1}-\\left(1-\\alpha \\Delta t \\lambda^{h} z\\right) d_{n}+\\Delta t(1-\\alpha)\\left(\\lambda^{h}\n",
      "z d_{n}-z F_{n}\\right)=\\alpha \\Delta t z F_{n+1} $$ Therefore, $$ d_{n+1}-\\left(1-\\Delta t \\lambda^{h} z\\right) d_{n}=\\Delta t z F_{n+\\alpha} $$ i.\n",
      "Amplification factor: $A=1-\\Delta t \\lambda^{h}\\left(1-\\alpha \\Delta t \\lambda^{h}\\right)=1-\\Delta t \\lambda^{h}+\\alpha\\left(\\Delta t \\lambda^{h}\\right)^{2}$\\\\\n",
      "ii. Stability conditions: $|A|<1$, i.e., $-1<A<1$. $$ A<1 \\Rightarrow-\\Delta t \\lambda^{h}+\\alpha\\left(\\Delta t \\lambda^{h}\\right)^{2}<0 $$ Because\n",
      "$\\lambda^{h}>0$ and $\\Delta t>0$, it follows that $\\alpha \\Delta t \\lambda^{h}<1$ and $\\Delta t<1 / \\alpha \\lambda^{h}$. $$ A>-1 \\Rightarrow \\alpha\\left(\\Delta\n",
      "t \\lambda^{h}\\right)^{2}-\\Delta t \\lambda^{h}+2>0 $$ i.e., $$ \\begin{array}{r} \\left(\\Delta t \\lambda^{h}\\right)^{2}-\\frac{\\Delta t\n",
      "\\lambda^{h}}{\\alpha}+\\frac{2}{\\alpha}>0 \\quad(\\alpha>0) \\\\ f\\left(\\Delta t \\lambda^{h}\\right)=\\left[\\Delta t \\lambda^{h}-\\frac{1}{2 \\alpha}(1-\\sqrt{1-8\n",
      "\\alpha})\\right]\\left[\\Delta t \\lambda^{h}-\\frac{1}{2 \\alpha}(1+\\sqrt{1-8 \\alpha})\\right]>0 \\end{array} $$ If $\\alpha \\leq \\frac{1}{8}, \\Delta t\n",
      "\\lambda^{h}>\\frac{1}{2 \\alpha}(1+\\sqrt{1-8 \\alpha})$ or $\\Delta t \\lambda^{h}<\\frac{1}{2 \\alpha}(1-\\sqrt{1-8 \\alpha})$.\\\\ If $\\alpha>\\frac{1}{8}, f\\left(\\Delta\n",
      "t \\lambda^{h}\\right)>0$ for all $\\Delta t \\lambda^{h}$. Stability conditions\\\\ If $\\alpha \\leq \\frac{1}{8}, \\Delta t<\\frac{1}{2 \\alpha \\lambda^{h}}(1-\\sqrt{1-8\n",
      "\\alpha})$.\\\\ If $\\alpha>\\frac{1}{8}, \\Delta t<\\frac{1}{\\alpha \\lambda^{h}}$.\\\\ iii. Let $d\\left(t_{n}\\right)=d, \\dot{d}\\left(t_{n}\\right)=\\dot{d}, \\ldots$ $$\n",
      "\\begin{aligned} \\Delta t \\tau\\left(t_{n}\\right)= & d\\left(t_{n+1}\\right)-\\left(1-\\Delta t \\lambda^{h}+\\alpha\\left(\\Delta t \\lambda^{h}\\right)^{2}\\right)\n",
      "d\\left(t_{n}\\right) \\\\ & -\\alpha \\Delta t\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) F_{n+1}-\\Delta t(1-\\alpha)\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) F_{n}\n",
      "\\\\ = & \\cancel{\\alpha}+\\Delta t \\dot{d}+\\frac{{\\Delta} t^{2}}{2} \\ddot{d}+O\\left(\\Delta t^{3}\\right)-\\cancel{\\alpha}+\\Delta t \\lambda^{h} d-\\alpha\\left(\\Delta t\n",
      "\\lambda^{h}\\right)^{2} d \\\\ & -\\left(1-\\alpha \\Delta t \\lambda^{h}\\right) \\Delta t\\left\\{\\alpha \\cancel{F_{n}}+\\alpha \\Delta t \\dot{F}_{n}+\\alpha \\frac{\\Delta\n",
      "t^{2}}{2} \\ddot{F}_{n}+(1-\\cancel{\\alpha}) F_{n}\\right\\} \\\\ = & \\Delta t\\left(\\dot{d}+\\lambda^{h} d-F_{n}\\right)+\\frac{\\Delta t^{2}}{2}\\left(\\ddot{d}-2\n",
      "\\alpha\\left(\\lambda^{h}\\right)^{2} d-2 \\alpha \\dot{F}_{n}+2 \\alpha \\lambda^{h} F_{n}\\right) \\\\ & +O\\left(\\Delta t^{3}\\right) \\end{aligned} $$ But\n",
      "$\\dot{d}+\\lambda^{h} d=F_{n}$, so $$ \\begin{aligned} \\Delta t \\tau\\left(t_{n}\\right) & =\\frac{\\Delta t^{2}}{2}\\left(\\ddot{d}-2 \\alpha \\dot{F}_{n}+2 \\alpha\n",
      "\\lambda^{h}\\left(F_{n}-\\lambda^{h} d\\right)\\right)+O\\left(\\Delta t^{3}\\right) \\\\ & =\\frac{\\Delta t^{2}}{2}\\left(\\ddot{d}-2 \\alpha\\left(\\dot{F}_{n}-\\lambda^{n}\n",
      "\\dot{d}\\right)\\right)+O\\left(\\Delta t^{3}\\right) \\\\ & =\\frac{\\Delta t^{2}}{2}(1-2 \\alpha) \\ddot{d}+O\\left(\\Delta t^{3}\\right) \\end{aligned} $$ iv. If $\\alpha\n",
      "\\neq \\frac{1}{2}, \\quad\\left|\\tau\\left(t_{n}\\right)\\right| \\leq c \\Delta t$ $$ k=1, \\quad \\text { first order } $$ $$ \\text { If } \\alpha=\\frac{1}{2},\n",
      "\\quad\\left|\\tau\\left(t_{n}\\right)\\right| \\leq c \\Delta t^{2} $$ $k=2, \\quad$ second order\n",
      "\n",
      " 1: Right, so, remarks. Backward Euler. Right, which is alpha equals 1 damps out, well, I don't like the term damps, I'm going to say dissipates. Dissipates\n",
      "high order modes. Okay, and this is what we call a numerical dissipation. Okay, and because as I observed this has the same high-order behavior as the exact\n",
      "equation, it is often preferred. Okay? Numerical dissipation, right? And, let me also say here, it is similar to time exact equation. Right. The second note\n",
      "here is that there really isn't much to say about forward Euler by the way. It should come as no surprise to us as th, that that the amplification factor for\n",
      "forward Euler tends to minus infinity as lambda h delta t gets large. We've already seen some evidence of it. And in what way have we seen evidence of it? We've\n",
      "seen it in the fact that if delta t gets too large we know that the, that the forward Euler method is no longer stable, right? And that is also reflected in, in\n",
      "the, in the amplification factor going to minus, going to, getting unbounded in this case, minus infinity, as delta t gets too large, right?. Remember lambda h\n",
      "delta t can be can get large if either lambda h or delta t get large. If you have large times that, we know that there is a tendency for forward Euler to blow\n",
      "up, so to speak. Right. And we see that in the amplification factor, tending to minus infinity. Okay, so let me state that though. Forward Euler. Forward Euler,\n",
      "which is alpha equals zero. Has unbounded. Lambda has unbounded A. Okay, all of this, by the way, is in the context of high order modes, right. So I should say\n",
      "up here, this is all for high order modes. Okay, so everything I'm writing here applies to high order modes, okay? All right, so, this should come as no\n",
      "surprise to us, because we know that forward Euler has this tendency to, to sort of lose it, okay? Remark three, is that the surprising thing is that the\n",
      "midpoint rule. Or it may be a surprise. That the midpoint rule, which is alpha equals one half. Right has, has as we've seen A tending to mi, to -1, okay. What\n",
      "does this mean? What this means is that dn plus 1 for the high order modes, right if this is a high order mode I'm writing out, right? Kay, if dn plus 1 is the\n",
      "model coefficient of of a high order mode. What we're seeing is that dn plus 1 tends, is equal to minus dn. This leads to oscillatory behavior in the high order\n",
      "modes. And it's not uncommon to see solutions for from, from the mid-point rule which with respect to time, all right, if we're plotting certain modes with\n",
      "respect to time, what we will see is that if this is t1, t2, t3 and so on. Right? What we may very well see is that you get you get essentially oscillatory\n",
      "behavior. Okay? Something that's done is to simply form a time average, okay. And to take a time average you basically go damp out, well you don't truly damp\n",
      "out, but you, eliminate the effect of these oscillations in your post processed solution. Right, time average the, time average the solution, okay, to get\n",
      "around this. All right, this is important to know about the behavior of these methods in the high frequency limit, right, for high order modes. Okay. So we've\n",
      "looked at stability, we've understood high order, and the behavior of high order modes. What we are going to do next is essentially prepare ourselves for for\n",
      "talking about convergence. Alright? And the way we do that is by first looking at the notion of consistency. Okay? So, we look now at the idea of consistency.\n",
      "Alright. I order to get to consistency, let's go back to our discretized equation. Okay? And that equation, as you may recall, is the following. It is dn plus 1\n",
      "times 1 plus alpha, well, no, let me back up a little here. Let me write it in fact in the following manner. Let me write it all the way back as dn plus 1 minus\n",
      "dn over delta t plus lambda h d plus alpha equals. Now, up to recently, up to a few seconds ago we were looking at the homogeneous problem, right? Which we got\n",
      "to by turning off the high order modes, sorry by turning off the forcing. We're now bringing back the forcing. Okay? And I'm going to write this as F at N plus\n",
      "alpha, on the right hand side. Okay? If you wonder what FN plus alpha is, FN plus alpha, right, for this particular mode, is got by looking at F sub n plus\n",
      "alpha, the whole vector, right? And essentially, right, dotting it with whichever mode we are looking at. Okay? If we we're looking at mode L we would dot it\n",
      "with the Lth mode, okay. But since we've agreed that we're going to drop the explicit mention of the model number, right, we will just write it as FN plus alpha\n",
      "here. Okay, alright, now, let's multiply through by delta T and see what we get. We get dn plus 1 minus dn plus lambda h Delta t. I'm going to expand out dn\n",
      "plus alpha, as alpha dn plus 1, plus 1 minus alpha dn equals delta tFn plus alpha, alright. Working with this a little more, what we get is dn plus 1 times 1\n",
      "plus alpha delta t lambda H minus one minus, one minus alpha delta t lambda h, dn minus delta tFn plus alpha, equals zero. As a final step I'm going to divide\n",
      "through by one plus alpha delta t lambda h to give me dn plus 1 minus. Now when I divide this quantity by this I get back my amplification factor. So I'm going\n",
      "to jump a step and simply write that as A dn minus delta t divided by 1 plus alpha delta t lambda h, Fn plus alpha equals 0. Okay? Right, and I'm going to take\n",
      "the final step of calling this quantity here l at n plus one. Okay? Or I think I'll call it m, doesn't really matter, let me just call it l at n. Okay. All\n",
      "right? So this is how I want to write out my time discrete model equation, right, including the forcing. Now, the notion of consistency is the following. What\n",
      "would happen if you were to take the time exact modal equation, and plug it in here. Okay, so now let me now write that as d at tn plus 1 is the time exact mode\n",
      "corresponding. To lambda h. Okay? The particular value of lambda h gives us a certain mode, and that's it. Okay. Now the question we ask is what would happen if\n",
      "we were to take our time exact model solution and plug it back in here. All right? So what we are saying is that if we do that we get d at t and plus 1 minus A\n",
      "d at tn, right? This is also th, the time exact model value at time tn minus minus Ln. Okay, because the last term Ln does not depend upon on the, upon the\n",
      "modal values. Okay? Now, this equation is equal to zero. Correct, right? The left hand side on this equation is equal to zero. My question is, if we were to not\n",
      "plug in our exact solution here, right, as we, as I've done. Will the right hand, will what I've written here on the left hand side be equal to zero or not? In\n",
      "general, it's not equal to zero, okay? In general, however, one can write it out as delta t times some quantity tau. Okay? Tau which in general depends upon the\n",
      "time. Okay, right, the reason we are writing it out as delta T times tau is that, because we recognize that having started from this form of the equation, we've\n",
      "actually multiplied it, multiplied the ODE through by delta T. So we expect that whatever the right hand side is, it already has factor of delta T sitting in\n",
      "there. Right, so we choose to write it in this fashion. Okay, now. This is the general form that we would get for the exact equation. All right. It turns out\n",
      "that we can now make the following identification. Okay. What we say is that. Sorry. See, let me back up to the previous slide. Let me just say one more thing\n",
      "here. If we can show that our exact solution satisfies an equation of this type, okay, we had what we call a consistent method. Hour is no what no let me back\n",
      "up a little more its not quite a consistent method, this is a consistency condition. Okay? This is a consistency condition where we have a consistent method if\n",
      "we can show further. That tau, which I've written as depending upon the, the actual time. If you can show that tau is lesser than or equal to sum constant c\n",
      "times delta t, to some power K, okay? Where K is greater than zero. Okay? If we can show this, then we have a consistent method. Here's why. If we can show that\n",
      "this holds, one can say that as in, in the limit, as delta T tends to zero. Right? What we will find is that d at tn plus 1, minus Ad at tn, minus Ln, which is\n",
      "equal to delta t c delta t to the power of k, right? This thing also tends to zero. Right? This entire thing also tends to zero. Okay? All right, if k is\n",
      "greater than zero. Alright, and therefore what this suggests is that, yes the equation we're working with, the finite difference equation we're working with in\n",
      "time, is one that when we plug in the exact solution, though it does not immediately satisfy the exact solution, at least it does satisfy the limit of, of,\n",
      "vanishing time scales. Okay, so in the limit delta t tends to 0, the time discrete equation. Admits. The exact solution. Right, and which I'm sure you will\n",
      "agree is a useful property for a method. Right, for a numerical method. Okay? I should mention, of course, that in that, in that result t of tao N eq, is lesser\n",
      "than or equal to c delta t to the power k. C is a constant, and K is what we call, k is the order of accuracy. Okay? And it turns out that k equals 2 if alpha\n",
      "equals half, right. The midpoint rule. And it's equal to 1 otherwise. Okay. All right? So, the midpoint rule, as you may expect, because it does look at the, it\n",
      "does apply the algorithm between n and n plus 1. Gives you higher accuracy then any of the other methods. Right, so backward Euler and forward Euler are, are,\n",
      "are down here. And any other method also has Euler factor of c 1. Okay? Only the midpoint rule is second order accurate. Okay, this would be a great place to\n",
      "end this segment.\n",
      "\n",
      " 2: In [14] the following error estimates are obtained (c denotes a constant). Parabolic case \\begin{align*} \\|e(t)\\|_{0} & \\leq\n",
      "\\operatorname{ch}^{\\mu}\\left[\\|u(t)\\|_{k+1}+\\exp \\left(-\\lambda_{1}^{h} t\\right)\\|u(0)\\|_{k+1}\\right. \\\\ & \\left.+\\int_{0}^{t} \\exp\n",
      "\\left(\\lambda_{1}^{h}(\\tau-t)\\right)\\|\\dot{u}(\\tau)\\|_{k+1} d \\tau\\right] \\tag{7.I.1} \\end{align*} where $e=u^{h}-u$ and $\\mu=\\min \\{k+1,2(k+1-m)\\}$\\\\ For each\n",
      "fixed $t$, the term in (7.I.1) in square brackets will be bounded: Thus this result establishes the convergence of $u^{h}(t)$ to $u(t)$ in the $L_{2}$-norm as\n",
      "$h \\rightarrow 0$. \\textbf{Hyperbolic case} Let $E(u, \\dot{u})=\\frac{1}{2}[(\\dot{u}, \\rho \\dot{u})+a(u, u)]$ denote the total energy. The square root of $E$\n",
      "defines a norm on $W \\times L_{2}$ equivalent to the $H^{m} \\times L_{2}$ norm. The main result is \\begin{align*} E(e(t), \\dot{e}(t))^{1/2} \\leq & c \\{ h^{\\nu}\n",
      "\\left[ \\|u(0)\\|_{k+1}+\\|u(t)\\|_{k+1} \\right] \\\\ +h^{\\mu} [ \\|\\dot{u}(0)\\|_{k+1}+&\\|\\dot{u}(t)\\|_{k+1}+\\int_{0}^{t}\\|\\ddot{u}(\\tau)\\|_{k+1} d \\tau ] \\}\n",
      "\\tag{7.I.2} \\end{align*} where $\\nu=k+1-m$ and $\\mu=\\min \\{k+1,2(k+1-m)\\}$. For each fixed $t$, the terms in square brackets are bounded. Thus $E(e(t),\n",
      "\\dot{e}(t))^{1 / 2} \\rightarrow 0$ as $h \\rightarrow 0$, which in turn implies the convergence of $u^{h}(t)$ to $u(t)$ in $H^{m}$ and $\\dot{u}^{h}(t)$ to\n",
      "$\\dot{u}(t)$ in $L_{2}$. Because $\\nu \\leq \\mu$, the rate of convergence is $\\nu$. However, note that the integral in (7.I.2) is $O(t)$. Therefore, this\n",
      "estimate is only good for times no smaller than $O\\left(h^{-m}\\right)^{3}$. Otherwise, the error is $O\\left(h^{\\mu} t\\right)$. If you are interested in learning\n",
      "how to obtain results of this kind, consult [14].\\\\ Exercise 1. In the parabolic case, assume $\\ell, q$, and $h$ are zero. Obtain the following growth / decay\n",
      "estimates: $$ \\begin{gathered} \\|u(t)\\|_{0} \\leq \\exp \\left(-\\lambda_{1} t\\right)\\|u(0)\\|_{0} \\\\ \\left\\|u^{h}(t)\\right\\|_{0} \\leq \\exp \\left(-\\lambda_{1}^{h}\n",
      "t\\right)\\left\\|u^{h}(0)\\right\\|_{0} \\end{gathered} $$ Exercise 2. In the hyperbolic case, assume f, $\\boldsymbol{g}$, and $\\boldsymbol{k}$ are zero. Establish\n",
      "the conservation of total energy: $$ \\begin{aligned} E(u(t), \\dot{u}(t)) & =E(u(0), \\dot{u}(0)) \\\\ E\\left(u^{h}(t), \\dot{u}^{h}(t)\\right) & =E\\left(u^{h}(0),\n",
      "\\dot{u}^{h}(0)\\right) \\end{aligned} $$\n",
      "\n",
      " 3: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these\n",
      "facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution\n",
      "$\\boldsymbol{u}^{\\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\\delta_{y}(x)=\\delta(x-y)$ denote\n",
      "the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions.\n",
      "Let $w$ be continuous on $[0,1]$; then we write \\begin{align*} \\left(w, \\delta_{y}\\right) & =\\int_{0}^{1} w(x) \\delta(x-y) d x \\tag{1.10.1}\\\\ & =w(y)\n",
      "\\end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at\n",
      "$y$, its value would be ambiguous. In mechanics, we think of $\\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$.\n",
      "The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \\begin{align*} g_{, x\n",
      "x}+\\delta_{y} & =0 \\quad \\text { on } \\Omega \\tag{1.10.2}\\\\ g(1) & =0 \\tag{1.10.3}\\\\ g_{, x}(0) & =0 \\tag{1.10.4} \\end{align*} Note that (1.10.2)-(1.10.4) are\n",
      "simply $(S)$ in which $f$ is replaced by $\\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with\n",
      "distributions, or generalized functions, such as $\\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal\n",
      "calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.)\n",
      "To this end we note that the (formal) integral of $\\delta_{y}$ is the Heaviside, or unit step, function: \\[ H_{y}(x)=H(x-y)= \\begin{cases}0, & x<y\n",
      "\\tag{1.10.5}\\\\ 1, & x>y\\end{cases} \\] The integral of $\\dot{H}_{y}$ is the Macaulay bracket: \\[ \\langle x-y\\rangle=\\left\\{\\begin{array}{cl} 0, & x \\leq y\n",
      "\\tag{1.10.6}\\\\ x-y, & x>y \\end{array}\\right. \\] The preceding functions are depicted in Fig. 1.10.1.\\\\ \\includegraphics[max width=\\textwidth,\n",
      "center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\\\ \\includegraphics[max\n",
      "width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function\n",
      "problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \\begin{equation*} g_{, x}+H_{y}=c_{1} \\tag{1.10.7} \\end{equation*} where $c_{1}$ is a\n",
      "constant of integration. A second integration and use of (1.10.6) yields \\begin{equation*} g(x)+\\langle x-y\\rangle=c_{1} x+c_{2} \\tag{1.10.8} \\end{equation*}\n",
      "where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary\n",
      "conditions. This results in (see Fig. 1.10.2) \\begin{equation*} g(x)=(1-y)-\\langle x-y\\rangle \\tag{1.10.9} \\end{equation*} Observe that $g$ is piecewise linear.\n",
      "Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \\in \\mathcal{U}^{h}$.\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\\\\n",
      "\\includegraphics[max width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\\\ In the ensuing analysis we will need\n",
      "the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\\delta_{y}$, and $g$ and\n",
      "$h$ by 0 , viz., \\begin{equation*} a(w, g)=\\left(w, \\delta_{y}\\right)=w(y) \\tag{1.10.10} \\end{equation*} Equation (1.10.10) holds for all continuous $w \\in\n",
      "\\mathcal{U}$. The square-integrability of derivatives of functions $w \\in \\mathcal{U}$ actually implies the continuity of all $w \\in \\mathcal{U}$ by a well-\n",
      "known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure\n",
      "the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\\left(x_{A}\\right)=u\\left(x_{A}\\right), A=1,2, \\ldots, n+1$ (i.e.,\n",
      "$u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\\left(u-u^{h}, w^{h}\\right)=0$ for all $w^{h} \\in\n",
      "\\mathcal{U}^{h}$.\\\\ Proof. We have observed previously that $\\mathcal{U}^{h} \\in \\mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation:\n",
      "\\begin{equation*} a\\left(w^{h}, u\\right)=\\left(w^{h}, f\\right)+w^{h}(0) h \\tag{1.10.11} \\end{equation*} Equation (1.10.11) holds for all $w^{h} \\in\n",
      "\\mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\\\\n",
      "from (1.10.11) and using the bilinearity and symmetry of $a(\\cdot, \\cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\\left(u-u^{h}, g\\right)$, where\n",
      "$g$ is the Green's function.\\\\ Proof $$ \\begin{aligned} u(y)-u^{h}(y) & =\\left(u-u^{h}, \\delta_{y}\\right) & & \\text { (definition of } \\left.\\delta_{y}\\right)\n",
      "\\\\ & =a\\left(u-u^{h}, g\\right) & & \\text { (by (1.10.10)) } \\end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\\mathcal{U}$.\\\\ Proof of Theorem.\n",
      "As we have remarked previously, if $y=x_{A}$, a node, $g \\in \\mathcal{U}^{h}$. Let us take this to be the case. Then $$ \\begin{aligned}\n",
      "u\\left(x_{A}\\right)-u^{h}\\left(x_{A}\\right) & =a\\left(u-u^{h}, g\\right) & & \\text { (Lemma 2) } \\\\ & =0 & & \\text { (Lemma 1) } \\end{aligned} $$ The theorem is\n",
      "valid for $A=1,2, \\ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy\n",
      "characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be\n",
      "able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal\n",
      "accuracy properties of some sort may often be guaranteed. \\subsection*{Accuracy of the Derivatives} In considering the convergence properties of the\n",
      "derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as\n",
      "they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \\subsection*{Taylor's Formula with Remainder} Let\n",
      "$f:[0,1] \\rightarrow \\mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$\n",
      "such that \\begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \\frac{1}{2}(z-y)^{2} f_{,xx}(y) \\\\ & + \\frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \\cdots + \\tag{1.10.12} \\\\\n",
      "& + \\frac{1}{k!}(z-y)^{k} f_{,\\underbrace{x \\dots x}_{k \\text{ times}}}(c) \\end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is\n",
      "sometimes called a finite Taylor expansion. \\subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k\n",
      "\\geq 1$ (i.e., $f$ is continuously differentiable): \\begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \\tag{1.10.13} \\end{equation*} Consider a typical subinterval\n",
      "$\\left[x_{A}, x_{A+1}\\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\\boldsymbol{h}}$ in $]\n",
      "x_{A}, x_{A+1}[$ is constant: \\begin{equation*} \\left.u_{, x}^{h}(x)=\\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}, \\quad x \\in\\right] x_{A},\n",
      "x_{A+1}[ \\tag{1.10.14} \\end{equation*} \\begin{center} \\includegraphics[max width=\\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \\end{center} Figare 1.10.3\\\\\n",
      "Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean\n",
      "value theorem, there exists a point $c \\in] x_{A}, x_{A+1}[$ such that \\begin{equation*} \\frac{u\\left(x_{A+1}\\right)-u\\left(x_{A}\\right)}{h_{A}}=u_{, x}(c)\n",
      "\\tag{1.10.15} \\end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since\n",
      "$u\\left(x_{A}\\right)=u^{h}\\left(x_{A}\\right)$ and $u\\left(x_{A+1}\\right)=u^{h}\\left(x_{A+1}\\right)$, we may rewrite (1.10.15) as \\begin{equation*}\n",
      "\\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}=u_{, x}(c) \\tag{1.10.16} \\end{equation*} Comparison of (1.10.16) with (1.10.14) yields the\n",
      "desired result. \\subsection*{Remarks} \\begin{enumerate} \\item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere\n",
      "on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \\item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The\n",
      "following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\\\ \\includegraphics[max\n",
      "width=\\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \\end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\\alpha) \\stackrel{\\operatorname{def} .}{=} u_{,\n",
      "x}^{h}(\\alpha)-u_{, x}(\\alpha)=\\frac{u^{h}\\left(x_{A+1}\\right)-u^{h}\\left(x_{A}\\right)}{h_{A}}-u_{, x}(\\alpha) $$ the error in the derivative at $\\alpha\n",
      "\\in\\left[x_{A}, x_{A+1}\\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume\n",
      "$\\boldsymbol{u}$ is three times continuously differentiable. Then \\begin{align*} e_{, x}(\\alpha)= & \\left(\\frac{x_{A+1}+x_{A}}{2}-\\alpha\\right) u_{, x\n",
      "x}(\\alpha) \\\\ & +\\frac{1}{3!h_{A}}\\left[\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right)-\\left(x_{A}-\\alpha\\right)^{3} u_{, x x\n",
      "x}\\left(c_{2}\\right)\\right] \\tag{1.10.17} \\end{align*} where $c_{1}$ and $c_{2}$ are in $\\left[x_{A}, x_{A+1}\\right]$.\\\\ Proof. Expand $u\\left(x_{A+1}\\right)$\n",
      "and $u\\left(x_{A}\\right)$ in finite Taylor expansions about $\\alpha \\in\\left[x_{A}, x_{A+1}\\right]$, viz., $$ \\begin{aligned} u\\left(x_{A+1}\\right)= &\n",
      "u(\\alpha)+\\left(x_{A+1}-\\alpha\\right) u_{, x}(\\alpha)+\\frac{1}{2}\\left(x_{A+1}-\\alpha\\right)^{2} u_{, x x}(\\alpha) \\\\ &\n",
      "+\\frac{1}{3!}\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right), \\quad c_{1} \\in\\left[\\alpha, x_{A+1}\\right] \\\\ u\\left(x_{A}\\right)= &\n",
      "u(\\alpha)+\\left(x_{A}-\\alpha\\right) u_{, x}(\\alpha)+\\frac{1}{2}\\left(x_{A}-\\alpha\\right)^{2} u_{, x x}(\\alpha) \\\\ & +\\frac{1}{3!}\\left(x_{A}-\\alpha\\right)^{3}\n",
      "u_{, x x x}\\left(c_{2}\\right), \\quad c_{2} \\in\\left[x_{A}, \\alpha\\right] \\end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \\begin{aligned}\n",
      "\\frac{u\\left(x_{A+1}\\right)-u\\left(x_{A}\\right)}{h_{A}}= & u_{, x}(\\alpha)+\\left(\\frac{x_{A+1}+x_{A}}{2}-\\alpha\\right) u_{, x x}(\\alpha) \\\\ &\n",
      "+\\frac{1}{3!h_{A}}\\left[\\left(x_{A+1}-\\alpha\\right)^{3} u_{, x x x}\\left(c_{1}\\right)-\\left(x_{A}-\\alpha\\right)^{3} u_{1, x x x}\\left(c_{2}\\right)\\right]\n",
      "\\end{aligned} $$ Replacing $u\\left(x_{A+1}\\right)$ by $u^{h}\\left(x_{A+1}\\right)$ and $u\\left(x_{A}\\right)$ by $u^{h}\\left(x_{A}\\right)$ in the left-hand side\n",
      "and rearranging terms completes the proof. \\subsection*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think\n",
      "of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than\n",
      "$h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\\left(h_{\\mathrm{A}}^{2}\\right),{ }^{3}$ the error in the derivatives will be much\n",
      "smaller than if the right-hand side is only $O\\left(h_{A}\\right)$. The exponent of $h_{\\mathrm{A}}$ is called the order of convergence or order of accuracy. In\n",
      "the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example,\n",
      "assume $\\alpha \\rightarrow x_{A}$. Then $$ e_{, x}\\left(x_{A}\\right)=\\frac{h_{A}}{2} u_{, x x}\\left(x_{A}\\right)+\\frac{h_{A}^{2}}{3!} u_{, x x\n",
      "x}\\left(c_{1}\\right)=O\\left(h_{A}\\right) $$ As $\\boldsymbol{h}_{A} \\rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec.\n",
      "1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\\alpha \\in\\left[x_{A}, x_{A+1}\\right]$ achieves first-\n",
      "order accuracy. We are thus naturally led to asking the question, are there any values of $\\alpha$ at which higher-order accuracy is achieved? Corollary. Let\n",
      "$x_{A+1 / 2} \\equiv\\left(x_{A}+x_{A+1}\\right) / 2$ (i.e., the midpoint). Then $$ \\begin{aligned} e_{, x}\\left(x_{A+1 / 2}\\right) & =\\frac{h_{A}^{2}}{24} u_{, x\n",
      "x x}(c), \\quad c \\in\\left[x_{A}, x_{A+1}\\right] \\\\ & =O\\left(h_{A}^{2}\\right) \\end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\\left(x_{A+1 /\n",
      "2}\\right)=\\frac{h_{A}^{2}}{48}\\left[u_{, x x x}\\left(c_{1}\\right)+u_{, x x x}\\left(c_{2}\\right)\\right] $$ By the continuity of $u_{1 x x x}$, there is at least\n",
      "one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\\frac{1}{2}\\left[u_{, x x x}\\left(c_{1}\\right)+u_{, x x x}\\left(c_{2}\\right)\\right] $$\n",
      "Combining these facts completes the proof. \\subsection*{Remarks} \\begin{enumerate} \\item From the corollary we see that the derivatives are second-order\n",
      "accurate at the midpoints. \\end{enumerate} \\footnotetext{${ }^{3} \\mathrm{~A}$ function $f(x)$ is said to be $O\\left(x^{k}\\right)$ (i.e., order $x^{k}$ ) if\n",
      "$f(x) / x^{k} \\rightarrow$ a constant as $x \\rightarrow 0$. For example, $f(x)=x^{k}$ is $O\\left(x^{k}\\right)$, as is $f(x)=\\sum_{j=k}^{k+1} x^{j}, l \\geq 0$.\n",
      "But neither is $O\\left(x^{k+1}\\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$\n",
      "), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\\\ 3. In linear elastic rod theory,\n",
      "the derivatives are proportional to the stresses. The midpoints of linear \"elements\" are sometimes called the Barlow stress points, after Barlow [8], who first\n",
      "noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \\ldots, n$ ). Consider the\n",
      "standard finite difference \"stencil\" for $u_{, x x}+\\phi=0$ at a typical internal node, namely, \\begin{equation*} \\frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0\n",
      "\\tag{1.10.18} \\end{equation*} Assuming \\& varies in piecewise linear fashion and so can be expanded as \\begin{equation*} f=\\sum_{A=1}^{n+1} f_{A} N_{A}\n",
      "\\tag{1.10.19} \\end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with\n",
      "(1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite\n",
      "element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further\n",
      "comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem\n",
      "(S), possesses the following properties:\\\\ i. It is exact at the nodes.\\\\ ii. There exists at least one point in each element at which the derivative is\n",
      "exact.\\\\ iii. The derivative is second-order accurate at the midpoints of the elements.\n",
      "\n",
      " 4: It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in\n",
      "the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase\n",
      "the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme\n",
      "is applicable to systems of equations\\\\ $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example,\n",
      "symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \\subsection*{Gauss Elimination} \\begin{itemize} \\item\n",
      "Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \\item Solve the second equation for $d_{2}$ and eliminate $d_{2}$\n",
      "from the remaining $n-2$ equations. \\item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \\item Solve the $n$-th\n",
      "equation for $d_{n}$. \\end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example,\n",
      "suppose we began with a system of four equations as follows: $$ \\left[\\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\\\ K_{21} & K_{22} & K_{23} & K_{24}\n",
      "\\\\ K_{31} & K_{32} & K_{33} & K_{34} \\\\ K_{41} & K_{42} & K_{43} & K_{44} \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4}\n",
      "\\end{array}\\right\\}=\\left\\{\\begin{array}{l} F_{1} \\\\ F_{2} \\\\ F_{3} \\\\ F_{4} \\end{array}\\right\\} $$ The augmented matrix corresponding to this system is \\[\n",
      "\\left[ \\begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\\\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\\\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\\\\n",
      "K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{K}} & \\underbrace{\\phantom{F_1}}_{F}\n",
      "\\end{array} \\right] \\] After the forward reduction, the augmented matrix becomes\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K‘_{14} & F'_1 \\\\ 0 &\n",
      "1 & K'_{23} & K'_{24} & F'_2 \\\\ 0 & 0 & 1 & K'_{34} & F'_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\ \\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\,\n",
      "K_{14}}}_{U}} & \\underbrace{\\phantom{F_1}}_{F'} \\end{array} \\tag{1.11.1} \\right] \\] corresponding to the upper triangular system $\\boldsymbol{U}\n",
      "\\boldsymbol{d}=\\boldsymbol{F}^{\\prime} \\cdot{ }^{4}$ It is a simply verified fact that if $\\boldsymbol{K}$ is banded, then $\\boldsymbol{U}$ will be also.\n",
      "Employing the reduced augmented matrix, proceed as follows: \\begin{itemize} \\item Eliminate $d_{n}$ from equations $n-1, n-2, \\ldots, 1$.\\\\ \\footnotetext{${\n",
      "}^{4} \\text{Primes will be used to denote intermediate quantities throughout this section}.$} \\item Eliminate $d_{n-1}$ from equations $n-2, n-3, \\ldots, 1$.\n",
      "\\item Eliminate $d_{2}$ from the first equation. \\end{itemize} This procedure is called back substitution. For example, in the example just given, after back\n",
      "substitution we obtain\\\\ \\[ \\left[ \\begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\\\ 0 & 1 & 0 & 0 & d_2 \\\\ 0 & 0 & 1 & 0 & d_3 \\\\ 0 & 0 & 0 & 1 & d_4 \\\\\n",
      "\\multicolumn{4}{c|}{\\underbrace{\\phantom{K_{11}\\, K_{12}\\, K_{13}\\, K_{14}}}_{I}} & \\underbrace{\\phantom{F_1}}_{d} \\end{array} \\tag{1.11.2} \\right] \\]\n",
      "corresponding to the identity $1 \\boldsymbol{d}=\\boldsymbol{d}$. The solution winds up in the last column. \\subsection*{Hand-Calculation Algorthm} In a hand\n",
      "calculation, Gauss elimination can be performed on the augmented matrix as follows. \\subsection*{Forward reduction} \\begin{itemize} \\item Divide row 1 by\n",
      "$K_{11}$. \\item Subtract $K_{21} \\times$ row 1 from row 2. \\item Subtract $K_{31} \\times$ row 1 from row 3. \\item Subtract $K_{n 1} \\times$ row 1 from row $n$.\n",
      "\\end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \\left[\\begin{array}{llll|l} 1 &\n",
      "\\boldsymbol{K}_{12}^{\\prime} & \\boldsymbol{K}_{3}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & \\boldsymbol{K}_{22}^{\\prime\n",
      "\\prime} & \\boldsymbol{K}_{23}^{\\prime \\prime} & \\boldsymbol{K}_{24}^{\\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime} \\\\ 0 & \\boldsymbol{K}_{32}^{\\prime} &\n",
      "\\boldsymbol{K}_{33}^{\\prime 3} & \\boldsymbol{K}_{34}^{\\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime} \\\\ \\mathbf{0} & \\boldsymbol{K}_{42}^{\\prime} &\n",
      "\\boldsymbol{K}_{43}^{3} & \\boldsymbol{K}_{44}^{\\prime \\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime} \\end{array}\\right] $$ Note that if $\\boldsymbol{K}_{\\mathbf{A\n",
      "1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \\begin{itemize} \\item Divide row 2 by $K_{22}^{\\prime \\prime}$. \\item\n",
      "Subtract $K_{32}^{\\prime \\prime} \\times$ row 2 from row 3. \\item Subtract $K_{42}^{n} \\times$ row 2 from row 4. \\item Subtract $K_{n 2}^{\\prime \\prime} \\times$\n",
      "row 2 from row $n$. \\end{itemize} The result for the example will look like\\\\ $\\left[\\begin{array}{cccc|c}1 & \\boldsymbol{K}_{12}^{\\prime} &\n",
      "\\boldsymbol{K}_{13}^{\\prime} & \\boldsymbol{K}_{14}^{\\prime} & \\boldsymbol{F}_{1}^{\\prime} \\\\ 0 & 1 & \\boldsymbol{K}_{23}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{24}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{2}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{33}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{34}^{\\prime \\prime \\prime} & \\boldsymbol{F}_{3}^{\\prime \\prime \\prime} \\\\ \\mathbf{0} & \\mathbf{0} & \\boldsymbol{K}_{43}^{\\prime \\prime \\prime} &\n",
      "\\boldsymbol{K}_{44}^{\\prime \\prime\\prime} & \\boldsymbol{F}_{4}^{\\prime \\prime \\prime} \\\\ & & & & \\end{array}\\right]$ Note that only the submatrix enclosed in\n",
      "dashed lines is affected in this procedure.\\\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \\subsection*{Back\n",
      "substitution} \\begin{itemize} \\item Subtract $K_{n-1, n}^{\\prime} \\times$ row $n$ from row $n-1$. \\item Subtract $K_{n-2, n}^{\\prime} \\times$ row $n$ from row\n",
      "$n-2$.\\\\ \\vdots \\item Subtract $K_{1, n}^{\\prime} \\times$ row $n$ from row 1 \\end{itemize} After these steps the augmented matrix, for this example, will look\n",
      "like $$ \\left[\\begin{array}{cccc|c} 1 & \\bar{K}_{12}^{\\prime} & \\bar{K}_{3}^{\\prime} & 0 & F_{1}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 1 & K_{23}^{\\prime} & 0 &\n",
      "F_{2}^{\\prime \\prime \\prime \\prime} \\\\ 0 & 0 & 1 & 0 & d_{3} \\\\ 0 & 0 & 0 & 1 & d_{4} \\end{array}\\right] $$ Note that the submatrix enclosed in dashed lines is\n",
      "unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\\prime}$ is\n",
      "altered. Now clear the second-to-last column in the coefficient matrix: \\begin{itemize} \\item Subtract $K_{n-2, n-1}^{\\prime} \\times$ row $n-1$ from row $n-2$.\n",
      "\\item Subtract $K_{n-3, n-1}^{\\prime} \\times$ row $n-1$ from row $n-3$.\\\\ \\vdots \\item Subtract $K_{1 . n-1}^{\\prime} \\times$ row $n-1$ from row 1.\n",
      "\\end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\\boldsymbol{F}$ ). Repeat as above until\n",
      "columns $\\boldsymbol{n}-2, n-3, \\ldots, 2$ are cleared. The result is (1.11.2). \\subsection*{Remarks} \\begin{enumerate} \\item In passing we note that the above\n",
      "procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss\n",
      "elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the\n",
      "procedure. However, it is felt that the given scheme is the clearest for hand calculations. \\item The numerical example with which we close this section\n",
      "illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain\n",
      "zero throughout the calculations). The reader is urged to perform the calculations. \\end{enumerate} \\subsection*{Example of Gauss ellmination} $$\n",
      "\\left[\\begin{array}{rrrr} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 2 \\end{array}\\right]\\left\\{\\begin{array}{l} d_{1} \\\\ d_{2} \\\\\n",
      "d_{3} \\\\ d_{4} \\end{array}\\right\\}=\\left\\{\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right\\} $$ \\subsection*{Augmented matrix} $$ \\left[\\begin{array}{rrrr|r}\n",
      "1 & -1 & 0 & 0 & 1 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right] $$ Forward reduction $$ \\begin{aligned} &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & -1 & 2 & 0 \\end{array}\\right]} \\\\ &\n",
      "{\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & -1 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\end{aligned} $$\n",
      "\\subsection*{Back substitution} $$ \\begin{aligned} & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & -1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1\n",
      "& 1 \\end{array}\\right]} \\\\ & {\\left[\\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]}\n",
      "\\\\ & {\\left[\\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\\\ 0 & 1 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 2 \\\\ 0 & 0 & 0 & 1 & 1 \\end{array}\\right]} \\\\ \\begin{array}{l}\n",
      "\\left\\{ \\begin{array}{l} d_{1} \\\\ d_{2} \\\\ d_{3} \\\\ d_{4} \\end{array} \\right\\} = \\left\\{ \\begin{array}{l} 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{array} \\right\\} \\end{array}\n",
      "\\end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \\begin{aligned} u_{, x x}(x)+f(x) & =0 \\quad x \\in] 0,1[\n",
      "\\\\ u(1) & =g \\\\ -u_{, x}(0) & =h \\end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\\\ a. Employing the linear finite element space with\n",
      "equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\\left(h=\\right.$ mesh parameter $\\left.=\\frac{1}{4}\\right)$. Recall that in\n",
      "Sec. 1.7 this was carried out for $n=1$ and $n=2\\left(h=1\\right.$ and $h=\\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss\n",
      "elimination to solve $\\boldsymbol{K} \\boldsymbol{d}=\\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your\n",
      "answers since they must be exact at the nodes.\\\\ b. Let $r e_{, x}=\\left|u_{, x}^{h}-u_{. x}\\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{,\n",
      "x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\\\ c. Employing the data for $h=1, \\frac{1}{2}$, and\n",
      "$\\frac{1}{4}$, plot $\\ln r e_{, x}$ versus $\\ln h$.\\\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following\n",
      "questions:\\\\ i. What is the significance of the slope of the graph in part (c)?\\\\ ii. What is the significance of the $y$-intercept?\n",
      "\n",
      " 5: Okay, so what this gives us then is that. This tells us then that e at n plus 1 equals minus sum i going from 0 to n A to the power delta t tao t at n minus\n",
      "i. Okay. All right. Let's work with this. And we're going to work now, by invoking some inequality. Okay. In particular what we can say, first of all, is that\n",
      "the first con, the first step that we will take is not an inequality. It is to say that the magnitude of e n plus 1, right, is equal to essentially magnitude of\n",
      "sum i going from 0 to n, A to the power i delta t tao t n minus i, okay? Right, we're taking the magnitude of that sum, the absolute value of that sum. Now, is\n",
      "where things get really interesting. We, now's where the inequalities come up, okay? And, and remember when we write the inequalities, I'm going to start out by\n",
      "writing this. And you recall when we looked at the, our error analysis for the finite element method I made the point that when I write such an inequality, what\n",
      "I mean is that the previous right-hand side, this one is bounded from above by what I'm about to write now as the new right-hand side, okay? So, what I get is\n",
      "that, that is lesser than or equal to i going from 0 to n the magnitude of the, the absolute value of each one of those terms, and that's A to the power i, A to\n",
      "the power of i delta t tau add t n minus i. Okay? All right the absolute value of a sum is bounded from above by the sum of the absolute values. And this result\n",
      "is a very standard step in analysis. It is called the triangle inequality. Okay? But there is more. We can say further that that, that our most recent right-\n",
      "hand side itself is bounded from above by this expression. Okay? Essentially what we're seeing here that is that the that any absolute value, which is a\n",
      "product, right? So the product, the absolute value of a product is boundary from above by the product of the absolute values. Right, and this result is called\n",
      "the Cauchy–Schwarz inequality. Okay? But now we have more. We have stability, right? If our method is stable, what we are able to say is that sum i going from\n",
      "0, to n of 1 times magnitude of delta t times tau tn minus i, bounds from above our previous right-hand side. Okay? Essentially, what I've done is replace, A i\n",
      "with 1, right. And why, why am I able to do that? It's a property of the methods we are looking at. What property are we applying here? It's stability, right?\n",
      "Because we know that the magnitude of A has to be lesser than or equal to 1. The absolute value of A has to be less than or equal to one for stability,\n",
      "therefore, A to the power i has an absolute value also lesser than or equal to 1. Okay, so if you re-substitute that with one, we get a bounding from above of\n",
      "what we had as our previous right-hand side, okay. But now the story goes on, now we know that delta t is a positive quantity, right. So what we can do here is,\n",
      "we know that delta t's a positive quantity and, furthermore, we know that for tau, the condition of consistency allowed us to say something about it. Okay, so\n",
      "in one step I'm going to do this. I'm going to pull the delta t out. All right. And I have inside my sum. C delta t to the power of k. Okay. Why am I able to\n",
      "say this? It's because we said that we have consistency, okay? All right, we have consistency. Now one could introduce another step inside here, which would be\n",
      "to first say well, if all the tau's over all the n minus i's are take maximum of them. Okay and then I get to this point, right. So let me say that one can see\n",
      "inside of here, when they first, if one wanted to be really careful about this then I guess one should be really careful. So one would say, that first of all\n",
      "you would have a previous step inside here, which is to say that the, that this right-hand side, right? Using that, one could say that sorry, the entire right-\n",
      "hand side, not just that. One could say that this entire right-hand side, first of all would be bounded from above by sum i equals 0 to n delta t times the\n",
      "maximum over all i, t n minus i. Right. One could take that step, right? Which is to say, that well since each of the tau, you know you have a different value\n",
      "of tau for each time step. Let's consider the maximum over all i, okay. But then we know that that maximum has to be bounded from above by C delta t to the\n",
      "power of k, because we have consistency. Right, so then we get to this step that I have here. Right. Okay, all right. Okay now well, what do we get here? We\n",
      "have delta t times sorry, the sum here goes from 0 to n. Sorry, right. Okay, we have delta t times a sum of n plus 1 steps, because i is going from 0 to n, of\n",
      "that quantity c delta t times c delta t to the power of k. 'Kay, so we can very well say that it is just like multiplying delta t essentially n plus 1 times,\n",
      "right. So this thing is lesser than or equal to t to the power n plus 1. Sorry, t at n plus 1, not t to the power n plus 1, but t at n plus 1 times c plus delta\n",
      "t, to the power of k. Okay. All right, just summing over those n plus 1 steps. Okay, right right. Now and now from here, we see however, we see that as delta t\n",
      "tends to 0, right, because k is greater than 0, right? Then as delta d turns to 0, we see that the righ- hand side also turns to zero, okay? All right, but.\n",
      "Limit delta t tends to 0 of c delta t to the power k is equal to 0 for k, greater than 0. Right, and where do we get this? Why are we allowed to see a case\n",
      "greater than zero? Once again it is the result of consistency. Okay, so what we see finally, is that the absolute value of our modal coefficient of the error is\n",
      "bounded from about by, essentially by 0. Right. Which means it, it, it still tends at 0, right, and the limit as delta t tends to 0, right. What we are seeing\n",
      "is that, that quantity is lesser than or equal to 0. Okay, so indeed we have conversions. Right, and that's the end of our proof. Okay? Make a quick remark\n",
      "here, which is that what we've seen here is a demonstration of the use of consistency. And stability. Implies convergence. Okay, which is a very standard it's\n",
      "actually a tier in numerical analysis. Okay it's called the Lax theorem. Okay? All right we're actually done with this entire topic of methods for parabolic\n",
      "problems. And what and, and the approach we've taken if you just to summarize is to carry out a standard spacial discretization using the finite element method.\n",
      "But to the time discretization using finite difference methods, and since here we're looking at first order of problems. We looked at the Euler family. We\n",
      "analyzed its stability understood the behavior of its higher order modes looked at consistency, and looked at how stability and consistency give us convergence.\n",
      "Let me make one more remark now about about the be, about the use of the different algorithms, okay. In particular, let me do this, okay. Let me say, let me say\n",
      "that here I have alpha, okay, and let's look at alpha equals 0, alpha equals one-half, and alpha equals 1. Okay, and just for connection to other things. Let's\n",
      "give this, this algorithms their names, right? So this is forward Euler. Right, this is the mid-point rule, or the Crank Nicolson method. And this is backward\n",
      "Euler. Okay let's look here at the stability. Let's look at order of accuracy. And let's draw a straight line. Okay better, and let's look finally at high order\n",
      "modes, right. Okay? Stability of forward Euler is conditional. Right, and we've seen the cons, the stability condition. Midpoint rule and backward Euler are\n",
      "unconditionally stable. Okay? Order of accuracy, forward Euler has order of accuracy 1, midpoint Euler has order of accuracy 2, backward Euler has order of\n",
      "accuracy 1. For high order modes, limit as lambda h delta t tends to infinity. Limit of A in the tends to infinity not 0. Okay forward Euler is let me just see\n",
      "it tends to minus infinity right, because nothing can be equal to minus infinity, right. Midpoint rule. Limit lender each delta t, tends to infinity is equal to\n",
      "minus 1. Okay? Oscillatory behavior. For backward Euler we see that limit lender h delta t tends to infinity. Oh, sorry, limit of A here. Okay, here too, limit\n",
      "of A equals 0. Okay, so it damps out high order modes, dissipates them away. So this is the, broadly speaking, the behavior of the three main are the three most\n",
      "commonly used members of this family. Depending upon the problem you choose your method, okay. All right, we're done with this segment and with this unit here\n",
      "when we return we will take up the problem of Elastodynamics.\n",
      "\n",
      " 6: Now, in order to proceed from here, let me actually write this time discrete ODE. In ord, for, for, for, for purposes of argument let me write it at time,\n",
      "tn plus one. Right, so this of course is the time discretize ODE at tn. Right, because everything is at n. Okay. Let me know write this at time tn plus one.\n",
      "Right? So, at T n plus one, that becomes in canonical form, it becomes N V and N plus one plus K, d at n plus one equals F at n plus 1. All right. All I've done\n",
      "is, is write out the ODE at two different. Times okay two different times and as well. All right so this is generally what we would call the the time discrete\n",
      "version of the ODE. And, and when we write it in terms of the philosophy it's often sort of canonical All right now, this is where we need to invoke special\n",
      "integration methods, right? Our integration methods are based upon what we call the same inte- it's a family of integration algorithms. Okay? That are sometimes\n",
      "called the Euler family for first-order ODE's. Okay. And here is how they are posed. Supposing we're considering the following ODE. Suppose we're looking at, y\n",
      "dot equals f of y. Okay? This is the, algorithm we're looking at. So, what we will see here is that the time discretized version of this or the ARD or the, the,\n",
      "the, the solution as post by the Euler family of algorithms is the following. All right, the algorithm then is Is the following. It is that y n plus 1 minus y n\n",
      "over delta t, okay? Is equal to f at y n plus alpha. All right? This is the algorithm as defined by the Euler family. I'll have some explanation to do here of\n",
      "course right? So what we have is the following. Just the algorithm where, delta t is simply the what is called the time stamp. It's d n plus 1 minus t n. Okay?\n",
      "And this is the time stamp Okay? Alpha is a real number. And alpha belongs to the closed interval 0 to 1. All right, okay. So. We have the setting, and, since\n",
      "we're talking about the Euler family for the very first time let me, so to speak, introduce you to the members of the Euler family. So, we have the following\n",
      "sort of setting. Four alpha equals 0. Right, this is called the forward Euler method. Okay alpha equals 1, is called backward Euler. And alpha equals one-half\n",
      "is called various things. I will tend to call it the Midpoint Rule. Okay? It's also sometimes called the Crank-Nicolson method. Okay, so those are special\n",
      "members of the Euler family. Of course as as indicated back here you can one can design algorithms. One can develop algorithms and use them for any value of\n",
      "between zero and one. Okay, so, one one final step I want to take in this segment is to note that the way this can be sometimes rewritten is as simply. All\n",
      "right, this, this think can, is sometimes rewritten as y n plus 1 equals y n. Plus delta t, times f at y n plus alpha. Okay? Now, in order to complete this\n",
      "description I need to tell you what y n plus alpha is. Okay? Y n + alpha is constructed simply as a linear interpolation between values at alpha equals zero and\n",
      "alpha equals one. Okay? And that tells you that, that you construct it as Y N plus 1, right? Let me see, alpha times y n+1 plus 1 minus alpha times y n. Okay?\n",
      "All right. So really so when you sit back and look at these Euler family of At the Euler family of algorithms, what it is doing is to approximate our time\n",
      "derivative, right so we approximate the time derivative as a linear, as a linearly varying quantity over every time interval. Right? Why don't, we'll just\n",
      "sayinh well,let me just take a linear, approximation of it over the sub, over the time stamp of the time derivative. This is our approximation for the time\n",
      "derivative, right? And by saying that we are requiring it to be equal f at n plus alpha, this is where the design of our algorithm also comes in over the\n",
      "various possibilities in the Euler family because if we are trying to go from tn to tn plus one. Right? We're seeing that the rate in the average rate across\n",
      "the entity of tn and tn plus one depends upon f and we could, we are free to evaluate f or we're leaving ourselves the flexibility to evaluate f at any value\n",
      "inside here. Right, so at, at any point inside this. So that point is a gen, is generically n plus alpha, t at n plus alpha. Okay. By choosing different values\n",
      "of alpha, including alpha equals 0, or alpha equals 1, or alpha equals one-half, or, or really anything else, we get particular properties for our method.\n",
      "Right? So, so this is where the approximation of the time derivative comes in. We have this side of it, which is the linear approximation, and here we are\n",
      "approximating the ODE by evaluating the right hand side at some T equals alpha. Okay? So this is the basis of the Euler method and we are going to apply it to\n",
      "our time discretized ODE when we return in the next segment.\n",
      "\n",
      " 7: \\subsection*{9.3.1 LMS Mothods for First-order Equations} Consider a system of first-order, linear, ordinary differential equations \\begin{equation*}\n",
      "\\dot{y}=f(y, t)=G y+H(t) \\tag{9.3.1} \\end{equation*} where $\\boldsymbol{G}$ is a given constant matrix and $\\boldsymbol{H}$ is a given vector-valued function of\n",
      "$\\boldsymbol{t}$. A $k$-step linear multistep method for (9.3.1) is defined by the following expression: \\begin{equation*} \\sum_{i=0}^{k}\\left\\{\\alpha_{i}\n",
      "y_{n+1-i}+\\Delta t \\beta_{l} f\\left(y_{n+1-i}, t_{n+1-i}\\right)\\right\\}=0 \\tag{9.3.2} \\end{equation*} The $\\alpha_{i}$ 's and $\\beta_{i}$ 's are parameters\n",
      "which define the method. Note that the word \"linear\" in linear multistep method has nothing to do with the linearity of (9.3.1). Indeed, (9.3.2) is perfectly\n",
      "well defined for a nonlinear function $f(y, t)$. Rather, the linearity pertains to the form of (9.3.2). An excellent reference for LMS methods of this type is\n",
      "Gear [15]. An LMS method is called explicit if $\\beta_{0}=0$; otherwise it is called implicit. It is called a backward-difference method if $\\beta_{i}=0$ for\n",
      "all $i \\geq 1$. An example of a one-step LMS method of first-order type is the generalized trapezoidal family discussed in Chapter 8 for which\n",
      "$\\alpha_{0}=-\\alpha_{1}=1, \\beta_{0}=-\\alpha$, and $\\beta_{1}=\\alpha-1$. Recall tht $\\alpha=0$ defines an explicit method, and $\\alpha=1$ defines a backward\n",
      "difference method. The greater the number of steps involved in the definition of an LMS method, the greater the \"historical data pool\" that must be stored in\n",
      "computing-a practical disadvantage. Exercise 1. Put the semidiscrete heat equation into the first-order form defined by (9.3.1). Conclude that the eigenvalues\n",
      "of $G$ are nonpositive real numbers. Thus the spectrum of $\\boldsymbol{G}$ resides upon the negative real axis in the complex plane $\\mathbf{C}$ (i.e.,\n",
      "$\\lambda(\\boldsymbol{G}) \\leq 0$ ).\\\\ Exercise 2. Put the semidiscrete equation of motion in first-order form. Take $y=\\{d, d\\}^{\\boldsymbol{T}}$. Conclude that\n",
      "the $2 n_{e q}$ eigenvalues of $\\boldsymbol{G}$ take on the following forms:\\\\ i. $0 \\leq \\xi<1$, underdamped: $$ \\lambda_{1,2}(G)=-\\xi \\omega^{h} \\pm i\n",
      "\\omega^{h} \\sqrt{1-\\xi^{2}} $$ ii. $\\xi=1$, critically damped: $$ \\lambda_{1,2}(G)=-\\omega^{h} \\quad \\text { (double root) } $$ iii. $\\xi>1$, overdamped: $$\n",
      "\\lambda_{1,2}(G)=-\\xi \\omega^{h} \\pm \\omega^{h} \\sqrt{\\xi^{2}-1} $$ When $\\xi=0$, the eigenvalues are complex conjugate, $\\pm i \\omega^{h}$, and thus reside on\n",
      "the imaginary axis. In all cases, the eigenvalues of $\\boldsymbol{G}$ are confined to the negative half-plane of $\\mathbb{C}$ including the negative real axis\n",
      "(i.e., $\\operatorname{Re}(\\lambda(G)) \\leq 0$ ). Exercise 3. Make a sketch of the complex plane and identify where the eigenvalues, $\\lambda(\\boldsymbol{G})$,\n",
      "as determined in Exercises 1 and 2, fall. Many algorithms of practical interest take the form (9.3.2). The analysis of LMS methods may be performed using modal\n",
      "techniques, as described previously. Stability is generally phrased in terms of the roots of the polynomial associated to (9.3.2), namely, \\begin{equation*}\n",
      "\\sum_{i=0}^{k}\\left(\\alpha_{i}+\\Delta t \\lambda \\beta_{i}\\right) \\zeta^{n+1-i}=0 \\tag{9.3.3} \\end{equation*} where $\\lambda$ is an eigenvalue of\n",
      "$\\boldsymbol{G}$. (To derive (9.3.3), assume $\\boldsymbol{G}$ admits the representation $\\boldsymbol{G}=\\boldsymbol{P} \\boldsymbol{\\Lambda}\n",
      "\\boldsymbol{P}^{-1}$, where $\\boldsymbol{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{N}\\right)$ and $N$ is the dimension of\n",
      "$\\boldsymbol{G}$. Show that (9.3.2) can be reduced to an uncoupled system of scalar equations having the form $\\Sigma_{i=0}^{k}\\left(\\alpha_{i}+\\Delta t \\lambda\n",
      "\\beta_{i}\\right) z_{n+1-i}=0$, which has solutions $z_{n} \\sim c \\zeta^{n}$.) An LMS method of the form (9.3.2) is said to be absolutely stable, at a fixed\n",
      "$\\lambda \\Delta t$, if all $\\zeta$ satisfying (9.3.3) are such that $|\\zeta| \\leq 1$. The region of absolute stability of an LMS method is the set of $\\lambda\n",
      "\\Delta t \\in \\mathbb{C}$ at which it is absolutely stable. An LMS method is $A$-stable if solutions of the SDOF analog of $(\\mathbf{9} .3 .2) \\rightarrow 0$ as\n",
      "$n \\rightarrow \\infty$ when $\\operatorname{Re}(\\lambda)<0$. Physically speaking, an $A$-stable algorithm is one which produces solutions which decay to zero\n",
      "whenever the corresponding exact solutions decay to zero. Clearly, if an algorithm is $A$-stable then the region of absolute stability contains the left half-\n",
      "plane of $\\mathbb{C}$. The condition of $A$-stability places no limitation on the size of $\\Delta t$, consequently it is closely related to what we termed\n",
      "\"unconditional stability\" previously. The only difference is that spectral stability allows eigenvalues of unit modulus which potentially could conserve a\n",
      "solution rather than contract it to zero. It should be apparent that $A$-stable algorithms are important for many physical problem classes. However, within the\n",
      "class of LMS methods, the subclass of $A$-stable methods is severely limited. This is the content of a celebrated theorem:\\\\ Dabiquist’s Theorem [16] \\\\ 1. An\n",
      "explicit A-stable LMS method does not exist.\\\\ 2. A third-order accurate A-stable LMS method does not exist.\\\\ 3. The second-order accurate A-stable LMS method\n",
      "with the smallest error constant is the trapezoidal rule. The upshot of this theorem is that if we seek an $A$-stable LMS method that possesses some special\n",
      "feature (such as high-frequency numerical dissipation), we necessarily entail some loss of accuracy with respect to the trapezoidal rule. Thus it would appear\n",
      "that the trapezoidal rule is the canonical A-stable LMS method for structural dynamics. The widespread use and popularity of the trapezoidal rule in this\n",
      "context appears to confirm this observation. However, there are other useful $A$-stable LMS methods for structural dynamics; for example, Park's method [17].\n",
      "Many structural dynamics algorithms do not fall within the class of first-order LMS methods considered so far. The Newmark family of methods may be mentioned in\n",
      "this regard. Another class of LMS methods for second-order equations will be described subsequently; it contains the Newmark family, among others. Since\n",
      "$A$-stable LMS methods are at most second-order accurate, the question arises, \"Is there some weakened notion of $A$-stability which pertains to a physically\n",
      "relevant class of problems and allows for the development of higher-order-accurate methods?\" An affirmative answer is provided by the concept of stiff stability\n",
      "proposed \\footnotetext{${ }^{3}$ The error constant is the coefficient of $\\Delta t^{k}$ in the local truncation error. } by Gear [15]. A method is said to be\n",
      "stiffly stable if it is absolutely stable in the region of the $\\lambda \\Delta t$-plane defined by $\\operatorname{Re}(\\lambda \\Delta t)<-\\delta$, where $\\delta$\n",
      "is a positive constant. Gear has developed a family of $k$-step, $k$ th-order-accurate LMS methods which are stiffly stable for $k=2,3, \\ldots, 6$. Each method\n",
      "is a backward-difference LMS method. The second-order method is, in addition, $\\boldsymbol{A}$-stable; however, it is significantly less accurate than the\n",
      "trapezoidal rule. For $k \\geq 3$, the regions of instability include portions of the imaginary axis. Consequently, these stiffly stable algorithms also appear\n",
      "inappropriate for typical structural dynamics applications. Because the region of stability of each of the methods, $k=2,3, \\ldots, 6$, includes the negative\n",
      "real axis, one would anticipate good behavior in application to problems of heat conduction. Park [17] has developed a second-order accurate, A-stable\n",
      "algorithm, which retains good accuracy in the low frequencies and strong dissipative characteristics in the high frequencies, by combining Gear's two-step and\n",
      "three-step stiffly stable algorithms. The resultant scheme is defined by Park's Method $$ \\begin{gathered} k=3, \\quad \\alpha_{0}=-1, \\quad\n",
      "\\alpha_{1}=\\frac{15}{10}, \\quad \\alpha_{2}=\\frac{-6}{10}, \\quad \\alpha_{3}=\\frac{1}{10}, \\quad \\beta_{0}=\\frac{6}{10} \\\\ \\beta_{1}=\\beta_{2}=\\beta_{3}=0\n",
      "\\end{gathered} $$ \\subsection*{9.3.2 LMS Methods for Second-order Equations} Consider a system of second-order, linear, ordinary differential equations written\n",
      "in the form: \\begin{equation*} \\ddot{y}=f(y, \\dot{y}, t)=G_{0} y+G_{1} \\dot{y}+H(t) \\tag{9.3.5} \\end{equation*} where $G_{0}$ and $G_{1}$ are given constant\n",
      "matrices. A $\\boldsymbol{k}$-step LMS method for (9.3.5) is given by (Geradin [18]): $$ \\sum_{i=0}^{k}\\left\\{\\alpha_{i} y_{n+1-i}+\\Delta t \\beta_{i} G_{1}\n",
      "y_{n+1-i}+\\Delta t^{2} \\gamma_{i}\\left[G_{0} y_{n+1-i}+H\\left(t_{n+1-i}\\right)\\right]\\right\\}=0 $$ The method is defined by the values of $\\alpha_{i},\n",
      "\\beta_{i}$ and $\\gamma_{i}, i=0,1, \\ldots, k$. An LMS method of this type is called explicit if $\\beta_{0}$ and $\\gamma_{0}=0$. (If $\\boldsymbol{G}_{1}=0$, then\n",
      "clearly only $\\gamma_{0}$ need be zero.) The method is a backward-difference method if $\\beta_{i}$ and $\\gamma_{i}=0$, $i \\geq 1$. (Likewise, if $G_{1}=0$, only\n",
      "the $\\gamma_{i}^{\\prime} s, i \\geq 1$, need be zero.) The equation of motion may be put in the form of ( 9.3 .5 ) simply by multiplying through by $M^{-1}$.\n",
      "Observe that with $y=d$, (9.3.6) is a displacement difference-equation form of the algorithm. The commonly used algorithms of structural dynamics can all be put\n",
      "into this form, although very few are naturally cast this way. In the sense of an LMS method for second-order systems, Newmark's method is a two-step method.\n",
      "LMS methods applied to the first-order form of the equation of motion may likewise be recast as second-order-type LMS methods (e.g., Park's method). The\n",
      "stability properties of second-order-type LMS methods may be investigated in similar fashion to first-order LMS methods. Briefly, modal reduction may be\n",
      "employed which, for the equation of motion, leads to the following polynomial: \\begin{equation*} \\sum_{i=0}^{k}\\left(\\alpha_{i}+2 \\xi \\omega^{h} \\Delta t\n",
      "\\beta_{i}+\\left(\\omega^{h} \\Delta t\\right)^{2} \\gamma_{i}\\right) \\zeta^{n+1-i}=0 \\tag{9.3.7} \\end{equation*} An analog of Dahlquist's theorem also holds for LMS\n",
      "methods for second-order equations (see Krieg [19]). Exercise 4. Derive the following displacement difference-equation form of Newmark's algorithm:\n",
      "\\begin{align*} &\\left(M+\\gamma \\Delta t C+\\beta \\Delta t^{2} K\\right) d_{n+1}+\\left[-2 M+(1-2 \\gamma) \\Delta t C+\\left(\\frac{1}{2}-2 \\beta+\\gamma\\right) \\Delta\n",
      "t^{2} K\\right] d_{n} \\\\ &+\\left[M-(1-\\gamma) \\Delta t C+\\left(\\frac{1}{2}+\\beta-\\gamma\\right) \\Delta t^{2} K\\right] d_{n-1}+\\Delta t^{2} \\bar{F}_{n}=0\n",
      "\\tag{9.3.8} \\end{align*} Define $\\bar{F}_{n}$ in terms of $\\boldsymbol{F}_{n-1}, \\boldsymbol{F}_{n}$, and $\\boldsymbol{F}_{n+1}$.\\\\ Exercise 5. Modally reduce\n",
      "the homogeneous version of (9.3.8) to the form (9.3.7). Define $\\alpha_{i}$ and $\\beta_{i}, i=0,1,2$. Show that this results in an identical equation to the one\n",
      "derived from the $2 \\times 2$ amplification matrix form, (9.1.44). Deduce that the roots of the stability polynomial (9.3.7) are identical to the eigenvalues of\n",
      "the amplification matrix (i.e., $\\zeta=\\lambda(\\boldsymbol{A})$ ). Exercise 6. Consider the scalar difference equation that emanates from (9.3.1), (9.3.2) after\n",
      "modal reduction: \\begin{equation*} \\sum_{i=0}^{k}\\left(a_{i}+\\Delta t \\lambda \\beta_{i}\\right) z_{n+1-i}=0 \\tag{9.3.9} \\end{equation*} Derive a one-step,\n",
      "multivalue method corresponding to (9.3.9), which has the form \\begin{equation*} \\boldsymbol{Z}_{n+1}=\\boldsymbol{A} \\boldsymbol{Z}_{n} \\tag{9.3.10}\n",
      "\\end{equation*} where $Z_{n}=\\left\\{z_{n}, z_{n-1}, \\ldots, z_{n+1-k}\\right\\}^{T}$. That is, explicitly define the $k \\times k$ amplification matrix\n",
      "$\\boldsymbol{A}$. Establish that the eigenvalues of $\\boldsymbol{A}$ are the same as the roots, $\\boldsymbol{\\zeta}$, of the stability polynomial, (9.3.3),\n",
      "emanating from (9.3.9). See (9.1.14). Exercise 7. Following along the lines of the previous exercise, consider the stability polynomial: \\begin{equation*}\n",
      "\\zeta^{3}-2 A_{1} \\zeta^{2}+A_{2} \\zeta-A_{3}=0 \\tag{9.3.11} \\end{equation*} Derive a $3 \\times 3$ amplification matrix, as in (9.3.10), which gives rise to\n",
      "(9.3.11). Show that \\begin{align*} & A_{1}=\\frac{1}{2} \\text { trace } A \\tag{9.3.12}\\\\ & A_{2}=\\text { sum of the principal minors of } A \\tag{9.3.13}\\\\ &\n",
      "A_{3}=\\operatorname{det} A \\tag{9.3.14} \\end{align*} (Equations (9.3.12) through (9.3.14) define the principal invariants of $A$. They arise naturally in\n",
      "calculating the stability polynomial by way of $\\operatorname{det}(A-\\zeta I)=0$; cf. (9.3.11).) Exercise 8. The region of absolute stability of a fourth-order\n",
      "explicit Runge-Kutta LMS method for the model equation $\\dot{y}=\\lambda y$, where $\\lambda$ is a complex number, is depicted in the accompanying figure. (Assume\n",
      "the eigenvalues of the amplification matrix are distinct.)\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_9cbaa12b79ae51b9f5cfg-39} Determine the\n",
      "stability conditions for the following cases:\\\\ i. Parabolic case. [Ans.: $-2.7 \\leq \\lambda^{h} \\Delta t \\leq 0$ ]\\\\ ii. Undamped hyperbolic case. [Ans.:\n",
      "$\\omega^{\\boldsymbol{h}} \\Delta t \\leq 2.5$ ] Exercise 9. Consider the trapezoidal-rule algorithm. Show that it is $A$-stable. Determine its region of absolute\n",
      "stability. [Ans.: The left-hand complex plane including the imaginary axis; i.e., it is stable for all $\\lambda^{h} \\Delta t$ such that\n",
      "$\\operatorname{Re}\\left(\\lambda^{h} \\Delta t\\right) \\leq 0$.] Exercise 10. Show that the Euler backward-difference method is $A$-stable. Determine its region of\n",
      "absolute stability. Exercise 11. Determine the region of absolute stability of the Euler forward-difference algorithm. Exercise 12. Determine local truncation\n",
      "error expressions for LMS methods of type (9.3.2) and $(9.3 .6)$.\\\\ Answer: $$ \\begin{aligned} \\Delta t \\tau & =\\sum_{i=0}^{k}\\left\\{\\alpha_{i}\n",
      "y\\left(t_{n+1-i}\\right)+\\Delta t \\beta_{i} f\\left(y\\left(t_{n+1-i}\\right), t_{n+1-i}\\right)\\right\\} \\\\ \\Delta t^{2} \\tau & =\\sum_{i=0}^{k}\\left\\{\\alpha_{i}\n",
      "y\\left(t_{n+1-i}\\right)+\\Delta t \\beta_{i} G_{1} y\\left(t_{n+1-i}\\right)+\\Delta t^{2} \\gamma_{i}\\left[G_{0}\n",
      "y\\left(t_{n+1-i}\\right)+H\\left(t_{n+1-i}\\right)\\right]\\right\\} \\end{aligned} $$\n",
      "\n",
      " 8: Okay. So we're now shaping up to get to the main result related to finite element analysis, right? An, an analysis of the conversions of the method. To get\n",
      "there, to, to actually get to that result, there is one more class of results that we need. So the title of this segment is going to be Sobolev estimates.\n",
      "Sobolev of course, refers to the type of space that we are working with and that I'll tell you what we're trying to estimate here. In order to say what we are\n",
      "trying to estimate let us sort of reuse our symbol U, capital U of h that we introduced before. It continues to represent a general member of the space Sh,\n",
      "which means it satisfies the, the rationally boundary condition and it lives in H1. Okay? For, for our particular problem, it, it lives in H1. But in the\n",
      "context of Sobolev estimates, which are applicable to spaces with which, which are applicable in general to Hn spaces. Let me just redefine this for now. Okay.\n",
      "Let me say that Belongs to Sh, which now consists of all functions. That for our, for our purposes now, which actually hold in more general settings. Okay?\n",
      "Consist of Hn functions. And additionally, of course, they need to satisfy the Dirichlet boundary condition. Right? And so, if we were doing the, the Dirichlet-\n",
      "Neumann problem, we would have At 0 equals 0. Okay. So we're re-invoking this, this same function capital. Remember, U, capital Does not need to be the, the\n",
      "finite element solution. It could be, right? But it does not need to be, right? So let me state that. Does not necessarily represent little Right? Which is the\n",
      "finite element solution. It is any other solution in Sh, right? Where Sh now has been defined to include all functions that belong, that live in Hn. Okay? All\n",
      "right. I want now to consider a very special member a member of this set of functions. Okay? Consider Such that. At any nodal point let me say, this as X let me\n",
      "just say, x sub A. Okay? So At x sub A equals little at x sub A. Right? Okay. But if A here just denotes our global numbering of degrees of freedom, then this\n",
      "remember, using our finite element vector notation is just the degree of freedom, d sub A. Okay? All right. All right. So what this, what this says is that A is\n",
      "a global degree of freedom, right? And this, this then implies that xA is a globally numbered node and dA is a globally numbered Trial solution degree of\n",
      "freedom. Right. Or finite element solution degree of freedom. Okay? All of that should be clear. So what we're saying is that we want to consider a special\n",
      "member of this class, where Is equal to the finite element solution at the node, the degrees of freedom. Right? At the nodes, okay? But you know that if we do\n",
      "this, we would have capital Actually equals to little everywhere, right? Because of the fact of the finite element representation. Okay? Right? So this is not,\n",
      "not exactly the solution we want. Not exactly the the function we want. Consider another function, U tilde h which is such, that U tilde h. Okay? At xA, now\n",
      "equals the exact solution evaluated at that point. Okay? The idea is that if you had the exact solution and you could evaluated it at a nodal point, right? Take\n",
      "that value and let's suppose that u tilde h is such that it hits the exact solution right there on the nodes. So what we're seeing here is that u tilde h,\n",
      "right? Expressed now as a function of parametrize by little x, right? By position. U tilde h is what is said to be nodally exact, okay? I'm going to sketch it\n",
      "out for us, suppose this is our domain. And for simplicity, let's just suppose we have linear basis functions. Okay? And suppose our exact solution is the\n",
      "following. Let me go to a different color here. Our exact solution I'm going to plot in green, okay? Let's suppose this is our exact solution, okay? All right.\n",
      "This is our exact solution and what we're talking of doing here is following. We are talking of we're talking of looking at these nodal points. And identifying\n",
      "the value of the exact solution at the nodes. Okay? And then we're talking of so, so this, so this is u, the exact solution, okay? All right, and what we are,\n",
      "what we want to do now is identify the u tilde h. Okay? U tilde h, remember, is a member of S h, right, which means it inherits the representation that we get\n",
      "from our basis functions. If we are working with linears here, u tilde h can at, can at best be linear for each element. However ,we want it to be such that\n",
      "actually hits the exact solution at the nodes. Okay? For many of the other elements it's, it's not going to be very different from the exact solution because\n",
      "perhaps the exact solution is not too different a linear, sorry, it's not too different from a linear solution elsewhere. Okay. You see that there is a\n",
      "difference, right? See, this is our u tilde h. And this is what we mean by saying that u tilde h is nodally exact, right, it's exactly the nodes. It hits the\n",
      "exact solution in the nodes, but it is a member of S h, so over an element in this particular case it is as, at, at, it is, it is, it is linear. Okay? And let\n",
      "me go back to my red here, okay. So this, this is just plotting up a solution up here. All right? Okay, so this is what we mean by saying u tilde h is nodally\n",
      "exact. Such a function, u tilde h, is what is often called an interpolate of the exact solution. Because what we are doing is taking the exact solution at the\n",
      "nodes, and then using the basis functions to essentially interpolate between those exact nodal values to get u tilde h, okay? U tilde h is nodally exact and is\n",
      "also called the interpolate. Okay? All right. Okay, and just remember that u tilde h belongs to S h, right, and that's re, reflected in the fact that it is\n",
      "linear over elements. In this case we are considering linears, okay? So we're considering, in this particular example only linear basis functions. All right,\n",
      "okay. So this is what we have. Now, here is a result. The theory of Sobolev spaces, right, or, or mathematics of Sobolev spaces, functional analysis on Sobolev\n",
      "spaces, gives us the following so called interpolation Error estimate. In Sobolev spaces. Okay, the interpolation error estimate is for the following quantity.\n",
      "It holds for the m norm, the h m norm of the difference between this interpolate and the exact solution. Okay? It's important to note why this is called the\n",
      "interpolation error, right? So the term on the left is my interpolation error. Interpolation error is this, okay? U tilde h minus u. The reason it's called\n",
      "interpolation error should be pretty obvious, right? And what is that? Think about it. What it is saying is that, suppose you had the exact solution, right, and\n",
      "at the nodes you were to hit the exact solution, which is what U tilde h does, right? But then because of your choice of a finite dimensional basis over the\n",
      "element, you are now interpolating from that exact solution. But since you are interpolating with a finite dimensional basis, you fail to hit the exact solution\n",
      "over an element, right, most prominently here. Therefore, this difference that we are looking at is indeed the interpolation error, okay? All right. So the\n",
      "Sobolev, estimate here, and, and I'm, I'm just going to state it, we're not going to prove it, is the following. It is that this interpolation error estimate is\n",
      "bounded from above by c as a constant. h e is our element size. Okay, and in these error estimates the we assume, or the error estimates have been derived for\n",
      "the case in which we have a uniform element size, okay? So the c h e to the power of a number alpha, which I will define in a little bit, okay, c h e to the\n",
      "power alpha times the h r norm of u, okay? So what do we have here? U tilde h minus u is the interpolation error. It's the error incurred by the fact that we're\n",
      "using finite dimensional basis functions even if we had the exact solution at the nodes, okay. H e, as before, is the element size. c is a constant. r is what\n",
      "is called the regularity of the exact solution. Okay? So note that when we're talking about u r the r norm of u, we're seeing that yes, if we were to square\n",
      "integrate the exact solution and also square integrate its derivatives up to r, right, up to its rth derivative, we would get a quantity which is bounded,\n",
      "right, and that is the r norm. Okay? All right, so let's just recall that this quantity, the r notm of our u Is a measure of smoothness, of regularity, really.\n",
      "r comes from regularity. Right, which is really a measure of smoothness Of u, all right? If you have the r norm of u, it means that you can take up to r\n",
      "derivatives of u and still have that quantity bounded, which gives you some sense of how small the u is, okay? All right, the next thing I need to tell you is\n",
      "about what alpha is. Right, alpha is the exponent in to which h e is raised, right, the power of h e. Alpha satisfies it, it's an exponent, obviously. And it\n",
      "satisfies. The following condition. Alpha is equal to the minimum of right, k plus 1 minus m, and r minus m. Well, what is k now? k, k is the order of the\n",
      "polynomial order of our finite dimensional basis. k is the polynomial order of the finite element, sorry, finite dimensional basis. Okay? All right. Let me do\n",
      "just well, let, let, let me rewrite the result here, okay. So that is h minus u interpolation error. We are computing the m norm of it, right? And this is\n",
      "lesser than or equal to c h e to the power alpha. Okay, so what this is telling us is that supposing in most cases, let us suppose that we have a solution just\n",
      "to fix ideas. Let's suppose that our exact solution is such that we can actually take derivatives of it up to a very high order and you know, we're, we're able\n",
      "to take derivatives of it to a very high order, which means it's very smooth, okay? What it means is that in this definition of alpha, r is going to be very\n",
      "big, okay? So even though we want to take, so, so, we, we are going to take m derivatives of the solution and we are interested in knowing what the m norm of it\n",
      "is. If r is very big, what it says is that the order of our of our exponent here, the size of our exponent is controlled by the polynomial order of the basis\n",
      "functions that we have. Okay? Now why does this matter? The question is, what happens with our interpolation error as we refine the mesh? Okay, if r is big\n",
      "enough, then alpha essentially reduces to k plus 1 minus m, right? So if r is large, okay, which means the exact solution is very smooth. Right, we have a nice,\n",
      "well-behaved problem. Okay? In this setting what we will see is that alpha being the minimum of those two quantities, is indeed going to come down to k plus 1\n",
      "minus m, okay? And therefore, what this thing is saying, then, is that our result is that the, the interpolation error. The m norm of the interpolation error is\n",
      "lesser than or equal to c h e to the power k plus 1 minus m times this r norm of u. But then if u is very smooth, we expect that the r norm is not very big,\n",
      "right? We won't get a very large number when we integrate the functions, to integrate the function and its r derivatives up to r, okay? But that tells us then\n",
      "that now as as h e tends to 0, right, if our quantity k plus 1 minus m is greater than 0, what happens with the interpolation error? What happens with the norm\n",
      "of the interpolation error? It vanishes, right? Okay? It vanishes at the rate k plus 1 minus m, right? So it tells us that any nor, that, that this h, sorry,\n",
      "this m norm that we want to compute of the interpolation error also tends to 0 at the rate k plus 1 minus m. Okay? So as we refine the mesh, as we make h e\n",
      "smaller, right, remember h e going to 0 means we are looking at mesh refinement. We are going to smaller and smaller elements. Okay? So what the Sobolev\n",
      "estimate tells us is that yeah, as you refine the mesh eh, if you were to look at this interpolation error it will vanish, provided, provided what? Provided\n",
      "that number is greater than 0. How can we make that number greater than 0? We can make the number greater than 0 by taking a higher polynomial order, right, or\n",
      "making sure that we are not taking too many derivatives in computing the norm. So we don't, if we are not looking for very high m norms, right, we're not\n",
      "looking for taking many derivatives when we compute the m norm, right, then this holds. Right, and this is an important thing to know. This, this property, note\n",
      "we should note that this property comes to us directly from the Sobolev space. Right, we're not saying anything yet about the finite element solution. This\n",
      "result is a property of our Sobolev space S h. Okay? How is it a property of our Sobolev space S h? Well, that's what determines the polynomial order k. Right?\n",
      "That is determined by, by the space we have picked for S h, right? So it says as long as you pick that to be high enough, and as long as you're not looking for\n",
      "too many derivatives in this interpolation error it does converge. Okay? No talk yet about the finite element solution. Okay? That will come next. And that will\n",
      "come in the next segment.\n",
      "\n",
      " 9: The primary requirement of the algorithms given in the last section is that they converge. We shall call an algorithm convergent if for $t_{n}$ fixed and\n",
      "$\\Delta t=t_{n} / n$, $d_{n} \\rightarrow d\\left(t_{n}\\right)$ as $\\Delta t \\rightarrow 0$. To establish the convergence of an algorithm, two additional notions\n",
      "must be considered: stability and consistency. We shall show later on that once stability and consistency are verified, convergence is automatic. In addition,\n",
      "we shall be concerned with the accuracy of an algorithm, i.e., the rate of convergence as $\\Delta t \\rightarrow 0$, and allied topics such as the behavior of\n",
      "the (spurious) higher modes of the semidiscrete system. There are several techniques that can be employed to study the characteristics of an algorithm. In the\n",
      "present context the most revealing approach appears to be the \"modal approach\" (sometimes called spectral, or Fourier, analysis) in which the problem is\n",
      "decomposed into $n_{\\text {eq }}$ uncoupled scalar equations. It can be rigorously established that the behavior of the entire coupled system reduces to\n",
      "consideration of the individual modal equations that comprise it. Our first step in analyzing the family of algorithms introduced in Sec. 8.1 will be to perform\n",
      "the reduction to single-degree-of-freedom (SDOF) form. \\subsection*{8.2.1 Modal Reductlon to SDOF Form} The essential property used in reducing to SDOF form is\n",
      "the orthogonality of the eigenvectors of the associated eigenvalue problem. Recall that \\begin{equation*} \\left(K-\\lambda_{l}^{h} M\\right) \\Psi_{l}=0, \\quad l\n",
      "\\in\\left\\{1,2, \\ldots, n_{e q}\\right\\} \\tag{8.2.1} \\end{equation*} where \\begin{equation*} 0 \\leq \\lambda_{1}^{h} \\leq \\lambda_{2}^{h} \\leq \\cdots \\leq\n",
      "\\lambda_{n_{\\text {eq }}}^{h} \\tag{8.2.2} \\end{equation*} and \\begin{equation*} \\psi_{l}^{T} M \\psi_{m}=\\delta_{l m} \\quad \\text { (orthonormality) }\n",
      "\\tag{8.2.3} \\end{equation*} where $\\delta_{\\text {Im }}$ is the Kronecker delta. Furthermore, the eigenvectors $\\left\\{\\psi_{l}\\right\\}_{1_{10}}{ }^{n_{0}}$\n",
      "constitute a basis for $\\mathbb{R}^{n_{4}}$, meaning that any element in $\\mathbb{R}^{n^{n}}$ can be written as a linear combination of the $\\psi_{i}$ 's. From\n",
      "the orthonormality property, it immediately follows that \\begin{equation*} \\psi_{l}^{T} K \\psi_{m}=\\lambda_{l}^{h} \\delta_{l m} \\quad \\text { (no sum) }\n",
      "\\tag{8.2.4} \\end{equation*} These properties will be used to decompose both the semidiscrete heat equation and the generalized trapezoidal algorithm.\n",
      "Semidiscrete heat equation. Let \\begin{equation*} d(t)=\\sum_{m=1}^{n_{eq}} d_{(m)}(t) \\psi_{m} \\tag{8.2.5} \\end{equation*} from which it follows that\n",
      "\\begin{equation*} \\dot{d}(t)=\\sum_{m=1}^{n_{\\text {eq }}} \\dot{d}_{(m)}(t) \\psi_{m} \\tag{8.2.6} \\end{equation*} The scalar-valued functions $d_{(m)}(t)$\n",
      "(Fourier coefficients) are obtained by premultiplying (8.2.5) by $\\psi_{T}^{T} M$ and invoking the orthonormality property. The result is \\begin{equation*}\n",
      "d_{(l)}(t)=\\psi_l^{T} M d(t) \\tag{8.2.7} \\end{equation*} (We have included the subscript in parentheses to avoid notational confusion.) The coefficients in\n",
      "(8.2.6) are obtained by differentiating (8.2.7): \\begin{equation*} \\dot{d}_{(l)}(t)=\\psi_l^T M \\dot{d}(t) \\tag{8.2.8} \\end{equation*} Employing (8.2.5) and\n",
      "(8.2.6) in (8.1.1) and premultiplying by $\\psi T$ yields \\begin{equation*} \\sum_{m=1}^{n_{e q}}\\left(\\dot{d}_{(m)} \\psi_{l}^{T} \\boldsymbol{M} \\psi_{m} +\n",
      "d_{(m)} \\psi_{l}^{T} \\boldsymbol{K} \\psi_{m}\\right) = \\psi_{l}^{T} \\boldsymbol{F} \\tag{8.2.9} \\end{equation*} from which it follows that \\begin{equation*}\n",
      "\\dot{d}_{(l)}+\\lambda_{l}^{h} d_{(l)}=F_{(l)} \\tag{8.2.10} \\end{equation*} the $l$ th-modal equation, where $F_{(l)}(t)=\\psi_{l}^{T} F(t)$.\\\\ The initial\n",
      "condition for (8.2.10) is obtained by premultiplying (8.1.2) by $\\psi_{l}^{T} M$ : \\begin{align*} d_{(l)}(0) & =\\psi_{l}^{T} M d(0) \\\\ & =\\psi_{l}^{T} M d_{0}\n",
      "\\\\ & \\stackrel{\\text { def }}{=} d_{0(l)} \\tag{8.2.11} \\end{align*} Solution of (8.2.10) for each mode allows us to construct the solution $d$ of the original\n",
      "problem via (8.2.5). To simplify the subsequent writing further we shall omit the $l$ th -modal subscript in (8.2.10) and (8.2.11). Thus our typical modal\n",
      "initial-value problem consists of the following equations: \\begin{align*} \\dot{d}+\\lambda^{h} d & =F, \\quad t \\in[0, T] \\tag{8.2.12}\\\\ d(0) & =d_{0}\n",
      "\\end{align*} (SDOF model problem) Generalized trapezoidal algorithm. The procedure for decomposing the generalized trapezoidal algorithm is similar to that for\n",
      "the semidiscrete equation. The main results are collected here: \\begin{align*} & \\boldsymbol{d}_{n}=\\sum_{m=1}^{n_{\\text {cq }}} d_{n(m)} \\boldsymbol{\\psi}_{m}\n",
      "\\tag{8.2.14}\\\\ & \\boldsymbol{d}_{n+1}=\\sum_{m=1}^{n_{e q}} d_{n+1(m)} \\boldsymbol{\\psi}_{m} \\tag{8.2.15}\\\\ & d_{n(l)}=\\boldsymbol{\\psi}_l^{T} M d_{n}\n",
      "\\tag{8.2.16}\\\\ & d_{n+l(l)}=\\boldsymbol{\\psi}_{l}^{T} M d_{n+1} \\tag{8.2.17} \\end{align*} \\begin{align*} & \\sum_{m=1}^{n_{e q}}\\left[d_{n+1(m)}\n",
      "\\psi_{l}^{T}(M+\\alpha \\Delta t K) \\Psi_{m}\\right. \\\\ &\\left.-d_{n(m)} \\psi_{l}^{T}(M-(1-\\alpha) \\Delta t K) \\psi_{m}\\right]=\\Delta t \\Psi_{l}^{T} F_{n+\\alpha}\n",
      "\\tag{8.2.18}\\\\ & F_{n+\\alpha}=(1-\\alpha) F_{n}+\\alpha F_{n+1} \\tag{8.2.19}\\\\ &\\left(1+\\alpha \\Delta t \\lambda_{l}^{h}\\right) d_{n+1(l)}=\\left(1-(1-\\alpha)\n",
      "\\Delta t \\lambda_{l}^{h}\\right) d_{n(l)}+\\Delta t F_{n+\\alpha(l)} \\tag{8.2.20} \\end{align*} The initial value, $d_{0(n)}$, is defined by (8.2.11). As before, we\n",
      "will omit the $l$ th-modal subscript in (8.2.20). Thus we may write (temporally discretized SDOF model problem) \\[ \\begin{array}{c} \\left(1+\\alpha \\Delta t\n",
      "\\lambda^{h}\\right) d_{n+1}=\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right) d_{n}+\\Delta t F_{n+\\alpha} \\tag{8.2.21}\\\\ d_{0} \\text { given } \\end{array} \\]\n",
      "\\subsubsection*{Remarks} \\begin{enumerate} \\item The convergence of $d_{n}$ to $d\\left(t_{n}\\right)$ can be established by showing the Fourier coefficients\n",
      "converge [i.e., $d_{n(l)}$ converges to $d_{(l)}\\left(t_{n}\\right)$ for each $l \\in\\left\\{1,2, \\ldots, n_{e q}\\right\\}$ ]. The proof of this goes as follows.\n",
      "Let $e\\left(t_{n}\\right)=d_{n}-d\\left(t_{n}\\right)$ denote the error in $d_{n}$, and let $e_{(l)}\\left(t_{n}\\right)=d_{n(n)}-d_{(n)}\\left(t_{n}\\right)$ denote\n",
      "the $l$ th-Fourier component of $e\\left(t_{n}\\right)$. Then \\end{enumerate} \\begin{align*} e\\left(t_{n}\\right)^{T} M e\\left(t_{n}\\right) & =\\sum_{l, m=1}^{n_{e\n",
      "q}}\\left(e_{(l)}\\left(t_{n}\\right) \\psi_{l}\\right)^{T} M\\left(e_{(m)}\\left(t_{n}\\right) \\psi_{m}\\right) \\\\ & =\\sum_{l,m=2}^{n_{eq}} e_{(l)}\\left(t_{n}\\right)\n",
      "e_{(\\dot{m})}\\left(t_{n}\\right) \\psi_{l}^{T} M \\psi_{m} \\\\ & =\\sum_{l,m=1}^{n_{eq}} e_{(l)}\\left(t_{n}\\right) e_{(m)}\\left(t_{n}\\right) \\delta_{l m} \\quad \\text\n",
      "{ (orthonormality) } \\\\ & =\\sum_{l=1}^{n_{e q}}\\left(e_{(l)}\\left(t_{n}\\right)\\right)^{2} \\tag{8.2.22} \\end{align*} and so $e\\left(t_{n}\\right)^{T} M\n",
      "e\\left(t_{n}\\right) \\rightarrow 0$ if and only if $e_{(l)}\\left(t_{n}\\right) \\rightarrow 0$ for each $l \\in\\left\\{1,2, \\ldots n_{e q}\\right\\}$. Because $M$ is\n",
      "assumed positive-definite, $e\\left(t_{n}\\right)^{T} M e\\left(t_{n}\\right) \\rightarrow 0$ if and only if $e\\left(t_{n}\\right) \\rightarrow 0$. Consequently, we\n",
      "need consider only the SDOF problems in subsequent discussion.\\\\ 2. Note that directly applying the generalized trapezoidal method to (8.2.12) also results in\n",
      "(8.2.20). This fact is depicted in the following commutative diagram:\\\\ \\includegraphics[max width=\\textwidth, center]{2024_10_04_10b9e5948d77e8b27b2dg-07}\n",
      "\\subsection*{8.2.2 Stability} To motivate the appropriate notion of stability for the case under consideration, we shall investigate the behavior of the\n",
      "homogeneous model equation \\begin{equation*} \\dot{d}+\\lambda^{h} d=0 \\tag{8.2.23} \\end{equation*} This is a first-order ordinary differential equation, which\n",
      "can be easily solved. The solution at time $t_{n+1}$ for initial value $d\\left(t_{n}\\right), t_{n+1}>t_{n}$, is \\begin{equation*} d\\left(t_{n+1}\\right)=\\exp\n",
      "\\left(-\\lambda^{h}\\left(t_{n+1}-t_{n}\\right)\\right) d\\left(t_{n}\\right) \\tag{8.2.24} \\end{equation*} from which it follows that \\[ |d(t_{n+1})| < |d(t_n)|,\n",
      "\\lambda^h>0 \\] \\[ d(t_{n+1}) = d(t_n), \\lambda^h = 0 \\] The conditions in (8.2.25) are what we wish to mimic in the temporally discrete case.\\\\ The homogeneous\n",
      "temporally discrete model equation is \\begin{equation*} \\left(1+\\alpha \\Delta t \\lambda^{h}\\right) d_{n+1}=\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right) d_{n}\n",
      "\\tag{8.2.26} \\end{equation*} Noting that $\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)>0$ for all allowable values of the parameters, we can write (8.2.26) as\n",
      "\\begin{equation*} d_{n+1}=A d_{n} \\tag{8.2.27} \\end{equation*} where $A=\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right) /\\left(1+\\alpha \\Delta t\n",
      "\\lambda^{h}\\right)$ is the amplification factor.\\\\ Our stability requirements will be that \\[ \\left.\\begin{array}{rlrl} \\left|d_{n+1}\\right| &\n",
      "<\\left|d_{n}\\right|, & & \\lambda^{h}>0 \\tag{8.2.28}\\\\ d_{n+1} & =d_{n}, & & \\lambda^{h}=0 \\end{array}\\right\\} \\] From the definition of $A$, the second of\n",
      "(8.2.28) is automatic. The first condition is equivalent to insisting that \\begin{equation*} |A|<1 \\tag{8.2.29} \\end{equation*} for $\\lambda^{h}>0$. To\n",
      "determine the restrictions imposed upon $\\alpha, \\Delta t$, and $\\lambda^{h}$, it is convenient to rewrite (8.2.29) as $-1<A<1$, viz., \\begin{equation*}\n",
      "-1<\\frac{\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right)}{\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)}<1 \\tag{8.2.30} \\end{equation*} The right-hand inequality is\n",
      "satisfied for all allowable values of the parameters, and the left-hand inequality is satisfied whenever $\\alpha \\geq \\frac{1}{2}$. However, if\n",
      "$\\alpha<\\frac{1}{2}$, the left-hand inequality requires $\\lambda^{h} \\Delta t<2 /(1-2 \\alpha)$. For a given $\\lambda^{h}$ this imposes an upper bound on the\n",
      "size of the allowable time step. The greater the $\\lambda^{h}$, the smaller the time step required. \\subsubsection*{Remark} An algorithm for which stability\n",
      "imposes a time step restriction is called conditionally stable. An algorithm for which there is no time step restriction imposed by stability is called\n",
      "unconditionally stable. The significance of the stability concept introduced may be seen from the following example: \\subsubsection*{Example} Note that the\n",
      "solution of (8.2.27) may be written \\begin{equation*} d_{n}=A^{n} d_{0} \\tag{8.2.31} \\end{equation*} If $d_{n}$ is to behave like the solution of (8.2.23)\n",
      "(i.e., decay), $A^{n}$ must converge (i.e., as $n \\rightarrow \\infty, A^{n} \\rightarrow 0$ ). If $|A|<1$, clearly this will be the case. On the other hand, if\\\\\n",
      "$|A|>1$, growth will occur. Even if the value of $|A|$ is only slightly larger than 1 , disastrous growth can occur. To see what can happen, consider the\n",
      "numerical data in Table 8.2.1. The necessity of keeping $|A|<1$ should be clearly apparent, or else virtually unbounded errors will enter a computation. TABLE\n",
      "8.2.1 $A^{n}$ for Various Values of $A$ and $n$ \\begin{center} \\begin{tabular}{|c|c|c|} \\hline $A$ & $n$=100 & $n$=1000 \\\\ \\hline 0.99 & .37 & $4.32 \\times\n",
      "10^{-5}$ \\\\ 1.01 & 2.70 & $2.09 \\times 10^{4}$ \\\\ .9 & $2.66 \\times 10^{-5}$ & $1.75 \\times 10^{-46}$ \\\\ 1.1 & $1.39 \\times 10^{4}$ & $2.47 \\times 10^{41}$ \\\\\n",
      "\\hline \\end{tabular} \\end{center} \\subsubsection*{Remarks} \\begin{enumerate} \\item In the conditionally stable case, the stability condition $\\Delta t<2\n",
      "/\\left[(1-2 \\alpha) \\lambda^{h}\\right]$ must hold for all modes (i.e., all $\\lambda_{l}^{h}, l \\in\\left\\{1,2, \\ldots, n_{\\text {eq }}\\right\\}$ ) in the system.\n",
      "The greatest $\\lambda_{l}^{h}$, namely, $\\lambda_{n_{c q}}^{h}$, imposes the most stringent restriction upon the time step (i.e., $\\left.\\Delta t<2 /\\left[(1-2\n",
      "\\alpha) \\lambda_{n_{e q}}^{h}\\right]\\right)$. In fact, for the heat conduction problem, it can be shown that $\\lambda_{n_{\\text {eq\n",
      "}}}^{h}=O\\left(h^{-2}\\right)$, where $h$ is the mesh parameter, and thus the critical time step must satisfy $\\Delta t<$ constant $\\cdot h^{2}$. In a large\n",
      "system of equations (i.e., $n_{e q} \\gg 1$ ), this condition is a severe constraint; thus unconditionally stable algorithms are generally preferred. \\item In\n",
      "Fig. 8.2.1 the behavior of $A$ as a function of $\\lambda^{h} \\Delta t$ is depicted for several values of $\\alpha$. The value of $\\lambda^{h} \\Delta t$ for which\n",
      "$A=0$ is called the oscillation limit because for greater values, the sign of $A^{n}$ changes from step to step. For the unconditionally stable algorithms\n",
      "(i.e., ones for which $\\alpha \\geq \\frac{1}{2}$ ), the asymptotic value of the amplification factor satisfies $\\left|A_{\\infty}\\right| \\leq 1$. Thus for all\n",
      "fixed $\\lambda^{h} \\Delta t,|A|<1$ and all spurious high modal components decay. However, for $\\alpha=\\frac{1}{2}$ (or very near $\\frac{1}{2}$ ) and\n",
      "$\\lambda^{k} \\Delta t \\gg 1, A \\cong-1$, and thus high modal components will behave like $(-1)^{n}$. This \"sawtooth\" pattern in time manifests itself frequently\n",
      "in computations. In reporting data for a calculation in which this is the case, these spurious higher modes may be filtered out by reporting the step-to-step\n",
      "averages, $\\left(d_{n+1}+d_{n}\\right) / 2$, because $(-1)^{n+1}+$ $(-1)^{n}=0$. \\end{enumerate} Summary: Stability for the generalized trapezoidal methods\\\\\n",
      "Amplification factor: $A=\\frac{1-(1-\\alpha) \\Delta t \\lambda^{h}}{1+\\alpha \\Delta t \\lambda^{h}}$\\\\ Stability requirement: $|A|<1$ for\n",
      "$\\lambda^{h}=\\lambda_{n_{\\text {eq }}}^{h} \\quad$ (= maximum eigenvalue)\\\\ Unconditional stability: $\\quad \\alpha \\geq \\frac{1}{2}$\\\\ Conditional stability:\n",
      "$\\quad \\alpha<\\frac{1}{2}, \\quad \\Delta t<\\frac{2}{(1-2 \\alpha) \\lambda_{n_{e q}}^{h}}$\\\\ \\includegraphics[max width=\\textwidth,\n",
      "center]{2024_10_04_10b9e5948d77e8b27b2dg-10} Figure 8.2.1 Amplification factor for typical one-step methods. \\subsection*{8.2.3 Convergence} The temporally\n",
      "discrete model problem may be written in the form \\begin{equation*} d_{n+1}-A d_{n}-L_{n}=0 \\tag{8.2.32} \\end{equation*} where the load $L_{n}=\\Delta t\n",
      "F_{n+\\alpha} /\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)$. If we replace $d_{n}$ and $d_{n+1}$ in the left-hand side of (8.2.32) by the corresponding exact\n",
      "values, we obtain an expression of the form \\begin{equation*} d\\left(t_{n+1}\\right)-\\operatorname{Ad}\\left(t_{n}\\right)-L_{n}=\\Delta t \\cdot\n",
      "\\tau\\left(t_{n}\\right) \\tag{8.2.33} \\end{equation*} where $\\tau\\left(t_{n}\\right)$ is called the local truncation error. If $|\\tau(t)| \\leq c \\Delta t^{k}$, for\n",
      "all $t \\in[0, T]$, where $c$ is a constant independent of $\\Delta t$, and $k>0$, the algorithm defined by (8.2.32) is called consistent; $k$ is called the order\n",
      "of accuracy or rate of convergence. Proposition. The generalized trapezoidal methods are consistent, and furthermore $k=1$ for all $\\alpha \\in[0,1]$, except\n",
      "$\\alpha=\\frac{1}{2}$, in which case $k=2$. Proof. Expand $d\\left(t_{n+1}\\right)$ and $d\\left(t_{n}\\right)$ about $t_{n+\\alpha}$ in finite Taylor expansions and\n",
      "use the model equation to eliminate $t$-derivatives of $d\\left(t_{n+\\alpha}\\right)$: $$ \\begin{aligned} & d\\left(t_{n+1}\\right)=\n",
      "d\\left(t_{n+\\alpha}\\right)+(1-\\alpha) \\Delta t \\dot{d}\\left(t_{n+\\alpha}\\right) \\\\ &+\\frac{((1-\\alpha) \\Delta t)^{2}}{2}\n",
      "\\ddot{d}\\left(t_{n+\\alpha}\\right)+\\frac{((1-\\alpha) \\Delta t)^{3}}{3!} \\stackrel{...}{d}\\left(t_{n+\\alpha}\\right)+O\\left(\\Delta t^{4}\\right) \\\\ &\n",
      "d\\left(t_{n}\\right)= d\\left(t_{n+\\alpha}\\right)+(-\\alpha \\Delta t) \\dot{d}\\left(t_{n+\\alpha}\\right)+\\frac{(-\\alpha \\Delta t)^{2}}{2}\n",
      "\\ddot{d}\\left(t_{n+\\alpha}\\right) \\\\ &+\\frac{(-\\alpha \\Delta t)^{3}}{3!} \\stackrel{...}{d}\\left(t_{n+\\alpha}\\right)+O\\left(\\Delta t^{4}\\right) \\\\ & \\Delta\n",
      "t\\left(1+\\alpha \\Delta t \\lambda^{h}\\right) \\tau\\left(t_{n}\\right) \\\\ &=\\left(1+\\alpha \\Delta t \\lambda^{h}\\right) d\\left(t_{n+1}\\right)-\\left(1-(1-\\alpha)\n",
      "\\Delta t \\lambda^{h}\\right) d\\left(t_{n}\\right)-\\Delta t F_{n+\\alpha} \\\\ &=\\left\\{\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)-\\left(1-(1-\\alpha) \\Delta t\n",
      "\\lambda^{h}\\right)\\right\\} d\\left(t_{n+\\alpha}\\right) \\\\ &+\\left\\{\\left(1+\\alpha \\Delta t \\lambda^{h}\\right)(1-\\alpha) \\Delta t-\\left(1-(1-\\alpha) \\Delta t\n",
      "\\lambda^{h}\\right)(-\\alpha \\Delta t)\\right\\} \\dot{d}\\left(t_{n+\\alpha}\\right) \\\\ &+\\left\\{\\left(1+\\alpha \\Delta t \\lambda^{h}\\right) \\frac{((1-\\alpha) \\Delta\n",
      "t)^{2}}{2}-\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right) \\frac{(-\\alpha \\Delta t)^{2}}{2}\\right\\} \\ddot{d}\\left(t_{n+\\alpha}\\right) \\\\ &+\\left\\{\\left(1+\\alpha\n",
      "\\Delta t \\lambda^{h}\\right) \\frac{((1-\\alpha) \\Delta t)^{3}}{3!}-\\left(1-(1-\\alpha) \\Delta t \\lambda^{h}\\right) \\frac{(-\\alpha \\Delta t)^{3}}{3!}\\right\\}\n",
      "\\stackrel{...}{d}\\left(t_{n+\\alpha}\\right) \\\\ &-\\Delta t\\left([\\alpha+(1-\\alpha)] F\\left(t_{n+\\alpha}\\right)+[\\alpha(1-\\alpha) \\Delta t+(1-\\alpha)(-\\alpha\n",
      "\\Delta t)] \\dot{F}\\left(t_{n+\\alpha}\\right)\\right. \\\\ &+\\left\\{\\alpha \\frac{((1-\\alpha) \\Delta t)^{2}}{2}+(1-\\alpha) \\frac{(-\\alpha \\Delta t)^{2}}{2}\\right\\}\n",
      "\\ddot{F}\\left(t_{n+\\alpha}\\right) \\\\ &\\left.+\\left\\{\\alpha \\frac{((1-\\alpha) \\Delta t)}{3!}+(1-\\alpha) \\frac{(-\\alpha \\Delta t)^{3}}{3!}\\right\\}\n",
      "\\stackrel{...}{F}\\left(t_{n+\\alpha}\\right)\\right)+O\\left(\\Delta t^{4}\\right) \\end{aligned} $$ Using $\\dot{d}+\\lambda^{h} d=F$ and time derivatives of same, it\n",
      "can be shown that $$ \\tau=(1-2 \\alpha) O\\left(\\Delta t^{1}\\right)+O\\left(\\Delta t^{2}\\right) $$ (Fill in the remaining details as an exercise.)\n",
      "\\subsubsection*{Remark} Thus the trapezoidal rule $\\left(\\alpha=\\frac{1}{2}\\right)$ is the only member of the family of methods that is second-order accurate.\n",
      "Theorem. Consider equations (8.2.32) and (8.2.33). Let $t_{n}$ be fixed ( $n$, and consequently $\\Delta t$, are allowed to vary), and assume the following\n",
      "conditions hold:\\\\ i. $|A| \\leq 1 \\quad$ (stability)\\\\ ii. $|\\tau(t)| \\leq c \\Delta t^{k}, \\quad t \\in[0, T], \\quad k>0 \\quad$ (consistency) Then\n",
      "$e\\left(t_{n}\\right) \\rightarrow 0$ as $\\Delta t \\rightarrow 0$.\\\\ Proof. Subtract (8.2.33) from (8.2.32): \\begin{equation*} e\\left(t_{n+1}\\right)=A\n",
      "e\\left(t_{n}\\right)-\\Delta t \\cdot \\tau\\left(t_{n}\\right) \\quad \\text { error equation } \\tag{8.2.34} \\end{equation*} Replace $e\\left(t_{n}\\right)$ on the\n",
      "right-hand side by the error equation for the previous step, i.e., $$ e\\left(t_{n}\\right)=A e\\left(t_{n-1}\\right)-\\Delta t \\cdot \\tau\\left(t_{n-1}\\right) $$ to\n",
      "obtain $$ e\\left(t_{n+1}\\right)=A^{2} e\\left(t_{n-1}\\right)-\\Delta t A \\tau\\left(t_{n-1}\\right)-\\Delta t \\cdot \\tau\\left(t_{n}\\right) $$ Now repeat this\n",
      "procedure to eliminate $e\\left(t_{n-1}\\right)$ from the right-hand side, viz., $$ e\\left(t_{n+1}\\right)=A^{3} e\\left(t_{n-2}\\right)-\\Delta t A^{2}\n",
      "\\tau\\left(t_{n-2}\\right)-\\Delta t A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9  # try different QAs\n",
    "\n",
    "print('Q:')\n",
    "print_wrapped(df.iloc[i,:]['question'])\n",
    "print('A:')\n",
    "print_wrapped(df.iloc[i,:]['answer'])\n",
    "print('\\nChunk used for Q generation:')\n",
    "print_wrapped(df.iloc[i,:]['question_chunk'])\n",
    "print('\\nRetrieved context:')\n",
    "for item in df.iloc[i,:]['context'].split('Additional context'):\n",
    "    print_wrapped(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

,question_chunk,context,coverage,question,answer
0,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify

 Additional context 1: Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate} \maketitle \section*{Formulation of Two and Three-dimensional Boundary-value Problems} \subsection*{2.1 INTRODUCTORY REMARKS} It makes no sense to attempt to ""solve"" a boundary-value problem without a precise knowledge of what the problem is. The truth of this statement seems self-evident. Unfortunately, attempts are often made to solve vaguely defined problems, creating considerable confusion and, sometimes, totally erroneous results. In this chapter we present precise statements of multidimensional boundary-value problems in classical linear heat conduction and elastostatics. The presentation is similar in many respects to that for the one-dimensional model problem of Chapter 1. In particular, we discuss strong and weak forms, their equivalence, corresponding Galerkin formulations, the definitions of element arrays, and pertinent data processing concepts. In multidimensions, the data

 Additional context 2: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of",80,What are the main constituents of the finite element method for solving boundary-value problems as described in the text?,"The main constituents of a finite element method for the solution of a boundary-value problem are:

i. The variational or weak statement of the problem; and

ii. The approximate solution of the variational equations through the use of ""finite element functions."""
1,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.",70,Explain the significance of the variational or weak statement of a boundary-value problem in the context of the finite element method. How does it differ from the strong form?,"The variational or weak statement of a boundary-value problem is significant in the context of the finite element method (FEM) because it provides a framework for obtaining approximate solutions to complex problems where exact solutions may not be feasible. The weak form allows for the incorporation of trial solutions and weighting functions, which are essential for the variational approach used in FEM.

In the weak formulation, the problem is expressed in terms of integrals involving trial solutions and weighting functions, leading to the variational equation. This formulation is particularly useful because it relaxes some of the requirements of the strong form, such as the need for solutions to satisfy all boundary conditions explicitly. Instead, the weak form requires that trial solutions satisfy essential boundary conditions (like $u(1) = g$) while allowing natural boundary conditions (like $-u_{, x}(0) = h$) to be satisfied implicitly through the variational equation.

The strong form, on the other hand, involves the differential equation and explicit boundary conditions that the solution must satisfy directly. For example, in the strong form (S), the equation $u_{,xx} + f = 0$ must hold on the domain $\Omega$, along with the boundary conditions $u(1) = g$ and $-u_{,x}(0) = h$. 

In summary, the weak form is advantageous for FEM because it allows for the approximation of solutions in a more flexible manner, accommodating a broader class of functions and making it easier to handle complex geometries and boundary conditions. This flexibility is crucial for the application of FEM to real-world problems where exact solutions are often unattainable."
2,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 1: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 2: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify",60,"Given the differential equation $u_{,xx} + f = 0$, what does the notation $u_{,xx}$ represent, and how is it related to the boundary conditions imposed on $u$?","The notation $u_{,xx}$ represents the second derivative of the function $u$ with respect to the variable $x$, which can be expressed mathematically as $u_{,xx} = \frac{d^2 u}{dx^2}$. This term is part of the differential equation $u_{,xx} + f = 0$, indicating how the function $u$ changes with respect to $x$.

The boundary conditions imposed on $u$ are related to the values and behavior of $u$ at specific points in the domain. In this context, the boundary conditions are given by:

1. \( u(1) = g \) (Equation 1.2.1), which requires that the function $u$ takes on the value $g$ at the endpoint $x=1$.
2. \( -u_{,x}(0) = h \) (Equation 1.2.2), which specifies that the derivative of $u$ (the slope) at the endpoint $x=0$ must equal $-h$.

These boundary conditions are essential for defining the solution to the boundary-value problem, as they constrain the possible forms of $u$ and ensure that the solution satisfies physical or geometrical requirements at the boundaries of the domain. The first condition is an essential boundary condition, while the second is a natural boundary condition, which is implied by the variational equation rather than explicitly stated."
3,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[

 Additional context 1: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 2: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.",75,Define the sets of trial solutions and weighting functions in the context of the weak formulation of a boundary-value problem. How do these sets relate to the boundary conditions specified?,"In the context of the weak formulation of a boundary-value problem, the sets of trial solutions and weighting functions are defined as follows:

1. **Trial Solutions ($\mathcal{S}$)**:
   The set of trial solutions consists of functions that satisfy the boundary condition at $x=1$. Specifically, it is defined as:
   $$
   \mathcal{S} = \left\{ u \mid u \in H^{1}, u(1) = g \right\}
   $$
   Here, $H^{1}$ denotes the space of functions whose derivatives are square-integrable, and $g$ is a given constant. Thus, the trial solutions must take on the value $g$ at the boundary point $x=1$. The other boundary condition, which involves the derivative of $u$, is not required in this definition.

2. **Weighting Functions ($\mathcal{U}$)**:
   The set of weighting functions consists of functions that satisfy the homogeneous counterpart of the boundary condition at $x=1$. It is defined as:
   $$
   \mathcal{U} = \{ w \mid w \in H^{1}, w(1) = 0 \}
   $$
   This means that the weighting functions must also have square-integrable derivatives, but they must equal zero at the boundary point $x=1$.

**Relation to Boundary Conditions**:
- The trial solutions are required to satisfy the boundary condition $u(1) = g$, which means they must take a specific value at that boundary.
- The weighting functions, on the other hand, must satisfy the condition $w(1) = 0$, which allows them to be used in the variational formulation without imposing additional constraints at that boundary.

Together, these sets facilitate the formulation of the weak problem, where the trial solutions are used to approximate the solution of the boundary-value problem, and the weighting functions are used to derive the variational equation."
4,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 1: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 2: formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[",65,"Derive the weak form of the boundary-value problem given the strong form $(S)$, and explain the significance of the variational equation in this context.","To derive the weak form of the boundary-value problem given the strong form $(S)$, we start with the strong form, which is stated as follows:

\[
\text{(S)} \quad \left\{ 
\begin{array}{l}
\text{Given } f:\overline{\boldsymbol{\Omega}} \to \mathbb{R} \text{ and constants } g \text{ and } h, \\
\text{find } u:\overline{\boldsymbol{\Omega}} \to \mathbb{R} \text{ such that:} \\
u_{,xx} + f = 0 \quad \text{on } \Omega \\
u(1) = g \\
-u_{,x}(0) = h 
\end{array} 
\right.
\]

### Step 1: Define Trial and Weighting Functions

We define two classes of functions:

1. **Trial Solutions ($\mathfrak{f}$)**: These are functions that satisfy the boundary condition $u(1) = g$ and have square-integrable derivatives. Formally, this is given by:
   \[
   \mathcal{S} = \left\{ u \mid u \in H^{1}, u(1) = g \right\}
   \]

2. **Weighting Functions ($\mathcal{U}$)**: These functions satisfy the homogeneous boundary condition $w(1) = 0$ and also have square-integrable derivatives. Formally, this is defined as:
   \[
   \mathcal{U} = \{ w \mid w \in H^{1}, w(1) = 0 \}
   \]

### Step 2: Formulate the Weak Form

The weak form of the boundary-value problem, denoted as $(W)$, is stated as follows:

\[
(\mathcal{W}) \quad \left\{ 
\begin{array}{l}
\text{Given } f, g, \text{ and } h, \text{ find } u \in \mathfrak{f}, \text{ such that for all } w \in \mathcal{U} \\
\int_{0}^{1} w_{,x} u_{,x} \, dx = \int_{0}^{1} w f \, dx + w(0) h 
\end{array} 
\right.
\]

### Step 3: Significance of the Variational Equation

The variational equation, given by:

\[
\int_{0}^{1} w_{,x} u_{,x} \, dx = \int_{0}^{1} w f \, dx + w(0) h
\]

is significant for several reasons:

1. **Generalization**: It allows for the formulation of approximate solutions to the boundary-value problem, particularly in complex situations where exact solutions may not be feasible.

2. **Weak Solutions**: The variational equation provides a framework for defining weak or generalized solutions, which are essential in the context of finite element methods. This is particularly useful when dealing with functions that may not be smooth or may not satisfy the strong form directly.

3. **Physical Interpretation**: In mechanics, this formulation is often associated with principles of virtual work, where the left-hand side represents the work done by internal forces and the right-hand side represents the work done by external forces.

4. **Equivalence**: The weak form is equivalent to the strong form under certain conditions, meaning that solutions to one form can be translated into solutions of the other, thus providing a robust framework for analysis.

In summary, the weak form $(W)$ serves as a crucial step in the finite element method, allowing for the approximation of solutions to boundary-value problems while accommodating a broader class of functions."
5,"\maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a","

 Additional context 0: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[",80,Discuss the relationship between the strong and weak forms of a boundary-value problem. What conditions must be satisfied for the solutions of both forms to be considered identical?,"The relationship between the strong and weak forms of a boundary-value problem is that they are equivalent; that is, a solution to one form is also a solution to the other. This equivalence is established under the assumption that all functions involved are smooth.

To summarize the conditions for the solutions of both forms to be considered identical:

1. **Smoothness of Functions**: It is assumed that all functions, including the trial solutions and weighting functions, are smooth. This allows for the application of certain mathematical operations without encountering technical difficulties.

2. **Boundary Conditions**: The strong form imposes specific boundary conditions on the function $u$, while the weak form incorporates these conditions in a variational framework. The weak form requires that trial solutions satisfy the boundary condition $u(1) = g$, and the weighting functions satisfy the homogeneous boundary condition $w(1) = 0$.

3. **Variational Equation**: The weak form is expressed through a variational equation that relates the trial solutions and weighting functions. The solutions must satisfy this equation for all weighting functions in the defined space.

4. **Identical Solutions**: The proposition states that if $u$ is a solution of the strong form (denoted as $(S)$), then $u$ is also a solution of the weak form (denoted as $(W)$), and vice versa. This means that the solutions obtained from either formulation will yield the same function $u$ under the specified conditions.

In conclusion, the strong and weak forms are interconnected through the smoothness of functions and the satisfaction of boundary conditions, leading to the conclusion that their solutions are identical when these conditions are met."
6,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify subsequent developments. Before going on, we introduce a few additional notations and terminologies. Let ]0, 1[ denote the unit interval without end points (i.e., the set of points $x$ such that $0<x<1).] 0,1[$ and $[0,1]$ are referred to as \textbf{\textit{open and closed unit intervals,}} respectively. To simplify subsequent writing and tie in with notation employed later on in multidimensional situations, we shall adopt the definitions \[ \boldsymbol{\Omega}=] 0,1[ \quad \text { (open) } \tag{1.1.3} \] \[ \overline{\boldsymbol{\Omega}}=[0,1] \quad \text { (closed) } \tag{1.1.4} \] See Fig. 1.1.1. \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-02} \vspace{0.5em} \textbf{Figure 1.1.1} \end{figure} At this point, considerations such as these may seem pedantic. Our purpose, however, is to develop a language for the precise articulation of boundary-value problems, which is necessary for good finite element work. \section*{1.2 STRONG, OR CLASSICAL, FORM OF THE PROBLEM} A boundary-value problem for (1.1.1) involves imposing \textbf{\textit{boundary conditions}} on the function $u$. There are a variety of possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}}",80,"What is the significance of the variational equation in the context of boundary-value problems, and how does it relate to the concepts of virtual work and virtual displacements?","The variational equation plays a crucial role in the context of boundary-value problems as it provides a weak formulation of the problem, which is essential for obtaining approximate solutions using methods like the finite element method. Specifically, the variational equation is expressed as:

$$
\int_{0}^{1} w_{, x} u_{, x} d x = \int_{0}^{1} w f d x + w(0) h
$$

This equation relates the trial solution $u$ to the test function $w$, where $w$ represents the virtual displacements. The significance of this equation lies in its ability to transform the original boundary-value problem into a form that can be more easily solved, especially when dealing with complex geometries or higher dimensions.

The concepts of virtual work and virtual displacements are closely tied to the variational equation. The variational equation is often referred to as the equation of virtual work, which stems from the principle of virtual work in mechanics. In this context, the virtual displacements $w$ are hypothetical displacements that do not necessarily occur but are used to derive the equations governing the system's behavior. The left-hand side of the variational equation represents the work done by the internal forces (related to the trial solution $u$), while the right-hand side accounts for the work done by external forces (represented by $f$ and the boundary condition $h$).

Thus, the variational equation encapsulates the balance of virtual work in the system, allowing for the derivation of solutions that satisfy both the governing differential equations and the associated boundary conditions. This connection between virtual work, virtual displacements, and the variational equation is fundamental in the formulation and solution of boundary-value problems in the finite element method."
7,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[",90,Explain the relationship between strong and weak formulations of boundary-value problems. How do the solutions of these formulations relate to each other?,"The relationship between strong and weak formulations of boundary-value problems is established through the equivalence of their solutions. 

1. **Definitions**:
   - The **strong formulation** (denoted as $(S)$) involves differential equations and boundary conditions that the solution must satisfy explicitly. For example, it requires that $u_{,xx} + f = 0$ on the domain $\Omega$, along with boundary conditions such as $u(1) = g$ and $-u_{,x}(0) = h$.
   - The **weak formulation** (denoted as $(W)$) is derived from the strong formulation by integrating the differential equation against a set of weighting functions (variations) and applying integration by parts. This leads to a variational equation that must hold for all functions in a specific function space.

2. **Equivalence of Solutions**:
   - The context provides a proposition that states:
     a. If $u$ is a solution of the strong formulation $(S)$, then it is also a solution of the weak formulation $(W)$.
     b. Conversely, if $u$ is a solution of the weak formulation $(W)$, then it is also a solution of the strong formulation $(S)$.
   - This means that the solutions of both formulations are identical, indicating that they represent the same physical problem.

3. **Proof Outline**:
   - The proof for part (a) involves showing that the integration by parts of the strong formulation leads to the variational equation of the weak formulation.
   - For part (b), it is shown that if the weak formulation holds, it implies the differential equation and boundary conditions of the strong formulation.

4. **Boundary Conditions**:
   - The weak formulation inherently satisfies natural boundary conditions, while the strong formulation requires essential boundary conditions to be explicitly satisfied by trial solutions.

In summary, the strong and weak formulations of boundary-value problems are equivalent in terms of their solutions, allowing for flexibility in solving complex problems through either formulation. The weak formulation is particularly useful in the context of finite element methods, where approximate solutions are sought."
8,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 1: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 2: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is",70,"In the context of the weak formulation, what are natural boundary conditions and essential boundary conditions? How do they differ in terms of their implications for the solutions of the variational equation?","Natural boundary conditions and essential boundary conditions are two types of boundary conditions that arise in the context of the weak formulation of boundary-value problems.

**Natural Boundary Conditions:**
- These are boundary conditions that are not explicitly stated in the weak formulation but are implied by the satisfaction of the variational equation. 
- In the provided context, the natural boundary condition is given by the equation $-u_{, x}(0) = h$, which relates the derivative of the solution at the boundary $x=0$ to a constant $h$.
- The fact that solutions of the variational equation satisfy natural boundary conditions is significant, especially in more complex situations.

**Essential Boundary Conditions:**
- These are boundary conditions that are explicitly required to be satisfied by the trial solutions in the weak formulation. 
- In the context provided, the essential boundary condition is $u(1) = g$, which requires that the solution takes on a specific value $g$ at the boundary $x=1$.

**Differences:**
- The key difference lies in how they are treated in the formulation: essential boundary conditions must be satisfied by the trial solutions, while natural boundary conditions emerge from the variational formulation itself and are satisfied as a consequence of the variational equation.
- Essential boundary conditions are directly imposed on the solution, whereas natural boundary conditions arise from the behavior of the solution at the boundaries as dictated by the variational principles.

In summary, essential boundary conditions are explicitly required for trial solutions, while natural boundary conditions are implied by the variational equation and are satisfied by the solutions of the weak formulation."
9,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 1: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 2: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.",85,Derive the variational equation from the strong form of the boundary-value problem using integration by parts. What assumptions must be made about the functions involved?,"To derive the variational equation from the strong form of the boundary-value problem, we start with the strong form (S):

\[
\text{(S)} \quad \left\{ 
\begin{array}{l}
u_{,xx} + f = 0 \quad \text{on } \Omega \\
u(1) = g \\
-u_{,x}(0) = h 
\end{array} 
\right.
\]

1. **Assumptions about Functions**: 
   - We assume that the functions involved are smooth, meaning they are continuously differentiable. Specifically, we require that the trial solution $u$ and the weighting function $w$ are in the Sobolev space $H^1$, which implies that their derivatives are square-integrable.

2. **Starting from the Strong Form**: 
   - We can rewrite the strong form of the boundary-value problem as:
   \[
   u_{,xx} + f = 0 \implies u_{,xx} = -f
   \]

3. **Multiplying by a Weighting Function**: 
   - Multiply both sides of the equation by a weighting function $w \in \mathcal{U}$ (where $w(1) = 0$):
   \[
   w(u_{,xx} + f) = 0
   \]

4. **Integrating Over the Domain**: 
   - Integrate over the domain $\Omega = [0, 1]$:
   \[
   \int_{0}^{1} w(u_{,xx} + f) \, dx = 0
   \]

5. **Applying Integration by Parts**: 
   - We apply integration by parts to the term involving $u_{,xx}$:
   \[
   \int_{0}^{1} w u_{,xx} \, dx = \left[ w u_{,x} \right]_{0}^{1} - \int_{0}^{1} w_{,x} u_{,x} \, dx
   \]
   - The boundary term $\left[ w u_{,x} \right]_{0}^{1}$ evaluates to:
     - At $x=1$: $w(1) u_{,x}(1) = 0 \cdot u_{,x}(1) = 0$ (since $w(1) = 0$).
     - At $x=0$: $w(0) u_{,x}(0) = w(0)(-h)$ (using the boundary condition $-u_{,x}(0) = h$).

6. **Substituting Back**: 
   - Thus, we have:
   \[
   \int_{0}^{1} w u_{,xx} \, dx = 0 - \int_{0}^{1} w_{,x} u_{,x} \, dx + w(0)(-h)
   \]
   - Therefore, the equation becomes:
   \[
   -\int_{0}^{1} w_{,x} u_{,x} \, dx + w(0)(-h) = -\int_{0}^{1} w f \, dx
   \]

7. **Rearranging**: 
   - Rearranging gives us the variational equation:
   \[
   \int_{0}^{1} w_{,x} u_{,x} \, dx = \int_{0}^{1} w f \, dx + w(0) h
   \]

Thus, the variational equation derived from the strong form is:

\[
\int_{0}^{1} w_{,x} u_{,x} \, dx = \int_{0}^{1} w f \, dx + w(0) h
\]

This is the variational equation (1.3.4) in the context provided."
10,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify",75,"What is the role of the fundamental lemma in the calculus of variations, and how does it facilitate the transition from weak formulations to differential equations and boundary conditions?","The fundamental lemma in the calculus of variations plays a crucial role in deducing the differential equations and boundary conditions implied by the weak formulation of a problem. It allows us to establish a connection between the weak form and the strong form of the problem.

Specifically, the fundamental lemma enables us to show that if a weak solution satisfies the variational equation, then it must also satisfy the corresponding differential equations and boundary conditions. This is achieved by selecting appropriate test functions (in the context of the variational formulation) and demonstrating that the integrals involved lead to the necessary conditions.

In the context provided, the proof of part (b) of the proposition illustrates this process. By assuming that $u$ is a weak solution, the variational equation is manipulated through integration by parts, leading to an expression that must hold for all test functions. This ultimately allows us to conclude that the weak solution satisfies the differential equation $u_{, x x} + f = 0$ on the domain and the boundary condition $-u_{, x}(0) = h$.

Thus, the fundamental lemma serves as a bridge that facilitates the transition from weak formulations to the corresponding differential equations and boundary conditions, ensuring that the weak and strong solutions are equivalent."
11,"form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$. Another result, which we shall not bother to verify but is in fact easily established, is that both $(S)$ and $(W)$ possess unique solutions. Thus, by (a) and (b), the strong and weak solutions are one and the same. Consequently, $(W)$ is equivalent to $(S)$. \subsection*{Formal Proof} \bfseries{a}. Since $u$ is assumed to be a solution of (S), we may write \begin{equation*} 0=-\int_{0}^{1} w(u_{, x x}+f) d x \tag{1.4.1} \end{equation*} for any $w \in \mathcal{U}$. Integrating (1.4.1) by parts results in \begin{equation*} 0=\int_{0}^{1} w_{, x} u_{, x} d x-\int_{0}^{1} w f d x-\left.w u_{, x}\right|_{0} ^{1} \tag{1.4.2} \end{equation*} Rearranging and making use of the fact that $-u_{. x}(0)=h$ and $w(1)=0$ results in \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.4.3} \end{equation*} Furthermore, since $u$ is a solution of $(S)$, it satisfies $u(1)=g$ and therefore is in $\mathfrak{f}$. Finally, since $u$ also satisfies (1.4.3) for all $w \in \mathcal{U}, u$ satisfies the definition of a weak solution given by ( $W$ ).\\ \\ b. Now $u$ is assumed to be a weak solution. Thus $u \in \mathcal{S}$; consequently $u(1)=g$, and $$ \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the","

 Additional context 0: we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We

 Additional context 1: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 2: for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary",80,Describe the Galerkin method for obtaining approximate solutions to boundary-value problems. What are the key steps involved in constructing finite-dimensional approximations of the function spaces?,"The Galerkin method for obtaining approximate solutions to boundary-value problems involves the following key steps:

1. **Construct Finite-Dimensional Approximations**: The first step is to create finite-dimensional approximations of the function spaces involved in the problem. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, where the superscript $h$ refers to a mesh or discretization of the domain $\Omega$, parameterized by a characteristic length scale $h$. 

2. **Subset Relationships**: The approximations are considered subsets of the original function spaces:
   $$
   f^{h} \subset f \quad \text{and} \quad \mathcal{U}^{h} \subset \mathcal{U}
   $$
   This means that if a function $u^{h}$ belongs to $f^{h}$, then it also belongs to $f$, and similarly for $w^{h}$ in $\mathcal{U}^{h}$.

3. **Boundary Conditions**: The collections $f^{h}$ and $\mathcal{U}^{h}$ must satisfy specific boundary conditions. For example, if $u^{h} \in f^{h}$, it must satisfy $u^{h}(1) = g$, and if $w^{h} \in \mathcal{U}^{h}$, it must satisfy $w^{h}(1) = 0$.

4. **Constructing Approximate Solutions**: For each member $v^{h} \in \mathcal{U}^{h}$, an approximate solution $u^{h}$ is constructed using the relation:
   $$
   u^{h} = v^{h} + g^{h}
   $$
   where $g^{h}$ is a given function that satisfies the essential boundary condition.

5. **Formulating the Variational Equation**: The next step is to write a variational equation in the form:
   $$
   a(w^{h}, u^{h}) = (w^{h}, f) + w^{h}(0) h
   $$
   where $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are symmetric bilinear forms defined as:
   $$
   a(w, u) = \int_{0}^{1} w_{, x} u_{, x} dx
   $$
   $$
   (w, f) = \int_{0}^{1} w f dx
   $$

6. **Solving the Variational Equation**: The variational equation is then solved to obtain the approximate (weak) solution $u^{h}$.

These steps outline the process of applying the Galerkin method to approximate solutions for boundary-value problems, emphasizing the construction of finite-dimensional function spaces and the formulation of the variational equation."
12,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin

 Additional context 1: for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary

 Additional context 2: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is",70,"What is the significance of the inhomogeneous boundary condition in the context of function spaces $f$ and $f^{h}$, and how does it affect the linearity property of these spaces?","The significance of the inhomogeneous boundary condition in the context of function spaces $f$ and $f^{h}$ is that it prevents these spaces from possessing the property of a linear space. Specifically, if $u_{1}$ and $u_{2}$ are members of the function space $f$, the sum $u_{1} + u_{2}$ does not belong to $f$. This is illustrated by the condition that at the boundary, $u_{1}(1) + u_{2}(1) = g + g = 2g$, which violates the definition of the space $f$, where functions must satisfy specific boundary conditions.

In contrast, the spaces $\mathcal{U}$ and $\mathcal{U}^{h}$ do possess the linearity property, meaning that if $w^{h}$ and $v^{h}$ are in these spaces, then any linear combination of them is also in the same space. Thus, while $\mathcal{U}$ and $\mathcal{U}^{h}$ are linear spaces, the inhomogeneous boundary condition leads to the conclusion that $f$ and $f^{h}$ do not share this property, making them non-linear spaces."
13,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary

 Additional context 1: condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin

 Additional context 2: (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and",80,Explain the Bubnov-Galerkin method and how it constructs the function $u^{h}$ from a member $v^{h}$ of the collection $\\mathcal{U}^{h}$. What role does the function $g^{h}$ play in this construction?,"The Bubnov-Galerkin method is a finite element method that constructs an approximate solution to a differential equation by utilizing a collection of functions. In this method, given a member $v^{h}$ from the collection $\mathcal{U}^{h}$, the function $u^{h}$ is constructed as follows:

$$
u^{h} = v^{h} + g^{h}
$$

Here, $g^{h}$ is a specific function that satisfies the essential boundary condition, which is defined such that:

$$
g^{h}(1) = g
$$

This means that $g^{h}$ is chosen to ensure that the boundary condition at the point $x=1$ is satisfied. The construction of $u^{h}$ ensures that it belongs to the function space $f^{h}$, which is defined to include all functions of the form $u^{h}$. 

The role of the function $g^{h}$ is crucial because it allows the construction of $u^{h}$ to meet the boundary conditions required by the problem. Specifically, it adjusts the value of $u^{h}$ at the boundary to be equal to $g$, while $v^{h}$ contributes to the solution in the interior of the domain. Thus, $g^{h}$ effectively captures the essential boundary condition, ensuring that the overall function $u^{h}$ adheres to the necessary constraints imposed by the problem."
14,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 1: condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin

 Additional context 2: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional",60,"Derive the variational equation in terms of $w^{h} \\in \\mathcal{W}^{h}$ and $u^{h} \\in \\delta^{h}$ as presented in the text, and explain the significance of this equation in defining an approximate solution.","To derive the variational equation in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$, we start from the context provided. The variational equation is expressed as:

$$
a\left(w^{h}, u^{h}\right) = \left(w^{h}, f\right) + w^{h}(0) h
$$

This equation is derived from the weak formulation of the boundary value problem, where $a(\cdot, \cdot)$ is a bilinear form defined as:

$$
a(w, u) = \int_{0}^{1} w_{, x} u_{, x} \, dx
$$

and $(w, f)$ is defined as:

$$
(w, f) = \int_{0}^{1} w f \, dx.
$$

In this context, $u^{h}$ is an approximate solution constructed from a member $v^{h} \in \mathcal{U}^{h}$ and a function $g^{h}$ that satisfies the essential boundary condition. Specifically, we have:

$$
u^{h} = v^{h} + g^{h}
$$

where $g^{h}$ is defined such that $g^{h}(1) = g$.

The significance of this variational equation lies in its role in defining an approximate (weak) solution $u^{h}$. By requiring that this equation holds for all test functions $w^{h} \in \mathcal{W}^{h}$, we ensure that the approximate solution $u^{h}$ satisfies the weak form of the original boundary value problem. This weak formulation is crucial because it allows for the inclusion of functions that may not be differentiable in the classical sense, thus broadening the types of problems that can be addressed using the finite element method.

In summary, the variational equation serves as a bridge between the weak formulation of the problem and the finite-dimensional approximation provided by the Galerkin method, facilitating the computation of approximate solutions to boundary value problems."
15,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of

 Additional context 1: condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin

 Additional context 2: we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is \begin{align*} a(u, v) & =a(v, u) \tag{1.4.11}\\ (u, v) & =(v, u) \tag{1.4.12} \end{align*} Bilinearity means linearity in each ""slot""; for example, \begin{align*} a\left(c_{1} u+c_{2} v, w\right) & =c_{1} a(u, w)+c_{2} a(v, w) \tag{1.4.13}\\ \left(c_{1} u+c_{2} v, w\right) & =c_{1}(u, w)+c_{2}(v, w) \tag{1.4.14} \end{align*} \\ Exercise 1. Use the definitions of $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ to verify the properties of symmetry and bilinearity.\\ The above notations are very concise; at the same time they capture essential mathematical features and thus are conducive to a mathematical understanding of variational and finite element methods. Diverse classes of physical problems can be written in essentially similar fashion to (1.4.10). Thus ideas developed and results obtained are seen at once to have very broad applicability. \section*{1.6 GALERKIN'S APPROXIMATION METHOD} We shall now describe a method of obtaining approximate solutions to boundary-value problems based upon weak formulations. Our introduction to this subject is somewhat of an abstract treatment. However, the meaning should be significantly reinforced by the remaining sections of the chapter. It may be worthwhile for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We",75,"Discuss the implications of the bilinearity of the function $a(\\cdot, \\cdot)$ in the context of the Galerkin method. How does this property facilitate the formulation of the Galerkin equation?","The bilinearity of the function $a(\cdot, \cdot)$ plays a crucial role in the formulation of the Galerkin method. Bilinearity means that the function $a(u, v)$ is linear in each of its arguments separately. This property can be expressed mathematically as follows:

1. For any constants $c_1$ and $c_2$, and functions $u$, $v$, and $w$, the bilinearity property states:
   $$
   a(c_1 u + c_2 v, w) = c_1 a(u, w) + c_2 a(v, w)
   $$
   and
   $$
   a(w, c_1 u + c_2 v) = c_1 a(w, u) + c_2 a(w, v).
   $$

In the context of the Galerkin method, this bilinear property allows for the substitution of approximations into the variational equation. Specifically, when we substitute the expression for $u^h$ (which is defined as $u^h = v^h + g^h$) into the variational equation, we can utilize the bilinearity of $a(\cdot, \cdot)$ to separate the terms involving $v^h$ and $g^h$. This leads to the formulation of the Galerkin equation as follows:

$$
a(w^h, v^h) = (w^h, f) + w^h(0) h - a(w^h, g^h).
$$

Here, $w^h$ is any test function from the collection $\mathcal{U}^h$, and $f$ represents the given data. The right-hand side consists of known terms, while the left-hand side involves the unknown function $v^h$. 

The bilinearity of $a(\cdot, \cdot)$ thus facilitates the linearization of the problem, allowing us to express the Galerkin equation in a form that can be solved for the unknown coefficients associated with the basis functions. This leads to a system of linear algebraic equations that can be solved to find the approximate solution to the boundary-value problem.

In summary, the bilinearity of the function $a(\cdot, \cdot)$ is essential for the formulation of the Galerkin equation, as it enables the separation of terms and the construction of a solvable system of equations, thereby facilitating the application of the Galerkin method in finite element analysis."
16,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This

 Additional context 1: from which it follows that $c_{p}=0$; hence $\boldsymbol{c}=\mathbf{0}$. \section*{Remark} Positive definiteness of $\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\mathcal{V}^{h}$. \subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\boldsymbol{k}^{e}$ and $f^{\boldsymbol{c}}$ : \begin{align*} & \boldsymbol{k}^{e}=\left[k_{p q}^{e}\right], \quad \boldsymbol{f}^{e}=\left\{f_{p}^{e}\right\}, \quad 1 \leq p, q \leq n_{ee}=n_{ed} n_{e n} \tag{2.9.1}\footnotemark\\ & k_{p q}^{e}=e_{i}^{T} \int_{\Omega e} B_{a}^{T} D B_{b} d \Omega e_{j}, \quad p=n_{e d}(a-1)+i, \\ & q=n_{e d}(b-1)+j \tag{2.9.2}\\ & \left(n_{s d}=2\right) \quad B_{a}=\left[\begin{array}{cc} N_{a, 1} & 0 \\ 0 & N_{a, 2} \\ N_{a, 2} & N_{a, 1} \end{array}\right] \tag{2.9.3}\\ & \left(n_{s d}=3\right) \quad \boldsymbol{B}_{a}=\left[\begin{array}{ccc} N_{a, 1} & 0 & 0 \\ 0 & N_{a, 2} & 0 \\ 0 & 0 & N_{a, 3} \\ 0 & N_{a, 3} & N_{a, 2} \\ N_{a, 3} & 0 & N_{a, 1} \\ N_{a, 2} & N_{a, 1} & 0 \end{array}\right] \tag{2.9.4} \end{align*} \footnotetext{$n_{\text{ee}}$ stands for the number of element equations and $n_{\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\text {ed}} \leq n_{\text{dof}}$, although they are usually equal. } and \[ f_{p}^{e}=\int_{\Omega^{e}} N_{a} \ell_{i} d \Omega+\int_{\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \Gamma-\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \quad \begin{array}{r} \Gamma_{h_{i}}^{e}=\Gamma_{h_{i}} \cap \Gamma^{e} \tag{2.9.5}\\ (\text { no sum on } i) \end{array} \] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\left(x_{b}^{e}\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \begin{equation*} \underbrace{\boldsymbol{k}_{ab}^{e}}_{n_{e d} \times n_{e d}}=\int_{\boldsymbol{\Omega}^{e}} B_{a}^{T} \boldsymbol{D} B_{b} d \boldsymbol{\Omega} \tag{2.9.6a} \end{equation*} From (2.9.2) we see that

 Additional context 2: $\boldsymbol{\ell}=$ constant.\\ ii. $\ell=\delta(x-\bar{x})$, the delta function, where $x_{1}^{e} \leq \bar{x} \leq x_{2}^{e}$. Specialize for the cases $\bar{x}=x_{b}^{e}$ and $\bar{x}=\left(x_{1}^{e}+x_{2}^{e}\right) / 2$. Solution $$ \begin{aligned} & \text { i. } f_{a}^{e}=\ell \int_{x_{1}^{e}}^{x^ e_{2}} N_{a}(x) d x=\frac{\ell h^{e}}{2} \underbrace{\int_{-1}^{+1} N_{a}(\xi) d \xi}_{1} \\ & f^{e}=\frac{\ell h^{e}}{2}\left\{\begin{array}{l} 1 \\ 1 \end{array}\right\} \end{aligned} $$ ii. $f_{a}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{a}(x) \delta(x-\bar{x}) d x=N_{a}(\bar{x})$ For $x=x_{b}^{e}$, $$ \begin{aligned} f_{a}^{e} & =N_{a}(\bar{x})=N_{a}\left(x_{b}^{e}\right)=\delta_{a b} \quad \text { (Kronecker delta) } \\ f^{e} & =\left\{\begin{array}{l} \delta_{1 b} \\ \delta_{2 b} \end{array}\right\} \end{aligned} $$ For $\bar{x}=\left(x_{1}^{e}+x_{2}^{e}\right) / 2$, $$ f_{a}^{e}=N_{a}(\bar{x})=N_{a}\left(\frac{x_{1}^{e}+x_{2}^{e}}{2}\right)=\frac{1}{2} $$ Therefore, $$ f^{e}=\frac{1}{2}\left\{\begin{array}{l} 1 \\ 1 \end{array}\right\} $$ \subsection*{Exercise 4.} Consider the boundary-value problem for classical linear elastostatics discussed previously. In the linearized theory of small displacements superposed upon large, the stiffness term in the variational equation, $$ \int_{\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \Omega $$ is replaced by $$ \int_{\Omega} w_{i, j} d_{i j k l} u_{k, l} d \Omega $$ where $$ \begin{aligned} d_{i j k l} & =c_{i j k l}+\delta_{i k} \sigma_{jl}^{0} \\ \sigma_{j l}^{0} & =\sigma_{lj}^{0} \end{aligned} $$ and the $\sigma_{j l}^{0}$ 's (i.e., initial stresses) are given functions of $\boldsymbol{x} \in \boldsymbol{\Omega}$. It follows from the symmetries of $c_{i j k l}$ and $\sigma_{jl}^{0}$ that $$ d_{i j k l}=d_{k li j} $$ Assume $\boldsymbol{n}_{s d}=2$. An index-free formulation of the stiffness term is given by $$ \int_{\Omega}\left\{\begin{array}{c} w_{1,1} \\ w_{2,2} \\ w_{1,2}+w_{2,1} \\ w_{1,2}-w_{2,1} \end{array}\right\}^{T} \quad \underbrace{D}_{4 \times 4}\left\{\begin{array}{c} u_{1,1} \\ u_{2,2} \\ u_{1,2}+u_{2,1} \\ u_{1,2}-u_{2,1} \end{array}\right\} d \Omega $$ which leads to the following definition of the element stiffness matrix: $$ k_{p q}^{e}=e_{i}^{T} \int_{\boldsymbol{\Omega}^{e}} \underbrace{B_{a}^{T}}_{2 \times",85,"Given the definitions of the stiffness matrix $K$, force vector $F$, and displacement vector $d$, explain how the matrix equation $K d = F$ is derived from the Galerkin method. What does each term represent in a physical context?","To derive the matrix equation $K d = F$ from the Galerkin method, we start with the Galerkin equation given in the context:

$$
\sum_{B=1}^{n} K_{A B} d_{B} = F_{A}, \quad A=1,2, \ldots, n
$$

This equation arises from substituting the approximations for the trial and test functions into the variational formulation of the problem. The terms in this equation are defined as follows:

1. **Stiffness Matrix $K$**:
   - The stiffness matrix is defined as:
   $$
   K_{A B} = a(N_{A}, N_{B})
   $$
   where $a(\cdot, \cdot)$ is a bilinear form that typically represents the relationship between the strain and stress in the system. In physical terms, the stiffness matrix quantifies how much the system resists deformation under applied loads. It encapsulates the material properties and geometry of the structure being analyzed.

2. **Force Vector $F$**:
   - The force vector is defined as:
   $$
   F_{A} = \left(N_{A}, f\right) + N_{A}(0) h - a(N_{A}, N_{n+1}) g
   $$
   Here, $f$ represents the external forces applied to the system, $h$ is a term related to boundary conditions, and $g$ is a function that satisfies the essential boundary condition. Physically, the force vector represents the total effect of external forces and boundary conditions acting on the system.

3. **Displacement Vector $d$**:
   - The displacement vector is defined as:
   $$
   d = \{d_{B}\} = \{d_{1}, d_{2}, \ldots, d_{n}\}
   $$
   This vector contains the unknown displacements at the nodes of the finite element mesh. In a physical context, the displacement vector represents how much each node of the structure moves in response to the applied forces.

By organizing these terms into matrix notation, we can express the system of equations as:

$$
\boldsymbol{K} \boldsymbol{d} = \boldsymbol{F}
$$

This matrix equation succinctly captures the relationship between the stiffness of the system, the displacements of the nodes, and the external forces acting on the system. The solution of this equation provides the approximate displacements, which can then be used to evaluate the approximate solution of the original differential equation governing the physical problem."
17,"the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary condition, i.e., \begin{equation*} g^{h}(1)=g \tag{1.5.6} \end{equation*} Note that (1.5.5) satisfies the requisite boundary condition also: \begin{align*} u^{h}(1) & =v^{h}(1)+g^{h}(1) \tag{1.5.7}\\ & =0+g \end{align*} Thus (1.5.5) constitutes a definition of $f^{h}$; that is, $f^{h}$ is all functions of the form (1.5.5). The key point to observe is that, up to the function $g^{h}, f^{h}$ and $\mathcal{U}^{h}$ are composed of identical collections of functions. This property will be shown later on to have significant consequences for certain classes of problems. We now write a variational equation, of the form of (1.4.10), in terms of $w^{h} \in \mathcal{W}^{h}$ and $u^{h} \in \delta^{h}$ \begin{equation*} a\left(w^{h}, u^{h}\right)=\left(w^{h}, A\right)+w^{h}(0) h \tag{1.5.8} \end{equation*} This equation is to be thought of as defining an approximate (weak) solution, $u^{\boldsymbol{h}}$. \footnotetext{${ }^{2}$ This condition may be considered standard. However, it is often violated in practice. Strang [2] coined the terminology ""variational crimes"" to apply to this, and other, situations in which the classical rules of variational methods are violated. Many ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of $\mathcal{U}^{h}$. Let $\mathcal{U}^{h}$ consist of all linear combinations of given functions denoted by $N_{A}: \bar{\Omega} \rightarrow \mathbb{R}$, where $A=1,2, \ldots, n$. By this we mean that if $w^{h} \in \mathcal{U}^{h}$, then there exist constants $c_{A}, A=1,2, \ldots, n$, such that \begin{align*} w^{h} & =\sum_{A=1}^{n} c_{A} N_{A} \\ & =c_{1} N_{1}+c_{2} N_{2}+\cdots+c_{n} N_{n} \tag{1.6.1} \end{align*} The $N_{A}$ 's are referred to as shape, basis, or interpolation functions. We require that each $N_{A}$ satisfies \begin{equation*} N_{A}(1)=0, \quad A=1,2, \ldots, n \tag{1.6.2} \end{equation*} from which it follows by (1.6.1) that $w^{h}(1)=0$, as is necessary. $W^{h}$ is said to have dimension $n$, for obvious reasons. To define members of $\delta^{h}$ we need to specify $g^{h}$. To this end, we introduce another shape function, $N_{n+1}: \bar{\Omega} \rightarrow \mathbb{R}$, which has the property \begin{equation*} N_{n+1}(1)=1 \tag{1.6.3} \end{equation*} (Note $N_{n+1} \notin \mathcal{U}^{h}$. ) Then $g^{h}$ is given by \begin{equation*} g^{h}=g N_{n+1} \tag{1.6.4} \end{equation*} and thus \begin{equation*} g^{h}(1)=g \tag{1.6.5} \end{equation*} With these definitions, a typical $u^{h} \in f^{h}$ may be written as \begin{align*} u^{h} & =v^{h}+g^{h} \\ & =\sum_{A=1}^{n} d_{A} N_{A}+g N_{n+1} \tag{1.6.6} \end{align*} where the $d_{A}$ 's are constants and from which it is apparent that $u^{h}(1)=g$.\\ Substitution of (1.6.1) and (1.6.6) into the Galerkin equation yields \begin{align*} a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} d_{B} N_{B}\right)=\left(\sum_{A=1}^{n} c_{A} N_{A}, f\right)+ & {\left[\sum_{A=1}^{n} c_{A} N_{A}(0)\right] h } \\ & -a\left(\sum_{A=1}^{n} c_{A} N_{A}, g N_{n+1}\right) \tag{1.6.7} \end{align*} By using the bilinearity of $a(\cdot, \cdot)$ and $(\cdot, \cdot),(1.6 .7)$ becomes \begin{equation*} 0=\sum_{A=1}^{n} c_{A} G_{A} \tag{1.6.8} \end{equation*} where \begin{equation*} G_{A}=\sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}-\left(N_{A}, f\right)-N_{A}(0) h+a\left(N_{A}, N_{n+1}\right) q \tag{1.6.9} \end{equation*} Now the Galerkin equation is to hold for all $w^{h} \in \mathcal{U}^{h}$. By (1.6.1), this means for all $c_{A}$'s, $A=1,2, \ldots, n$. Since the $c_{A}$'s are arbitrary in (1.6.8), it necessarily follows that each $G_{A}, A=1,2, \ldots, n$, must be identically zero, i.e., from (1.6.9) \begin{equation*} \sum_{B=1}^{n} a\left(N_{A}, N_{B}\right) d_{B}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.10} \end{equation*} Note that everything is known in (1.6.10) except the $d_{B}$ 's. Thus (1.6.10) constitutes a system of $n$ equations in $n$ unknowns. This can be written in a more concise form as follows: Let \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \tag{1.6.11}\\ F_{A} & =\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{n+1}\right) g \tag{1.6.12} \end{align*} Then (1.6.10) becomes \begin{equation*} \sum_{B=1}^{n} K_{A B} d_{B}=F_{A}, \quad A=1,2, \ldots, n \tag{1.6.13} \end{equation*} Further simplicity is gained by adopting a matrix notation. Let \begin{align*} & \boldsymbol{K}=\left[K_{A B}\right]=\left[\begin{array}{cccc} K_{11} & \boldsymbol{K}_{12} & \cdots & K_{1 n} \\ \boldsymbol{K}_{21} & K_{22} & \cdots & K_{2 n} \\ \vdots & \vdots & & \vdots \\ K_{n 1} & K_{n 2} & \cdots & K_{n n} \end{array}\right] \tag{1.6.14}\\ & \boldsymbol{F}=\left\{F_{A}\right\}=\left\{\begin{array}{c} F_{1} \\ F_{2} \\ \vdots \\ F_{n} \end{array}\right\} \tag{1.6.15} \end{align*} and \[ d=\left\{d_{B}\right\}=\left\{\begin{array}{c} d_{1} \tag{1.6.16}\\ d_{2} \\ \vdots \\ d_{n} \end{array}\right\} \] Now (1.6.13) may be written as \begin{equation*} \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F} \tag{1.6.17} \end{equation*} The following terminologies are frequently applied, especially when the problem under consideration pertains to a mechanical system: $$ \begin{aligned} \boldsymbol{K} & =\text { stiffness matrix } \\ \boldsymbol{F} & =\text { force vector } \\ \boldsymbol{d} & =\text { displacement vector } \end{aligned} $$ A variety of physical interpretations are of course possible. At this point, we may state the matrix equivalent, $(M)$, of the Galerkin problem.\\ (M) $\left\{\begin{array}{c}\text { Given the coefficient matrix } \boldsymbol{K} \text { and vector } \boldsymbol{F}, \text { find } \boldsymbol{d} \text { such that } \\ \boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}\end{array}\right.$ The solution of $(M)$ is, of course, just $d=K^{-1} \boldsymbol{F}$ (assuming the inverse of $\boldsymbol{K}$, $K^{-1}$, exists). Once $d$ is known, the solution of ( $G$ ) may be obtained at any point $x \in \bar{\Omega}$ by employing (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding","

 Additional context 0: of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of

 Additional context 1: (1.6.6), viz., \begin{equation*} u^{h}(x)=\sum_{A=1}^{n} d_{A} N_{A}(x)+g N_{n+1}(x) \tag{1.6.18} \end{equation*} Likewise, derivatives of $u^{h}$, if required, may be obtained by term-by-term differentiation. It should be emphasized, that the solution of $(G)$ is an approximate solution of ( $W$ ). Consequently, the differential equation and natural boundary condition are only approximately satisfied. The quality of the approximation depends upon the specific choice of $N_{A}$ 's and the number $n$. \section*{Remarks} \begin{enumerate} \item The matrix $K$ is symmetric. This follows from the symmetry of $a(\cdot, \cdot)$ and use of Galerkin's method (i.e., the same shape functions are used for the variations and trial solutions): \end{enumerate} \begin{align*} K_{A B} & =a\left(N_{A}, N_{B}\right) \\ & =a\left(N_{B}, N_{A}\right) \\ & =K_{B A} \tag{1.6.19} \end{align*} In matrix notation \begin{equation*} K=K^{\boldsymbol{T}} \tag{1.6.20} \end{equation*} where the superscript $\boldsymbol{T}$ denotes transpose. The symmetry of $\boldsymbol{K}$ has important computational consequences.\\ \\ 2. Let us schematically retrace the steps leading to the matrix problem, as they are typical of the process one must go through in developing a finite element method for any given problem: \begin{equation*} (S) \Leftrightarrow(W) \approx(G) \Leftrightarrow(M) \tag{1.6.21} \end{equation*} The only apparent approximation made thus far is in approximately solving ( $W$ ) via $(G)$. In more complicated situations, encountered in practice, the number of approximations increases. For example, the data $f, g$, and $h$ may be approximated, as well as the domain $\Omega$, calculation of integrals, and so on. Convergence proofs and error analyses involve consideration of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and

 Additional context 2: zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can",90,"In the context of the example provided for 1 degree of freedom, derive the expressions for $K_{11}$ and $F_{1}$, and explain how these relate to the overall solution $u^{h}(x)$. What does this example illustrate about the finite element method?","To derive the expressions for $K_{11}$ and $F_{1}$ in the context of the example for 1 degree of freedom, we start with the definitions provided in the context.

1. **Expression for $K_{11}$**:
   The stiffness matrix entry $K_{11}$ is defined as:
   $$
   K_{11} = a(N_{1}, N_{1}) = \int_{0}^{1} N_{1,x} N_{1,x} \, dx
   $$
   Given that $N_{1}(x) = 1 - x$, we first compute its derivative:
   $$
   N_{1,x} = -1
   $$
   Therefore, we have:
   $$
   K_{11} = \int_{0}^{1} (-1)(-1) \, dx = \int_{0}^{1} 1 \, dx = 1
   $$

2. **Expression for $F_{1}$**:
   The force vector entry $F_{1}$ is defined as:
   $$
   F_{1} = \left(N_{1}, f\right) + N_{1}(0) h - a(N_{1}, N_{2}) q
   $$
   We need to compute each term:
   - The first term:
   $$
   \left(N_{1}, f\right) = \int_{0}^{1} (1 - x) f(x) \, dx
   $$
   - The second term:
   $$
   N_{1}(0) = 1 \quad \text{(since $N_{1}(0) = 1 - 0 = 1$)}
   $$
   Thus, the second term contributes $h$.
   - The third term involves $N_{2}(x) = x$:
   $$
   a(N_{1}, N_{2}) = \int_{0}^{1} N_{1,x} N_{2,x} \, dx = \int_{0}^{1} (-1)(1) \, dx = -1
   $$
   Therefore, the third term contributes $-(-1)q = q$.

   Combining these, we get:
   $$
   F_{1} = \int_{0}^{1} (1 - x) f(x) \, dx + h + q
   $$

3. **Relation to the Overall Solution $u^{h}(x)$**:
   The overall solution $u^{h}(x)$ is given by:
   $$
   u^{h}(x) = d_{1} N_{1}(x) + g N_{2}(x)
   $$
   where $d_{1} = K_{11}^{-1} F_{1} = F_{1}$ (since $K_{11} = 1$). Thus, we can express $d_{1}$ as:
   $$
   d_{1} = \int_{0}^{1} (1 - y) f(y) \, dy + h + g
   $$
   Substituting this back into the expression for $u^{h}(x)$ gives:
   $$
   u^{h}(x) = \left[\int_{0}^{1} (1 - y) f(y) \, dy + h + g\right](1 - x) + g x
   $$

4. **Illustration of the Finite Element Method**:
   This example illustrates the finite element method by demonstrating how a complex problem can be approximated using simple shape functions. The method involves breaking down the problem into smaller, manageable parts (elements) and using piecewise linear functions to approximate the solution. The derivation of $K_{11}$ and $F_{1}$ shows how the stiffness matrix and force vector are constructed from the shape functions and the problem's physical parameters, leading to a system of equations that can be solved to obtain an approximate solution. This highlights the flexibility and applicability of the finite element method in solving various engineering and physical problems."
18,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 1: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 2: ""variational crimes"" have been given a rigorous mathematical basis (e.g., see [2]). We shall have more to say about this subject in subsequent chapters. }Substitution of (1.5.5) into (1.5.8), and the bilinearity of $a(\cdot, \cdot)$ enables us to write \begin{equation*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \tag{1.5.9} \end{equation*} The right-hand side consists of the totality of terms associated with given data (i.e., $f, q$, and $h$ ). Equation (1.5.9) is to be used to define $v^{h}$, the unknown part of $u^{h}$. The (Bubnov-) Galerkin form of the problem, denoted by ( $G$ ), is stated as follows:\\ \[ \text{(G)} \quad \left\{ \parbox{0.8\textwidth}{ \text { Given } $f, q$, \text { and } $h$, \text {, as before, find } $u^{h}=v^{h}+q^{h}$ \text {, where } $v^{h} \in \mathcal{U}^{h}$ \text { such that for all } $w^{h} \in \mathcal{U}^{h}$ \\ \begin{align*} a\left(w^{h}, v^{h}\right)=\left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, g^{h}\right) \end{align*} } \right. \] Note that $(G)$ is just a version of $(W)$ posed in terms of a finite-dimensional collection of functions, namely, $\mathcal{U}^\text{h}$. To make matters more specific, $g^{h}$ and $\mathcal{U}^{h}$ have to be explicitly defined. Before doing this, it is worthwhile to mention a larger class of approximation methods, called \textit{\textbf{Petrov-Galerkin methods}}, in which $v^{h}$ is contained in a collection of functions other than $\mathcal{U^\text{h}}$. Recent attention has been paid to methods of this type, especially in the context of fluid mechanics. For the time being, we will be exclusively concerned with the Bubnov-Galerkin method. The Bubnov-Galerkin method is commonly referred to as simply the Galerkin method, terminology we shall adopt henceforth. Equation (1.5.9) is sometimes referred to as the \textit{\textbf{Galerkin equation.}} Approximation methods of the type considered are examples of so-called \textit{\textbf{weighted residual methods}}. The standard reference dealing with this subject is Finlayson [3]. For a more succinct presentation containing an interesting historical account, see Finlayson and Scriven [4]. \section*{1.6 MATRIX EQUATIONS; STIFFNESS MATRIX K} The Galerkin method leads to a coupled system of linear algebraic equations. To see this we need to give further structure to the definition of",80,"What is the significance of the Galerkin method in the context of finite element analysis, and how does it relate to the formulation of the finite element problem?","The significance of the Galerkin method in the context of finite element analysis lies in its ability to provide a systematic approach for approximating solutions to boundary-value problems. It does this by transforming the problem into a finite-dimensional space, where the solution is expressed as a linear combination of basis functions that have local support and piecewise smoothness.

In the formulation of the finite element problem, the Galerkin method involves the following key aspects:

1. **Weak Formulation**: The method starts with the weak statement of the problem, which is derived from the original differential equation. This weak formulation allows for the inclusion of functions that may not be differentiable in the classical sense.

2. **Trial and Test Functions**: The Galerkin method uses trial functions (the approximate solution) and test functions (the weighting functions) from the same finite-dimensional space, denoted as $\mathcal{U}^{h}$. This ensures that the method is consistent and converges to the exact solution as the mesh is refined.

3. **Matrix Formulation**: The application of the Galerkin method leads to a coupled system of linear algebraic equations, represented in matrix form. The stiffness matrix $K$ and the force vector $F$ are constructed based on the inner products of the shape functions and the given data.

4. **Local Support**: The basis functions used in the Galerkin method have local support, meaning they are non-zero only in a neighborhood of their associated nodes. This property simplifies the computations and allows for the assembly of the global system from local element contributions.

5. **Exact Representation of Homogeneous Solutions**: The method guarantees that the homogeneous part of the solution is exactly represented, which is crucial for the accuracy of the approximation.

Overall, the Galerkin method serves as a foundational technique in finite element analysis, enabling the formulation and solution of complex engineering problems by approximating the behavior of physical systems through discretization."
19,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized

 Additional context 1: of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of

 Additional context 2: in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ &",90,"In the example with 1 degree of freedom, how are the shape functions $N_1(x)$ and $N_2(x)$ defined, and what conditions must they satisfy?","In the example with 1 degree of freedom, the shape functions are defined as follows:

- \( N_{1}(x) = 1 - x \)
- \( N_{2}(x) = x \)

These shape functions must satisfy the following conditions:

- \( N_{1}(1) = 0 \)
- \( N_{2}(1) = 1 \) 

These conditions ensure that the shape functions are appropriately defined for the finite element formulation."
20,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A},

 Additional context 1: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[

 Additional context 2: element stiffness, $\boldsymbol{k}^{e}$, and element force vector, $\boldsymbol{f}^{\boldsymbol{e}}$, may be deduced from these equations: \begin{align*} & k^{e}=\left[k_{a b}^{e}\right], \quad f^{e}=\left\{f_{a}^{e}\right\}, \quad 1 \leq a, b \leq n_{e n} \tag{2.5.6}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}}\left(\nabla N_{a}\right)^{T} \kappa\left(\nabla N_{b}\right) d \Omega \tag{2.5.7}\\ & f_{a}^{e}=\int_{\boldsymbol{\Omega}^{e}} N_{a} \ell d \Omega+\int_{\Gamma_{h}^{e}} N_{a} h d \Gamma-\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \tag{2.5.8} \end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\left(x_{b}^{e}\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all' $x \in \bar{\Omega}^{\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be formed from the element arrays $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \begin{equation*} k^{e}=\int_{\boldsymbol{\Omega}^{e}} B^{T} D B d \Omega \tag{2.5.9} \end{equation*} where, in the present case, \begin{align*} & \underbrace{\boldsymbol{D}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{s d}}=\boldsymbol{\kappa} \tag{2.5.10}\\ & \underbrace{\boldsymbol{B}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{en}}=\left[\boldsymbol{B}_{1}, \boldsymbol{B}_{2}, \ldots, \boldsymbol{B}_{n_{e n}}\right] \tag{2.5.11}\\ & \underbrace{B_{a}}_{n_{s d} \times 1}=\nabla N_{a} \tag{2.5.12} \end{align*} The component version of $(2.5 .9)$ is \begin{equation*} k_{a b}^{e}=\int_{\Omega^{e}} B_{a}^{T} D B_{b} d \Omega \tag{2.5.13} \end{equation*} \subsection*{Exercise 1.} Let\\ \[ \underbrace{d^{e}}_{n_{e n} \times 1}=\left\{d_{a}^{e}\right\}=\left\{\begin{array}{c} d_{1}^{e} \tag{2.5.14}\\ d_{2}^{e} \\ \vdots \\ d_{n_{e n}^{e}}^{e} \end{array}\right\} \] where \begin{equation*}",70,Derive the expression for the stiffness matrix $K_{11}$ given the shape function $N_1(x) = 1 - x$ and its derivative. What does this matrix represent in the context of the finite element method?,"To derive the expression for the stiffness matrix $K_{11}$ given the shape function $N_1(x) = 1 - x$, we start with the definition of the stiffness matrix entry:

$$
K_{11} = a(N_1, N_1) = \int_{0}^{1} N_{1,x} N_{1,x} \, dx
$$

First, we need to compute the derivative of the shape function $N_1(x)$:

$$
N_{1,x} = \frac{d}{dx}(1 - x) = -1
$$

Now, substituting this into the expression for $K_{11}$:

$$
K_{11} = \int_{0}^{1} (-1)(-1) \, dx = \int_{0}^{1} 1 \, dx
$$

Calculating the integral:

$$
K_{11} = [x]_{0}^{1} = 1 - 0 = 1
$$

Thus, we have:

$$
K_{11} = 1
$$

In the context of the finite element method, the stiffness matrix entry $K_{11}$ represents the contribution of the shape function $N_1$ to the overall stiffness of the system. Specifically, it quantifies how the displacement associated with the shape function $N_1$ affects the internal forces in the element, reflecting the relationship between the nodal displacements and the resulting forces due to the stiffness of the material."
21,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized

 Additional context 1: of each approximation.\\ 3. It is sometimes convenient to write \begin{equation*} u^{h}(x)=\sum_{A=1}^{n+1} N_{A}(x) d_{A} \tag{1.6.22} \end{equation*} where $d_{n+1}=g$. \section*{1.7 EXAMPLES: 1 AND 2 DECREES OF FREEDOM} In this section we will carry out the detailed calculations involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of

 Additional context 2: in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ &",85,"In Example 2, how do the shape functions for 2 degrees of freedom differ from those in the 1 degree of freedom example, and what implications does this have for the formulation of the finite element equations?","In Example 2, the shape functions for 2 degrees of freedom are defined as follows:

- For $N_1(x)$:
$$
N_{1}(x)=\left\{
\begin{array}{cc}
1-2 x & 0 \leq x \leq \frac{1}{2} \\
0 & \frac{1}{2} \leq x \leq 1
\end{array}
\right.
$$

- For $N_2(x)$:
$$
N_{2}(x)=\left\{
\begin{array}{cc}
2 x & 0 \leq x \leq \frac{1}{2} \\
2(1-x) & \frac{1}{2} \leq x \leq 1
\end{array}
\right.
$$

- For $N_3(x)$:
$$
N_{3}(x)=\left\{
\begin{array}{cc}
0 & 0 \leq x \leq \frac{1}{2} \\
2 x-1 & \frac{1}{2} \leq x \leq 1
\end{array}
\right.
$$

In contrast, in the 1 degree of freedom example, the shape functions were defined as:
- $N_1(x) = 1 - x$
- $N_2(x) = x$

### Differences:
1. **Complexity**: The shape functions in Example 2 are piecewise linear, meaning they have different expressions over different intervals. Specifically, $N_1$ and $N_2$ change their definitions at $x = \frac{1}{2}$. In contrast, the shape functions in Example 1 are linear across the entire interval [0, 1].

2. **Number of Shape Functions**: Example 2 introduces a third shape function, $N_3(x)$, which is necessary to accommodate the additional degree of freedom. This is not present in Example 1, which only has two shape functions.

3. **Support**: The shape functions in Example 2 have a more complex support structure, with $N_1$ and $N_2$ being non-zero over different segments of the domain, while $N_3$ is only non-zero in the interval $[\frac{1}{2}, 1]$.

### Implications for Finite Element Equations:
1. **Matrix Formulation**: The introduction of piecewise shape functions leads to a more complex stiffness matrix $K$ in Example 2, which is represented as:
$$
K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right]
$$
This contrasts with the simpler formulation in Example 1, where the matrix collapses to a single value.

2. **Force Vector**: The force vector $F$ in Example 2 also becomes a vector with components $F_1$ and $F_2$, reflecting the contributions from the different shape functions and their respective intervals. In Example 1, there was only one component.

3. **Higher Flexibility**: The piecewise nature of the shape functions in Example 2 allows for better approximation of solutions that may not be linear across the entire domain, thus providing greater flexibility and accuracy in modeling complex behaviors.

4. **Continuity and Discontinuity**: The shape functions in Example 2 exhibit discontinuities in their derivatives at $x = \frac{1}{2}$, which necessitates careful treatment in the formulation of the finite element equations, particularly in the evaluation of integrals over the subintervals.

In summary, the differences in shape functions between the two examples lead to a more complex formulation of the finite element equations in Example 2, allowing for greater flexibility in approximating solutions but requiring careful consideration of continuity and integration across piecewise-defined intervals."
22,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are

 Additional context 1: for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary

 Additional context 2: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of",75,Explain the role of the mesh parameter $h$ in the finite element method and how it affects the accuracy of the solution. What is the relationship between the mesh refinement and the length of the subintervals?,"The mesh parameter $h$ in the finite element method is defined as the length of the maximum subinterval, which is given by $h = \max h_{A}$, where $h_{A} = x_{A+1} - x_{A}$ for each subinterval $[x_{A}, x_{A+1}]$. The smaller the value of $h$, the more refined the partition or mesh is considered to be. 

As the mesh is refined (i.e., as $h$ decreases), the accuracy of the solution generally improves. This is because a finer mesh allows for a better approximation of the solution within each subinterval, capturing more details of the function being approximated. Conversely, a coarser mesh (larger $h$) may lead to a less accurate representation of the solution, as it may not adequately capture the variations in the function over larger intervals.

In summary, the relationship between mesh refinement and the length of the subintervals is that as the lengths of the subintervals decrease (resulting in a smaller $h$), the mesh becomes more refined, which typically leads to increased accuracy in the finite element solution."
23,"involved in formulating and solving the Galerkin problem. The functions employed are extremely simple, thus expediting computations, but they are also primitive examples of typical finite element functions. \subsection*{Example 1 (1 degree of freedom)} In this case $n=1$. Thus $w^{h}=c_{1} N_{1}$ and $u^{h}=v^{h}+g^{h}=d_{1} N_{1}+g N_{2}$. The only unknown is $d_{1}$. The shape functions must satisfy $N_{1}(1)=0$ and $N_{2}(1)=1$ (see (1.6.2) and (1.6.3)). Let us take $N_{1}(x)=1-x$ and $N_{2}(x)=x$. These are illustrated in Fig. 1.7.1 and clearly satisfy the required conditions. Since we are dealing with only 1 degree of freedom, the matrix paraphernalia collapses as follows: \begin{align*} K & =\left[K_{11}\right]=K_{11} \tag{1.7.1}\\ F & =\left\{F_{1}\right\}=F_{1} \tag{1.7.2}\\ d & =\left\{d_{1}\right\}=d_{1} \tag{1.7.3}\\ K_{11} & =a\left(N_{1}, N_{1}\right)=\int_{0}^{1} \underbrace{N_{1,x}}_{-1} \underbrace{N_{1, x}}_{-1} d x=1 \tag{1.7.4}\\ F_{1} & =\left(N_{1}, f\right)+N_{1}(0) h-a\left(N_{1}, N_{2}\right) q \\ & =\int_{0}^{1}(1-x) f(x) d x+h-\int_{0}^{1} \underbrace{N_{1, x}}_{-1} \underbrace{N_{2, x}}_{+1} d x g \\ & =\int_{0}^{1}(1-x) f(x) d x+h+q \tag{1.7.5}\\ d_{1} & =K_{11}^{-1} F_{1}=F_{1} \tag{1.7.6} \end{align*} Consequently \begin{equation*} u^{h}(x)=[\underbrace{\int_{0}^{1}(1-y) f(y) d y+h+g}_{d_{1}}](1-x)+g x \tag{1.7.7} \end{equation*} In (1.7.7), $y$ plays the role of a dummy variable. An illustration of (1.7.7) appears in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6} \tag{1.7.12} \end{equation*} which are compared in Fig. 1.7.4. Again we note that the $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$. There is one point, $x=1 / \sqrt{3}$, at which $u_{\text {part, } x}^{h}$ is exact. Let us summarize what we have observed in this example:\\ a. The homogeneous part of $u^{h}$ is exact in all cases.\\ b. In the presence of nonzero $f, u^{h}$ is exact at $x=0$ and $x=1$.\\ c. For each case, there is at least one point at which $u_{, x}^{h}$ is exact.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(3)} Figure 1.7.4 Comparison of exact and Galerkin particular solutions, Example 1, case (iii). \section*{Example 2 (2 degrees of freedom)} In this case $n=2$. Thus $w^{h}=c_{1} N_{1}+c_{2} N_{2}$, where $N_{1}(1)=N_{2}(1)=0$, and $u^{h}=$ $d_{1} N_{1}+d_{2} N_{2}+g N_{3}$, where $N_{3}(1)=1$. Let us define the $N_{A}$ 's as follows \begin{align*} & N_{1}(x)=\left\{\begin{array}{cc} 1-2 x & 0 \leq x \leq \frac{1}{2} \\ 0 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.6}\\ & N_{2}(x)=\left\{\begin{array}{cc} 2 x & 0 \leq x \leq \frac{1}{2} \\ 2(1-x) & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.7}\\ & N_{3}(x)=\left\{\begin{array}{cc} 0 & 0 \leq x \leq \frac{1}{2} \\ 2 x-1 & \frac{1}{2} \leq x \leq 1 \end{array}\right. \tag{1.7.8} \end{align*} The shape functions are illustrated in Fig. 1.7.5. Typical $w^{h} \in \mathcal{U}^{h}$ and $u^{h} \in f^{h}$ and their derivatives are shown in Fig. 1.7.6. Since $\boldsymbol{n}=$ 2, the matrix paraphernalia takes the following form: \begin{align*} & K=\left[\begin{array}{ll} K_{11} & K_{12} \\ K_{21} & K_{22} \end{array}\right] \tag{1.7.9}\\ & F=\left\{\begin{array}{l} F_{1} \\ F_{2} \end{array}\right\} \tag{1.7.10}\\ & d=\left\{\begin{array}{l} d_{1} \\ d_{2} \end{array}\right\} \tag{1.7.11}\\ & K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x=\int_{0}^{1 / 2} N_{A, x} N_{B, x} d x+\int_{1 / 2}^{1} N_{A, x} N_{B, x} d x \tag{1.7.12}\\ & K_{11}=2, \quad K_{12}=K_{21}=-2, \quad K_{22}=4 \tag{1.7.13}\\ & K=2\left[\begin{array}{rr} 1 & -1 \\ -1 & 2 \end{array}\right] \tag{1.7.14}\\ & F_{A}=\left(N_{A}, f\right)+N_{A}(0) h-a\left(N_{A}, N_{3}\right) g \\ &=\int_{0}^{1} N_{A} f d x+N_{A}(0) h-\int_{1 / 2}^{1} N_{A, x} N_{3, x} d x g \tag{1.7.15} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-16} \end{center} Figure 1.7.5 Functions for the 2 degree of freedom examples. (These functions are secretly the simplest finite element functions in a two-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-17} Figure 1.7.6 Typical weighting function and trial solution for the 2 degree of freedom example. \begin{align*} & F_{1}=\int_{0}^{1 / 2}(1-2 x) f(x) d x+h \tag{1.7.16}\\ & F_{2}=2 \int_{0}^{1 / 2} x f(x) d x+2 \int_{1 / 2}^{1}(1-x) f(x) d x+2 g \tag{1.7.17} \end{align*} Note that due to the shape functions' discontinuities in slope at $x=\frac{1}{2}$, it is convenient to express integrals over the subintervals $\left[0, \frac{1}{2}\right]$ and $\left[\frac{1}{2}, 1\right]$ (e.g., see (1.7.12) and (1.7.15)). We need not worry about the value of the derivative of $N_{A}$ at $x=\frac{1}{2}$ (it suffers a discontinuity there and thus is not well-defined classically) since it has no effect on the integrals in (1.7.12). This amounts to employing the notion of a generalized derivative. We shall again analyze the three cases considered in Example 1.\\ i. $f=0$. \begin{align*} F & =\left\{\begin{array}{c} h \\ 2 g \end{array}\right\} \tag{1.7.18}\\ d & =K^{-1} F \end{align*} \begin{align*} & =\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} h \\ 2 g \end{array}\right\} \\ & =\left\{\begin{array}{l} g+h \\ g+\frac{h}{2} \end{array}\right\} \tag{1.7.19} \end{align*} This results in \begin{align*} u^{h} & =(g+h) N_{1}+\left(g+\frac{h}{2}\right) N_{2}+g N_{3} \\ & =g\left(N_{1}+N_{2}+N_{3}\right)+h\left(N_{1}+\frac{N_{2}}{2}\right) \tag{1.7.20}\\ u^{h}(x) & =g+h(1-x) \tag{1.7.21} \end{align*} Again, the exact homogeneous solution is obtained. (The reason for this is that the exact solution is linear, and our trial solution is capable of exactly representing any linear function. Galerkin's method will give the exact answer whenever possible-that is, whenever the collection of trial solutions contains the exact solution among its members.)\\ ii. $f(x)=p=$ constant. \begin{align*} & F_{1}=\frac{p}{4}+h \tag{1.7.22}\\ & F_{2}=\frac{p}{2}+2 g \tag{1.7.23}\\ & d=\left[\begin{array}{ll} 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21}","

 Additional context 0: in Fig. 1.7.2. To get a feel for the nature of the approximation, let us compare (1.7.7) with the exact solution (see (1.2.3)). It is helpful to consider specific forms for $f$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14(1)} Figure 1.7.1 Functions for the 1 degree of freedom examples. (These functions are secretly the simplest finite element interpolation functions in a one-element context.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-14} Figure 1.7.2 The Galerkin solution for the 1 degree of freedom example.\\ i. Let $f=0$. Then \begin{equation*} u^{h}(x)=u(x)=g+(1-x) h \tag{1.7.8} \end{equation*} That is, the approximate solution is exact. In fact, it is clear by inspecting (1.7.7) and (1.2.3) that the homogeneous solution (i.e., the part of the solution corresponding to $f=0$ ) is always exactly represented. The only approximation pertains to the particular solution (i.e., the part of the solution corresponding to $f \neq 0$ ).\\ \\ ii. Now let us introduce a nonzero $f$. Assume $f(x)=p$, a constant. Then the particular solutions take the form \begin{equation*} u_{\text {part }}(x)=\frac{p\left(1-x^{2}\right)}{2} \tag{1.7.9} \end{equation*} and \begin{equation*} u_{\text {part }}^{h}(x)=\frac{p(1-x)}{2} \tag{1.7.10} \end{equation*} Equations (1.7.9) and (1.7.10) are compared in Fig. 1.7.3. Note that $u_{\text {part }}^{h}$ is exact at $x=0$ and $x=1$ and that $u_{\text {part, } x}^{h}$ is exact at $x=\frac{1}{2}$. (It should be clear that it is impossible for $u_{\text {part }}^{h}$ to be exact at all $x$ in the present circumstances. The exact solution, (1.7.9), contains a quadratic term in $x$, whereas the approximate solution is restricted to linear variation in $x$ by the definitions of $N_{1}$ and $N_{2}$.)\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-15} Figure 1.7.3 Comparison of exact and Galerkin particular solutions, Example 1, case (ii).\\ iii. This time let $f(x)=q x$, where $q$ is a constant. This choice for $f$ leads to \begin{equation*} u_{\text {part }}(x)=\frac{q\left(1-x^{3}\right)}{6} \tag{1.7.11} \end{equation*} and \begin{equation*} u_{\text {pata }}^{h}(x)=\frac{q(1-x)}{6}

 Additional context 1: 1 & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} \end{array}\right]\left\{\begin{array}{l} \frac{p}{4}+h \\ \frac{p}{2}+2 g \end{array}\right\}=\left\{\begin{array}{l} \frac{p}{2}+g+h \\ \frac{3 p}{8}+g+\frac{h}{2} \end{array}\right\} \tag{1.7.24} \end{align*} The solution takes the form \begin{align*} & u^{h}(x)=g+h(1-x)+u_{\text {part }}^{h}(x) \tag{1.7.25}\\ & u_{\text {part }}^{h}=\frac{p}{2} N_{1}+\frac{3 p}{8} N_{2} \tag{1.7.26} \end{align*} The approximate particular solution is compared with the exact in Fig. 1.7.7, from which we see that agreement is achieved at $x=0, \frac{1}{2}$ and 1 , and derivatives coincide at $x=\frac{1}{4}$ and $\frac{3}{4}$.\\ iii. $f(x)=q x, q=$ constant. \begin{align*} & F_{1}=\frac{q}{24}+h \tag{1.7.27}\\ & F_{2}=\frac{q}{4}+2 g \tag{1.7.28} \end{align*} \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19} Figure 1.7.7 Comparison of exact and Galerkin particular solutions, Example 2, case (ii). \[ d=\left\{\begin{array}{l} \frac{q}{6}+g+h \tag{1.7.29}\\ \frac{7 q}{48}+g+\frac{h}{2} \end{array}\right\} \] Again $u^{h}$ may be expressed in the form (1.7.25), where \begin{equation*} u_{\text {part }}^{h}=\frac{q}{6} N_{1}+\frac{7 q}{48} N_{2} \tag{1.7.30} \end{equation*} A comparison is presented in Fig. 1.7.8. The Galerkin solution is seen to be exact once again at $x=0, \frac{1}{2}$, and 1 , and the derivative is exact at two points. Let us summarize the salient observations of Example 2:\\ a. The homogeneous part of $u^{h}$ is exact in all cases, as in Example 1. (A rationale for this is given after Equation (1.7.21).\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section,

 Additional context 2: that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive",80,"Compare the approximate solutions $u_{	ext{part}}^{h}(x)$ and $u_{	ext{part}}(x)$ for the case where $f(x) = p$, a constant. What conclusions can be drawn regarding the accuracy of the finite element approximation in this scenario?","To compare the approximate solutions $u_{\text{part}}^{h}(x)$ and $u_{\text{part}}(x)$ for the case where $f(x) = p$, a constant, we have the following expressions from the context:

1. The exact particular solution is given by:
   $$
   u_{\text{part}}(x) = \frac{p(1-x^2)}{2} \tag{1.7.9}
   $$

2. The approximate particular solution is given by:
   $$
   u_{\text{part}}^{h}(x) = \frac{p(1-x)}{2} \tag{1.7.10}
   $$

Now, let's analyze these two solutions:

- **At the endpoints**:
  - At $x = 0$:
    $$
    u_{\text{part}}(0) = \frac{p(1-0^2)}{2} = \frac{p}{2}
    $$
    $$
    u_{\text{part}}^{h}(0) = \frac{p(1-0)}{2} = \frac{p}{2}
    $$
    Thus, both solutions are equal at $x = 0$.

  - At $x = 1$:
    $$
    u_{\text{part}}(1) = \frac{p(1-1^2)}{2} = 0
    $$
    $$
    u_{\text{part}}^{h}(1) = \frac{p(1-1)}{2} = 0
    $$
    Thus, both solutions are equal at $x = 1$.

- **At $x = \frac{1}{2}$**:
  - For the exact solution:
    $$
    u_{\text{part}}\left(\frac{1}{2}\right) = \frac{p\left(1-\left(\frac{1}{2}\right)^2\right)}{2} = \frac{p\left(1-\frac{1}{4}\right)}{2} = \frac{p\left(\frac{3}{4}\right)}{2} = \frac{3p}{8}
    $$
  
  - For the approximate solution:
    $$
    u_{\text{part}}^{h}\left(\frac{1}{2}\right) = \frac{p(1-\frac{1}{2})}{2} = \frac{p\left(\frac{1}{2}\right)}{2} = \frac{p}{4}
    $$

From this comparison, we can conclude that:

- The approximate solution $u_{\text{part}}^{h}(x)$ is exact at the endpoints $x = 0$ and $x = 1$.
- However, at $x = \frac{1}{2}$, the approximate solution does not match the exact solution, indicating a discrepancy in the middle of the interval.

### Conclusion:
The finite element approximation $u_{\text{part}}^{h}(x)$ is accurate at the endpoints of the interval but fails to capture the exact behavior of the solution in between, specifically at $x = \frac{1}{2}$. This highlights a limitation of linear finite element approximations when dealing with functions that exhibit curvature, as the exact solution $u_{\text{part}}(x)$ contains a quadratic term, while the approximation is restricted to linear variation. Thus, while the method provides a reasonable approximation, it may not be sufficient for capturing the full behavior of the solution across the entire domain."
24,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 1: center]{2024_10_04_fba7dc36d090c246379ag-19(1)} Figure 1.7.8 Comparison of exact and Galerkin particular solutions, Example 2, case (iii).\\ b. The Galerkin solution is exact at the endpoints of each subinterval for all cases.\\ c. In each case, there is at least one point in each subinterval at which $u_{, x}^{h}$ is exact. After generalizing to the case of $\boldsymbol{n}$ subintervals in the following section, we shall show in Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are

 Additional context 2: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify",80,"Define the piecewise linear finite element space and explain how it is constructed from a partition of the domain [0,1] into nonoverlapping subintervals. What are the implications of having unequal lengths of these subintervals?","The piecewise linear finite element space, denoted as $\mathcal{U}^{h}$, is constructed by partitioning the domain $[0,1]$ into $n$ nonoverlapping subintervals. Each typical subinterval is represented as $\left[x_{A}, x_{A+1}\right]$, where $x_{A} < x_{A+1}$ and $A = 1, 2, \ldots, n$. The endpoints of these subintervals, $x_{A}$, are referred to as nodal points or nodes, with the requirement that $x_{1} = 0$ and $x_{n+1} = 1$.

The lengths of the elements are defined as $h_{A} = x_{A+1} - x_{A}$, and these lengths are not required to be equal. The mesh parameter, $h$, is typically taken to be the length of the maximum subinterval, i.e., $h = \max h_{A}, A = 1, 2, \ldots, n$. A smaller value of $h$ indicates a more refined partition or mesh. If all subinterval lengths are equal, then $h = \frac{1}{n}$.

The implications of having unequal lengths of these subintervals include:

1. **Flexibility in Mesh Design**: Unequal lengths allow for a more flexible mesh design that can adapt to the characteristics of the problem being solved. For example, finer meshes can be placed in regions where the solution is expected to change rapidly, while coarser meshes can be used in regions where the solution is smoother.

2. **Computational Efficiency**: By allowing for varying lengths, the finite element method can achieve a balance between accuracy and computational efficiency. Regions requiring higher accuracy can be refined without unnecessarily increasing the number of elements in regions where less accuracy is needed.

3. **Piecewise Linear Functions**: The shape functions associated with the nodes are defined piecewise, leading to linear polynomials within each subinterval. This means that while the overall function $w^{h}$ is continuous, its derivative can be discontinuous at the boundaries of the elements.

In summary, the piecewise linear finite element space is constructed through a partition of the domain into nonoverlapping subintervals, allowing for unequal lengths which enhance the flexibility and efficiency of the finite element analysis."
25,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the

 Additional context 1: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 2: zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can",70,What are the properties of the shape functions $N_A(x)$ associated with the piecewise linear finite element space? How do these properties contribute to the construction of finite element functions?,"The properties of the shape functions $N_A(x)$ associated with the piecewise linear finite element space are as follows:

1. **Local Support**: The shape functions $N_A(x)$ are nonzero only in the subintervals that contain the node $x_A$. This means that $N_A(x) = 0$ outside a neighborhood of node $A$. This property ensures that each shape function influences the finite element function only in its local region, allowing for localized variations in the solution.

2. **Value at Nodes**: The shape functions satisfy the property $N_A(x_B) = \delta_{AB}$, where $\delta_{AB}$ is the Kronecker delta. This means that $N_A(x)$ takes the value 1 at node $A$ and is 0 at all other nodes. This property is crucial for ensuring that the finite element function interpolates the values at the nodes correctly.

3. **Piecewise Linear**: The shape functions are linear polynomials within each element domain. This means that within each subinterval $[x_A, x_{A+1}]$, the shape functions can be expressed as linear functions of $x$. This linearity allows for simple and efficient calculations in the finite element method.

4. **Continuity with Discontinuous Slope**: The overall finite element function constructed from these shape functions is continuous but has a discontinuous slope across each element boundary. This characteristic allows the finite element method to approximate solutions to problems with varying gradients while maintaining continuity in the function itself.

These properties contribute to the construction of finite element functions by allowing for the formulation of a piecewise linear approximation of the solution over the entire domain. A typical member $w^h \in \mathcal{U}^h$ can be expressed as:

$$
w^h(x) = \sum_{A=1}^{n} c_A N_A(x)
$$

where $c_A$ are coefficients associated with each node. The local support ensures that the influence of each coefficient is limited to its corresponding element, while the value at nodes property guarantees that the finite element function matches the prescribed values at the nodes. The piecewise linear nature of the shape functions allows for a flexible and accurate representation of the solution across the domain, making the piecewise linear finite element functions widely used in one-dimensional problems."
26,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is

 Additional context 1: zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can

 Additional context 2: w f d x+w(0) h $$ for all $w \in \mathcal{U}$. Integrating by parts and making use of the fact $w(1)=0$ results in \begin{equation*} 0=\int_{0}^{1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}(0)+h\right]. \tag{1.4.4} \end{equation*} To prove $u$ is a solution of $(S)$ it suffices to show that (1.4.4) implies ${ }^{1}$\\ i. $u_{, x x}+f=0$ on $\Omega$; and\\ ii. $u_{, x}(0)+h=0$ First we shall prove (i). Define $\boldsymbol{w}$ in (1.4.4) by \begin{equation*} w=\phi\left(u_{, x x}+f\right) \tag{1.4.5} \end{equation*} where $\phi$ is smooth; $\phi(x)>0$ for all $x \in \Omega=] 0,1[$; and $\phi(0)=\phi(1)=0$. For example, we can take $\phi(x)=x(1-x)$, which satisfies all the stipulated requirements (see Figure 1.4.1). It follows that $w(1)=0$ and thus $w \in \mathcal{U}$, so (1.4.5) defines a\\ \begin{figure}[h] \centering \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-05} \vspace{0.5em} \textbf{Figure 1.4.1} \end{figure} \footnotetext{${ }^{1}$ These equations are sometimes called the Euler-Lagrange equations of the weak formulation. } legitimate member of $\mathcal{U}$. Substituting (1.4.5) into (1.4.4) results in\\ \begin{equation*} 0=\int_{0}^{1} \phi \underbrace{\left(u_{, x x}+f\right)^{2}}_{\geq 0} d x+0 \tag{1.4.6} \end{equation*} Since $\phi>0$ on $\Omega$, it follows from (1.4.6) that (i) must be satisfied.\\ Now that we have established (i), we may use it in (1.4.4) to prove (ii), namely, \begin{equation*} 0=w(0)\left[u_{, x}(0)+h\right] \tag{1.4.7} \end{equation*} That $w \in \mathcal{U}$ puts no restriction whatsoever on its value at $x=0$. Therefore, we may assume that the $w$ in (1.4.7) is such that $w(0) \neq 0$. Thus (ii) is also shown to hold, which completes the proof of the proposition. \section*{Remarks} \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the",90,"Given the weak formulation of the one-dimensional model problem, derive the Euler-Lagrange conditions from the equation $$\int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h$$ and explain the significance of each condition in the context of finite element analysis.","To derive the Euler-Lagrange conditions from the weak formulation of the one-dimensional model problem given by 

$$
\int_{0}^{1} w_{, x} u_{, x} d x = \int_{0}^{1} w f d x + w(0) h,
$$ 

we start by integrating by parts the left-hand side. The integration by parts gives us:

$$
\int_{0}^{1} w_{, x} u_{, x} d x = -\int_{0}^{1} w u_{, x x} d x + w(1) u_{, x}(1) - w(0) u_{, x}(0).
$$

Since we have the essential boundary condition $u(1) = g$, we can assume $w(1) = 0$ (as $w$ is a member of the space $\mathcal{U}$ which satisfies this condition). Thus, the term $w(1) u_{, x}(1)$ vanishes. We are left with:

$$
-\int_{0}^{1} w u_{, x x} d x - w(0) u_{, x}(0) = \int_{0}^{1} w f d x + w(0) h.
$$

Rearranging this gives:

$$
\int_{0}^{1} w (u_{, x x} + f) d x + w(0) (h + u_{, x}(0)) = 0.
$$

For this equation to hold for all test functions $w \in \mathcal{U}$, the integrand must vanish. This leads us to the following Euler-Lagrange conditions:

1. **Condition (i)**: 
   $$ u_{, x x}(x) + f(x) = 0, \quad \text{for } x \in ]x_A, x_{A+1}[, A=1,2,\ldots,n. $$
   - **Significance**: This condition represents the governing differential equation of the problem in the interior of each element. It indicates that the second derivative of the solution $u$ is balanced by the source term $f$, which is essential in understanding the behavior of the system being modeled.

2. **Condition (ii)**: 
   $$ -u_{, x}(0^{+}) = h. $$
   - **Significance**: This is a natural boundary condition that relates the derivative of the solution at the boundary $x=0$ to a given value $h$. It reflects how the system reacts at the boundary, which is crucial for ensuring that the physical behavior at the edges of the domain is accurately captured.

3. **Condition (iii)**: 
   $$ u_{, x}(x_{A}^{-}) = u_{, x}(x_{A}^{+}), \quad A=2,3,\ldots,n. $$
   - **Significance**: This condition ensures continuity of the derivative of the solution across element boundaries. It is important for maintaining the physical realism of the model, as it prevents unphysical jumps in the slope of the solution, which would be unrealistic in many physical systems.

In summary, these Euler-Lagrange conditions derived from the weak formulation are fundamental in finite element analysis as they provide the necessary equations and boundary conditions that the approximate solution must satisfy, ensuring that the numerical method accurately reflects the underlying physical problem."
27,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A},

 Additional context 1: The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on

 Additional context 2: $$ \section*{Remark} Note that the symmetry of $\boldsymbol{K}$ followed from the symmetry of $\boldsymbol{D}$, which was a consequence of the major symmetry of the $c_{i j k l}$ 's (see (2.7.3)). Proof of 2. Positive definite (Recall from Sec. 1.9 that we must show (i) $\boldsymbol{c}^{T} \boldsymbol{K} c \geq 0$ and (ii) $\boldsymbol{c}^{T} \boldsymbol{K} \boldsymbol{c}=0$ implies $\boldsymbol{c}=0$.) Let $w_{i}^{h}=\Sigma_{A \in \eta_{-\eta_{g_i}}} N_{A} c_{iA}$ be a member of $\mathcal{V}_{i}^{h}$. Then $c_{P}=c_{iA}$, where $P=$ $\operatorname{ID}(i, A), 1 \leq P \leq n_{e q}$, defines the components of an $n_{e q}$-vector $c$.\\ i. $$ \begin{aligned} & \boldsymbol{c}^{T} K c=\sum_{P, Q=1}^{n_{eq}} c_{P} K_{P Q} c_{Q} \\ & =\sum_{i, j=1}^{n_{dof}}\left(\sum_{\substack{A \in \eta-\eta_{g_i} \\ B \in \eta-\eta_{g_j}}} c_{i A} a\left(N_{A} e_{i}, N_{B} e_{j}\right) c_{j B}\right) \quad \text { (definition of } K_{P Q} \text { ) } \\ & =a\left(\sum_{i=1}^{n_{dof}}\left(\sum_{A \in \eta-\eta_{g_i}} c_{iA} N_{A} e_{i}\right), \sum_{j=1}^{n_ {dof}}\left(\sum_{B \in \eta-\eta_{g_j}} c_{j B} N_{B} e_{j}\right)\right) \quad \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) \quad \text { (definition of } w^{h} \text { ) } \\ & =\int_{\Omega} \underbrace{w_{(i, j)}^{h} c_{ijkl} w_{(k, l)}^{h} d \Omega}_{\geq 0} \quad \text { (by (2.7.5) and (2.7.17)) } \\ & \geq 0 \end{aligned} $$ ii. Assume $\boldsymbol{c}^{\boldsymbol{T}} \boldsymbol{K} \boldsymbol{c}=\mathbf{0}$. By the proof of part ( $\left.\mathbf{(}\right)$, we deduce that $$ w_{(i,j)}^{h} c_{i j k l} w_{(k, l)}^{h}=0 $$ From (2.7.6), this means that $w_{(i, j)}^{h}=0$, and so $w^{h}$ is an infinitesimal rigid motion. By Assumption $R, \boldsymbol{w}^{\boldsymbol{h}}=\mathbf{0}$, from which it follows that $c_{p}=0$; hence $\boldsymbol{c}=\mathbf{0}$. \section*{Remark} Positive definiteness of $\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\mathcal{V}^{h}$. \subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the",85,Explain the concept of a positive definite matrix in the context of the stiffness matrix $K$ in finite element analysis. How can one prove that the stiffness matrix $K$ is positive definite based on the properties of the shape functions?,"In the context of finite element analysis, a matrix $\boldsymbol{K}$ is said to be positive definite if it satisfies two conditions:

1. For any non-zero vector $\boldsymbol{c}$, the quadratic form $c^{T} K c \geq 0$.
2. If $c^{T} K c = 0$, then it must imply that $\boldsymbol{c} = 0$.

To prove that the stiffness matrix $K$ is positive definite, we can follow these steps based on the properties of the shape functions:

1. **Construct a member of the finite element space**: Let $c_{A}$, where $A=1,2,\ldots,n$, be the components of a vector $\boldsymbol{c}$. We can construct a member of the piecewise linear finite element space $\mathcal{U}^{h}$ as follows:
   $$
   w^{h} = \sum_{A=1}^{n} c_{A} N_{A}
   $$
   where $N_{A}$ are the shape functions associated with the nodes.

2. **Evaluate the quadratic form**: The quadratic form can be expressed as:
   $$
   c^{T} K c = \sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} = a\left(w^{h}, w^{h}\right)
   $$
   Here, $a(\cdot, \cdot)$ is a bilinear form defined in the context of the finite element method.

3. **Show non-negativity**: By the properties of the bilinear form, we have:
   $$
   a\left(w^{h}, w^{h}\right) = \int_{0}^{1} \left(w_{, x}^{h}\right)^{2} d x \geq 0
   $$
   This integral is non-negative since it represents the integral of the square of the derivative of $w^{h}$.

4. **Show that zero implies trivial solution**: If we assume $c^{T} K c = 0$, then:
   $$
   \int_{0}^{1} \left(w_{, x}^{h}\right)^{2} d x = 0
   $$
   This implies that $w^{h}$ must be constant. Given that $w^{h} \in \mathcal{U}^{h}$ and satisfies the boundary condition $w^{h}(1) = 0$, we conclude that $w^{h}(x) = 0$ for all $x \in [0,1]$. This can only occur if each $c_{A} = 0$, leading to $\boldsymbol{c} = 0$.

Thus, we have shown that the stiffness matrix $K$ is positive definite, satisfying both conditions required for positive definiteness."
28,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: from which it follows that $c_{p}=0$; hence $\boldsymbol{c}=\mathbf{0}$. \section*{Remark} Positive definiteness of $\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\mathcal{V}^{h}$. \subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\boldsymbol{k}^{e}$ and $f^{\boldsymbol{c}}$ : \begin{align*} & \boldsymbol{k}^{e}=\left[k_{p q}^{e}\right], \quad \boldsymbol{f}^{e}=\left\{f_{p}^{e}\right\}, \quad 1 \leq p, q \leq n_{ee}=n_{ed} n_{e n} \tag{2.9.1}\footnotemark\\ & k_{p q}^{e}=e_{i}^{T} \int_{\Omega e} B_{a}^{T} D B_{b} d \Omega e_{j}, \quad p=n_{e d}(a-1)+i, \\ & q=n_{e d}(b-1)+j \tag{2.9.2}\\ & \left(n_{s d}=2\right) \quad B_{a}=\left[\begin{array}{cc} N_{a, 1} & 0 \\ 0 & N_{a, 2} \\ N_{a, 2} & N_{a, 1} \end{array}\right] \tag{2.9.3}\\ & \left(n_{s d}=3\right) \quad \boldsymbol{B}_{a}=\left[\begin{array}{ccc} N_{a, 1} & 0 & 0 \\ 0 & N_{a, 2} & 0 \\ 0 & 0 & N_{a, 3} \\ 0 & N_{a, 3} & N_{a, 2} \\ N_{a, 3} & 0 & N_{a, 1} \\ N_{a, 2} & N_{a, 1} & 0 \end{array}\right] \tag{2.9.4} \end{align*} \footnotetext{$n_{\text{ee}}$ stands for the number of element equations and $n_{\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\text {ed}} \leq n_{\text{dof}}$, although they are usually equal. } and \[ f_{p}^{e}=\int_{\Omega^{e}} N_{a} \ell_{i} d \Omega+\int_{\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \Gamma-\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \quad \begin{array}{r} \Gamma_{h_{i}}^{e}=\Gamma_{h_{i}} \cap \Gamma^{e} \tag{2.9.5}\\ (\text { no sum on } i) \end{array} \] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\left(x_{b}^{e}\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \begin{equation*} \underbrace{\boldsymbol{k}_{ab}^{e}}_{n_{e d} \times n_{e d}}=\int_{\boldsymbol{\Omega}^{e}} B_{a}^{T} \boldsymbol{D} B_{b} d \boldsymbol{\Omega} \tag{2.9.6a} \end{equation*} From (2.9.2) we see that

 Additional context 1: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[

 Additional context 2: and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A},",75,Discuss the significance of the banded structure of the stiffness matrix $K$ in finite element analysis. How does this property affect computational efficiency when solving the system of equations $K d = F$?,"The banded structure of the stiffness matrix $K$ in finite element analysis is significant for several reasons:

1. **Sparsity**: The entries of the stiffness matrix $K$ are mostly zero, particularly for entries that are far from the main diagonal. This sparsity arises because the shape functions $N_{A}$ are zero outside a neighborhood of their corresponding nodes. As a result, many of the integrals that contribute to the entries of $K$ evaluate to zero when the nodes are not adjacent.

2. **Storage Efficiency**: Since $K$ is banded, it means that only the nonzero entries need to be stored in memory. This reduces the amount of memory required to store the matrix, which is particularly beneficial for large-scale problems where the number of elements (and thus the size of $K$) can be very large.

3. **Computational Efficiency**: The banded structure allows for more efficient algorithms to be used when solving the system of equations $K d = F$. Specifically, specialized numerical methods can take advantage of the banded nature of $K$ to reduce the computational complexity of matrix operations. For example, direct solvers can use techniques like Gaussian elimination that are optimized for banded matrices, leading to faster solution times compared to general dense matrix algorithms.

4. **Reduced Operations**: The zero elements outside the band mean that operations involving $K$, such as matrix-vector multiplications, can skip these zero entries, further speeding up computations. This is particularly important in iterative solvers where many matrix-vector products are required.

In summary, the banded structure of the stiffness matrix $K$ enhances both storage and computational efficiency, making it feasible to solve large finite element problems effectively."
29,"Sec. 1.10 that the above observations are not accidental. Exercise 1. If the reader has not had experience with calculations of the type presented in this section, it would be worthwhile to reproduce all results, providing all omitted details. \section*{1.8 PIECEWISE LNEAR FINITE ELEMENT SPACE} The examples of the preceding section employed definitions of $\mathcal{U}^{h}$ and $f^{h}$ which were special cases of the so-called piecewise linear finite element space. To define the general case in which $\mathcal{U}^{h}$ is $n$-dimensional, we partition the domain [0,1] into $n$ nonoverlapping subintervals. The typical subinterval is denoted by $\left[x_{A}, x_{A+1}\right]$, where $x_{A}<x_{A+1}$ and $A=1,2, \ldots, n$. We also require $x_{1}=0$ and $x_{n+1}=1$. The $x_{A}$ 's are called nodal points, or simply nodes. (The terminologies joints and knots are also used.) The subintervals are sometimes referred to as the finite element domains, or simply elements. Notice that the lengths of the elements, $h_{A}=x_{A+1}-x_{A}$, are not required to be equal. The mesh parameter, $h$, is generally taken to be the length of the maximum subinterval (i.e., $h=\max h_{A}, A=1,2, \ldots, n$ ). The smaller $h$, the more ""refined"" is the partition, or mesh. If the subinterval lengths are equal, then $h=1 / n$. The shape functions are defined as follows: Associated to a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the one-dimensional model problem: \begin{equation*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.8.4} \end{equation*} where $w \in \mathcal{U}$ and $u \in f$ are assumed to be smooth on element interiors (i.e., on $] x_{A}$, $x_{A+1}[, A=1,2, \ldots, n)$, but may suffer slope discontinuities across element boundaries. (Functions of this class contain the piecewise linear finite element space described earlier.) From (1.8.4) and the assumed continuity of the functions, show that: \begin{align*} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(u_{, x x}+f\right) d x+w(0)\left[u_{, x}\left(0^{+}\right)+h\right] \\ & +\sum_{A=2}^{n} w\left(x_{A}\right)\left[u_{, x}\left(x_{A}^{+}\right)-u_{, x}\left(x_{A}^{-}\right)\right] \tag{1.8.5} \end{align*} Arguing as in Sec. 1.4, it may be concluded that the Euler-Lagrange conditions of (1.8.5) are\\ i. $u_{, x x}(x)+f(x)=0$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$,\\ ii. $-u_{, x}\left(0^{+}\right)=h$; and\\ iii. $u_{, x}\left(x_{A}^{-}\right)=u_{, x}\left(x_{A}^{+}\right)$, where $A=2,3, \ldots, n$. Observe that (i) is the differential equation restricted to element interiors, and (iii) is a continuity condition across element boundaries. This may be contrasted with the case in which the solution is assumed smooth. In this case the continuity condition is identically satisfied and the summation of integrals over element interiors may be replaced by an integral over the entire domain (see Sec. 1.4). In the Galerkin finite element formulation, an approximate solution of (i)-(iii) is obtained. \section*{1.9 PROPERTIES OF K} The shape functions $N_{A}, A=1,2, \ldots, n+1$, are zero outside a neighborhood of node $A$. As a result, many of the entries of $K$ are zero. This can be seen as follows. Let $B>A+1$. Then (see Fig. 1.9.1) \begin{equation*} K_{A B}=\int_{0}^{1} \underbrace{N_{A, x} N_{B, x}}_{0} d x=0 \tag{1.9.1} \end{equation*} The symmetry of $K$ implies, in addition, that (1.9.1) holds for $A>B+1$. One says that $\boldsymbol{K}$ is banded (i.e., its nonzero entries are located in a band about the main diagonal). Figure 1.9.2 depicts this property. Banded matrices have significant advantages in that the zero elements outside the band neither have to be stored nor operated\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-23} Figure 1.9.2 Band structure of $\boldsymbol{K}$.\\ upon in the computer. The stiffness matrix arising in finite element analysis is, in general, narrowly banded, lending itself to economical formation and solution. Definition. An $\boldsymbol{n} \times \boldsymbol{n}$ matrix $\boldsymbol{A}$ is said to be positive definite if\\ i. $c^{T} A c \geq 0$ for all $n$-vectors $c$; and\\ ii. $c^{\boldsymbol{T}} \boldsymbol{A c}=0$ implies $\boldsymbol{c}=0$. \subsection*{Remarks} \begin{enumerate} \item A symmetric positive-definite matrix posesses a unique inverse. \item The eigenvalues of a positive-definite matrix are real and positive. \end{enumerate} Theorem. The $\boldsymbol{n} \times n$ matrix $K$ defined by (1.6.11) is positive definite.\\ Proof\\ i. Let $c_{A}, A=1,2, \ldots, n$, be the components of $c$ (i.e., $c=\left\{c_{A}\right\}$ ), an arbitrary vector. Use these $c_{A}$ 's to construct a member of $\mathcal{U}^{h}, w^{h}=\sum_{A=1}^{n} c_{A} N_{A}$, where\\ the $N_{A}$ 's are the basis functions for $\mathcal{U}^{h}$. Then $$ \begin{array}{rlrl} c^{T} K c & =\sum_{A, B=1}^{n} c_{A} K_{A B} c_{B} & \\ & =\sum_{A, B=1}^{n} c_{A} a\left(N_{A}, N_{B}\right) c_{B} & & \text { (definition of } \left.K_{A B}\right) \\ & =a\left(\sum_{A=1}^{n} c_{A} N_{A}, \sum_{B=1}^{n} c_{B} N_{B}\right) & & \text { (bilinearity of } a(\cdot, \cdot)) \\ & =a\left(w^{h}, w^{h}\right) & & \text { (definition of } \left.w^{h}\right) \\ & =\int_{0}^{1} \underbrace{\left(w_{0}^{h} x\right.}_{\geq 0})^{2} d x & & \text { (by (1.4.8) } \\ & \geq 0 & \end{array} $$ ii. Assume $c^{T} K c=0$. By the proof of part (i), $$ \int_{0}^{1}\left(w_{, x}^{h}\right)^{2} d x=0 $$ and consequently $w^{h}$ must be constant. Since $w^{h} \in \mathcal{U}^{h}, w^{h}(1)=0$. Combining these facts, we conclude that $w^{h}(x)=0$ for all $x \in[0,1]$, which is possible only if each $c_{A}=0, A=1,2, \ldots, n$. Thus $c=0$. Note that part (ii) depended upon the definition of $\boldsymbol{K}$ and the zero essential boundary condition built into the definition of $\mathcal{U}^{h}$. Summary. $K$, defined by (1.6.11), is\\ i. Symmetric\\ ii. Banded\\ iii. Positive-definite The practical consequence of the above properties is that a very efficient computer solution of $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ may be performed. \section*{1.10 MATHEMATICAL ANALYSIS} In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making","

 Additional context 0: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max

 Additional context 1: to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one

 Additional context 2: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional",80,What is the role of the Dirac delta function in the context of the Green's function problem in finite element analysis? How does it relate to the formulation of the problem and the solution process?,"In the context of the Green's function problem in finite element analysis, the Dirac delta function, denoted as $\delta_{y}(x) = \delta(x - y)$, plays a crucial role as it represents a concentrated force of unit amplitude located at point $y$. The Green's function problem is formulated to find a function $g$ such that:

$$
g_{, xx} + \delta_{y} = 0 \quad \text{on } \Omega
$$

This formulation indicates that the Green's function $g$ must satisfy the differential equation with the Dirac delta function acting as a source term. The boundary conditions for the problem are given by:

$$
g(1) = 0 \quad \text{and} \quad g_{, x}(0) = 0.
$$

The Dirac delta function is not a function in the classical sense but rather an operator that acts on continuous functions. When integrated against a continuous function $w$, it ""sifts out"" the value of $w$ at the point $y$, as shown by the equation:

$$
\left(w, \delta_{y}\right) = \int_{0}^{1} w(x) \delta(x - y) dx = w(y).
$$

This property is essential for establishing the variational equation corresponding to the Green's function problem, which is expressed as:

$$
a(w, g) = \left(w, \delta_{y}\right) = w(y).
$$

This equation holds for all continuous functions $w \in \mathcal{U}$. The use of the Dirac delta function allows for the formulation of the problem in a way that captures the influence of point sources on the system being analyzed.

In summary, the Dirac delta function serves as a mathematical tool to model point sources in the Green's function problem, facilitating the formulation and solution process by allowing the extraction of values from continuous functions at specific points."
30,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one

 Additional context 1: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max

 Additional context 2: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional",80,Explain the significance of the Dirac delta function $oldsymbol{	ext{}}_y$ in the context of the Green's function problem and how it relates to the concept of concentrated forces in mechanics.,"The Dirac delta function $\delta_y$ plays a crucial role in the context of the Green's function problem. It is defined as an operator that acts on continuous functions, effectively ""sifting out"" the value of the function at a specific point $y$. This is expressed mathematically as:

$$
\left(w, \delta_{y}\right) = \int_{0}^{1} w(x) \delta(x-y) dx = w(y)
$$

This property highlights that if the function $w$ were discontinuous at $y$, the value would be ambiguous, emphasizing the need for continuity in the functions involved.

In mechanics, the Dirac delta function is interpreted as representing a concentrated force of unit amplitude located at the point $y$. This means that when we apply the delta function in the Green's function problem, we are essentially modeling the effect of a concentrated force acting at a specific point in the domain. The Green's function $g$ is then sought to satisfy the equation:

$$
g_{, xx} + \delta_{y} = 0 \quad \text{on } \Omega
$$

This equation indicates that the response of the system (represented by $g$) is influenced by the concentrated force represented by $\delta_y$. Thus, the Dirac delta function is significant as it allows us to formulate problems involving point loads or concentrated forces, which are common in mechanics, and to derive solutions that describe the system's behavior under such conditions."
31,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one

 Additional context 1: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max

 Additional context 2: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional",90,"Derive the Green's function $g(x)$ for the problem defined by the equations: $$ g_{, xx} + oldsymbol{	ext{}}_y = 0 $$ with the boundary conditions $$ g(1) = 0 $$ and $$ g_{, x}(0) = 0 $$, and explain the steps involved in the integration process.","To derive the Green's function $g(x)$ for the problem defined by the equations 

$$ g_{, xx} + \delta_y = 0 $$ 

with the boundary conditions 

$$ g(1) = 0 $$ 
$$ g_{, x}(0) = 0, $$ 

we will follow these steps:

1. **Integrate the Differential Equation**: Start with the equation 

   $$ g_{, xx} + \delta_y = 0. $$ 

   Rearranging gives us 

   $$ g_{, xx} = -\delta_y. $$ 

   We will integrate this equation with respect to $x$.

2. **First Integration**: Integrating both sides gives:

   $$ g_{, x} = -\int \delta_y \, dx + c_1, $$ 

   where $c_1$ is a constant of integration. The integral of the Dirac delta function $\delta_y$ is the Heaviside step function $H_y(x)$:

   $$ g_{, x} = -H_y(x) + c_1. $$ 

3. **Second Integration**: Now, integrate again:

   $$ g(x) = -\int H_y(x) \, dx + c_1 x + c_2, $$ 

   where $c_2$ is another constant of integration. The integral of the Heaviside function gives us the Macaulay bracket:

   $$ g(x) = -\langle x - y \rangle + c_1 x + c_2. $$ 

4. **Apply Boundary Conditions**: We now have 

   $$ g(x) = -\langle x - y \rangle + c_1 x + c_2. $$ 

   We will apply the boundary conditions to find $c_1$ and $c_2$.

   - **Boundary Condition 1**: From $g(1) = 0$, we substitute $x = 1$:

     $$ 0 = -\langle 1 - y \rangle + c_1(1) + c_2. $$ 

   - **Boundary Condition 2**: From $g_{, x}(0) = 0$, we substitute $x = 0$ into the expression for $g_{, x}$:

     $$ 0 = -H_y(0) + c_1. $$ 

     Since $H_y(0) = 0$ for $y > 0$, we find that $c_1 = 0$.

5. **Substituting Back**: With $c_1 = 0$, our expression for $g(x)$ simplifies to:

   $$ g(x) = -\langle x - y \rangle + c_2. $$ 

   Now we substitute back into the first boundary condition:

   $$ 0 = -\langle 1 - y \rangle + c_2. $$ 

   This gives us 

   $$ c_2 = \langle 1 - y \rangle. $$ 

6. **Final Expression for Green's Function**: Thus, we arrive at the final expression for the Green's function:

   $$ g(x) = \langle 1 - y \rangle - \langle x - y \rangle. $$ 

   This can be expressed as:

   $$ g(x) = (1 - y) - \langle x - y \rangle. $$ 

This function $g(x)$ is piecewise linear and satisfies the given boundary conditions and the differential equation."
32,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one

 Additional context 1: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max

 Additional context 2: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional",70,"Discuss the relationship between the Heaviside function $H_y(x)$ and the Dirac delta function $oldsymbol{	ext{}}_y$. How does the integral of $oldsymbol{	ext{}}_y$ yield the Heaviside function, and what is its significance in the context of distributions?","The Heaviside function $H_y(x)$ and the Dirac delta function $\delta_y$ are closely related in the context of distributions. The Dirac delta function $\delta_y(x)$ is defined as an operator that ""sifts out"" the value of a continuous function at a specific point $y$. Specifically, for a continuous function $w$, the action of the Dirac delta function is given by:

$$
\left(w, \delta_y\right) = \int_{0}^{1} w(x) \delta(x-y) dx = w(y.
$$

This property highlights that the Dirac delta function is not a traditional function but rather a distribution that is defined by its effect on continuous functions.

The Heaviside function $H_y(x)$, also known as the unit step function, is obtained by integrating the Dirac delta function. The relationship can be expressed as follows:

$$
H_y(x) = \int_{-\infty}^{x} \delta_y(t) dt.
$$

This integral results in:

$$
H_y(x) = 
\begin{cases}
0, & x < y \\
1, & x \geq y
\end{cases}
$$

The significance of this relationship in the context of distributions is that the Heaviside function represents a cumulative effect of the Dirac delta function. While the Dirac delta function indicates a concentrated force or impulse at a point, the Heaviside function represents the step change that occurs at that point. This is particularly important in mechanics and other fields where such concentrated forces are analyzed, as it allows for the modeling of discontinuities and sudden changes in systems. 

In summary, the integral of the Dirac delta function yields the Heaviside function, illustrating the transition from a pointwise effect (the delta function) to a cumulative effect (the Heaviside function) in the framework of distributions."
33,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional

 Additional context 1: to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one

 Additional context 2: In this section we will show that the observations made with reference to the example problems of Sec. 1.7 are, in fact, general results. To establish these facts rigorously requires only elementary mathematical techniques. Our first objective is to establish that the Galerkin finite element solution $\boldsymbol{u}^{\boldsymbol{h}}$ is exact at the nodes. To do this we must introduce the notion of a Green's function. Let $\delta_{y}(x)=\delta(x-y)$ denote the Dirac delta function. The Dirac function is not a function in the classical sense but rather an operator defined by its action on (continuous) functions. Let $w$ be continuous on $[0,1]$; then we write \begin{align*} \left(w, \delta_{y}\right) & =\int_{0}^{1} w(x) \delta(x-y) d x \tag{1.10.1}\\ & =w(y) \end{align*} By (1.10.1), we see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max",85,"What is the variational equation corresponding to the Green's function problem, and how does it relate to the concept of continuous functions in the space $oldsymbol{	ext{U}}$? Provide the equation and discuss its implications for the solution of the Green's function problem.","The variational equation corresponding to the Green's function problem is given by:

$$
a(w, g) = \left(w, \delta_{y}\right) = w(y) \tag{1.10.10}
$$

This equation holds for all continuous functions $w \in \mathcal{U}$. 

### Implications for the Solution of the Green's Function Problem:

1. **Continuous Functions**: The requirement that $w$ be continuous is crucial because the Dirac delta function $\delta_{y}$ acts on continuous functions by ""sifting out"" the value of $w$ at the point $y$. If $w$ were discontinuous at $y$, the value would be ambiguous, which is why attention is restricted to continuous functions.

2. **Green's Function**: The function $g$ in the variational equation represents the Green's function, which is a solution to the differential equation involving the delta function. The variational formulation allows us to express the problem in terms of functionals, which can be advantageous for numerical methods like the finite element method.

3. **Existence of Solutions**: The variational equation indicates that the solution $g$ must satisfy the condition that when paired with any continuous function $w$, it yields the value of $w$ at the point $y$. This establishes a direct relationship between the Green's function and the behavior of the solution at specific points, which is essential for ensuring that the solution is accurate at the nodes of the finite element mesh.

4. **Square-Integrability**: The context mentions that the square-integrability of derivatives of functions in $\mathcal{U}$ implies their continuity. This is significant because it ensures that the functions we are working with in the variational formulation are well-behaved, which is necessary for the convergence and stability of numerical solutions.

In summary, the variational equation encapsulates the relationship between the Green's function and continuous functions, ensuring that the solution behaves correctly at the specified points, which is fundamental for the accuracy of the finite element method."
34,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{,

 Additional context 2: u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0",75,"Using Taylor's formula with remainder, explain how the accuracy of derivatives in finite element methods can be assessed. What does the order of convergence imply about the accuracy of the numerical solution as the mesh is refined?","To assess the accuracy of derivatives in finite element methods using Taylor's formula with remainder, we start by considering the Taylor expansion of a function $u$ that possesses $k$ continuous derivatives. For two points $y$ and $z$ in the interval, Taylor's formula states:

$$
f(z) = f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c)
$$

where $c$ is a point between $y$ and $z$. This expansion allows us to express the function value at $z$ in terms of its value and derivatives at $y$, along with a remainder term that depends on the derivatives of $f$ at some point $c$.

In the context of finite element methods, the accuracy of the numerical solution can be evaluated by examining the error in the derivative, denoted as $e_{, x}(\alpha)$, which is defined as:

$$
e_{, x}(\alpha) = u_{, x}^{h}(\alpha) - u_{, x}(\alpha)
$$

The analysis shows that as the mesh is refined (i.e., as $h_A$ approaches zero), the error can be expressed in terms of the derivatives of $u$ at some points within the interval. Specifically, if the right-hand side of the error expression is $O(h_A^k)$ for some integer $k$, this indicates the order of convergence or order of accuracy of the numerical method.

For example, if the error in the derivative is $O(h_A)$, it implies first-order convergence, meaning that as the mesh is refined, the error decreases linearly. Conversely, if the error is $O(h_A^2)$, it indicates second-order convergence, suggesting that the error decreases quadratically as the mesh is refined.

The corollary presented in the context states that at the midpoints of the elements, the derivatives are second-order accurate:

$$
e_{, x}\left(x_{A+1/2}\right) = \frac{h_A^2}{24} u_{, xxx}(c), \quad c \in [x_A, x_{A+1}]
$$

This means that the numerical solution's accuracy improves significantly at these midpoints, allowing for a more precise approximation of the derivatives compared to other points within the element.

In summary, the order of convergence implies that as the mesh is refined, the accuracy of the numerical solution improves, with higher-order convergence leading to a more rapid decrease in error. This understanding is crucial for ensuring that finite element methods yield reliable and accurate results in practical applications."
35,"see why attention is restricted to continuous functions- $\delta$, sifts out the value of $w$ at $y$. If $w$ were discontinuous at $y$, its value would be ambiguous. In mechanics, we think of $\delta_{y}$, visually as representing a concentrated force of unit amplitude located at point $y$. The Green's function problem corresponding to ( $S$ ) may be stated as follows: Find a function $g$ (i.e., the Green's function) such that \begin{align*} g_{, x x}+\delta_{y} & =0 \quad \text { on } \Omega \tag{1.10.2}\\ g(1) & =0 \tag{1.10.3}\\ g_{, x}(0) & =0 \tag{1.10.4} \end{align*} Note that (1.10.2)-(1.10.4) are simply $(S)$ in which $f$ is replaced by $\delta_{y}$, and $q$ and $h$ are taken to be zero. This problem may be solved by way of formal calculations with distributions, or generalized functions, such as $\delta_{y}$. (The theory of distributions is dealt with in Stakgold [5]. A good elementary account of formal calculations with distributions is presented in Popov [9]. This latter reference is recommended to readers having had no previous experience with this topic.) To this end we note that the (formal) integral of $\delta_{y}$ is the Heaviside, or unit step, function: \[ H_{y}(x)=H(x-y)= \begin{cases}0, & x<y \tag{1.10.5}\\ 1, & x>y\end{cases} \] The integral of $\dot{H}_{y}$ is the Macaulay bracket: \[ \langle x-y\rangle=\left\{\begin{array}{cl} 0, & x \leq y \tag{1.10.6}\\ x-y, & x>y \end{array}\right. \] The preceding functions are depicted in Fig. 1.10.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(1)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25(2)}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-25} Figure 1.10.1 Elementary generalized functions (distributions). To solve the Green's function problem, (1.10.2) is integrated, making use of (1.10.5), to obtain: \begin{equation*} g_{, x}+H_{y}=c_{1} \tag{1.10.7} \end{equation*} where $c_{1}$ is a constant of integration. A second integration and use of (1.10.6) yields \begin{equation*} g(x)+\langle x-y\rangle=c_{1} x+c_{2} \tag{1.10.8} \end{equation*} where $c_{2}$ is another constant of integration. Evaluation of $c_{1}$ and $c_{2}$ is performed by requiring (1.10.7) and (1.10.8) to satisfy the boundary conditions. This results in (see Fig. 1.10.2) \begin{equation*} g(x)=(1-y)-\langle x-y\rangle \tag{1.10.9} \end{equation*} Observe that $g$ is piecewise linear. Thus if $y=x_{A}$ (i.e., if $y$ is a node), $g \in \mathcal{U}^{h}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26}\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-26(1)} Figure 1.10.2 Green's function.\\ In the ensuing analysis we will need the variational equation corresponding to the Green's function problem. This can be deduced from ( $W$ ) by replacing $u$ by $g, f$ by $\delta_{y}$, and $g$ and $h$ by 0 , viz., \begin{equation*} a(w, g)=\left(w, \delta_{y}\right)=w(y) \tag{1.10.10} \end{equation*} Equation (1.10.10) holds for all continuous $w \in \mathcal{U}$. The square-integrability of derivatives of functions $w \in \mathcal{U}$ actually implies the continuity of all $w \in \mathcal{U}$ by a well-known theorem in analysis due to Sobolev. (This result is true only in one dimension. The square-integrability of second derivatives is also required to ensure the continuity of functions defined on two- and three-dimensional domains.) Theorem. $u^{h}\left(x_{A}\right)=u\left(x_{A}\right), A=1,2, \ldots, n+1$ (i.e., $u^{h}$ is exact at the nodes). To prove the theorem, we need to establish two preliminary results. Lemma 1. $a\left(u-u^{h}, w^{h}\right)=0$ for all $w^{h} \in \mathcal{U}^{h}$.\\ Proof. We have observed previously that $\mathcal{U}^{h} \in \mathcal{U}$, so we may replace $w$ by $w^{h}$ in the variational equation: \begin{equation*} a\left(w^{h}, u\right)=\left(w^{h}, f\right)+w^{h}(0) h \tag{1.10.11} \end{equation*} Equation (1.10.11) holds for all $w^{h} \in \mathcal{U}^{h}$. Recall that the Galerkin equation is identical to (1.10.11) except that $u^{h}$ appears instead of $u$. Subtracting the Galerkin equation\\ from (1.10.11) and using the bilinearity and symmetry of $a(\cdot, \cdot)$ yields the required result. Lemma 2. $u(y)-u^{h}(y)=a\left(u-u^{h}, g\right)$, where $g$ is the Green's function.\\ Proof $$ \begin{aligned} u(y)-u^{h}(y) & =\left(u-u^{h}, \delta_{y}\right) & & \text { (definition of } \left.\delta_{y}\right) \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{, x}^{h}(x)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}, \quad x \in\right] x_{A}, x_{A+1}[ \tag{1.10.14} \end{equation*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-28} \end{center} Figare 1.10.3\\ Theorem. Assume $u$ is continuously differentiable. Then there exists at least one point in $] x_{A}, x_{A+1}[$ at which (1.10.14) is exact. Proof. By the mean value theorem, there exists a point $c \in] x_{A}, x_{A+1}[$ such that \begin{equation*} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.15} \end{equation*} (We have used (1.10.13) with $u, x_{A}$, and $x_{A+1}$, in place of $f, y$, and $z$, respectively.) Since $u\left(x_{A}\right)=u^{h}\left(x_{A}\right)$ and $u\left(x_{A+1}\right)=u^{h}\left(x_{A+1}\right)$, we may rewrite (1.10.15) as \begin{equation*} \frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}=u_{, x}(c) \tag{1.10.16} \end{equation*} Comparison of (1.10.16) with (1.10.14) yields the desired result. \subsection*{Remarks} \begin{enumerate} \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference","

 Additional context 0: \\ & =a\left(u-u^{h}, g\right) & & \text { (by (1.10.10)) } \end{aligned} $$ Note that line 2 is true since $u-u^{h}$ is in $\mathcal{U}$.\\ Proof of Theorem. As we have remarked previously, if $y=x_{A}$, a node, $g \in \mathcal{U}^{h}$. Let us take this to be the case. Then $$ \begin{aligned} u\left(x_{A}\right)-u^{h}\left(x_{A}\right) & =a\left(u-u^{h}, g\right) & & \text { (Lemma 2) } \\ & =0 & & \text { (Lemma 1) } \end{aligned} $$ The theorem is valid for $A=1,2, \ldots, n+1$. Strang and Fix [6] attribute this argument to Douglas and Dupont. Results of this kind, embodying exceptional accuracy characteristics, are often referred to as superconvergence phenomena. However, the reader should appreciate that, in more complicated situations, we will not be able, in practice, to guarantee nodal exactness. Nevertheless, as we shall see later on, weighted residual procedures provide a framework within which optimal accuracy properties of some sort may often be guaranteed. \section*{Accuracy of the Derivatives} In considering the convergence properties of the derivatives, certain elementary notions of numerical analysis arise. The reader should make sure that he or she has a complete understanding of these ideas as they subsequently arise in other contexts. We begin by introducing some preliminary mathematical results. \subsection*{Taylor's Formula with Remainder} Let $f:[0,1] \rightarrow \mathbb{R}$ possess $k$ continuous derivatives and let $y$ and $z$ be two points in $[0,1]$. Then there is a point $c$ between $y$ and $z$ such that \begin{align*} f(z) = & f(y) + (z-y) f_{,x}(y) + \frac{1}{2}(z-y)^{2} f_{,xx}(y) \\ & + \frac{1}{3!}(z-y)^{3} f_{,xxx}(y) + \cdots + \tag{1.10.12} \\ & + \frac{1}{k!}(z-y)^{k} f_{,\underbrace{x \dots x}_{k \text{ times}}}(c) \end{align*} The proof of this formula may be found in [7]. Equation (1.10.12) is sometimes called a finite Taylor expansion. \subsection*{Mean-Value Theorem} The mean-value theorem is a special case of (1.10.12) which is valid as long as $k \geq 1$ (i.e., $f$ is continuously differentiable): \begin{equation*} f(z)=f(y)+(z-y) f_{, x}(c) \tag{1.10.13} \end{equation*} Consider a typical subinterval $\left[x_{A}, x_{A+1}\right]$. We have already shown that $u^{h}$ is exact at the endpoints (see Fig. 1.10.3). The derivative of $u^{\boldsymbol{h}}$ in $] x_{A}, x_{A+1}[$ is constant: \begin{equation*} \left.u_{,

 Additional context 1: for the reader to consult this section again after completing the rest of the chapter to make sure a full comprehension of the material is attained. The first step in developing the method is to construct finite-dimensional approximations of $f$ and $\mathcal{U}$. These collections of functions are denoted by $f^{h}$ and $\mathcal{U}^{h}$, respectively. The superscript refers to the association of $f^{h}$ and $\mathcal{U}^{h}$ with a \textit{\textbf{mesh}}, or \textit{\textbf{discretization}}, of the domain $\Omega$, which is parameterized by a characteristic length scale $h$. We wish to think of $f^{h}$ and $\mathcal{U}^{h}$ as being subsets of $f$ and $\mathcal{U}$, respectively. This is written as \begin{align*} f^{h} \subset f & \text { (i.e., if } \left.u^{h} \in f^{h}, \text { then } u^{h} \in f\right) \tag{1.5.1}\\ \mathcal{U}^{h} \subset \mathcal{U} & \text { (i.e., if } w^{h} \in \mathcal{U}^{h}, \text { then } w^{h} \in\mathcal{U}) \tag{1.5.2} \end{align*} where the precise meaning is given in parentheses. ${ }^{2}$ Consequences of (1.5.1) and (1.5.2) are (respectively) that if $u^{h} \in f^{h}$ and $w^{h} \in \mathcal{U}^{h}$, then \begin{align*} & u^{h}(1)=q \tag{1.5.3}\\ & w^{h}(1)=0 \tag{1.5.4} \end{align*} The collections, $f, \mathcal{U}, d^{h}$, and $\mathcal{U}^{h}$, are often referred to as \textit{\textbf{function space}}. The terminology space in mathematics usually connotes a linear structure. This has the following meaning: If $c_{1}$ and $c_{2}$ are constants and $v$ and $w$ are in $\mathcal{U}$, then $c_{1} v+c_{2} w$ is also in $\mathcal{U}$. Both $\mathcal{U}$ and $\mathcal{U}^{h}$ are thus seen to possess the property of a linear space. However, this property is clearly not shared by $f$ and $f^{h}$ due to the inhomogeneous boundary condition. For example, if $u_{1}$ and $u_{2}$ are members of $f$, then $u_{1}+u_{2} \notin f$, since $u_{1}(1)+u_{2}(1)=g+g=2 g$ in violation of the definition of $f$. Nevertheless, the terminology function space is still (loosely) applied to $f$ and $f^{h}$. \section*{(Bubnov-) Galerkin Method} Assume the collection $\mathcal{U}^{h}$ is given. Then, to each member $v^{h} \in \mathcal{U}^{h}$, we construct a function $u^{h} \in f^{h}$ by \begin{equation*} u^{h}=v^{h}+g^{h} \tag{1.5.5} \end{equation*} where $g^{\boldsymbol{h}}$ is a given function satisfying the essential boundary

 Additional context 2: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The",80,"In the context of the provided text, explain the concept of superconvergence phenomena. How does the theorem regarding $u^h(x_A) = u(x_A)$ illustrate this phenomenon, and what are the implications for numerical methods in finite element analysis?","Superconvergence phenomena refer to exceptional accuracy characteristics observed in numerical methods, particularly in the context of finite element analysis. Specifically, it indicates that the numerical solution $u^h$ can achieve higher accuracy at certain points (often at the nodes) than would typically be expected based on the overall order of convergence of the method.

The theorem stating that $u^h(x_A) = u(x_A)$ illustrates this phenomenon by demonstrating that the finite element solution $u^h$ is exact at the nodes $x_A$. This means that at these specific points, the numerical approximation matches the exact solution, which is a stronger result than merely having a general convergence rate across the entire domain. 

The implications for numerical methods in finite element analysis are significant:
1. **Accuracy at Nodes**: The guarantee of exactness at the nodes allows for more reliable results when evaluating the solution at these critical points, which is particularly important in engineering applications where nodal values are often of primary interest.
2. **Refinement Strategies**: Understanding where superconvergence occurs can inform strategies for mesh refinement, as it suggests that focusing on the behavior of the solution at nodes can yield better results without necessarily refining the entire mesh.
3. **Error Estimation**: Superconvergence phenomena can enhance error estimation techniques, as knowing that the solution is exact at certain points can help in assessing the overall accuracy of the numerical method.

In summary, superconvergence phenomena highlight the potential for numerical methods to achieve higher accuracy at specific locations, which can be leveraged to improve the effectiveness and reliability of finite element analyses."
36,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{,

 Additional context 2: u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0",80,"What is the significance of the order of convergence in the context of finite element methods, and how does it relate to the accuracy of derivatives?","The order of convergence in the context of finite element methods refers to the rate at which the numerical solution approaches the exact solution as the mesh size (denoted as $h_A$) is refined (i.e., as $h_A \rightarrow 0$). It is characterized by the exponent of $h_A$ in the error term, which indicates how the error decreases with respect to the mesh size.

In the provided context, it is stated that if the right-hand side of the error equation is $O(h_A^2)$, then the method achieves second-order convergence of the derivative. Conversely, if the right-hand side is only $O(h_A)$, the method achieves only first-order convergence. This means that:

1. **First-order convergence** implies that the error decreases linearly with the mesh size. For any point $\alpha \in [x_A, x_{A+1}]$, the accuracy of the derivative is first-order, meaning the error in the derivative is proportional to the mesh size $h_A$.

2. **Second-order convergence** indicates that the error decreases quadratically with the mesh size. Specifically, at the midpoints of the elements, the derivatives are second-order accurate, which means the error in the derivative is proportional to $h_A^2$. This is a significant improvement in accuracy compared to first-order convergence.

Thus, the significance of the order of convergence is that it provides a measure of how quickly the numerical solution improves as the mesh is refined, directly impacting the accuracy of the derivatives computed using the finite element method. Higher-order convergence leads to more accurate derivatives, which is crucial in applications where precise derivative values are necessary, such as in stress analysis in engineering contexts."
37,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: \item This result means that the constant value of $u_{, x}^{h}$ must coincide with $u_{, x}$ somewhere on $] x_{A}, x_{A+1}[$; see Fig. 1.10.4. \item Without knowledge of $u$ we have no way of determining the locations at which the derivatives are exact. The following results are more useful in that they tell us that the midpoints are, in a sense, optimally accurate, independent of $u$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-29} \end{enumerate} Figure 1.10.4 Let $$ e_{, x}(\alpha) \stackrel{\operatorname{def} .}{=} u_{, x}^{h}(\alpha)-u_{, x}(\alpha)=\frac{u^{h}\left(x_{A+1}\right)-u^{h}\left(x_{A}\right)}{h_{A}}-u_{, x}(\alpha) $$ the error in the derivative at $\alpha \in\left[x_{A}, x_{A+1}\right]$. To establish the superiority of the midpoints in evaluating the derivatives, we need a preliminary result. Lemma. Assume $\boldsymbol{u}$ is three times continuously differentiable. Then \begin{align*} e_{, x}(\alpha)= & \left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right)\right] \tag{1.10.17} \end{align*} where $c_{1}$ and $c_{2}$ are in $\left[x_{A}, x_{A+1}\right]$.\\ Proof. Expand $u\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ in finite Taylor expansions about $\alpha \in\left[x_{A}, x_{A+1}\right]$, viz., $$ \begin{aligned} u\left(x_{A+1}\right)= & u(\alpha)+\left(x_{A+1}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A+1}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right), \quad c_{1} \in\left[\alpha, x_{A+1}\right] \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x

 Additional context 1: \\ u\left(x_{A}\right)= & u(\alpha)+\left(x_{A}-\alpha\right) u_{, x}(\alpha)+\frac{1}{2}\left(x_{A}-\alpha\right)^{2} u_{, x x}(\alpha) \\ & +\frac{1}{3!}\left(x_{A}-\alpha\right)^{3} u_{, x x x}\left(c_{2}\right), \quad c_{2} \in\left[x_{A}, \alpha\right] \end{aligned} $$ Subtracting and dividing through by $h_{A}$ yields $$ \begin{aligned} \frac{u\left(x_{A+1}\right)-u\left(x_{A}\right)}{h_{A}}= & u_{, x}(\alpha)+\left(\frac{x_{A+1}+x_{A}}{2}-\alpha\right) u_{, x x}(\alpha) \\ & +\frac{1}{3!h_{A}}\left[\left(x_{A+1}-\alpha\right)^{3} u_{, x x x}\left(c_{1}\right)-\left(x_{A}-\alpha\right)^{3} u_{1, x x x}\left(c_{2}\right)\right] \end{aligned} $$ Replacing $u\left(x_{A+1}\right)$ by $u^{h}\left(x_{A+1}\right)$ and $u\left(x_{A}\right)$ by $u^{h}\left(x_{A}\right)$ in the left-hand side and rearranging terms completes the proof. \section*{Discussion} To determine what (1.10.17) tells us about the accuracy of the derivatives, we wish to think of the situation in which the mesh is being systematically refined (i.e., we let $h_{A}$ approach zero). In this case $h_{A}^{2}$ will be much smaller than $h_{A}$. Thus, for a given $u$, if the right-hand side of $(1.10 .17)$ is $O\left(h_{\mathrm{A}}^{2}\right),{ }^{3}$ the error in the derivatives will be much smaller than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{,

 Additional context 2: u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0",70,"Given the expression for the error in the derivative at the midpoint, $$ e_{, x}(x_{A+1/2}) = rac{h_A^2}{24} u_{, xxx}(c), $$ explain how this relates to the concept of superconvergence in finite element analysis.","The expression for the error in the derivative at the midpoint is given by 

$$ e_{, x}(x_{A+1/2}) = \frac{h_A^2}{24} u_{, xxx}(c), $$ 

where $c$ is a point in the interval $\left[x_{A}, x_{A+1}\right]$. This expression indicates that the error in the derivative at the midpoint is proportional to the square of the mesh size $h_A$. 

In finite element analysis, superconvergence refers to the phenomenon where the numerical solution exhibits a higher order of accuracy at certain points (such as midpoints) than what is generally expected based on the order of the finite element method used. In this case, while the overall method may have a first-order accuracy, the error at the midpoint is shown to be second-order accurate, as indicated by the $h_A^2$ term in the error expression.

This means that at the midpoint, the derivative is not only more accurate than at other points in the interval but also that the error decreases faster as the mesh is refined (i.e., as $h_A \rightarrow 0$). The presence of the $u_{, xxx}(c)$ term suggests that the accuracy is influenced by the third derivative of the function, which is a characteristic of superconvergence.

Thus, the relationship between the error expression and superconvergence is that the midpoint achieves a higher order of accuracy (second-order) compared to the general first-order accuracy of the finite element method, demonstrating the concept of superconvergence in this context."
38,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 2: associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering",60,"In the context of finite element methods, derive the finite element equation associated with node $A$ for the problem $$ u_{, xx} + 	heta = 0 $$ and discuss how it contrasts with the finite difference equation $$ rac{u_{A+1} - 2u_A + u_{A-1}}{h^2} + f_A = 0 $$.","To derive the finite element equation associated with node $A$ for the problem 

$$ u_{, xx} + \phi = 0, $$ 

we start by considering the standard finite element formulation. The weak form of the equation can be obtained by multiplying the equation by a test function $v(x)$ and integrating over the domain. This leads to:

$$ \int_{x_A}^{x_{A+1}} u_{, xx} v \, dx + \int_{x_A}^{x_{A+1}} \phi v \, dx = 0. $$

Using integration by parts on the first term, we have:

$$ \int_{x_A}^{x_{A+1}} u_{, xx} v \, dx = \left[ u_{, x} v \right]_{x_A}^{x_{A+1}} - \int_{x_A}^{x_{A+1}} u_{, x} v_{, x} \, dx. $$

Assuming that the test function $v$ vanishes at the boundaries (which is common in finite element methods), the boundary term becomes zero. Thus, we obtain:

$$ -\int_{x_A}^{x_{A+1}} u_{, x} v_{, x} \, dx + \int_{x_A}^{x_{A+1}} \phi v \, dx = 0. $$

Now, we can express $u$ in terms of the shape functions $N_A$ associated with node $A$:

$$ u(x) = N_A(x) d_A + N_B(x) d_B, $$

where $d_A$ and $d_B$ are the degrees of freedom at nodes $A$ and $B$. Substituting this expression into the weak form and using the properties of the shape functions, we can derive the finite element equation associated with node $A$.

The resulting finite element equation can be expressed in matrix form as:

$$ \boldsymbol{K} \boldsymbol{d} = \boldsymbol{F}, $$

where $\boldsymbol{K}$ is the stiffness matrix, $\boldsymbol{d}$ is the vector of nodal values, and $\boldsymbol{F}$ is the force vector.

Now, contrasting this with the finite difference equation given by 

$$ \frac{u_{A+1} - 2u_A + u_{A-1}}{h^2} + f_A = 0, $$ 

we see several key differences:

1. **Formulation Approach**: The finite difference method uses a direct approximation of the derivatives at discrete points, while the finite element method derives its equations from a variational principle and considers the entire element.

2. **Local vs Global**: The finite difference equation is local, relying solely on the values at nodes $A$, $A+1$, and $A-1$. In contrast, the finite element method incorporates the contributions from the entire element, allowing for more flexibility in handling complex geometries and boundary conditions.

3. **Accuracy and Convergence**: The finite element method can achieve higher-order accuracy at specific points (like midpoints) as indicated in the context, while the finite difference method typically has a fixed order of accuracy based on the stencil used.

In summary, the finite element equation associated with node $A$ is derived from a weak formulation and considers the contributions of the entire element, while the finite difference equation is a straightforward approximation of the second derivative using neighboring node values."
39,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 2: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are",75,"Discuss the implications of the Galerkin finite element solution properties, specifically how it being exact at the nodes and having exact derivatives at certain points influences the overall accuracy of the finite element method.","The properties of the Galerkin finite element solution have significant implications for the overall accuracy of the finite element method (FEM). 

1. **Exactness at Nodes**: The Galerkin finite element solution $u^{h}$ is exact at the nodes. This means that the solution accurately represents the true value of the function at the discrete points where the nodes are located. As a result, any numerical solution derived from the finite element method will match the exact solution at these specific points, ensuring that the method captures the behavior of the solution accurately at the nodes.

2. **Exact Derivatives at Certain Points**: The property that there exists at least one point in each element where the derivative is exact further enhances the accuracy of the FEM. Specifically, it is noted that the derivative is second-order accurate at the midpoints of the elements. This means that not only does the solution match the exact values at the nodes, but the rate of change (derivative) of the solution is also accurately captured at these midpoints. This is particularly important in applications where the behavior of the solution's gradient is critical, such as in stress analysis in materials.

3. **Higher-Order Accuracy**: The ability to achieve higher-order accuracy at specific points (like the midpoints) indicates that the finite element method can provide a more refined approximation of the solution than methods that only guarantee accuracy at the nodes. This is beneficial in problems where the solution may exhibit rapid changes or where high precision is required.

4. **Superconvergence Phenomena**: The context mentions that under certain conditions, the finite difference stencil can also exhibit superconvergence phenomena. This suggests that there are scenarios where the accuracy of the numerical solution can exceed the expected order of convergence, leading to even more precise results. Understanding the conditions under which this occurs can help in optimizing the mesh and the choice of elements in FEM.

In summary, the exactness of the Galerkin finite element solution at the nodes and the exact derivatives at certain points contribute to the overall accuracy and reliability of the finite element method. These properties ensure that the method not only approximates the solution well at discrete points but also captures the behavior of the solution's gradient effectively, which is crucial for many engineering and physical applications."
40,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U}

 Additional context 1: \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and

 Additional context 2: & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} &",90,"Explain the process of Gauss elimination as applied to finite element analysis, detailing both forward reduction and back substitution steps, and how these steps contribute to solving the system of equations $$ oldsymbol{K} oldsymbol{d} = oldsymbol{F} $$ efficiently.","Gauss elimination is a systematic method used to solve systems of linear equations, particularly in the context of finite element analysis. The process consists of two main steps: forward reduction and back substitution.

### Forward Reduction
1. **Initial Setup**: Start with the system of equations represented in matrix form as $$ \boldsymbol{K} \boldsymbol{d} = \boldsymbol{F} $$, where $\boldsymbol{K}$ is the coefficient matrix, $\boldsymbol{d}$ is the vector of unknowns, and $\boldsymbol{F}$ is the force vector.

2. **Augmented Matrix**: Construct the augmented matrix that combines the coefficient matrix and the force vector:
   $$
   \left[ \begin{array}{cccc|c}
   K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\
   K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\
   K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\
   K_{41} & K_{42} & K_{43} & K_{44} & F_4
   \end{array} \right]
   $$

3. **Elimination Process**: 
   - Solve the first equation for $d_1$ and eliminate $d_1$ from the remaining equations. This is done by subtracting appropriate multiples of the first row from the subsequent rows.
   - Repeat this process for the second equation to eliminate $d_2$ from the remaining equations, and so on, until the last equation is reached. This results in an upper triangular form of the matrix.

4. **Resulting Upper Triangular Matrix**: After completing the forward reduction, the augmented matrix will look like:
   $$
   \left[ \begin{array}{cccc|c}
   1 & K'_{12} & K'_{13} & K'_{14} & F'_1 \\
   0 & 1 & K'_{23} & K'_{24} & F'_2 \\
   0 & 0 & 1 & K'_{34} & F'_3 \\
   0 & 0 & 0 & 1 & d_4
   \end{array} \right]
   $$
   This matrix corresponds to the upper triangular system $\boldsymbol{U} \boldsymbol{d} = \boldsymbol{F}'$.

### Back Substitution
1. **Solving for Unknowns**: Once the matrix is in upper triangular form, the next step is back substitution. This involves solving for the unknowns starting from the last equation and moving upwards.
   - Begin with the last equation to find $d_n$.
   - Substitute $d_n$ back into the second-to-last equation to find $d_{n-1}$, and continue this process until $d_1$ is found.

2. **Final Result**: The final augmented matrix after back substitution will look like:
   $$
   \left[ \begin{array}{cccc|c}
   1 & 0 & 0 & 0 & d_1 \\
   0 & 1 & 0 & 0 & d_2 \\
   0 & 0 & 1 & 0 & d_3 \\
   0 & 0 & 0 & 1 & d_4
   \end{array} \right]
   $$
   This indicates that the solution for the vector of unknowns $\boldsymbol{d}$ is now complete.

### Efficiency of the Process
The combination of forward reduction and back substitution allows for an efficient solution of the system of equations. The forward reduction systematically simplifies the problem, reducing the number of equations and unknowns at each step, while back substitution provides a straightforward method to extract the solutions from the simplified system. This method is particularly effective for symmetric, positive-definite matrices, which are common in finite element analysis."
41,"than if the right-hand side is only $O\left(h_{A}\right)$. The exponent of $h_{\mathrm{A}}$ is called the order of convergence or order of accuracy. In the former case we would have second-order convergence of the derivative, whereas in the latter case we would have only first-order convergence. As an example, assume $\alpha \rightarrow x_{A}$. Then $$ e_{, x}\left(x_{A}\right)=\frac{h_{A}}{2} u_{, x x}\left(x_{A}\right)+\frac{h_{A}^{2}}{3!} u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The procedure is as follows: \subsection*{Gauss Elimination} \begin{itemize} \item Solve the first equation for $d_{1}$ and elminate $d_{1}$ from the remaining $n-1$ equations. \item Solve the second equation for $d_{2}$ and eliminate $d_{2}$ from the remaining $n-2$ equations. \item Solve the $n-1$ st equation for $d_{n-1}$ and eliminate $d_{n-1}$ from the $n$th equation. \item Solve the $n$-th equation for $d_{n}$. \end{itemize} The preceding steps are called forward reduction. The original matrix is reduced to upper triangular form. For example, suppose we began with a system of four equations as follows: $$ \left[\begin{array}{llll} K_{11} & K_{12} & K_{13} & K_{14} \\ K_{21} & K_{22} & K_{23} & K_{24} \\ K_{31} & K_{32} & K_{33} & K_{34} \\ K_{41} & K_{42} & K_{43} & K_{44} \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} F_{1} \\ F_{2} \\ F_{3} \\ F_{4} \end{array}\right\} $$ The augmented matrix corresponding to this system is \[ \left[ \begin{array}{cccc|c} K_{11} & K_{12} & K_{13} & K_{14} & F_1 \\ K_{21} & K_{22} & K_{23} & K_{24} & F_2 \\ K_{31} & K_{32} & K_{33} & K_{34} & F_3 \\ K_{41} & K_{42} & K_{43} & K_{44} & F_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{K}} & \underbrace{\phantom{F_1}}_{F} \end{array} \right] \] After the forward reduction, the augmented matrix becomes\\ \[ \left[ \begin{array}{cccc|c} 1 & K'_{12} & K'_{13} & K_{14} & F'_1 \\ 0 & 1 & K'_{23} & K'_{24} & F'_2 \\ 0 & 0 & 1 & K'_{34} & F'_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{U}} & \underbrace{\phantom{F_1}}_{F'} \end{array} \tag{1.11.1} \right] \] corresponding to the upper triangular system $\boldsymbol{U} \boldsymbol{d}=\boldsymbol{F}^{\prime} \cdot{ }^{4}$ It is a simply verified fact that if $\boldsymbol{K}$ is banded, then $\boldsymbol{U}$ will be also. Employing the reduced augmented matrix, proceed as follows: \begin{itemize} \item Eliminate $d_{n}$ from equations $n-1, n-2, \ldots, 1$.\\ \footnotetext{${ }^{4} \text{Primes will be used to denote intermediate quantities throughout this section}.$} \item Eliminate $d_{n-1}$ from equations $n-2, n-3, \ldots, 1$. \item Eliminate $d_{2}$ from the first equation. \end{itemize} This procedure is called back substitution. For example, in the example just given, after back substitution we obtain\\ \[ \left[ \begin{array}{cccc|c} 1 & 0 & 0 & 0 & d_1 \\ 0 & 1 & 0 & 0 & d_2 \\ 0 & 0 & 1 & 0 & d_3 \\ 0 & 0 & 0 & 1 & d_4 \\ \multicolumn{4}{c|}{\underbrace{\phantom{K_{11}\, K_{12}\, K_{13}\, K_{14}}}_{I}} & \underbrace{\phantom{F_1}}_{d} \end{array} \tag{1.11.2} \right] \] corresponding to the identity $1 \boldsymbol{d}=\boldsymbol{d}$. The solution winds up in the last column. \section*{Hand-Calculation Algorthm} In a hand calculation, Gauss elimination can be performed on the augmented matrix as follows. \subsection*{Forward reduction} \begin{itemize} \item Divide row 1 by $K_{11}$. \item Subtract $K_{21} \times$ row 1 from row 2. \item Subtract $K_{31} \times$ row 1 from row 3. \item Subtract $K_{n 1} \times$ row 1 from row $n$. \end{itemize} Consider the example of four equations. The preceding steps reduce the first column to the form $$ \left[\begin{array}{llll|l} 1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{3}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & \boldsymbol{K}_{22}^{\prime \prime} & \boldsymbol{K}_{23}^{\prime \prime} & \boldsymbol{K}_{24}^{\prime \prime} & \boldsymbol{F}_{2}^{\prime \prime} \\ 0 & \boldsymbol{K}_{32}^{\prime} & \boldsymbol{K}_{33}^{\prime 3} & \boldsymbol{K}_{34}^{\prime} & \boldsymbol{F}_{3}^{\prime \prime} \\ \mathbf{0} & \boldsymbol{K}_{42}^{\prime} & \boldsymbol{K}_{43}^{3} & \boldsymbol{K}_{44}^{\prime \prime} & \boldsymbol{F}_{4}^{\prime \prime} \end{array}\right] $$ Note that if $\boldsymbol{K}_{\mathbf{A 1}}=0$, then the computation for the Ath row can be ignored. Now reduce the second column \begin{itemize} \item Divide row 2 by $K_{22}^{\prime \prime}$. \item Subtract $K_{32}^{\prime \prime} \times$ row 2 from row 3. \item Subtract $K_{42}^{n} \times$ row 2 from row 4. \item Subtract $K_{n 2}^{\prime \prime} \times$ row 2 from row $n$. \end{itemize} The result for the example will look like\\ $\left[\begin{array}{cccc|c}1 & \boldsymbol{K}_{12}^{\prime} & \boldsymbol{K}_{13}^{\prime} & \boldsymbol{K}_{14}^{\prime} & \boldsymbol{F}_{1}^{\prime} \\ 0 & 1 & \boldsymbol{K}_{23}^{\prime \prime \prime} & \boldsymbol{K}_{24}^{\prime \prime \prime} & \boldsymbol{F}_{2}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{33}^{\prime \prime \prime} & \boldsymbol{K}_{34}^{\prime \prime \prime} & \boldsymbol{F}_{3}^{\prime \prime \prime} \\ \mathbf{0} & \mathbf{0} & \boldsymbol{K}_{43}^{\prime \prime \prime} & \boldsymbol{K}_{44}^{\prime \prime\prime} & \boldsymbol{F}_{4}^{\prime \prime \prime} \\ & & & & \end{array}\right]$ Note that only the submatrix enclosed in dashed lines is affected in this procedure.\\ Repeat until columns 3 to $n$ are reduced and the upper triangular form (1.11.1) is obtained. \subsection*{Back substitution} \begin{itemize} \item Subtract $K_{n-1, n}^{\prime} \times$ row $n$ from row $n-1$. \item Subtract $K_{n-2, n}^{\prime} \times$ row $n$ from row $n-2$.\\ \vdots \item Subtract $K_{1, n}^{\prime} \times$ row $n$ from row 1 \end{itemize} After these steps the augmented matrix, for this example, will look like $$ \left[\begin{array}{cccc|c} 1 & \bar{K}_{12}^{\prime} & \bar{K}_{3}^{\prime} & 0 & F_{1}^{\prime \prime \prime \prime} \\ 0 & 1 & K_{23}^{\prime} & 0 & F_{2}^{\prime \prime \prime \prime} \\ 0 & 0 & 1 & 0 & d_{3} \\ 0 & 0 & 0 & 1 & d_{4} \end{array}\right] $$ Note that the submatrix enclosed in dashed lines is unaffected by these steps, and, aside from zeroing the appropriate elements of the last column of the coefficient matrix, only the vector $F^{\prime}$ is altered. Now clear the second-to-last column in the coefficient matrix: \begin{itemize} \item Subtract $K_{n-2, n-1}^{\prime} \times$ row $n-1$ from row $n-2$. \item Subtract $K_{n-3, n-1}^{\prime} \times$ row $n-1$ from row $n-3$.\\ \vdots \item Subtract $K_{1 . n-1}^{\prime} \times$ row $n-1$ from row 1. \end{itemize} Again we mention that the only nontrivial calculations are being performed on the last column (i.e., on $\boldsymbol{F}$ ). Repeat as above until columns $\boldsymbol{n}-2, n-3, \ldots, 2$ are cleared. The result is (1.11.2). \subsection*{Remarks} \begin{enumerate} \item In passing we note that the above procedure is not the same as the way one would implement Gauss elimination on a computer, which we shall treat later. In a computer program for Gauss elimination of symmetric matrices we would want all intermediate results to retain symmetry and thus save storage. This can be done by a small change in the procedure. However, it is felt that the given scheme is the clearest for hand calculations. \item The numerical example with which we close this section illustrates the preceding elimination scheme. Note that the band is maintained (i.e., the zeros in the upper right-hand comer of the coefficient matrix remain zero throughout the calculations). The reader is urged to perform the calculations. \end{enumerate} \section*{Example of Gauss ellmination} $$ \left[\begin{array}{rrrr} 1 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{array}\right]\left\{\begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array}\right\}=\left\{\begin{array}{l} 1 \\ 0 \\ 0 \\ 0 \end{array}\right\} $$ \subsection*{Augmented matrix} $$ \left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right] $$ Forward reduction $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & -1 & 2 & 0 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \end{aligned} $$ \subsection*{Back substitution} $$ \begin{aligned} & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & -1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: u_{, x x x}\left(c_{1}\right)=O\left(h_{A}\right) $$ As $\boldsymbol{h}_{A} \rightarrow 0$, the first term dominates. (We have seen from the example calculations in Sec. 1.8 that the endpoints of the subintervals are not very accurate for the derivatives.) Clearly any point $\alpha \in\left[x_{A}, x_{A+1}\right]$ achieves first-order accuracy. We are thus naturally led to asking the question, are there any values of $\alpha$ at which higher-order accuracy is achieved? Corollary. Let $x_{A+1 / 2} \equiv\left(x_{A}+x_{A+1}\right) / 2$ (i.e., the midpoint). Then $$ \begin{aligned} e_{, x}\left(x_{A+1 / 2}\right) & =\frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right] \\ & =O\left(h_{A}^{2}\right) \end{aligned} $$ Proof. By (1.10.17) $$ e_{, x}\left(x_{A+1 / 2}\right)=\frac{h_{A}^{2}}{48}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ By the continuity of $u_{1 x x x}$, there is at least one point $c$ between $c_{1}$ and $c_{2}$ such that $$ u_{. x x x}(c)=\frac{1}{2}\left[u_{, x x x}\left(c_{1}\right)+u_{, x x x}\left(c_{2}\right)\right] $$ Combining these facts completes the proof. \section*{Remarks} \begin{enumerate} \item From the corollary we see that the derivatives are second-order accurate at the midpoints. \end{enumerate} \footnotetext{${ }^{3} \mathrm{~A}$ function $f(x)$ is said to be $O\left(x^{k}\right)$ (i.e., order $x^{k}$ ) if $f(x) / x^{k} \rightarrow$ a constant as $x \rightarrow 0$. For example, $f(x)=x^{k}$ is $O\left(x^{k}\right)$, as is $f(x)=\sum_{j=k}^{k+1} x^{j}, l \geq 0$. But neither is $O\left(x^{k+1}\right)$. (Verify.) } 2. If the exact solution is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ), then $u_{, x x x}=0$ and-by (1.10.17)-the derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0

 Additional context 2: n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$",65,"In the context of finite element methods, what is the significance of the continuity of the third derivative of the function $u$, and how does it affect the accuracy of the finite element solution at midpoints?","The continuity of the third derivative of the function $u$ is significant in the context of finite element methods because it directly influences the accuracy of the finite element solution, particularly at midpoints of the elements. 

From the provided context, we see that if the third derivative $u_{, x x x}$ is continuous, it allows for the following conclusions:

1. **Second-Order Accuracy at Midpoints**: The corollary states that at the midpoint $x_{A+1/2}$, the error in the derivative is given by:
   $$
   e_{, x}\left(x_{A+1/2}\right) = \frac{h_{A}^{2}}{24} u_{, x x x}(c), \quad c \in\left[x_{A}, x_{A+1}\right]
   $$
   This indicates that the error is of order $O(h_{A}^{2})$, which means that the derivative is second-order accurate at the midpoints of the elements.

2. **Existence of Exact Derivative Points**: The continuity of $u_{, x x x}$ ensures that there exists at least one point in each element where the derivative is exact. This is crucial for achieving higher accuracy in the finite element solution.

3. **Implications for Quadratic Functions**: If the exact solution is quadratic (i.e., $u_{, x x x} = 0$), then the derivative is exact at the midpoints. This highlights that the finite element method can achieve exactness under certain conditions, particularly when the function being approximated is smooth enough.

In summary, the continuity of the third derivative of $u$ enhances the accuracy of the finite element solution at midpoints, allowing for second-order convergence and ensuring that the method can yield exact derivatives at specific points, thereby improving the overall reliability of the numerical solution."
42,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 1: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify

 Additional context 2: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are",30,What is the significance of the slope of the graph in part (c) as mentioned in the context of finite element analysis?,"Answer: ""NOT ENOUGH INFO."""
43,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 1: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of

 Additional context 2: associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering",70,Explain the concept of local versus global viewpoints in the finite element method and how they relate to the basis functions used in the analysis.,"In the finite element method (FEM), there are two primary viewpoints: the global viewpoint and the local (or element) viewpoint.

1. **Global Viewpoint**:
   - In the global viewpoint, the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. This perspective emphasizes the mathematical properties of the finite element method, particularly the piecewise smoothness and ""local support"" of the selected basis functions. 
   - The global description includes parameters such as global coordinates, global shape functions, and global node ordering. For example, a linear finite element in the global description consists of a domain defined as $\left[x_{A}, x_{A+1}\right]$, nodes $\left\{x_{A}, x_{A+1}\right\}$, degrees of freedom $\left\{d_{A}, d_{A+1}\right\}$, and shape functions $\left\{N_{A}, N_{A+1}\right\}$.

2. **Local Viewpoint**:
   - The local viewpoint is more traditional in engineering and is particularly useful for the computer implementation of the finite element method and the development of finite elements. In this perspective, calculations for a typical element are standardized using a local set of quantities.
   - The local description involves a domain $\left[\xi_{1}, \xi_{2}\right]$, local nodes $\left\{\xi_{1}, \xi_{2}\right\}$, degrees of freedom $\left\{d_{1}, d_{2}\right\}$, and shape functions $\left\{N_{1}, N_{2}\right\}$. The interpolation function in the local description is given by $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$.

3. **Relation to Basis Functions**:
   - The basis functions used in FEM are characterized by their piecewise smoothness and local support, meaning that they are non-zero only in a neighborhood of their associated nodes. This property is crucial for both the global and local viewpoints as it allows for the construction of the finite element space.
   - The global viewpoint helps establish the overall mathematical framework and properties of the FEM, while the local viewpoint facilitates practical computations and the assembly of the global stiffness matrix and force vector from the contributions of individual elements.

In summary, the global viewpoint provides a comprehensive understanding of the FEM's mathematical properties, while the local viewpoint focuses on the practical aspects of element calculations and implementations, both relying on the characteristics of the basis functions."
44,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 1: associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering

 Additional context 2: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of",80,"Define a finite element in terms of its global and local descriptions, including the associated quantities such as nodes, degrees of freedom, and shape functions.","A finite element can be defined in terms of its global and local descriptions as follows:

**Global Description:**
1. **Domain:** The global domain of the finite element is given by the interval $\left[x_{A}, x_{A+1}\right]$.
2. **Nodes:** The nodes associated with the finite element are $\left\{x_{A}, x_{A+1}\right\}$.
3. **Degrees of Freedom:** The degrees of freedom are represented as $\left\{d_{A}, d_{A+1}\right\}$.
4. **Shape Functions:** The shape functions for the finite element are $\left\{N_{A}, N_{A+1}\right\}$.
5. **Interpolation Function:** The interpolation function is defined as:
   $$
   u^{h}(x) = N_{A}(x) d_{A} + N_{A+1}(x) d_{A+1}, \quad x \in \left[x_{A}, x_{A+1}\right].
   $$

**Local Description:**
1. **Domain:** The local domain of the finite element is given by the interval $\left[\xi_{1}, \xi_{2}\right]$.
2. **Nodes:** The local nodes are $\left\{\xi_{1}, \xi_{2}\right\}$.
3. **Degrees of Freedom:** The degrees of freedom in the local description are $\left\{d_{1}, d_{2}\right\}$.
4. **Shape Functions:** The shape functions in the local description are $\left\{N_{1}, N_{2}\right\}$.
5. **Interpolation Function:** The interpolation function in the local description is defined as:
   $$
   u^{h}(\xi) = N_{1}(\xi) d_{1} + N_{2}(\xi) d_{2}.
   $$

In summary, a finite element consists of a set of quantities that describe its behavior both globally and locally, including its domain, nodes, degrees of freedom, shape functions, and interpolation functions."
45,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 1: this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume

 Additional context 2: associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering",60,"Derive the affine transformation relationship between the global coordinates $x$ and the local coordinates $\xi$ for a linear finite element, and explain the significance of this transformation in the finite element method.","To derive the affine transformation relationship between the global coordinates $x$ and the local coordinates $\xi$ for a linear finite element, we start with the standard practice of defining the local coordinates $\xi_1 = -1$ and $\xi_2 = +1$. The global coordinates are defined over the interval $\left[x_A, x_{A+1}\right]$.

The affine transformation can be expressed as:

$$
\xi(x) = c_1 + c_2 x
$$

where $c_1$ and $c_2$ are constants determined by the conditions:

1. $\xi(x_A) = -1$
2. $\xi(x_{A+1}) = +1$

Substituting these conditions into the transformation gives us the following system of equations:

\[
\begin{align*}
-1 & = c_1 + x_A c_2 \tag{1.12.2} \\
1 & = c_1 + x_{A+1} c_2 \tag{1.12.3}
\end{align*}
\]

To solve for $c_1$ and $c_2$, we can subtract the first equation from the second:

$$
1 - (-1) = (c_1 + x_{A+1} c_2) - (c_1 + x_A c_2)
$$

This simplifies to:

$$
2 = (x_{A+1} - x_A) c_2
$$

Thus, we find:

$$
c_2 = \frac{2}{x_{A+1} - x_A} = \frac{2}{h_A}
$$

where $h_A = x_{A+1} - x_A$. Now substituting $c_2$ back into one of the equations to find $c_1$:

$$
-1 = c_1 + x_A \left(\frac{2}{h_A}\right)
$$

Solving for $c_1$ gives:

$$
c_1 = -1 - \frac{2x_A}{h_A}
$$

Now substituting $c_1$ and $c_2$ back into the affine transformation, we have:

$$
\xi(x) = -1 - \frac{2x_A}{h_A} + \frac{2}{h_A} x = \frac{2x - (x_A + x_{A+1})}{h_A}
$$

Thus, the final expression for the affine transformation is:

$$
\xi(x) = \frac{2x - x_A - x_{A+1}}{h_A} \tag{1.12.3}
$$

The inverse transformation, which maps local coordinates back to global coordinates, is given by:

$$
x(\xi) = \frac{h_A \xi + x_A + x_{A+1}}{2} \tag{1.12.4}
$$

**Significance of the Transformation:**

The affine transformation is significant in the finite element method for several reasons:

1. **Standardization**: It allows for a standardized way to map any element in the global coordinate system to a local coordinate system, simplifying the formulation of the finite element equations.

2. **Local Analysis**: By transforming to local coordinates, the analysis of each element can be performed independently, which is crucial for the assembly of the global stiffness matrix and force vector.

3. **Element Compatibility**: The transformation ensures that the shape functions and interpolation functions can be defined uniformly across different elements, facilitating the implementation of the finite element method in computational algorithms.

4. **Numerical Integration**: It simplifies the numerical integration process over the element domains, as the integration can be performed in the local coordinate system where the limits are constant (from -1 to 1). 

Overall, the affine transformation is a fundamental aspect of the finite element method that enhances both the mathematical formulation and computational efficiency."
46,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number }

 Additional context 1: element stiffness, $\boldsymbol{k}^{e}$, and element force vector, $\boldsymbol{f}^{\boldsymbol{e}}$, may be deduced from these equations: \begin{align*} & k^{e}=\left[k_{a b}^{e}\right], \quad f^{e}=\left\{f_{a}^{e}\right\}, \quad 1 \leq a, b \leq n_{e n} \tag{2.5.6}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}}\left(\nabla N_{a}\right)^{T} \kappa\left(\nabla N_{b}\right) d \Omega \tag{2.5.7}\\ & f_{a}^{e}=\int_{\boldsymbol{\Omega}^{e}} N_{a} \ell d \Omega+\int_{\Gamma_{h}^{e}} N_{a} h d \Gamma-\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \tag{2.5.8} \end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\left(x_{b}^{e}\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all' $x \in \bar{\Omega}^{\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be formed from the element arrays $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \begin{equation*} k^{e}=\int_{\boldsymbol{\Omega}^{e}} B^{T} D B d \Omega \tag{2.5.9} \end{equation*} where, in the present case, \begin{align*} & \underbrace{\boldsymbol{D}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{s d}}=\boldsymbol{\kappa} \tag{2.5.10}\\ & \underbrace{\boldsymbol{B}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{en}}=\left[\boldsymbol{B}_{1}, \boldsymbol{B}_{2}, \ldots, \boldsymbol{B}_{n_{e n}}\right] \tag{2.5.11}\\ & \underbrace{B_{a}}_{n_{s d} \times 1}=\nabla N_{a} \tag{2.5.12} \end{align*} The component version of $(2.5 .9)$ is \begin{equation*} k_{a b}^{e}=\int_{\Omega^{e}} B_{a}^{T} D B_{b} d \Omega \tag{2.5.13} \end{equation*} \subsection*{Exercise 1.} Let\\ \[ \underbrace{d^{e}}_{n_{e n} \times 1}=\left\{d_{a}^{e}\right\}=\left\{\begin{array}{c} d_{1}^{e} \tag{2.5.14}\\ d_{2}^{e} \\ \vdots \\ d_{n_{e n}^{e}}^{e} \end{array}\right\} \] where \begin{equation*}

 Additional context 2: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[",90,"Discuss the process of assembling the global stiffness matrix $K$ and force vector $F$ from the elemental stiffness matrices $k^{e}$ and force vectors $f^{e}$, including the role of the location matrix (LM).","To assemble the global stiffness matrix $K$ and force vector $F$ from the elemental stiffness matrices $k^{e}$ and force vectors $f^{e}$, we follow a systematic process that utilizes the location matrix (LM).

1. **Elemental Contributions**: Each element $e$ contributes its own stiffness matrix $k^{e}$ and force vector $f^{e}$. The elemental stiffness matrix is defined as:
   $$
   k^{e} = \left[k_{a b}^{e}\right], \quad f^{e} = \left\{f_{a}^{e}\right\}
   $$
   where $1 \leq a, b \leq n_{en}$, and $n_{en}$ is the number of element nodes.

2. **Global Assembly**: The global stiffness matrix $K$ and force vector $F$ are constructed by summing the contributions from all elements:
   $$
   K = \sum_{e=1}^{n_{el}} k^{e}, \quad F = \sum_{e=1}^{n_{el}} f^{e}
   $$
   Here, $n_{el}$ is the total number of elements.

3. **Location Matrix (LM)**: The LM array is crucial for determining where the components of the elemental matrices and vectors are placed in the global arrays. The dimensions of the LM array are $n_{en}$ (number of element nodes) by $n_{el}$ (number of elements). The LM array provides a mapping from local node numbers to global equation numbers:
   $$
   A = \mathrm{LM}(a, e) = \begin{cases}
   e & \text{if } a=1 \\
   e+1 & \text{if } a=2
   \end{cases}
   $$
   This means that for each element, the first local node corresponds to the global node number $e$, and the second local node corresponds to $e+1$.

4. **Assembly Procedure**: The assembly of the global stiffness matrix and force vector is done by adding the contributions from each element according to the LM array. For an element $e$, the contributions are added as follows:
   - For the stiffness matrix:
     $$
     K_{ee} \leftarrow K_{ee} + k_{11}^{e}
     $$
     $$
     K_{e, e+1} \leftarrow K_{e, e+1} + k_{12}^{e}
     $$
     $$
     K_{e+1, e} \leftarrow K_{e+1, e} + k_{21}^{e}
     $$
     $$
     K_{e+1, e+1} \leftarrow K_{e+1, e+1} + k_{22}^{e}
     $$
   - For the force vector:
     $$
     F_{e} \leftarrow F_{e} + f_{1}^{e}
     $$
     $$
     F_{e+1} \leftarrow F_{e+1} + f_{2}^{e}
     $$

5. **Handling Prescribed Values**: If an element is at the boundary or has prescribed values, the LM array indicates that certain degrees of freedom are not unknowns, and their contributions are not added to the global matrix and vector.

In summary, the assembly process involves systematically adding the contributions from each element's stiffness matrix and force vector to the global stiffness matrix and force vector, guided by the LM array to ensure correct placement of values."
47,"for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are built up from different classes of functions (i.e., Petrov-Galerkin methods), we would also have to specify a set of weighting functions, say\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-37} } (l2) Nodes: $\left\{\xi_{1}, \xi_{2}\right\}$\\ (l3) Degrees of freedom: $\left\{d_{1}, d_{2}\right\}$\\ (I4) Shape functions: $\left\{N_{1}, N_{2}\right\}$\\ (15) Interpolation function: $$ u^{h}(\xi)=N_{1}(\xi) d_{1}+N_{2}(\xi) d_{2} $$ Note that in the local description, the nodal numbering begins with 1.\\ We shall relate the domains of the global and local descriptions by an ""affine"" transformation $\xi:\left[x_{A}, x_{A+1}\right] \rightarrow\left[\xi_{1}, \xi_{2}\right]$, such that $\xi\left(x_{A}\right)=\xi_{1}$ and $\xi\left(x_{A+1}\right)=\xi_{2}$. It is standard practice to take $\xi_{1}=-1$ and $\xi_{2}=+1$. Thus $\xi$ may be represented by the expression \begin{equation*} \xi(x)=c_{1}+c_{2} x \tag{1.12.1} \end{equation*} where $c_{1}$ and $c_{2}$ are constants which are determined by \[ \left.\begin{array}{rl} -1 & =c_{1}+x_{A} c_{2} \tag{1.12.2}\\ 1 & =c_{1}+x_{A+1} c_{2} \end{array}\right\} \] Solving this system yields \begin{equation*} \xi(x)=\frac{2 x-x_{A}-x_{A+1}}{h_{A}} \tag{1.12.3} \end{equation*} (Recall $h_{A}=x_{A+1}-x_{A}$.) The inverse of $\xi$ is obtained by solving for $x$ : \begin{equation*} x(\xi)=\frac{h_{A} \xi+x_{A}+x_{A+1}}{2} \tag{1.12.4} \end{equation*} In (1.12.1), $\xi$ is a mapping and $x$ is a point, whereas in (1.12.4), $x$ is a mapping and $\xi$ is a point. In the sequel, we adopt the notational convention that subscripts $a, b, c, \ldots$ pertain to the local numbering system. The subscripts $A, B, C, \ldots$ will always pertain to the global numbering system. To control the proliferation of notations, we will frequently use the same notation for the local and global systems (e.g., $d_{a}$ and $d_{A}$ or $N_{a}$ and $N_{A}$ ). This generally should not cause confusion as the context will make clear which point of view is being adopted. If there is danger of confusion, a superscript $e$ will be introduced to denote a quantity in the local description associated with element number $e$ (e.g., $d_{a}^{e}=d_{A}, N_{a}^{e}(\xi)=N_{A}\left(x^{e}(\xi)\right)$, where $x^{e}:\left[\xi_{1}, \xi_{2}\right] \rightarrow$ $\left[x_{1}^{e}, x_{2}^{e}\right]=\left[x_{A}, x_{A+1}\right]$, etc.). In terms of $\xi$, the shape functions in the local description take on a standard form \begin{equation*} N_{a}(\xi)=\frac{1}{2}\left(1+\xi_{a} \xi\right), \quad a=1,2 \tag{1.12.5} \end{equation*} Note also that (1.12.4) may be written in terms of (1.12.5): \begin{equation*} x^{e}(\xi)=\sum_{a=1}^{2} N_{a}(\xi) x_{a}^{e} \tag{1.12.6} \end{equation*} This has the same form as the interpolation function (cf. 15).\\ For future reference, we note the following results: \begin{gather*} N_{a, \xi}=\frac{\xi_{a}}{2}=\frac{(-1)^{a}}{2} \tag{1.12.7}\\ x_{, \xi}^{e}=\frac{h^{e}}{2} \tag{1.12.8} \end{gather*} where $h^{e}=x_{2}^{e}-x_{1}^{e}$ and \begin{equation*} \xi_{, x}^{e}=\left(x_{, \xi}^{e}\right)^{-1}=\frac{2}{h^{e}} \tag{1.12.9} \end{equation*} The local and global descriptions of the eth element are depicted in Fig. 1.12.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-39} Figure 1.12.1 Local and global descriptions of the $e$ th element. \section*{1.13 ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} To develop the element point of view further, let us assume that our model consists of $n_{e l}$ elements, numbered as shown in Figure 1.13.1. Clearly $n_{e l}=n$ for this case. Let us take $e$ to be the variable index for the elements; thus $1 \leq e \leq n_{e l}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-40} Figure 1.13.1\\ Now recall the definitions of the (global) stiffness matrix and force vector\\ \\ \[ K = \underbrace{\left[ K_{AB} \right]}_{n \times n}, \quad F = \underbrace{\left\{ F_A \right\}}_{n \times 1} \tag{1.13.1} \] where \begin{gather*} K_{A B}=a\left(N_{A}, N_{B}\right)=\int_{0}^{1} N_{A, x} N_{B, x} d x \tag{1.13.2}\\ F_{A}=\left(N_{A}, f\right)+\delta_{A 1} h-a\left(N_{A}, N_{n+1}\right) g \\ =\int_{0}^{1} N_{A} f d x+\delta_{A 1} h-\int_{0}^{1} N_{A, x} N_{n+1, x} d x g \tag{1.13.3} \end{gather*} ( $\operatorname{In}(1.13 .3)$ we have assumed $N_{A}\left(x_{1}\right)=\delta_{A 1}$, as for the piecewise linear finite element space.) The integrals over $[0,1]$ may be written as sums of integrals over the element domains. Thus \[ \begin{array}{ll} \boldsymbol{K}=\sum_{e=1}^{n_{e l}} \boldsymbol{K}^{e}, & \boldsymbol{K}^{e}=\left[K_{A B}^{e}\right] \\ \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is","

 Additional context 0: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[

 Additional context 1: \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number }

 Additional context 2: from which it follows that $c_{p}=0$; hence $\boldsymbol{c}=\mathbf{0}$. \section*{Remark} Positive definiteness of $\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\mathcal{V}^{h}$. \subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\boldsymbol{k}^{e}$ and $f^{\boldsymbol{c}}$ : \begin{align*} & \boldsymbol{k}^{e}=\left[k_{p q}^{e}\right], \quad \boldsymbol{f}^{e}=\left\{f_{p}^{e}\right\}, \quad 1 \leq p, q \leq n_{ee}=n_{ed} n_{e n} \tag{2.9.1}\footnotemark\\ & k_{p q}^{e}=e_{i}^{T} \int_{\Omega e} B_{a}^{T} D B_{b} d \Omega e_{j}, \quad p=n_{e d}(a-1)+i, \\ & q=n_{e d}(b-1)+j \tag{2.9.2}\\ & \left(n_{s d}=2\right) \quad B_{a}=\left[\begin{array}{cc} N_{a, 1} & 0 \\ 0 & N_{a, 2} \\ N_{a, 2} & N_{a, 1} \end{array}\right] \tag{2.9.3}\\ & \left(n_{s d}=3\right) \quad \boldsymbol{B}_{a}=\left[\begin{array}{ccc} N_{a, 1} & 0 & 0 \\ 0 & N_{a, 2} & 0 \\ 0 & 0 & N_{a, 3} \\ 0 & N_{a, 3} & N_{a, 2} \\ N_{a, 3} & 0 & N_{a, 1} \\ N_{a, 2} & N_{a, 1} & 0 \end{array}\right] \tag{2.9.4} \end{align*} \footnotetext{$n_{\text{ee}}$ stands for the number of element equations and $n_{\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\text {ed}} \leq n_{\text{dof}}$, although they are usually equal. } and \[ f_{p}^{e}=\int_{\Omega^{e}} N_{a} \ell_{i} d \Omega+\int_{\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \Gamma-\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \quad \begin{array}{r} \Gamma_{h_{i}}^{e}=\Gamma_{h_{i}} \cap \Gamma^{e} \tag{2.9.5}\\ (\text { no sum on } i) \end{array} \] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\left(x_{b}^{e}\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \begin{equation*} \underbrace{\boldsymbol{k}_{ab}^{e}}_{n_{e d} \times n_{e d}}=\int_{\boldsymbol{\Omega}^{e}} B_{a}^{T} \boldsymbol{D} B_{b} d \boldsymbol{\Omega} \tag{2.9.6a} \end{equation*} From (2.9.2) we see that",50,What are the implications of the conditions $K_{A B}^{e}=0$ and $F_{A}^{e}=0$ for the assembly of the global stiffness matrix and force vector in the finite element method?,"The conditions $K_{A B}^{e}=0$ and $F_{A}^{e}=0$ have specific implications for the assembly of the global stiffness matrix $\boldsymbol{K}$ and force vector $\boldsymbol{F}$ in the finite element method:

1. **Condition $K_{A B}^{e}=0$**: This condition indicates that the contributions to the stiffness matrix from element $e$ do not affect the global stiffness matrix $\boldsymbol{K}$ at the indices $A$ and $B$ if $A$ and $B$ do not correspond to the nodes of the element $e$ or its adjacent elements. Specifically, it means that if $A \neq e$ or $e+1$ or $B \neq e$ or $e+1$, then the stiffness contributions from that element do not contribute to the global matrix. This is crucial for maintaining the sparsity of the global stiffness matrix, as it ensures that only relevant interactions between nodes are considered, thus simplifying the assembly process.

2. **Condition $F_{A}^{e}=0$**: Similarly, this condition implies that the force vector contributions from element $e$ do not affect the global force vector $\boldsymbol{F}$ at index $A$ if $A$ does not correspond to the nodes of element $e$ or its adjacent elements. Specifically, if $A \neq e$ or $e+1$, then the force contributions from that element are zero. This condition also helps in maintaining the sparsity of the global force vector and ensures that only the forces acting on the relevant nodes are included in the global vector.

In summary, both conditions ensure that only the contributions from the relevant elements and their associated nodes are included in the global stiffness matrix and force vector, thereby optimizing the assembly process and maintaining the structure of the matrices."
48,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number }

 Additional context 1: $\lambda$ is called the coefficient of heat transfer. This boundary condition applies to the case in which the heat flux is proportional to the difference of the surface temperatures of the body and surrounding medium, the latter formally represented by $h / \lambda$ in (2.5.17). \subsection*{2.6 HEAT CONDUCTION: DATA PROCESSING ARRAYS; \\ ID, IEN, AND LM} The element nodal data is stored in the array IEN, the element nodes array, which relates local node numbers to global node numbers, viz.,\\ \[ \text{IEN}(\underbrace{a}_{\text{Local node number}}, \underbrace{e}_{\text{Element number}}) = \underbrace{A}_{\text{Global node number}} \] The relationship between global node numbers and global equation numbers as well as nodal boundary condition information is stored in the ID array (see 2.4.7). In practice, the IEN and ID arrays are set up from input data. The LM array, which was described in the context of the one-dimensional model problem in Sec. 1.14, may then be constructed from the relation\\ \begin{equation*} \operatorname{LM}(a, e)=\operatorname{ID}(\operatorname{IEN}(a, e)) \tag{2.6.2} \end{equation*} Because of the previous relationship, we often think of LM as the element ""localization"" of ID. Strictly speaking the LM array is redundant. However, it is generally convenient in computing to set up LM once and for all, rather than make use of (2.6.2) repeatedly. Example 1 illustrates the structure of the ID, IEN, and LM arrays. \section*{Example 1} Consider the mesh of four-node, rectangular elements shown in Fig. 2.6.1. We assume that the local node numbering begins at the lower left-hand node of each element and proceeds in counterclockwise fashion. This is illustrated in Fig. 2.6.1 for element 2, which is typical. We also assume that essential boundary conditions (i.e., "" $g$-type"") are specified at nodes $1,4,7$, and 10 . Thus there will only be eight equations in the global system $\boldsymbol{K} d=\boldsymbol{F}$. We adopt the usual convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. The ID, IEN, and LM arrays are given in Fig. 2.6.2. The reader is urged to verify the details.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-16} Figure 2.6.1 Mesh of four-node, rectangular elements; global and local node numbers, element numbers, and essential

 Additional context 2: and $\boldsymbol{f}^{\boldsymbol{e}}$, respectively, by way of an assembly algorithm as outlined in Sec. 1.14. \subsection*{Exercise 1.} Let \begin{align*} & \underset{n_{ee} \times 1}{d^{e}}=\left\{d_{a}^{e}\right\}=\left\{\begin{array}{c} d_{1}^e \\ d_{2}^{e} \\ \vdots \\ d^{e}_{n_{en}} \end{array}\right\} \tag{2.9.10}\\ & \left(n_{\text{ed }}=2\right) \quad d_{a}^{e}=\left\{\begin{array}{l} d_{1a}^e \\ d_{2a}^{e} \end{array}\right\} \tag{2.9.11}\\ & \left(n_{e d}=3\right) \quad d_{a}^{e}=\left\{\begin{array}{l} d_{1a}^e \\ d_{2a}^e \\ d_{3a}^e \end{array}\right\} \tag{2.9.12} \end{align*} where \begin{equation*} d_{i a}^{e}=u_{i}^{h}\left(x_{a}^{e}\right) \tag{2.9.13} \end{equation*} $d^{e}$ is called the element displacement vector. Show that the stress vector (see Exercise 4, Sec. 2.7.) at point $x \in \Omega^{\boldsymbol{c}}$ can be calculated from the formula \begin{equation*} \sigma(x)=D(x) B(x) d^{e}=D(x) \sum_{a=1}^{n_{e n}} B_{a}(x) d_{a}^{e} \tag{2.9.14} \end{equation*} \subsection*{2.10 ELASTOSTATICS: DATA PROCESSING ARRAYS ID, IEN, AND LM} We have already noted that the definition of the ID array must be generalized for the present case as indicated in Sec. 2.8. We iniust also generalize our definition of the LM array. However, the IEN array remains the same as before. In the present and fully general cases, the LM array is three-dimensional, with dimensions $\boldsymbol{n}_{e d} \times \boldsymbol{n}_{e n} \times \boldsymbol{n}_{e l}$, and is defined by\\ \[ \text{LM}(i, a, e) = \text{ID}(i, \text{IEN}(a, e)) \] \begin{itemize} \item $i$: Degrees of freedom number \item $a$: Local node number \item $e$: Element number \end{itemize} Alternatively, it is sometimes convenient to think of LM as two-dimensional, with dimensions $n_{e e} \times n_{e l}$, viz., ${ }^{17}$ \begin{align*} & \mathrm{LM}(\underbrace{p}_{\text{Local equation number}}, \underbrace{e}_{\text{Element number}})=\mathrm{LM}(i, a, e), \quad p=n_{e d}(a-1)+i \end{align*} To see how everything works in practice, it is helpful to run through a simple example. \section*{Example 1} Consider the mesh of four-node, rectangular elements illustrated in Fig. 2.10.1. We assume that the local node numbering begins in the lower left-hand corner for each element and proceeds in counterclockwise fashion. \footnotetext[16]{The",70,"What is the purpose of the location matrix (LM) in the finite element method, and how is it constructed for a given problem?","The purpose of the location matrix (LM) in the finite element method is to store assembly information that relates local degrees of freedom of elements to their corresponding global equation numbers. This allows for the proper assembly of the global stiffness matrix $\boldsymbol{K}$ and global force vector $\boldsymbol{F}$ from the elemental contributions.

The LM array is constructed based on the number of element nodes ($n_{\text{en}}$) and the number of elements ($n_{el}$). In the given context, for a problem with 2 nodes per element, the dimensions of the LM array are $2 \times n_{el}$. The construction of the LM array is defined by the following rules:

1. For a particular degree of freedom number $a$ and an element number $e$, the value returned by the LM array is the corresponding global equation number $A$, given by:
   $$
   A = \mathrm{LM}(a, e) = 
   \begin{cases} 
   e & \text{if } a = 1 \\ 
   e + 1 & \text{if } a = 2 
   \end{cases}
   $$

2. The LM array is filled such that:
   - For the first node of each element (when $a = 1$), the global equation number corresponds directly to the element number $e$.
   - For the second node of each element (when $a = 2$), the global equation number is one greater than the element number, $e + 1$.

3. Additionally, if the second degree of freedom of the last element ($n_{el}$) is prescribed (not an unknown), it is indicated by setting $\mathrm{LM}(2, n_{el}) = 0$. This means that the corresponding terms for this degree of freedom will not be assembled into the global matrices.

Overall, the LM array serves as a crucial tool for organizing and managing the relationships between local and global degrees of freedom in the finite element analysis process."
49,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number }

 Additional context 1: \begin{cases}0, & \text { if } \mathrm{LM}(i, a, e) \neq 0 \tag{2.10.3}\\ g_{i A}, & \text { where } A=\operatorname{IEN}(a, e), \text { if } \mathrm{LM}(i, a, e)=0\end{cases} \] This definition may be easily programmed.\\ ${ }^{18}$ In practice, equation numbers are often renumbered internally to minimize the bandwidth of $\boldsymbol{K}$ and thus decrease storage and solution effort. This is especially important in analyzing large-scale systems involving tens of thousands of equations. An algorithm for reducing bandwidth is presented in [8]. \includegraphics[max width=\textwidth, center]{images/chapter2.10.2.png} \section*{Example 2} As a final example, we consider a typical four-node, elasticity element in some large mesh; see Fig. 2.10.3. We assume the pertinent entries of the ID array are given as follows: \[ \left.\begin{array}{l} \operatorname{ID}(1,32)=0 \\ \operatorname{ID}(2,32)=0 \\ \operatorname{ID}(1,59)=115 \\ \operatorname{ID}(2,59)=116 \\ \operatorname{ID}(1,164)=0 \tag{2.10.4}\\ \operatorname{ID}(2,164)=325 \\ \operatorname{ID}(1,168)=332 \\ \operatorname{ID}(2,168)=333 \end{array}\right\} \] The entries of IEN follow from Fig. 2.10.3: \[ \left.\begin{array}{l} \operatorname{IEN}(1, e)=164 \\ \operatorname{IEN}(2, e)=32 \\ \operatorname{IEN}(3, e)=168 \tag{2.10.5}\\ \operatorname{IEN}(4, e)=59 \end{array}\right\} \] \includegraphics[max width=\textwidth, center]{2024_10_04_037012b2cd72c3baccfbg-39}\\ (i) - Local node numbers Tigure 2.10.3 Typical four-node elasticity element; global and local node numbers. Combining (2.10.4) and (2.10.5), by way of (2.10.1), yields entries of the LM array: \[ \left.\begin{array}{l} \operatorname{LM}(1,1, e)=0 \tag{2.10.6}\\ \operatorname{LM}(2,1, e)=325 \\ \operatorname{LM}(1,2, e)=0 \\ \operatorname{LM}(2,2, e)=0 \\ \operatorname{LM}(1,3, e)=332 \\ \operatorname{LM}(2,3, e)=333 \\ \operatorname{LM}(1,4, e)=115 \\ \operatorname{LM}(2,4, e)=116 \end{array}\right\} \] The contribution to the global arrays may be deduced from LM:\\ Stiffness (due to symmetry, only the upper triangular portion need be assembled.) \[ \left.\begin{array}{l} K_{115,115} \leftarrow K_{115,115}+k_{77}^{e} \\ K_{115,116}^{e} \leftarrow K_{115,116}+k_{78}^{e} \\ K_{115,325} \leftarrow K_{115,325}+k_{72}^{e} \\ K_{115,332} \leftarrow

 Additional context 2: with dimensions $n_{e e} \times n_{e l}$, viz., ${ }^{17}$ \begin{align*} & \mathrm{LM}(\underbrace{p}_{\text{Local equation number}}, \underbrace{e}_{\text{Element number}})=\mathrm{LM}(i, a, e), \quad p=n_{e d}(a-1)+i \end{align*} To see how everything works in practice, it is helpful to run through a simple example. \section*{Example 1} Consider the mesh of four-node, rectangular elements illustrated in Fig. 2.10.1. We assume that the local node numbering begins in the lower left-hand corner for each element and proceeds in counterclockwise fashion. \footnotetext[16]{The $g$-boundary conditions are accounted for in this definition.\\ ${ }^{17}$ The reader knowledgeable in FORTRAN will realize that the intemal computer storage of (2.10.1) and (2.10.2) is identical. }This is shown for element 4, which is typical. Four displacement (i.e., "" $g$-type"") boundary conditions are specified; namely, the horizontal displacement is specified at nodes 1 and 10 , and the vertical displacement is specified at nodes 1 and 3. Since $n_{\text {np}}=12, n_{\text {dof }}=n_{\text {ed }}=2$, and 4 displacement degrees of freedom are specified, we have $n_{e q}=20$. As is usual, we adopt the convention that the global equation numbers run in ascending order with respect to the ascending order of global node numbers. ${ }^{18}$ The ID, IEN, and LM arrays are given in Figure 2.10.2. The reader is urged to verify the results.\\ \includegraphics[max width=\textwidth, center]{images/chapter2.10.1.png} Figure 2.10.1 Mesh of four-node, rectangular, elasticity elements; global and local node numbers, element numbers, and displacement boundary conditions. In terms of the IEN and LM arrays, a precise definition of the $g_{p}^{e}$ 's may be given (see (2.9.5)): \[ g_{p}^{e}=g_{i a}^{e}= \begin{cases}0, & \text { if } \mathrm{LM}(i, a, e) \neq 0 \tag{2.10.3}\\ g_{i A}, & \text { where } A=\operatorname{IEN}(a, e), \text { if } \mathrm{LM}(i, a, e)=0\end{cases} \] This definition may be easily programmed.\\ ${ }^{18}$ In practice, equation numbers are often renumbered internally to minimize the bandwidth of $\boldsymbol{K}$ and thus decrease storage and solution effort. This is especially important in analyzing large-scale systems involving tens of thousands of equations. An algorithm for reducing bandwidth is presented in [8]. \includegraphics[max width=\textwidth,",80,"Given the LM array, how do you determine the global equation number $A$ for a specific degree of freedom number $a$ and element number $e$? Provide the mathematical expression used to compute $A$.","To determine the global equation number $A$ for a specific degree of freedom number $a$ and element number $e$, you can use the following mathematical expression derived from the LM array:

$$
A = \mathrm{LM}(a, e) = 
\begin{cases} 
e & \text{if } a = 1 \\ 
e + 1 & \text{if } a = 2 
\end{cases}
$$

This expression indicates that for degree of freedom 1, the global equation number is equal to the element number $e$, and for degree of freedom 2, it is equal to the element number $e$ plus 1."
50,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[

 Additional context 1: \boldsymbol{F}=\sum_{e=1}^{n_{e l}} \boldsymbol{F}^{e}, & \boldsymbol{F}^{e}=\left\{F_{\hat{A}}^{e}\right\} \tag{1.13.5} \end{array} \] where \begin{align*} K_{A B}^{e} & =a\left(N_{A}, N_{B}\right)^{e}=\int_{\mathbf{Q}^{e}} N_{A, x} N_{B, x} d x \tag{1.13.6}\\ F_{A}^{e} & =\left(N_{A}, f\right)^{e}+\delta_{e 1} \delta_{A 1} h-a\left(N_{A}, N_{n+1}\right)^{e} g \\ & =\int_{\Omega^{e}} N_{A} f d x+\delta_{e 1} \delta_{A 1} h-\int_{\Omega^{e}} N_{A, x} N_{n+1, x} d x g \tag{1.13.7} \end{align*} and $\Omega^{e}=\left[x_{1}^{e}, x_{2}^{e}\right]$, the domain of the eth element.\\ The important observation to make is that $\boldsymbol{K}$ and $\boldsymbol{F}$ can be constructed by summing the contributions of elemental matrices and vectors, respectively. In the literature, this procedure is sometimes called the direct stiffmess method [10]. By the definitions of the $N_{A}$ 's, we have that \begin{equation*} K_{A B}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \text { or } B \neq e \text { or } e+1 \tag{1.13.8} \end{equation*} and \begin{equation*} F_{A}^{e}=0, \quad \text { if } A \neq e \text { or } e+1 \tag{1.13.9} \end{equation*} The situation for a typical element, $e$, is shown in Fig. 1.13.2. In practice we would not, of course, add in the zeros but merely add in the nonzero terms to the appropriate locations. For this purpose it is useful to define the eth element stiffiness matrix $k^{e}$ and element force vector $f^{e}$ as follows: \begin{align*} & k^{e}=\underbrace{\left[k_{a b}^{e}\right]}_{2 \times 2}, \quad f^{e}=\underbrace{\left\{f_{a}^{e}\right\}}_{2 \times 1} \tag{1.13.10}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}} N_{a, x} N_{b, x} d x \tag{1.13.11}\\ & f_{a}^{e}=\int_{\Omega^{e}} N_{a} f d x+\left\{\begin{array}{cl} \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires

 Additional context 2: \delta_{a 1} h & e=1 \\ 0 & e=2,3, \ldots, n_{e l}-1 \\ -k_{a 2 }^{e} g & e=n_{e l} \end{array}\right. \tag{1.13.12} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-41} \end{center} Here $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ are defined with respect to the local ordering, whereas $\boldsymbol{K}^{e}$ and $\boldsymbol{F}^{e}$ are defined with respect to the global ordering. To determine where the components of $k^{e}$ and $f^{e}$ ""go"" in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively, requires keeping additional information. This is discussed in the following section. \section*{1.14 ASSEMBLY OF GLOBAL STIFFNESS MATRIX AND FORCE VECTOR; LM ARRAY} In a finite element computer program, it is the task of a ""finite element subroutine"" to produce $k^{e}$ and $f^{e}, e=1,2, \ldots, n_{e l}$, from given data and to provide an ""assembly subroutine"" enough information so that the terms in $\boldsymbol{k}^{e}$ and $\boldsymbol{f}^{e}$ can be added to the appropriate locations in $\boldsymbol{K}$ and $\boldsymbol{F}$, respectively. This assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number }",90,Explain the assembly procedure for the stiffness matrix $oldsymbol{K}$ and force vector $oldsymbol{F}$ using the contributions from the eth element. What are the specific equations involved in this assembly process?,"The assembly procedure for the stiffness matrix $\boldsymbol{K}$ and force vector $\boldsymbol{F}$ using the contributions from the eth element involves adding the elemental contributions to the appropriate locations in the global matrices. The assembly process is guided by the location matrix (LM), which indicates where each element's contributions should be placed in the global matrices.

For the eth element, where $1 \leq e \leq n_{el-1}$, the assembly procedure is as follows:

1. **Stiffness Matrix Assembly**:
   - The contributions to the stiffness matrix from the eth element are given by the following equations:
   $$
   K_{ee} \leftarrow K_{ee} + k_{11}^{e} \tag{1.14.2}
   $$
   $$
   K_{e, e+1} \leftarrow K_{e, e+1} + k_{12}^{e} \tag{1.14.3}
   $$
   $$
   K_{e+1, e} \leftarrow K_{e+1, e} + k_{21}^{e} \tag{1.14.4}
   $$
   $$
   K_{e+1, e+1} \leftarrow K_{e+1, e+1} + k_{22}^{e} \tag{1.14.5}
   $$

2. **Force Vector Assembly**:
   - The contributions to the force vector from the eth element are given by:
   $$
   F_{e} \leftarrow F_{e} + f_{1}^{e} \tag{1.14.6}
   $$
   $$
   F_{e+1} \leftarrow F_{e+1} + f_{2}^{e} \tag{1.14.7}
   $$

For the last element, $n_{el}$, the assembly is slightly different:
- The contributions to the stiffness matrix and force vector are:
$$
K_{nn} \leftarrow K_{nn} + k_{11}^{n_{el}} \tag{1.14.8}
$$
$$
F_{n} \leftarrow F_{n} + f_{1}^{n_{el}} \tag{1.14.9}
$$

In summary, the assembly process involves updating the global stiffness matrix $\boldsymbol{K}$ and force vector $\boldsymbol{F}$ by adding the contributions from the local elemental stiffness matrix $k^{e}$ and force vector $f^{e}$ at the appropriate indices as determined by the LM array."
51,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[

 Additional context 1: element stiffness, $\boldsymbol{k}^{e}$, and element force vector, $\boldsymbol{f}^{\boldsymbol{e}}$, may be deduced from these equations: \begin{align*} & k^{e}=\left[k_{a b}^{e}\right], \quad f^{e}=\left\{f_{a}^{e}\right\}, \quad 1 \leq a, b \leq n_{e n} \tag{2.5.6}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}}\left(\nabla N_{a}\right)^{T} \kappa\left(\nabla N_{b}\right) d \Omega \tag{2.5.7}\\ & f_{a}^{e}=\int_{\boldsymbol{\Omega}^{e}} N_{a} \ell d \Omega+\int_{\Gamma_{h}^{e}} N_{a} h d \Gamma-\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \tag{2.5.8} \end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\left(x_{b}^{e}\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all' $x \in \bar{\Omega}^{\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be formed from the element arrays $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \begin{equation*} k^{e}=\int_{\boldsymbol{\Omega}^{e}} B^{T} D B d \Omega \tag{2.5.9} \end{equation*} where, in the present case, \begin{align*} & \underbrace{\boldsymbol{D}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{s d}}=\boldsymbol{\kappa} \tag{2.5.10}\\ & \underbrace{\boldsymbol{B}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{en}}=\left[\boldsymbol{B}_{1}, \boldsymbol{B}_{2}, \ldots, \boldsymbol{B}_{n_{e n}}\right] \tag{2.5.11}\\ & \underbrace{B_{a}}_{n_{s d} \times 1}=\nabla N_{a} \tag{2.5.12} \end{align*} The component version of $(2.5 .9)$ is \begin{equation*} k_{a b}^{e}=\int_{\Omega^{e}} B_{a}^{T} D B_{b} d \Omega \tag{2.5.13} \end{equation*} \subsection*{Exercise 1.} Let\\ \[ \underbrace{d^{e}}_{n_{e n} \times 1}=\left\{d_{a}^{e}\right\}=\left\{\begin{array}{c} d_{1}^{e} \tag{2.5.14}\\ d_{2}^{e} \\ \vdots \\ d_{n_{e n}^{e}}^{e} \end{array}\right\} \] where \begin{equation*}

 Additional context 2: $\boldsymbol{\ell}=$ constant.\\ ii. $\ell=\delta(x-\bar{x})$, the delta function, where $x_{1}^{e} \leq \bar{x} \leq x_{2}^{e}$. Specialize for the cases $\bar{x}=x_{b}^{e}$ and $\bar{x}=\left(x_{1}^{e}+x_{2}^{e}\right) / 2$. Solution $$ \begin{aligned} & \text { i. } f_{a}^{e}=\ell \int_{x_{1}^{e}}^{x^ e_{2}} N_{a}(x) d x=\frac{\ell h^{e}}{2} \underbrace{\int_{-1}^{+1} N_{a}(\xi) d \xi}_{1} \\ & f^{e}=\frac{\ell h^{e}}{2}\left\{\begin{array}{l} 1 \\ 1 \end{array}\right\} \end{aligned} $$ ii. $f_{a}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{a}(x) \delta(x-\bar{x}) d x=N_{a}(\bar{x})$ For $x=x_{b}^{e}$, $$ \begin{aligned} f_{a}^{e} & =N_{a}(\bar{x})=N_{a}\left(x_{b}^{e}\right)=\delta_{a b} \quad \text { (Kronecker delta) } \\ f^{e} & =\left\{\begin{array}{l} \delta_{1 b} \\ \delta_{2 b} \end{array}\right\} \end{aligned} $$ For $\bar{x}=\left(x_{1}^{e}+x_{2}^{e}\right) / 2$, $$ f_{a}^{e}=N_{a}(\bar{x})=N_{a}\left(\frac{x_{1}^{e}+x_{2}^{e}}{2}\right)=\frac{1}{2} $$ Therefore, $$ f^{e}=\frac{1}{2}\left\{\begin{array}{l} 1 \\ 1 \end{array}\right\} $$ \subsection*{Exercise 4.} Consider the boundary-value problem for classical linear elastostatics discussed previously. In the linearized theory of small displacements superposed upon large, the stiffness term in the variational equation, $$ \int_{\Omega} w_{(i, j)} c_{i j k l} u_{(k, l)} d \Omega $$ is replaced by $$ \int_{\Omega} w_{i, j} d_{i j k l} u_{k, l} d \Omega $$ where $$ \begin{aligned} d_{i j k l} & =c_{i j k l}+\delta_{i k} \sigma_{jl}^{0} \\ \sigma_{j l}^{0} & =\sigma_{lj}^{0} \end{aligned} $$ and the $\sigma_{j l}^{0}$ 's (i.e., initial stresses) are given functions of $\boldsymbol{x} \in \boldsymbol{\Omega}$. It follows from the symmetries of $c_{i j k l}$ and $\sigma_{jl}^{0}$ that $$ d_{i j k l}=d_{k li j} $$ Assume $\boldsymbol{n}_{s d}=2$. An index-free formulation of the stiffness term is given by $$ \int_{\Omega}\left\{\begin{array}{c} w_{1,1} \\ w_{2,2} \\ w_{1,2}+w_{2,1} \\ w_{1,2}-w_{2,1} \end{array}\right\}^{T} \quad \underbrace{D}_{4 \times 4}\left\{\begin{array}{c} u_{1,1} \\ u_{2,2} \\ u_{1,2}+u_{2,1} \\ u_{1,2}-u_{2,1} \end{array}\right\} d \Omega $$ which leads to the following definition of the element stiffness matrix: $$ k_{p q}^{e}=e_{i}^{T} \int_{\boldsymbol{\Omega}^{e}} \underbrace{B_{a}^{T}}_{2 \times",60,Discuss the significance of the change of variables formula in the context of finite element analysis. How does it relate to the computation of the element stiffness matrix $k^{e}$?,"The change of variables formula is significant in finite element analysis as it allows for the transformation of integrals over the physical domain into integrals over a standard reference domain. This is particularly useful when dealing with elements that may have varying shapes or sizes, as it simplifies the computation of integrals that are essential for deriving the element stiffness matrix.

In the context of computing the element stiffness matrix $k^{e}$, the change of variables formula is applied to convert the integral from the physical coordinates $x$ to the reference coordinates $\xi$. Specifically, the formula states:

$$
\int_{x_{1}}^{x_{2}} f(x) d x = \int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi
$$

This allows the computation of the stiffness matrix to be expressed as:

$$
k_{a b}^{e} = \int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x
$$

By applying the change of variables, this integral can be rewritten in terms of the reference coordinates:

$$
k_{a b}^{e} = \int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi
$$

This transformation is crucial because it allows the use of standard shape functions $N_a$ and $N_b$ defined over the reference element, making the numerical integration more straightforward and consistent across different elements. The resulting stiffness matrix can then be computed using the properties of these shape functions and their derivatives, leading to a more efficient assembly of the global stiffness matrix $\boldsymbol{K}$ in the finite element method. 

Thus, the change of variables formula is a foundational tool in finite element analysis that facilitates the computation of integrals necessary for constructing the element stiffness matrix, ensuring that the analysis can be performed accurately and efficiently."
52,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive

 Additional context 1: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are

 Additional context 2: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The",50,"In the context of finite element methods, how does the piecewise linear interpolation of nodal values affect the accuracy of approximating a function $f(x)$? What is the expected error in the approximation?","The piecewise linear interpolation of nodal values affects the accuracy of approximating a function $f(x)$ by providing a linear representation of the function over each element. This method is generally sufficient for most practical applications and is exact for constant or linear ""loading"" of the element.

The expected error in the approximation, under suitable hypotheses, is $O(h^2)$ in the data, where $h$ is the mesh size. This means that the interpolation error in the function $f$ leads to at worst $O(h^2)$ errors in the approximated solution $u^{h}$ and its derivative $u_{, x}^{h}$. 

In summary, while piecewise linear interpolation is effective for many applications, it introduces a quadratic error in the approximation of the function $f(x)$."
53,"assembly information is stored in an array named LM, the location matrix. Let us construct the LM array for the problem under consideration. The dimensions of LM are $n_{\text {en }}$, the number of element nodes, by the number of elements; in the present case, the numbers are 2 and $n_{e l}$, respectively. Given a particular degree of freedom number and an element number (say $a$ and $e$, respectively), the value returned by the LM array is the corresponding global equation number, $A$, viz., \[ A=\mathrm{LM}(a, e)=\left\{\begin{array}{cc} e & \text { if } a=1 \tag{1.14.1}\\ e+1 & \text { if } a=2 \end{array}\right. \] The complete LM array is depicted in Fig. 1.14.1. This is the way we envision it stored in the computer. Note that $\mathrm{LM}\left(2, n_{e l}\right)=0$. This indicates that degree of freedom 2 of element number $n_{e l}$ is prescribed and is not an unknown in the global matrix equation. Hence the terms $k_{12}^{n_{el}}, k_{21}^{n_{el}}, k_{22}^{n_{el}}$, and $f_{2}^{n_{e l}}$ are not assembled into $K$ and $F$, respectively. (There are no places for them to go!) Element numbers $1 \leq e \leq n_{e l}$ \begin{center} \begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline \multirow[b]{3}{*}{\( \begin{aligned} & \text { Local } \\ & \text { node } \\ & \text { number } \end{aligned} \)} & 1 & 2 & 3 & & e & & $n_{e l-1}$ & $n_{e l}$ \\ \hline & 1. & 2 & 3 & . . . & e &  & $n-1$ & n \\ \hline & 2 & 3 & 4 & . . . & $e+1$ & . . . & $n$ & 0 \\ \hline \end{tabular} \end{center} Figure 1.14.1 LM array for example problem.\\ As an example, assume we want to add the eth elemental contributions, where $1 \leq e \leq n_{e l-1}$, to the partially assembled $\boldsymbol{K}$ and $\boldsymbol{F}$. From the LM array, we deduce the following assembly procedure: \begin{align*} K_{e e} & \leftarrow K_{e e}+k_{11}^{e} \tag{1.14.2}\\ K_{e, e+1} & \leftarrow K_{e, e+1}+k_{12}^{e} \tag{1.14.3}\\ K_{e+1, e} & \leftarrow K_{e+1, e}+k_{21}^{e} \tag{1.14.4}\\ K_{e+1, e+1} & \leftarrow K_{e+1, e+1}+k_{22}^{e} \tag{1.14.5} \end{align*} \footnote{${ }^{6}$ Due to symmetry $k_{21}^{\prime}$ would not actually be assembled in practice.} \begin{align*} F_{e} & \leftarrow F_{e}+f_{i}^{e} \tag{1.14.6}\\ F_{e+1} & \leftarrow F_{e+1}+f_{2}^{e} \tag{1.14.7} \end{align*} where the arrow $(\leftarrow)$ is read ""is replaced by.""\\ For element $n_{e l}$ we have only that \begin{align*} K_{n n} & \leftarrow K_{m n}+k_{11}^{n_{el}} \tag{1.14.8}\\ F_{n} & \leftarrow F_{n}+f_{1}^{n_{t}} \tag{1.14.9} \end{align*} With these ideas, we may construct, in sketchy fashion, an algorithm for the assembly of $\boldsymbol{K}$ and $\boldsymbol{F}$; see Fig. 1.14.2.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-43} Figure 1.14.2 Flowchart of a finite element assembly algorithm. The action of the assembly algorithm is denoted throughout by $\mathbf{A}$, the assembly operator, vis., \begin{equation*} \boldsymbol{K}=\boldsymbol{A}_{e=1}^{n_{el}}\left(k^{e}\right), \quad \boldsymbol{F}=A_{e=1}^{n_{el}}\left(f^{e}\right) \tag{1.14.10} \end{equation*} \section*{1.15 EXPLICIT COMPUTATION OF ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} The explicit computation of $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{e}$, for the problem under consideration, provides some preliminary insight into the type of colculations that must be performed in a finite element subroutine. Some preliminary results are required. \subsection*{Change of Variables Formula (Ono-Dimensional Version)} Let $f:\left[x_{1}, x_{2}\right] \rightarrow \mathbb{R}$ be an integrable function and let $x:\left[\xi_{1}, \xi_{2}\right] \rightarrow\left[x_{1}, x_{2}\right]$ be continuously differentiable, with $x\left(\xi_{1}\right)=x_{1}$ and $x\left(\xi_{2}\right)=x_{2}$. Then \begin{equation*} \int_{x_{1}}^{x_{2}} f(x) d x=\int_{\xi_{1}}^{\xi_{2}} f(x(\xi)) x_{, \xi}(\xi) d \xi \tag{1.15.1} \end{equation*} \section*{Chain Rule} Let $f$ and $x$ be as above, and, in addition, assume $f$ is differentiable. Then \begin{equation*} \frac{\partial}{\partial \xi} f(x(\xi))=f_{, x}(x(\xi)) x_{, \xi}(\xi) \tag{1.15.2} \end{equation*} Proofs of these results may be found in [11].\\ The computation of $k^{e}$ proceeds as follows: $$ \begin{aligned} k_{a b}^{e} & =\int_{\Omega^{e}} N_{a, x}(x) N_{b, x}(x) d x \quad \text { (by definition) } \\ & =\int_{-1}^{+1} N_{a, x}(x(\xi)) N_{b, x}(x(\xi)) x_{, \xi}(\xi) d \xi \end{aligned} $$ (Change of variables, where $x(\xi)$ is defined by (1.12.6)) $$ =\int_{-1}^{+1} N_{a, \xi}(\xi) N_{b, \xi}(\xi)\left(x_{, \xi}(\xi)\right)^{-1} d \xi $$ (Chain rule; $\left.N_{a, \xi}(\xi)=(\partial / \partial \xi) N_{a}(x(\xi))=N_{a, x}(x(\xi)) x_{, \xi}(\xi)\right)$ $$ =(-1)^{a+b} / h^{e} \quad(\text { by }(1.12 .7)-(1.12 .9)) $$ Thus \[ k^{e}=\frac{1}{h^{e}}\left[\begin{array}{rr} 1 & -1 \tag{1.15.3}\\ -1 & 1 \end{array}\right] \] Observe that $N_{a, \xi}$ (see (1.12.7)) does not depend upon the particular element data, as $N_{a}=N_{a}(\xi)$. We shall see that this is generally true, and hence these computations may be done once and for all. The derivatives $\boldsymbol{x}_{, \xi}$ and $\boldsymbol{\xi}_{, x}$ do depend on the particular element data (in the present case $h^{e}$ ), and subroutines will be necessary to compute the analogs of these quantities in more general cases. Now we wish to compute $f^{e}$. However, this cannot be done without explicitly knowing what $f=\boldsymbol{f}(x)$ is. In practice, it would be inconvenient to reprogram every time we wanted to solve a problem involving a different function $f$. Generally a convenient approximation is made. For example, we might replace $f$ by its linear interpolate over each element, namely, \begin{equation*} f^{h}=\sum_{a=1}^{2} f_{a} N_{a} \tag{1.15.4} \end{equation*} where $f_{a}=f\left(x\left(\xi_{a}\right)\right)$; see Fig. 1.15.1. The notation $f^{h}$ is used to indicate that the approximation depends upon the mesh. This represents an approximation that is sufficient for most practical applications. (It is, of course, exact for constant or linear ""loading"" of the element.) Now standardization of input to the program may be facilitated; that is, the nodal values of $f$ are the required data. Let us employ this approximation in the explicit calculation of an element force vector: \begin{align*} \int_{\Omega^{e}} N_{a}(x) f^{h}(x) d x & =\int_{-1}^{+1} N_{a}(x(\xi)) f^{h}(x(\xi)) x_{, \xi}(\xi) d \xi \quad \text { (change of variables) } \\ & =\frac{h^{e}}{2} \sum_{b=1}^{2} \int_{-1}^{+1} N_{a}(\xi) N_{b}(\xi) d \xi f_{b} \quad \text { (by (1.12.8)) } \tag{1.15.5} \end{align*} \begin{center} \includegraphics[max width=\textwidth]{2024_10_04_fba7dc36d090c246379ag-45} \end{center} Figure 1.15.1 Approximation of $/$ by piecewise linear interpolation of nodal values. Carrying out the integrations $\left(\int_{-1}^{+1} N_{a} N_{b} d \xi=\left(1+\delta_{a b}\right) / 3\right)$ yields \[ \begin{array}{rlr} \mathfrak{f}^{e} & =\frac{h^{e}}{6}\left[\begin{array}{cc} 2 & 1 \\ 1 & 2 \end{array}\right]\left\{\begin{array}{l} f_{1} \\ f_{2} \end{array}\right\} & \begin{array}{c} \text { (+ boundary terms } \\ \text { cf. (1.13.12)) } \end{array} \\ & =\frac{h^{e}}{6}\left\{\begin{array}{l} 2 f_{1} + f_{2} \\ f_{1} + 2 f_{2} \end{array}\right\} & \text { (+ boundary terms) } \tag{1.15.6} \end{array} \] Remark. It can be shown that, under suitable hypotheses, piecewise linear nodal interpolation produces $O\left(h^{2}\right)$ errors in the data; in this case, $f$. (See [12], pp. 56-57, for basic estimates of interpolation errors.) It can be shown that, in appropriate measures of the error, this produces at worst $O\left(h^{2}\right)$ errors in $u^{\boldsymbol{h}}$ and $u_{, x}^{\boldsymbol{h}}$. The following exercise indicates that there may be better ways to approximate given data. \subsection*{Exercise 1}. Suppose $f(x)$ is quadratic (i.e., consists of a linear combination of the monomials $1, x, x^{2}$ ). Determine a piecewise linear approximation-not necessarily continuous-to $\boldsymbol{f}$ over each element which results in exact nodal values. Hint: The analysis may be performed with respect to one element. Exercise 2. The equation of a string on an elastic foundation is given by: $$ \left.u_{,xx}-\lambda u+f=0 \quad \text { on } \Omega=\right] 0,1[ $$ where $\lambda$, a positive constant, is a measure of the foundation stiffness. Assuming the same boundary conditions as for the problem discussed previously in this chapter, it can be shown that an equivalent weak formulation is: $$ \int_{\Omega}\left(w_{\cdot x} u_{, x}+w \lambda u\right) d x=\int_{\Omega} w f d x+w(0) h $$ where $u \in \mathfrak{f}, w \in \mathcal{U}$, and so on. This can also be written as $$ a(w, u)+(w, \lambda u)=(w, f)+w(0) h $$ i. Let $\boldsymbol{u}^{h}=v^{h}+g^{h}$. Write the Galerkin counterpart of the weak formulation: $$ \begin{array}{r} a\left(w^{h}, v^{h}\right)+\square= \\ \left(w^{h}, f\right)+w^{h}(0) h-a\left(w^{h}, gS^{h}\right) \\ -\square \end{array} $$ ii. Define $K_{A B}=a\left(N_{A}, N_{B}\right)+\square$\\ and $$ k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}+\square $$ iii. Determine $\boldsymbol{k}^{e}$ explicitly: $$ k^{e}=\left[k_{a b}^{e}\right]=[\square] $$ iv. Show that $\boldsymbol{K}$ is symmetric.\\ v. Show that $K$ is positive definite. Is it necessary to employ the boundary condition $w^{h}(1)=0$ ? Why?\\ vi. The Green's function for this problem satisfies $$ g_{. x x}-\lambda g+\delta_{y}=0 $$ and can be written as $$ g(x)= \begin{cases}c_{1} e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case","

 Additional context 0: e^{p x}+c_{2} e^{-p x}, & 0 \leq x \leq y \\ c_{3} e^{p x}+c_{4} e^{-p x}, & y \leq x \leq 1\end{cases} $$ where $p=\lambda^{1 / 2}$ and the $c$ 's are determined from the following four boundary and continuity conditions: $$ \begin{aligned} g(1) & =0 \\ g_{, x}(0) & =0 \\ g\left(y^{+}\right) & =g\left(y^{-}\right) \\ g_{, x}\left(y^{+}\right) & =g_{. x}\left(y^{-}\right)-1 \end{aligned} $$ Why is the piecewise linear finite element space incapable of attaining nodally exact solutions in this case?\\ vii. Construct exponential element shape functions $N_{1}(x)$ and $N_{2}(x)$ such that $$ u^{h}(x)=d_{1}^{e} N_{1}(x)+d_{2}^{e} N_{2}(x), \quad x \in \Omega^{e} $$ where $$ u^{h}(x)=c_{1} e^{p x}+c_{2} e^{-p x} $$ and the $c$ 's are determined from $$ d_{a}^{e}=u^{h}\left(x_{a}^{e}\right), \quad a=1,2 $$ What is the attribute which this choice of functions attains? \section*{1.16 EXERCISE: BERNOULL-EULER BEAM THEORY AND HERMITE CUBICS} This problem develops basic finite element results for Bernoulli-Euler beam theory. The strong form of a boundary-value problem for a thin beam (Bernoulli-Euler theory) fixed at one end and subjected to a shear force and moment at the other end, may be stated as follows: Let the beam occupy the unit interval (i.e., $\Omega=] 0,1[, \bar{\Omega}=[0,1]$ ).\\ \[ \text{(S)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text {Given } $f: \Omega \rightarrow \mathbb{R}$ \text { and constants } $M$ \text { and } $f$, \text { find } $u: \bar{\Omega} \rightarrow \mathbb{R}$ \text { such that:} \begin{align*} E I u_{,xxxx} = f \quad \text {on } \Omega \quad \text {(transverse equilibrium)}\\ u(1) &= 0 & \text{(zero transverse displacement)} \\ u_{x}(1) &= 0 & \text{(zero slope)} \\ E I u_{,xx}(0) &= M & \text{(prescribed moment)} \\ E I u_{,xxx}(0) &= Q & \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$

 Additional context 1: \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$.

 Additional context 2: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.",75,"For the Bernoulli-Euler beam theory problem, derive the weak formulation corresponding to the strong form of the boundary-value problem. What are the natural boundary conditions associated with this formulation?","To derive the weak formulation corresponding to the strong form of the boundary-value problem for the Bernoulli-Euler beam theory, we start with the strong form given by:

\[
E I u_{,xxxx} = f \quad \text{on } \Omega \quad \text{(transverse equilibrium)}
\]
with the boundary conditions:
\[
u(1) = 0 \quad \text{(zero transverse displacement)}
\]
\[
u_{,x}(1) = 0 \quad \text{(zero slope)}
\]
\[
E I u_{,xx}(0) = M \quad \text{(prescribed moment)}
\]
\[
E I u_{,xxx}(0) = Q \quad \text{(prescribed shear)}
\]

### Weak Formulation

1. **Test Function**: Let \( w \in \mathcal{U} \) be a test function that satisfies the boundary conditions \( w(1) = 0 \).

2. **Multiplying by Test Function**: Multiply the strong form equation by the test function \( w \) and integrate over the domain \( \Omega \):
\[
\int_{0}^{1} w E I u_{,xxxx} \, dx = \int_{0}^{1} w f \, dx
\]

3. **Integration by Parts**: We apply integration by parts to transfer derivatives from \( u \) to \( w \). The integration by parts formula states:
\[
\int w u_{,xxxx} \, dx = \left[ w u_{,xxx} \right]_{0}^{1} - \int u_{,xxx} w_{,x} \, dx
\]
Continuing this process, we will need to apply integration by parts three more times to fully transfer all derivatives from \( u \) to \( w \).

After applying integration by parts four times, we obtain:
\[
\int_{0}^{1} w E I u_{,xxxx} \, dx = \left[ w u_{,xxx} \right]_{0}^{1} - \int_{0}^{1} u_{,xxx} w_{,x} \, dx
\]
\[
= \left[ w u_{,xx} \right]_{0}^{1} - \left[ w_{,x} u_{,xx} \right]_{0}^{1} + \int_{0}^{1} u_{,xx} w_{,xx} \, dx
\]
\[
= \left[ w u_{,x} \right]_{0}^{1} - \left[ w_{,x} u_{,x} \right]_{0}^{1} + \int_{0}^{1} u_{,x} w_{,xxx} \, dx
\]
\[
= \int_{0}^{1} u w_{,xxxx} \, dx + \text{(boundary terms)}
\]

4. **Natural Boundary Conditions**: The natural boundary conditions arise from the terms evaluated at the boundaries. Specifically, the boundary terms will yield conditions on the test function \( w \) and its derivatives at the boundaries. 

### Summary of the Weak Formulation

The weak formulation can be summarized as follows:
Find \( u \in \mathfrak{f} \) such that for all \( w \in \mathcal{U} \):
\[
\int_{0}^{1} w E I u_{,xxxx} \, dx = \int_{0}^{1} w f \, dx + \text{(boundary terms)}
\]

### Natural Boundary Conditions

The natural boundary conditions associated with this formulation are derived from the boundary terms that arise during the integration by parts process. These conditions typically relate to the values of \( u \) and its derivatives at the boundaries, reflecting the physical constraints imposed by the problem.

In conclusion, the weak formulation captures the essence of the strong form while allowing for the use of test functions, leading to a variational approach that is fundamental in finite element analysis."
54,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: solution to this problem for $\boldsymbol{p}=5$. Sketch.\\ ii. State the weak formulation of the problem.\\ iii. State the Galerkin formulation.\\ iv. State the matrix formulation.\\ v. Solve the matrix problem assuming $p=5$ and using the piecewise linear finite element space for the following cases:\\ a. one element\\ b. two equal-length elements\\ vi. Compare the exact value of $u_{, x}(1)$ with the approximate values computed in part v. Explain why it is impossible for these results to compare favorably. \subsection*{Exercise 8.} In heat conduction, it is often of interest to accurately calculate the boundary heat flux over a portion of the boundary where temperature is specified. Suppose we use the usual Galerkin finite element formulation to calculate the temperature. However, instead of calculating the heat flux in the usual way (i.e., by differentiating the temperature), we introduce a post-processing which derives from the following weak formulation:\\ Find $u \in \delta$ and $h \in L_{2}\left(\Gamma_{g}\right)$ such that for all $w \in \mathcal{V}$, $$ -\int_{\Omega} w_{,i}q_{i} d \Omega=\int_{\Omega} w \ell d \Omega+\int_{\Gamma_{h}} w h d \Gamma+\int_{\Gamma_{g}} w h d \Gamma $$ where $h$ is the unknown heat flux on $\Gamma_{g}$\\ (Note: In this formulation, it is not assumed that $w=0$ on $\Gamma_{g}!$ )\\ i. Show, in addition to the usual differential equations and boundary conditions, that $$ h=-q_{i} n_{i} \text { on } \Gamma_{g} $$ arises naturally from the new weak formulation.\\ ii. State the Galerkin and matrix formulations corresponding to the new weak formulation assuming $h$ is approximated in the usual way, namely $$ h^{h}(x)=\sum_{A \in \eta_{g}} N_{A}(x) h_{A} $$ (Hint: The equations governing the temperature are unchanged.)\\ iii. Specialize this formulation to the one-dimensional problem described in Exercise 7 and calculate the boundary flux at $x=1$ by the new procedure (cf. parts $v$ and vi of Exercise 7).\\ (Hint: The new method should produce exact results for these cases.)\\ iv. Develop a counterpart of the new formulation for elasticity. That is, introduce the $i$th component of traction as an independent unknown on $\Gamma_{g_i}$ and carefully state the weak formulation.\\ v. Prove that the new method is exact for the one-dimensional model problem of Chapter 1. \section*{REFERENCES} \section*{Section 2.2} \begin{enumerate} \item

 Additional context 1: \maketitle \section*{1.1 INTRODUCTORY REMARKS AND PRELMMNARIES} The main constituents of a finite element method for the solution of a boundary-value problem are\\ i. The variational or weak statement of the problem; and\\ ii. The approximate solution of the variational equations through the use of ""finite element functions."" To clarify concepts we shall begin with a simple example.\\ Suppose we want to solve the following differential equation for $u$ : \begin{equation*} u_{, x x}+f=0 \tag{1.1.1} \end{equation*} where a comma stands for differentiation (i.e., $u_{, x x}=d^{2} u / d x^{2}$ ). We assume $f$ is a given smooth, scalar-valued function defined on the unit interval. We write \begin{equation*} f: [0,1] \to \mathbb{R} \tag{1.1.2} \end{equation*} where $[0,1]$ stands for the unit interval (i.e., the set of points $x$ such that $0 \leq x \leq 1$ ) and $\mathbb{R}$ stands for the real numbers. In words, (1.1.2) states that for a given $x$ in $[0,1]$, $f(x)$ is a real number. (Often we will use the notation $\in$ to mean ""in"" or ""a member of."" Thus for each $x \in[0,1], f(x) \in \mathbb{R}$.). Also, $[0,1]$ is said to be the domain of $f$, and $\mathbb{R}$ is its range. We have described the given function $f$ as being smooth. Intuitively, you probably know what this means. Roughly speaking, if we sketch the graph of the function $f$, we want it to be a smooth curve without discontinuities or kinks. We do this to avoid technical difficulties. Right now we do not wish to elaborate further as this would divert us from the main theme. At some point prior to moving on to the next chapter, the reader may wish to consult Appendix 1.I, ""An Elementary Discussion of Continuity, Differentiability and Smoothness,"" for further remarks on this important aspect of finite element work. The exercise in Sec. 1.16 already uses a little of the language described in Appendix 1.I. The terminology may be somewhat unfamiliar to engineering and physical science students, but it is now widely used in the finite element literature and therefore it is worthwhile to become accustomed to it. Equation (1.1.1) is known to govern the transverse displacement of a string in tension and also the longitudinal displacement of an elastic rod. In these cases, physical parameters, such as the magnitude of tension in the string, or elastic modulus in the case of the rod, appear in (1.1.1). We have omitted these parameters to simplify

 Additional context 2: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are",80,"What is the weak form of the problem as described in the text, and how does it relate to the finite element method?","The weak form of the problem, as described in the text, is given by:

$$
\text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} f, M, \text{, and} Q, \text{ find } u \in \mathfrak{f} \text{ such that for all } w \in \mathcal{U} \\ 
\begin{align*} 
a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q 
\end{align*} \end{minipage} \right.
$$

where the bilinear form $a(w, u)$ is defined as:

$$
a(w, u) = \int_{0}^{1} w_{, xx} E I u_{, xx} dx
$$

and the linear functional $(w, f)$ is defined as:

$$
(w, f) = \int_{0}^{1} w f dx.
$$

This weak formulation is a critical step in the finite element method (FEM) as it transforms the original differential problem into an equivalent variational problem. In the context of FEM, the weak form allows for the incorporation of boundary conditions and the use of approximation spaces (like $\mathcal{U}^h$) to find approximate solutions to the original problem.

The weak form is particularly useful because it relaxes the requirements on the solution $u$ and the test function $w$, allowing them to belong to Sobolev spaces, which can accommodate functions that are not necessarily smooth. This is essential for practical applications where exact solutions are often not attainable, and numerical methods like FEM are employed to approximate the solution.

In summary, the weak form serves as the foundation for the Galerkin formulation of the finite element method, where the problem is solved using piecewise polynomial approximations, leading to a system of algebraic equations that can be solved numerically."
55,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 1: formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable. That is, if $u$ is a trial solution, then \begin{equation*} \int_{0}^{1}(u_{,x})^{2} d x<\infty \tag{1.3.1} \end{equation*} Functions that satisfy (1.3.1) are called $H^{1}$-functions; we write $\boldsymbol{u} \in \boldsymbol{H}^{1}$. Sometimes the domain is explicitly included, i.e., $u \in H^{1}([0,1])$. Thus the collection of trial solutions, denoted by $\mathfrak{f}$, consists of all functions which have square-integrable derivatives and take on the value $q$ at $x=1$. This is written as follows: \begin{equation*} \mathcal{S}=\left\{u \mid u \in H^{1}, u(1)=g\right\} \quad \text { (trial solutions) } \tag{1.3.2} \end{equation*} The fact that $\mathfrak{f}$ is a collection, or set, of objects is indicated by the curly brackets (called braces) in (1.3.2). The notation for the typical member of the set, in this case $u$, comes first inside the left-hand curly bracket. Following the vertical line ( $\mid$ ) are the properties satisfied by members of the set. The second collection of functions is called the \textbf{\textit{weighting functions}}, or \textbf{\textit{variations}}. This collection is very similar to the trial solutions except we require the homogeneous counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[

 Additional context 2: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is",70,Explain the significance of the Sobolev space $H^{2}(	ext{})$ in the context of the weak formulation and the boundary conditions imposed on the functions $w$.,"The Sobolev space $H^{2}(\Omega)$ is significant in the context of the weak formulation and the boundary conditions imposed on the functions $w$ for several reasons:

1. **Smoothness and Integrability**: Functions in the Sobolev space $H^{2}(\Omega)$ possess square-integrable derivatives up to the second order. This means that both the function itself and its first and second derivatives are square-integrable over the domain $\Omega$. Specifically, this is expressed as:
   $$
   \int_{0}^{1} w^{2} dx < \infty, \quad \int_{0}^{1} (w_{,x})^{2} dx < \infty, \quad \int_{0}^{1} (w_{,xx})^{2} dx < \infty.
   $$

2. **Boundary Conditions**: The functions $w$ in the weak formulation are required to satisfy specific boundary conditions, namely $w(1) = 0$ and $w_{,x}(1) = 0$. These conditions ensure that the weighting functions are compatible with the physical constraints of the problem, particularly at the boundary $x=1$. The requirement that $w$ belongs to $H^{2}(\Omega)$ guarantees that these boundary conditions can be satisfied while maintaining the necessary smoothness of the functions.

3. **Kinematic Boundary Conditions**: The collection of functions $\mathcal{U}$, which consists of functions $w$ that are in $H^{2}(\Omega)$ and satisfy the boundary conditions, can be thought of as the space of finite strain-energy configurations of the beam. This is crucial for ensuring that the weak formulation accurately represents the physical behavior of the system being modeled.

4. **Existence of Solutions**: The assumption that $w \in H^{2}(\Omega)$, along with the imposed boundary conditions, is a consequence of Sobolev's theorem, which states that functions in $H^{k+1}$ are continuously differentiable. This smoothness is essential for the uniqueness and existence of solutions to the weak formulation of the problem.

In summary, the Sobolev space $H^{2}(\Omega)$ ensures that the functions used in the weak formulation are sufficiently smooth and integrable, allowing for the proper application of boundary conditions and the existence of unique solutions to the problem."
56,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: from which it follows that $c_{p}=0$; hence $\boldsymbol{c}=\mathbf{0}$. \section*{Remark} Positive definiteness of $\boldsymbol{K}$ is based upon two requirements: a positivedefiniteness condition on the constitutive coefficients and suitable boundary conditions being incorporated into $\mathcal{V}^{h}$. \subsection*{2.9 ELASTOSTATICS: ELEMENT STIFFNESS MATRIX AND FORCE VECTOR} As usual, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be decomposed into sums of elemental contributions. These results will be omitted here as the reader should now be familiar with the ideas involved (cf. Sec. 2.5). We will proceed directly to the definitions of $\boldsymbol{k}^{e}$ and $f^{\boldsymbol{c}}$ : \begin{align*} & \boldsymbol{k}^{e}=\left[k_{p q}^{e}\right], \quad \boldsymbol{f}^{e}=\left\{f_{p}^{e}\right\}, \quad 1 \leq p, q \leq n_{ee}=n_{ed} n_{e n} \tag{2.9.1}\footnotemark\\ & k_{p q}^{e}=e_{i}^{T} \int_{\Omega e} B_{a}^{T} D B_{b} d \Omega e_{j}, \quad p=n_{e d}(a-1)+i, \\ & q=n_{e d}(b-1)+j \tag{2.9.2}\\ & \left(n_{s d}=2\right) \quad B_{a}=\left[\begin{array}{cc} N_{a, 1} & 0 \\ 0 & N_{a, 2} \\ N_{a, 2} & N_{a, 1} \end{array}\right] \tag{2.9.3}\\ & \left(n_{s d}=3\right) \quad \boldsymbol{B}_{a}=\left[\begin{array}{ccc} N_{a, 1} & 0 & 0 \\ 0 & N_{a, 2} & 0 \\ 0 & 0 & N_{a, 3} \\ 0 & N_{a, 3} & N_{a, 2} \\ N_{a, 3} & 0 & N_{a, 1} \\ N_{a, 2} & N_{a, 1} & 0 \end{array}\right] \tag{2.9.4} \end{align*} \footnotetext{$n_{\text{ee}}$ stands for the number of element equations and $n_{\text{ed}}$ is the number of element degrees of freedom (per node). It is possible in practice to have $n_{\text {ed}} \leq n_{\text{dof}}$, although they are usually equal. } and \[ f_{p}^{e}=\int_{\Omega^{e}} N_{a} \ell_{i} d \Omega+\int_{\Gamma_{h_{i}}^{e}} N_{a} h_{i} d \Gamma-\sum_{q=1}^{n_{e e}} k_{p q} g_{q}^{e}, \quad \begin{array}{r} \Gamma_{h_{i}}^{e}=\Gamma_{h_{i}} \cap \Gamma^{e} \tag{2.9.5}\\ (\text { no sum on } i) \end{array} \] where $g_{q}^{e}=g_{j b}^{e}=g_{j}\left(x_{b}^{e}\right)$ if $g_{j}$ is prescribed at node $b$, and equals zero otherwise. It is useful for programming purposes to define the nodal submatrix \begin{equation*} \underbrace{\boldsymbol{k}_{ab}^{e}}_{n_{e d} \times n_{e d}}=\int_{\boldsymbol{\Omega}^{e}} B_{a}^{T} \boldsymbol{D} B_{b} d \boldsymbol{\Omega} \tag{2.9.6a} \end{equation*} From (2.9.2) we see that

 Additional context 1: element stiffness, $\boldsymbol{k}^{e}$, and element force vector, $\boldsymbol{f}^{\boldsymbol{e}}$, may be deduced from these equations: \begin{align*} & k^{e}=\left[k_{a b}^{e}\right], \quad f^{e}=\left\{f_{a}^{e}\right\}, \quad 1 \leq a, b \leq n_{e n} \tag{2.5.6}\\ & k_{a b}^{e}=a\left(N_{a}, N_{b}\right)^{e}=\int_{\Omega^{e}}\left(\nabla N_{a}\right)^{T} \kappa\left(\nabla N_{b}\right) d \Omega \tag{2.5.7}\\ & f_{a}^{e}=\int_{\boldsymbol{\Omega}^{e}} N_{a} \ell d \Omega+\int_{\Gamma_{h}^{e}} N_{a} h d \Gamma-\sum_{b=1}^{n_{el}} k_{a b}^{e} g_{b}^{e} \tag{2.5.8} \end{align*} where (recall) $n_{e n}$ is the number of element nodes, and $g_{b}^{e}=g\left(x_{b}^{e}\right)$ if $g$ is prescribed at node number $b$ and equals zero otherwise.\footnote{An implicit assumption in localizing the $g$-term is that if $x_{A}$ is not a node attached to element $e$, then $N_{A}(x)=0$ for all' $x \in \bar{\Omega}^{\circ}$. Otherwise, the last term in (2.5.4) may involve $g$-data of nodes not attached to element $e$, which is not accounted for in (2.5.8).} The global arrays, $\boldsymbol{K}$ and $\boldsymbol{F}$ may be formed from the element arrays $\boldsymbol{k}^{\boldsymbol{e}}$ and $\boldsymbol{f}^{\boldsymbol{e}}$, respectively, by way of an assembly algorithm as described in Sec. 1.14. The element stiffness matrix can be written in a standard form convenient for programming: \begin{equation*} k^{e}=\int_{\boldsymbol{\Omega}^{e}} B^{T} D B d \Omega \tag{2.5.9} \end{equation*} where, in the present case, \begin{align*} & \underbrace{\boldsymbol{D}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{s d}}=\boldsymbol{\kappa} \tag{2.5.10}\\ & \underbrace{\boldsymbol{B}}_{\boldsymbol{n}_{s d} \times \boldsymbol{n}_{en}}=\left[\boldsymbol{B}_{1}, \boldsymbol{B}_{2}, \ldots, \boldsymbol{B}_{n_{e n}}\right] \tag{2.5.11}\\ & \underbrace{B_{a}}_{n_{s d} \times 1}=\nabla N_{a} \tag{2.5.12} \end{align*} The component version of $(2.5 .9)$ is \begin{equation*} k_{a b}^{e}=\int_{\Omega^{e}} B_{a}^{T} D B_{b} d \Omega \tag{2.5.13} \end{equation*} \subsection*{Exercise 1.} Let\\ \[ \underbrace{d^{e}}_{n_{e n} \times 1}=\left\{d_{a}^{e}\right\}=\left\{\begin{array}{c} d_{1}^{e} \tag{2.5.14}\\ d_{2}^{e} \\ \vdots \\ d_{n_{e n}^{e}}^{e} \end{array}\right\} \] where \begin{equation*}

 Additional context 2: and $\sigma_{jl}^{0}$ that $$ d_{i j k l}=d_{k li j} $$ Assume $\boldsymbol{n}_{s d}=2$. An index-free formulation of the stiffness term is given by $$ \int_{\Omega}\left\{\begin{array}{c} w_{1,1} \\ w_{2,2} \\ w_{1,2}+w_{2,1} \\ w_{1,2}-w_{2,1} \end{array}\right\}^{T} \quad \underbrace{D}_{4 \times 4}\left\{\begin{array}{c} u_{1,1} \\ u_{2,2} \\ u_{1,2}+u_{2,1} \\ u_{1,2}-u_{2,1} \end{array}\right\} d \Omega $$ which leads to the following definition of the element stiffness matrix: $$ k_{p q}^{e}=e_{i}^{T} \int_{\boldsymbol{\Omega}^{e}} \underbrace{B_{a}^{T}}_{2 \times 4} \underbrace{D}_{4 \times 4} \underbrace{B_{b}}_{4 \times 2} d \Omega e_{j} $$ Set up $B_{a}$ in terms of the shape function $N_{a}$. Define the components of $D$ in terms of the $d_{i j kl}$ 's. (The $\sigma_{j l}^{0}$-contribution to the stiffness is sometimes called the initial-stress stiffiness matrix. It is important to account for it in the solution of many nonlinear problems.) \subsection*{Exercise 5.}Let $\boldsymbol{\Omega}$ be a region in $\mathbb{R}^{2}$ and let its boundary $\Gamma=\overline{\Gamma_{1} \cup \Gamma_{2} \cup \Gamma_{3} \cup \Gamma_{4}}$ where $\Gamma_{1}, \ldots, \Gamma_{4}$ are nonoverlapping subregions of $\Gamma$. Let $n$ be the unit outward normal vector to $\Gamma$ such that $\boldsymbol{s}$ and $\boldsymbol{n}$ form a right-hand rule basis; see Fig. 2.12.1. Consider the following boundary-value problem in classical linear elastostatics: Given $\ell_i: \Omega \rightarrow \mathbb{R} ; g_{i}: \Gamma_{1} \rightarrow \mathbb{R} ; h_{i}: \Gamma_{2} \rightarrow \mathbb{R} ; g_{n}$ and $h_{3}: \Gamma_{3} \rightarrow \mathbb{R}$; and $g_{s}$ and $h_{n}: \Gamma_{4} \rightarrow \mathbb{R}$; find $u_{i}: \overline{\boldsymbol{\Omega}} \rightarrow \mathbf{R}$ such that $$ \begin{aligned} & \sigma_{ij, j}+f_{i}=0 \quad \text { in } \Omega \\ & u_{i}=q_{i} \quad \text { on } \Gamma_{1} \\ & \sigma_{i j} n_{j}=h_{i} \quad \text { on } \Gamma_{2} \\ & \left.\left.\begin{array}{rl} u_{i} n_{i} & =g_{n} \\ \sigma_{y} n_{j} s_{i} & =h_{s} \end{array}\right\} \quad \text { on } \Gamma_{3} \quad \text { ( } \begin{array}{l} \text { normal displacement } \\ \text { tangential traction } \end{array}\right) \\ & \left.\begin{array}{rl} u_{i} s_{i} & =g_{s} \\ \sigma_{i j} n_{j} n_{i} & =h_{n} \end{array}\right\} \quad \text { on } \Gamma_{4}",60,Derive the expression for the stiffness matrix $k_{pq}^{e}$ given the element shape functions and the integral formulation provided in the text.,"To derive the expression for the stiffness matrix $k_{pq}^{e}$, we start with the integral formulation provided in the context. The stiffness matrix is defined as:

$$
k_{pq}^{e} = \int_{\Omega^{e}} N_{p, xx} E I N_{q, xx} \, dx
$$

where $N_{p}$ and $N_{q}$ are the shape functions associated with the element.

### Step-by-Step Derivation:

1. **Identify the Shape Functions**: 
   The shape functions $N_{p}$ and $N_{q}$ are cubic polynomials defined on the element interval. For a cubic polynomial, we can express it in the form:
   $$ 
   N_{p}(x) = c_{1} + c_{2} x + c_{3} x^{2} + c_{4} x^{3} 
   $$
   where $c_{1}, c_{2}, c_{3}, c_{4}$ are constants determined by the boundary conditions and nodal values.

2. **Calculate the Second Derivative**:
   The second derivative of the shape function is given by:
   $$
   N_{p, xx} = \frac{d^2 N_{p}}{dx^2} = 2c_{3} + 6c_{4}x
   $$
   Similarly, for $N_{q}$:
   $$
   N_{q, xx} = 2d_{3} + 6d_{4}x
   $$

3. **Substitute into the Stiffness Matrix Integral**:
   Substitute the second derivatives into the stiffness matrix integral:
   $$
   k_{pq}^{e} = \int_{x_1}^{x_2} (2c_{3} + 6c_{4}x) E I (2d_{3} + 6d_{4}x) \, dx
   $$

4. **Expand the Integral**:
   Expanding the integrand gives:
   $$
   k_{pq}^{e} = E I \int_{x_1}^{x_2} \left(4c_{3}d_{3} + 12c_{4}d_{3}x + 12c_{3}d_{4}x + 36c_{4}d_{4}x^2\right) \, dx
   $$

5. **Evaluate the Integral**:
   Now we evaluate the integral term by term:
   - The first term: 
     $$
     \int_{x_1}^{x_2} 4c_{3}d_{3} \, dx = 4c_{3}d_{3}(x_2 - x_1)
     $$
   - The second term:
     $$
     \int_{x_1}^{x_2} 12c_{4}d_{3}x \, dx = 12c_{4}d_{3} \left[\frac{x^2}{2}\right]_{x_1}^{x_2} = 6c_{4}d_{3}(x_2^2 - x_1^2)
     $$
   - The third term:
     $$
     \int_{x_1}^{x_2} 12c_{3}d_{4}x \, dx = 12c_{3}d_{4} \left[\frac{x^2}{2}\right]_{x_1}^{x_2} = 6c_{3}d_{4}(x_2^2 - x_1^2)
     $$
   - The fourth term:
     $$
     \int_{x_1}^{x_2} 36c_{4}d_{4}x^2 \, dx = 36c_{4}d_{4} \left[\frac{x^3}{3}\right]_{x_1}^{x_2} = 12c_{4}d_{4}(x_2^3 - x_1^3)
     $$

6. **Combine the Results**:
   Combining all these results gives the final expression for the stiffness matrix:
   $$
   k_{pq}^{e} = E I \left( 4c_{3}d_{3}(x_2 - x_1) + 6c_{4}d_{3}(x_2^2 - x_1^2) + 6c_{3}d_{4}(x_2^2 - x_1^2) + 12c_{4}d_{4}(x_2^3 - x_1^3) \right)
   $$

This expression represents the stiffness matrix for the element based on the shape functions and their derivatives."
57,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: counterpart of the $g$-boundary condition. That is, we require weighting functions, $w$, to satisfy $w(1)=0$. The collection is denoted by $\mathcal{U}$ and defined by\\ \begin{equation*} \mathcal{U} = \{w \mid w \in H^{1}, w(1)=0\} \quad \text{(weighting functions)} \tag{1.3.3} \end{equation*} It simplifies matters somewhat to continue to think of $f: \Omega \rightarrow \mathbb{R}$ as being smooth. (However, what follows holds for a considerably larger class of $f$'s.) In terms of the preceding definitions, we may now state a suitable weak form, $(W)$, of the boundary-value problem. \[ (\mathcal{W}) \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f,$q and $h$,\text{ as before. Find } $u \in \mathfrak{f}$, \text{ such that for all} \mbox{$w \in \mathcal{U}$} \begin{align*} \int_{0}^{1} w_{, x} u_{, x} d x=\int_{0}^{1} w f d x+w(0) h \tag{1.3.4} \end{align*} } \right. \] Formulations of this type are often called \textbf{\textit{virtual work}}, or \textbf{\textit{virtual displacement, principles}} in mechanics. The $w$'s are the \textbf{\textit{virtual displacements}}. Equation (1.3.4) is called the \textbf{\textit{variational equation}}, or (especially in mechanics) the \textbf{\textit{equation of virtual work.}} The solution of $(W)$ is called the \textbf{\textit{weak}}, or \textbf{\textit{generalized, solution}}. The definition given of a weak formulation is not the only one possible, but it is the most natural one for the problems we wish to consider. \section*{1.4 EQUIVALENCE OF STRONG AND WEAK FORMS: NATURAL BOUNDARY CONDITIONS} Clearly, there must be some relationship between the strong and weak versions of the problem, or else there would be no point in introducing the weak form. It turns out that the weak and strong solutions are identical. We shall establish this assuming all functions are smooth. This will allow us to proceed expeditiously without invoking technical conditions with which the reader is assumed to be unfamiliar. ""Proofs"" of this kind are sometimes euphemistically referred to as ""formal proofs."" The intent is not to be completely rigorous but rather to make plausible the truth of the proposition. With this philosophy in mind, we shall ""prove"" the following. \subsection*{Proposition} a. Let $u$ be a solution of (S). Then $u$ is also a solution of (W).\\ b. Let $u$ be a solution of $(W)$. Then $u$ is also a solution of $(S)$.

 Additional context 1: possibilities. We shall assume $u$ is required to satisfy \begin{align} u(1) &= g \tag{1.2.1} \\ -u_{, x}(0) &= h \tag{1.2.2} \end{align} where $g$ and $h$ are given constants. Equations (1.2.1) and (1.2.2) require that $u$ take on the value $g$ at $x=1$ and the derivative of $u$ (i.e., slope) take on the value $-h$ at $x=0$, respectively. This set of boundary conditions will later enable us to illustrate certain key features of variational formulations. For obvious reasons, boundary conditions of the type (1.2.1) and (1.2.2) lead to so-called \textbf{\textit{two-point boundary-value problems.}} The strong form of the boundary-value problem, $(S)$, is stated as follows: \[ \text{(S)} \quad \left\{ \parbox{0.8\textwidth}{ \text{Given } $f:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$ \text{ and constants } $g$ \text{ and } $h$, \text{ find } $u:\overline{\boldsymbol{\Omega}} \to \mathbb{R}$, \text{ such that:} \begin{align*} u_{,xx} + f &= 0 \quad \text{on } \Omega \\ u(1) &= g \\ -u_{,x}(0) &= h \end{align*} } \right. \] When we write $u_{, x x}+f=0$ on $\Omega$ we mean $u_{, x x}(x)+f(x)=0$ for all $x \in \Omega$. Of course, the exact solution of $(S)$ is trivial to obtain, namely, \begin{equation*} u(x)=g+(1-x) h+\int_{x}^{1}\left\{\int_{0}^{y} f(z) d z\right\} d y \tag{1.2.3} \end{equation*} where $y$ and $z$ are used to denote dummy variables. However, this is not the main concern here. We are interested in developing schemes for obtaining approximate solutions to ( $S$ ) that will be applicable to much more complex situations in which exact solutions are not possible. Some methods of approximation begin directly with the strong statement of the problem. The most notable example is the finite difference method (e.g., see [1]). The finite element method requires a different formulation, which is treated in the next section. \section*{1.3 WEAK, OR VABIATIOMAL, FORM OF THE PROBLEM} To define the weak, or variational, counterpart of ( $S$ ), we need to characterize two classes of functions. The first is to be composed of candidate, or trial, solutions. From the outset, we shall require these possible solutions to satisfy the boundary condition $u(1)=g$. The other boundary condition will not be required in the definition. Furthermore, so that certain expressions to be employed make sense, we shall require that the derivatives of the trial solutions be square-integrable.

 Additional context 2: \begin{enumerate} \item The boundary condition $-u_{, x}(0)=h$ is not explicitly mentioned in the statement of ( $W$ ). From the preceding proof, we see that this boundary condition is, however, implied by the satisfaction of the variational equation. Boundary conditions of this type are referred to as \textbf{\textit{natural boundary conditions}}. On the other hand, trial solutions are explicitly required to satisfy the boundary condition $u(1)=g$. Boundary conditions of this type are called \textbf{\textit{essential boundary conditions}}. The fact that solutions of the variational equation satisfy natural boundary conditions is extremely important in more complicated situations which we will consider later on. \item The method used to prove part (b) of the proposition goes under the name of the \textbf{\textit{fundamental lemma}} in the literature of the calculus of variations. In essence, it is the methodology that enables us to deduce the differential equations and boundary conditions implied by the weak formulation. To develop correct weak forms for complex, multidimensional problems, it is essential to have a thorough understanding of these procedures. \end{enumerate} Now we see that to obtain approximate solutions to the original boundary-value problem we have alternative starting points, i.e., the strong or weak statements of the problem. Finite element methods are based upon the latter. Roughly speaking, the basic idea is to approximate $f$ and $\mathcal{U}$ by convenient, finite-dimensional collections of functions. (Clearly, $f$ and $\mathcal{U}$ contain infinitely many functions.) The variational equations are then solved in this finite-dimensional context. An explicit example of how to go about this is the subject of the next section. However, we first introduce some additional notations to simplify subsequent writing. Let \begin{align*} a(w, u) & =\int_{0}^{1} w_{, x} u_{, x} d x \tag{1.4.8}\\ (w, f) & =\int_{0}^{1} w f d x \tag{1.4.9} \end{align*} In terms of (1.4.8) and (1.4.9), the variational equation takes the form \begin{equation*} a(w, u)=(w, f)+w(0) h \tag{1.4.10} \end{equation*} Here, $a(\cdot, \cdot)$ and $(\cdot, \cdot)$ are examples of \textbf{\textit{symmetric, bilinear forms}}. What this means is as follows: Let $c_{1}$ and $c_{2}$ be constants and let $u, v$, and $w$ be functions. Then the symmetry property is",50,"What are the natural boundary conditions that arise from the weak formulation, and how do they differ from essential boundary conditions?","The natural boundary conditions that arise from the weak formulation are implied by the variational equation and are associated with the boundary condition $-u_{, x}(0) = h$. This condition is not explicitly mentioned in the statement of the weak formulation but is satisfied by the solutions of the variational equation.

In contrast, essential boundary conditions are explicitly required for the trial solutions. In this context, the essential boundary condition is $u(1) = g$, which must be satisfied by the candidate or trial solutions.

To summarize:
- **Natural Boundary Conditions**: Implied by the variational equation, such as $-u_{, x}(0) = h$.
- **Essential Boundary Conditions**: Explicitly required for trial solutions, such as $u(1) = g$."
58,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain

 Additional context 1: \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the

 Additional context 2: a typical internal node (i.e., $2 \leq A \leq n$ ) \[ N_{A}(x)=\left\{\begin{array}{cl} \frac{\left(x-x_{A-1}\right)}{h_{A-1}}, & x_{A-1} \leq x \leq x_{A} \tag{1.8.1}\\ \frac{\left(x_{A+1}-x\right)}{h_{A}}, & x_{A} \leq x \leq x_{A+1} \\ 0, & \text { elsewhere } \end{array}\right. \] whereas for the boundary nodes we have \begin{align*} & N_{1}(x)=\frac{x_{2}-x}{h_{1}}, \quad x_{1} \leq x \leq x_{2} \tag{1.8.2}\\ & N_{n+1}(x)=\frac{x-x_{n}}{h_{n}}, \quad x_{n} \leq x \leq x_{n+1} \tag{1.8.3} \end{align*} The shape functions are sketched in Fig. 1.8.1. For obvious reasons, they are referred to variously as ""hat,"" ""chapeau,"" and ""roof"" functions. Note that $N_{A}\left(x_{B}\right)=\delta_{A B}$, where\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21} Figure 1.8.1 Basis functions for the piecewise linear finite element space.\\ $\delta_{A B}$ is the Kronecker delta (i.e., $\delta_{A B}=1$ if $A=B$, whereas $\delta_{A B}=0$ if $A \neq B$ ). In words, $N_{A}$ takes on the value 1 at node $A$ and is 0 at all other nodes. Furthermore, $N_{A}$ is nonzero only in the subintervals that contain $x_{A}$. A typical member $w^{h} \in \mathcal{U}^{h}$ has the form $\sum_{A=1}^{n} c_{A} N_{A}$ and appears as in Fig. 1.8.2. Note that $w^{\boldsymbol{h}}$ is continuous but has discontinuous slope across each element boundary. For this reason, $w_{, x}^{h}$, the generalized derivative of $w^{h}$, will be piecewise constant, experiencing discontinuities across element boundaries. (Such a function is sometimes called a generalized step function.) Restricted to each element domain, $w^{h}$ is a linear polynomial in $x$. In respect to the homogeneous essential boundary condition, $w^{h}(1)=0$. Clearly, $w^{h}$ is identically zero if and only if each $c_{A}=0, A=1,2$, . . . , $\boldsymbol{n}$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-21(1)} Figure 1.8.2 A typical member $w^{\boldsymbol{h}} \in \mathcal{U}^{\boldsymbol{k}}$.\\ Typical members of $f^{h}$ are obtained by adding $g^{h}=g N_{n+1}$ to typical members of $\mathcal{U}^{h}$. This ensures that $u^{h}(1)=g$. The piecewise linear finite element functions are the simplest and most widely used finite element functions for one-dimensional problems. Exercise 1. Consider the weak formulation of the",40,"Discuss the implications of using piecewise cubic Hermite shape functions in the finite element approximation, particularly regarding the continuity of derivatives at the nodes.","The use of piecewise cubic Hermite shape functions in the finite element approximation has specific implications regarding the continuity of derivatives at the nodes. 

1. **Continuity of First Derivatives**: The piecewise cubic Hermite shape functions are designed to ensure that the first derivatives of the approximating functions are continuous across the nodes. This means that at each internal node, the slopes (first derivatives) of the cubic polynomials on either side of the node match. This property is crucial for ensuring that the finite element solution behaves smoothly in terms of the first derivative, which is particularly important in applications such as beam bending where the slope represents the angle of deflection.

2. **Discontinuity of Second Derivatives**: While the first derivatives are continuous, the piecewise cubic Hermite shape functions do not guarantee continuity of the second derivatives at the nodes. This means that the curvature (second derivative) may exhibit discontinuities at the nodes. In the context of beam bending, this is significant because the curvature is directly related to the bending moment, and discontinuities in curvature can affect the accuracy of the moment calculations.

3. **Implications for Analysis**: The lack of continuity in the second derivatives implies that while the finite element method can provide accurate approximations for displacements and slopes, care must be taken when interpreting results related to curvature. In problems where curvature is of primary interest, such as in bending moment calculations, the discontinuities may lead to inaccuracies unless additional techniques (such as higher-order elements or special post-processing) are employed.

4. **Generalized Derivatives**: The context mentions that the second derivative of the piecewise cubic Hermite functions is a generalized step function, indicating that while the function itself is smooth within each element, the transition at the nodes introduces a form of discontinuity in the second derivative. This reinforces the need for understanding generalized derivatives when working with finite element methods.

In summary, the use of piecewise cubic Hermite shape functions allows for continuous first derivatives, which is beneficial for ensuring smooth slopes at nodes, but results in discontinuous second derivatives, which can complicate the analysis of curvature and bending moments in structural applications."
59,"Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$. This is a space of piecewise cubic Hermite shape functions. Observe that $w^{h} \in \mathcal{U}^{h}$ need not have continuous second derivatives at the nodes. For notational simplicity, we write $x_{1}$ and $x_{2}$ in place of $x_{\mathrm{A}}$ and $x_{\mathrm{A}+1}$, respectively. On each subinterval, show that $w^{h}$ may be written as $$ w^{h}(x)=N_{1}(x) w^{h}\left(x_{1}\right)+N_{3}(x) w^{h}\left(x_{2}\right)+N_{2}(x) w_{. x}^{h}\left(x_{1}\right)+N_{4}(x) w_{, x}^{h}\left(x_{2}\right) $$ where $$ \begin{aligned} & N_{1}(x)=\frac{-\left(x-x_{2}\right)^{2}\left[-h+2\left(x_{1}-x\right)\right]}{h^{3}} \\ & N_{2}(x)=\frac{\left(x-x_{1}\right)^{\prime}\left(x-x_{2}\right)^{2}}{h^{2}} \\ & N_{3}(x)=\frac{\left(x-x_{1}\right)^{2}\left[h+2\left(x_{2}-x\right)\right]}{h^{3}} \\ & N_{4}(x)=\frac{\left(x-x_{1}\right)^{2}\left(x-x_{2}\right)}{h^{2}} \end{aligned} $$ Hint: Let $w^{h}(x)=c_{1}+c_{2} x+c_{3} x^{2}+c_{4} x^{3}$, where the $c^{\prime}$ s are constants. Determine them by requiring the following four conditions hold: $$ \begin{aligned} w^{h}\left(x_{1}\right) & =c_{1}+c_{2} x_{1}+c_{3} x_{1}^{2}+c_{4} x_{1}^{3} \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the $4 \times 4$ element stiffness matrix, $$ k_{p q}^{e}=\int_{x_{1}^{e}}^{x_{2}^{e}} N_{p, x x} E I N_{q, x x} d x \quad 1 \leq p, q \leq 4 $$ where $h^{e}=x_{2}^{e}-x_{1}^{\mathrm{e}}$.\\ k. (See the exercise in Sec. 1.8.) Consider the weak formulation. Assume $w \in \mathcal{U}$ and $u \in \mathfrak{f}$ are smooth on element interiors (i.e., on $] x_{A}, x_{A+1}[$ ) but may exhibit discontinuities in second, and higher, derivatives across element boundaries. (Functions of this type contain the piecewise-cubic Hermite functions.) Show that $$ \begin{aligned} 0= & \sum_{A=1}^{n} \int_{x_{A}}^{x_{A}+1} w\left(E I u_{, x x x x}-f\right) d x \\ & -w_{, x}(0)\left(E I u_{, x x}\left(0^{+}\right)-M\right) \\ & +w(0)\left(E I u_{, x x x}\left(0^{+}\right)-Q\right) \\ & -\sum_{A=2}^{n} w_{, x}\left(x_{A}\right) E I\left(u_{, x x}\left(x_{A}^{+}\right)-u_{, x x}\left(x_{A}^{-}\right)\right) \\ & +\sum_{A=2}^{n} w\left(x_{A}\right) E I\left(u_{, x x x}\left(x_{A}^{+}\right)-u_{, x x x}\left(x_{A}^{-}\right)\right) \end{aligned} $$ from which it may be concluded that the Euler-Lagrange conditions are\\ i. EI $u_{, x x x x}(x)=f(x)$, where $\left.x \in\right] x_{A}, x_{A+1}[$ and $A=1,2, \ldots, n$\\ ii. $E I u_{, x x}\left(0^{+}\right)=M$\\ iii. EI $u_{, x x x}\left(0^{+}\right)=Q$\\ iv. $E I u_{, x x}\left(x_{A}^{+}\right)=E I u_{, x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$\\ v. $E I u_{, x x x}\left(x_{A}^{+}\right)=E I u_{, x x x}\left(x_{A}^{-}\right)$, where $A=2,3, \ldots, n$ Note that (i) is the equilibrium equation restricted to the element interiors, and (iv) and (v) are continuity conditions across element boundaries of moment and shear, respectively. Contrast these results with those obtained for functions $w$ and $u$, which are globally smooth. The Galerkin finite element formulation yields a solution that approximates (i) through (v). \section*{Appendix} \subsection*{An Elementary Discussion of Continuity, Differentiability, and Smoothness} Throughout Chapter 1 we have introduced mathematical terminologies and ideas in a gradual, as-needed format. Many of these ideas had to do with the continuity and differentiability of functions. The presentation was, admittedly, somewhat vague on these points in order that the main ideas would not be overencumbered. Careful characterization of the properties of functions is an essential ingredient in the development and analysis of finite element methods. However, to pursue this subject deeply would take us into the realm of serious mathematical analysis, which is outside the scope of this book. Nevertheless, we feel compelled to say a few additional words on the subject to round out the presentation in Chapter 1 and to expose the reader to notations and ideas that will probably be encountered if he or she attempts to read published papers on finite elements. The discussion here will be restricted to one dimension. In Chapter 1 we spoke of continuously differentiable functions. If we have a grasp of the notion of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta","

 Additional context 0: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 1: \\ w^{h}\left(x_{2}\right) & =c_{1}+c_{2} x_{2}+c_{3} x_{2}^{2}+c_{4} x_{2}^{3} \\ w_{,x}^{h}\left(x_{1}\right) & =c_{2}+2 c_{3} x_{1}+3 c_{4} x_{1}^{2} \\ w_{,x}^{h}\left(x_{2}\right) & =c_{2}+2 c_{3} x_{2}+3 c_{4} x_{2}^{2} \end{aligned} $$ \footnote{${ }^{8}$ The notation $w^{k} \in C^{1}$ means $w^{k}$ is continuously differentiable.} Sketch the element functions $N_{1}, N_{2}, N_{3}$, and $N_{4}$, and their typical global counterparts. The finite element space described in part (b) results in exact nodal displacements and slopes (first derivatives), analogous to the case presented in Sec. 1.10. In part ( g ), you are asked to prove this. In problems of beam bending we are generally interested in curvatures (second derivatives) for bending moment calculations.\\ c. Locate the optimal curvature points in the sense of Barlow. Warning: The algebraic manipulations can be tiresome unless certain simplifications are observed. If we work in the $\xi$-element coordinate system introduced in Sec. 1.12 (recall $\left.\xi=\left(2 x-x_{A}-x_{A+1}\right) / h_{A}\right)$, the location of the Barlow curvature points may be expressed as $\xi= \pm 1 / \sqrt{3}$. That is, there are two symmetrically spaced optimal locations to compute curvature.\\ d. What is the rate of convergence of curvature at these points? (Ans. $O\left(h^{3}\right)$ ).\\ e. If the segment of the beam $\left[x_{A}, x_{A+1}\right]$ is unloaded (i.e., $u_{, x x x}=0$, where $u$ is the exact solution), which points are optimal?\\ f. Assume $n_{e l}=1$ (i.e., one element) and $f(x)=c=$ constant. Set up and solve the Galerkin-finite element equations. Plot $u^{h}$ and $u ; u_{, x}^{h}$ and $u_{, x} ;$ and $u_{, x x}^{h}$ and $u_{, x x}$. Indicate the locations of the Barlow curvature points.\\ g. Prove that $$ \begin{gathered} u^{h}\left(x_{A}\right)=u\left(x_{A}\right) \\ u_{, x}^{h}\left(x_{A}\right)=u_{, x}\left(x_{A}\right) \end{gathered} $$ where $x_{A}$ is a typical node (i.e., prove the displacements and slopes are exact at the nodes). To do the second part you will have to be familiar with the dipole, $\delta_{x}\left(x-x_{A}\right)$, which is the generalized derivative of the delta function.\\ h. Show that the Barlow curvature points are exact when $f(x)=c=$ constant.\\ i. Why do we require that the functions in $\mathcal{U}^{h}$ have continuous first derivatives?\\ j. Calculate the

 Additional context 2: & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ & {\left[\begin{array}{rrrr|r} 1 & 0 & 0 & 0 & 4 \\ 0 & 1 & 0 & 0 & 3 \\ 0 & 0 & 1 & 0 & 2 \\ 0 & 0 & 0 & 1 & 1 \end{array}\right]} \\ \begin{array}{l} \left\{ \begin{array}{l} d_{1} \\ d_{2} \\ d_{3} \\ d_{4} \end{array} \right\} = \left\{ \begin{array}{l} 4 \\ 3 \\ 2 \\ 1 \end{array} \right\} \end{array} \end{aligned} $$ Exercise 1. Consider the boundary-value problem discussed in the previous sections: $$ \begin{aligned} u_{, x x}(x)+f(x) & =0 \quad x \in] 0,1[ \\ u(1) & =g \\ -u_{, x}(0) & =h \end{aligned} $$ Assume $f=g x$, where $g$ is constant, and $g=h=0$.\\ a. Employing the linear finite element space with equally spaced nodes, set up and solve the Galerkin finite element equations for $n=4\left(h=\right.$ mesh parameter $\left.=\frac{1}{4}\right)$. Recall that in Sec. 1.7 this was carried out for $n=1$ and $n=2\left(h=1\right.$ and $h=\frac{1}{2}$, respectively). Do not invert the ctiffness matrix $K$; use Gauss elimination to solve $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ or a more sophisticated direct factorization scheme if you know one. You can check your answers since they must be exact at the nodes.\\ b. Let $r e_{, x}=\left|u_{, x}^{h}-u_{. x}\right| /(q / 2)$, the relative error in $u_{. x}$. Compute $r e_{, x}$ at the midpoints of the four elements. They should all be equal. (This was also the case for $n=2$.)\\ c. Employing the data for $h=1, \frac{1}{2}$, and $\frac{1}{4}$, plot $\ln r e_{, x}$ versus $\ln h$.\\ d. Using the error analysis for $r e_{, x}$ at the midpoints presented in Sec. 1.10, answer the following questions:\\ i. What is the significance of the slope of the graph in part (c)?\\ ii. What is the significance of the $y$-intercept? \section*{1.12 THE ELEMENT POINT OF MEN} So far we have viewed the finite element method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of",30,"How does the Galerkin method ensure that the finite element solution converges to the exact solution as the mesh is refined, and what is the expected rate of convergence for curvature at optimal points?","The Galerkin method ensures that the finite element solution converges to the exact solution as the mesh is refined by constructing the finite element approximation in such a way that it satisfies the weak form of the governing equations. Specifically, the method involves selecting a finite-dimensional subspace of the function space (in this case, $\mathcal{U}^{h}$) and ensuring that the approximate solution minimizes the residual in the weak formulation. As the mesh is refined (i.e., as the size of the elements decreases), the approximation becomes more accurate, leading to convergence to the exact solution.

The expected rate of convergence for curvature at optimal points is given as $O(h^3)$, indicating that the error in the curvature decreases cubically as the mesh is refined. This means that if the mesh size $h$ is halved, the error in the curvature will decrease by a factor of approximately $8$."
60,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: of a continuous function, then continuously differentiable functions pose no problem. Definition: A function $f: \Omega \rightarrow \mathbb{R}$ (recall $\Omega=] 0,1[$ is said to be $k$-times continuously differentiable, or of class $C^{k}=C^{k}(\Omega)$, if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. A $C^{0}$ function is simply a continuous function. A $C^{\infty}$. function is one that possesses a continuous derivative of any order (i.e., $j=0,1, \ldots, \infty$ ). Definition: A function $f$ is said to be of class $C_{b}^{k}$ if it is $C^{k}$ and bounded (i.e., $|f(x)|<c$, where $c$ is a constant, for all $x \in \Omega$ ). \subsection*{Example 1} The functions defined by monomials (i.e., $f(x)=1, x, x^{2}$, etc.) are $C_{b}^{\infty}$. \subsection*{Example 2} The function $f(x)=1 / x$ is continuous on $\Omega$, as are all its derivatives; hence it is $C^{\infty}$, but it is not bounded (i.e., there does not exist a constant $c$ such that $|1 / x|<c$ for all $x \in \Omega$; see Fig. 1.I.1). Consequently this function is not of class $C_{b}^{k}$ for any $k \geq 0$.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-53} Figure 1.I. 1 A continuous function that is not bounded. \subsection*{Example 3} The function \[ f(x)= \begin{cases}x, & x \leq \frac{1}{2} \tag{1.I.1}\\ 1 / 2, & x>\frac{1}{2}\end{cases} \] is continuous but not continuously differentiable (i.e., it is $C_{b}^{0}$ but not $C_{b}{ }^{1}$ ). Punctions in $C_{b}^{k}, k \geq 1$, but not in $C_{b}^{k+1}$ may be constructed by integrating (1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and

 Additional context 1: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is

 Additional context 2: \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain",80,"What is the difference between the classes of functions $C^{k}$ and $C_{b}^{k}$, and under what conditions do they coincide?","The classes of functions $C^{k}$ and $C_{b}^{k}$ differ primarily in the boundedness of the functions they contain. 

1. **Definition of $C^{k}$**: A function $f: \Omega \rightarrow \mathbb{R}$ is said to be of class $C^{k}(\Omega)$ if its derivatives of order $j$, where $0 \leq j \leq k$, exist and are continuous functions. There are no restrictions on the values of the function; it can be unbounded.

2. **Definition of $C_{b}^{k}$**: A function $f$ is said to be of class $C_{b}^{k}(\Omega)$ if it is $C^{k}$ and additionally bounded, meaning there exists a constant $c$ such that $|f(x)| < c$ for all $x \in \Omega$.

3. **Conditions for Coincidence**: The difference between $C^{k}$ and $C_{b}^{k}$ disappears when the closed unit interval $\overline{\Omega} = [0,1]$ is used instead of the open interval $\Omega = ]0,1[$. In this case, if $f$ is $C^{k}([0,1])$, then $f(0)$ and $f(1)$ are real numbers and cannot be $\infty$, thus precluding unboundedness.

In summary, $C^{k}$ allows for unbounded functions, while $C_{b}^{k}$ requires boundedness. They coincide when considering functions defined on the closed interval $[0,1]$."
61,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is

 Additional context 1: is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate}

 Additional context 2: \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain",90,"Explain the significance of Sobolev spaces in the context of finite element analysis and boundary-value problems, particularly focusing on the relationship between $H^{k}$ functions and classical differentiable functions.","Sobolev spaces play a crucial role in finite element analysis and the study of boundary-value problems. They provide a framework for understanding the properties of functions that are not necessarily smooth in the classical sense but still possess certain integrability and differentiability characteristics.

1. **Definition of Sobolev Spaces**: The Sobolev space of degree $k$, denoted by $H^{k}(\Omega)$, consists of functions that have square-integrable generalized derivatives up to order $k$. Specifically, a function $w$ belongs to $H^{k}(\Omega)$ if it is in $L_{2}$ (i.e., $\int_{0}^{1} w^{2} dx < \infty$) and all its derivatives up to order $k$ are also in $L_{2}$.

2. **Relationship to Classical Functions**: The relationship between Sobolev spaces and classical differentiable functions is articulated by Sobolev's theorem, which states that in one dimension, if a function is in $H^{k+1}$, then it is also a $C_{b}^{k}$ function. This means that $H^{k+1}$ functions are not only square-integrable but also possess continuous and bounded classical derivatives up to order $k$. For instance, $H^{1}$ functions are continuous and bounded, while $H^{2}$ functions are $C_{b}^{1}$, indicating they have bounded, continuous first derivatives.

3. **Importance in Finite Element Analysis**: In finite element analysis, functions are often piecewise defined and may not be smooth across element boundaries. However, they are typically smooth within the interiors of elements. The Sobolev spaces allow for the characterization of these functions, acknowledging their generalized derivatives and integrability properties. This is particularly important when dealing with weak formulations of boundary-value problems, where the classical notion of differentiability may not hold.

4. **Handling Singularities**: Sobolev spaces also help in managing singularities that may arise in physical problems. For example, while $x^{-1/4}$ is in $L_{2}$, $x^{-1/2}$ is not, highlighting how square-integrability can preclude certain singular behaviors. This aspect is vital in applications such as fracture mechanics, where understanding the behavior of solutions near singularities is crucial.

In summary, Sobolev spaces provide a robust mathematical framework that bridges the gap between classical differentiability and the generalized functions used in finite element analysis, facilitating the study of elliptic boundary-value problems and ensuring that the functions used in analysis have the necessary properties for convergence and stability."
62,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is

 Additional context 1: is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate}

 Additional context 2: incorporated in the definition of $\mathcal{V}^{\boldsymbol{h}}$ which together result in the positive-definiteness of $\boldsymbol{K}$ and thus ensure its invertibility. \item The explicit structure of the shape functions, which will be delineated in Chapter 3, will also result in $\boldsymbol{K}$ being banded. \end{enumerate} \subsection*{Exercise 1.} (This exercise is a multidimensional analog of the one contained in Sec. 1.8.) Let $$ \Gamma_{\text{int }}=\left(\bigcup_{e=1}^{n_{el}} \Gamma^{e}\right)-\Gamma \quad \text { (interior element boundaries) } $$ One side of $\Gamma_{\text{int}}$ is (arbitrarily) designated to be the "" + side"" and the other is the "" - side."" Let $\boldsymbol{n}^{+}$and $\boldsymbol{n}^{-}$be unit normals to $\Gamma_{\text {int }}$ which point in the minus and plus directions, respectively. Clearly $n^{+}=-n^{-}$. Let $q_{i}^{+}$and $q_{i}^{-}$denote the values of $q_{i}$ obtained by approaching $x \in \Gamma_{\mathrm{int}}$ from + and - sides, respectively. The ""jump"" in $q_{n}=q_{i} n_{i}$ at $x$ is defined to be $$ \begin{aligned} {\left[q_{n}\right] } & =\left(q_{i}^{+}-q_{i}^{-}\right) n_{i}^{+} \\ & =q_{i}^{+} n_{i}^{+}+q_{i}^{-} n_{i}^{-} \end{aligned} $$ As may be easily verified, the jump is invariant with respect to reversing the + and - designations. Consider the weak formulation (i.e., (2.3.6)) and assume $w$ and $u$ are smooth on the element interiors but may experience discontinuities in gradient across element boundaries. (Functions of this type contain the standard $C^{0}$ finite element interpolations; see Chapter 3.) Show that $$ 0=\sum_{e=1}^{n_{e l}} \int_{\Omega^{e}} w\left(q_{i, i}-\ell\right) d \Omega-\int_{\Gamma_{h}} w\left(q_{n}+h\right) d \Gamma-\int_{\Gamma_{int}} w\left[q_{n}\right] d \Gamma $$ from which the Euler-Lagrange conditions may be readily deduced:\\ i. $q_{i, i}= \ell$ in $\bigcup_{e=1}^{n_{el}} \Omega^{e}$\\ ii. $-q_{n}=h$ on $\Gamma_{h}$\\ iii. $\left[q_{n}\right]=0$ on $\Gamma_{\text {int }}$ As may be seen, (i) is the heat equation on the element interiors and (iii) is a continuity condition across element boundaries on the heat flux. Contrast the present results with those obtained assuming $w$ and $u$ are globally smooth. The Galerkin finite element formulation obtains an approximate solution to (i) through (iii). \subsection*{2.5 HEAT",85,Define the Sobolev space $H^{k}$ and describe the conditions under which a function in this space possesses square-integrable generalized derivatives up to order $k$.,"The Sobolev space $H^{k}$, denoted as $H^{k}(\Omega)$, is defined as follows:

$$
H^{k} = H^{k}(\Omega) = \left\{ w \mid w \in L_{2}; w_{, x} \in L_{2}; \ldots; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2} \right\}
$$

This means that a function $w$ belongs to the Sobolev space $H^{k}$ if:

1. The function $w$ itself is square-integrable, i.e., it belongs to the space $L_{2}$, which is defined as:
   $$
   L_{2} = L_{2}(\Omega) = \left\{ w \mid \int_{0}^{1} w^{2} dx < \infty \right\}
   $$

2. The generalized derivatives of $w$ up to order $k$ (denoted as $w_{, x}, w_{xx}, \ldots, w_{x \ldots x}$) are also square-integrable, meaning each of these derivatives must also belong to the space $L_{2}$.

In summary, a function in the Sobolev space $H^{k}$ possesses square-integrable generalized derivatives up to order $k$ if it and its derivatives up to that order are all square-integrable functions."
63,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is

 Additional context 1: is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate}

 Additional context 2: \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain",95,"What are the implications of Sobolev's theorem regarding the smoothness of functions in the context of finite element methods, specifically how does it relate to the continuity and boundedness of $H^{1}$ and $H^{2}$ functions?","Sobolev's theorem has significant implications regarding the smoothness of functions in the context of finite element methods. Specifically, it states that in one dimension, if a function is of class $H^{k+1}$, then it is also a $C_{b}^{k}$ function. This means that such functions possess certain continuity and boundedness properties.

1. **For $H^{1}$ functions**:
   - According to Sobolev's theorem, $H^{1}$ functions are continuous and bounded. This is important in finite element methods because it ensures that the functions used in the analysis do not exhibit singularities or unbounded behavior, which could lead to numerical instability or inaccuracies in the solution.

2. **For $H^{2}$ functions**:
   - Similarly, $H^{2}$ functions are classified as $C_{b}^{1}$ functions by Sobolev's theorem. This means that they not only have bounded and continuous first derivatives but also that the second derivatives are generalized functions (specifically, they can be represented as generalized step functions). This property is crucial for ensuring that the finite element approximations are smooth enough for the analysis of higher-order derivatives, which is often required in the study of physical problems modeled by partial differential equations.

In summary, Sobolev's theorem guarantees that $H^{1}$ functions are continuous and bounded, while $H^{2}$ functions possess bounded, continuous classical derivatives. These properties are essential for the reliability and accuracy of finite element methods in solving boundary-value problems."
64,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain

 Additional context 1: derivative is exact at the midpoints. This is the case when $f(x)=p=$ constant.\\ 3. In linear elastic rod theory, the derivatives are proportional to the stresses. The midpoints of linear ""elements"" are sometimes called the Barlow stress points, after Barlow [8], who first noted that points of optimal accuracy existed within elements. Exercise 1. Assume the mesh length is constant (i.e., $h_{A}=h, A=1,2, \ldots, n$ ). Consider the standard finite difference ""stencil"" for $u_{, x x}+\phi=0$ at a typical internal node, namely, \begin{equation*} \frac{u_{A+1}-2 u_{A}+u_{A-1}}{h^{2}}+f_{A}=0 \tag{1.10.18} \end{equation*} Assuming \& varies in piecewise linear fashion and so can be expanded as \begin{equation*} f=\sum_{A=1}^{n+1} f_{A} N_{A} \tag{1.10.19} \end{equation*} where the $f_{A}$ 's are the nodal values of $f$, set up the finite element equation associated with node $A$ and contrast it with (1.10.18). Deduce when ( 1.10 .18 ) will also be capable of exhibiting superconvergence phenomena. (That is, what is the restriction on $f$?) Set up the finite element equation associated with node 1 , accounting for nonzero $h$. Discuss this equation from the point of view of finite differences. (For further comparisons along these lines, the interested reader is urged to consult [6], Chapter 1.) Summary. The Galerkin finite element solution $u^{h}$, of the problem (S), possesses the following properties:\\ i. It is exact at the nodes.\\ ii. There exists at least one point in each element at which the derivative is exact.\\ iii. The derivative is second-order accurate at the midpoints of the elements. \section*{1.11 INTERLUDE: CAUSS ELMINATION; HAND-CALCULATION VERSION} It is important for anyone who wishes to do finite element analysis to become familiar with the efficient and sophisticated computer schemes that arise in the finite element method. It is felt that the best way to do this is to begin with the simplest scheme, perform some hand calculations, and gradually increase the sophistication as time goes on. To do some of the problems we will need a fairly efficient method of solving matrix equations by hand. The following scheme is applicable to systems of equations\\ $\boldsymbol{K} \boldsymbol{d}=\boldsymbol{F}$ in which no pivoting (i.e., reordering) is necessary. For example, symmetric, positive-definite coefficient matrices never require pivoting. The

 Additional context 2: method simply as a particular Galerkin approximation procedure applied to the weak statement of the problem in question. What makes what we have done a finite element procedure is the character of the selected basis functions; particularly their piecewise smoothness and ""local support"" (i.e., $N_{A}=0$ outside a neighborhood of node $A$ ). This is the mathematical point of view; it is a global point of view in that the basis functions are considered to be defined everywhere on the domain of the boundary-value problem. The global viewpoint is useful in establishing the mathematical properties of the finite element method. This can be seen in Sec. 1.10 and will be made more apparent later on. Now we wish to discuss another point of view called the local, or element, point of view. This viewpoint is the traditional one in engineering and is useful in the computer implementation of the finite element method and in the development of finite elements. We begin our treatment of the local point of view with a question: What is a finite element? We shall attempt to give the answer in terms of the piecewise linear finite element space that we defined previously. An individual element consists of the following quantities. Linear finfte element (global description) \begin{center} \begin{tabular}{lll} $(g 1)$ & Domain: & $\left[x_{A}, x_{A+1}\right]$ \\ $(g 2)$ & Nodes: & $\left\{x_{A}, x_{A+1}\right\}$ \\ $(g 3)$ & Degrees of freedom: & $\left\{d_{A}, d_{A+1}\right\}$ \\ $(g 4)$ & Shape functions: & $\left\{N_{A}, N_{A+1}\right\}$ \\ $(g 5)$ & Interpolation function: & \\ $u^{h}(x)=N_{A}(x) d_{A}+N_{A+1}(x) d_{A+1}$, & $x \in\left[x_{A}, x_{A+1}\right]$ & \\ \end{tabular} \end{center} (Recall $d_{A}=u^{h}\left(x_{A}\right)$.) In words, a linear finite element is just the totality of paraphemalia associated with the globally defined function $u^{h}$ restricted to the element domain. The above quantities are in terms of global parameters-namely, the global coordinates, global shape functions, global node ordering, and so on. It is fruitful to introduce a local set of quantities, corresponding to the global ones, so that calculations for a typical element may be standardized. These are given as follows: Linear finite element (local description)\\ (l1) Domain: $\left[\xi_{1}, \xi_{2}\right]$ \footnotetext{${ }^{5}$ In weighted residual methods in which $g^{h}$ and $\sigma^{N}$ are",75,Discuss the concept of generalized derivatives in finite element functions and provide examples of how they differ for piecewise linear functions and Hermite cubics.,"In the context of finite element functions, generalized derivatives are essential for understanding the behavior of these functions, especially across element boundaries where traditional derivatives may not exist. 

1. **Piecewise Linear Finite Element Functions**:
   - These functions are characterized as being of class $C_{b}^{0}$, meaning they are continuous but not necessarily differentiable. 
   - The first derivative of a piecewise linear finite element function is a generalized step function. This means that while the function itself is continuous, its derivative has discontinuities at the element boundaries, reflecting the piecewise nature of the function.
   - The second derivative of a piecewise linear function is a generalized Dirac delta function. This indicates that the second derivative is not defined in the classical sense but can be represented as a distribution concentrated at the nodes of the elements.

2. **Hermite Cubics**:
   - In contrast, Hermite cubics are classified as $C_{b}^{1}$ functions, which means they are continuously differentiable. 
   - The first derivative of Hermite cubics is continuous across the entire domain, including at the element boundaries. This is a significant improvement over piecewise linear functions, where the first derivative is not continuous.
   - The second derivative of Hermite cubics is a generalized step function, similar to the first derivative of piecewise linear functions. This reflects that while the first derivative is smooth, the second derivative may still exhibit discontinuities at the element boundaries.

In summary, generalized derivatives in finite element functions highlight the differences in smoothness and continuity between piecewise linear functions and Hermite cubics. Piecewise linear functions exhibit discontinuities in their first derivatives, while Hermite cubics maintain continuity in their first derivatives but may still have generalized behavior in their second derivatives."
65,"(1.I.1) $k$ times. For example, \[ f(x)=\left\{\begin{array}{cc} \frac{x^{2}}{2}, & x \leq \frac{1}{2} \tag{1.I.2}\\ \frac{\left(x-\frac{1}{4}\right)}{2}, & x>\frac{1}{2} \end{array}\right. \] is in $C_{b}^{1}$ but not $C_{b}^{2}$. (The reader may wish to verify this.) There is no universally accepted definition of what is meant by a ""smooth"" function. However, it is generally taken to mean that at least one derivative exists and is continuous (i.e., either $C^{1}$ or $C_{b}^{l}$ ) and sometimes means $k>1$, even $\infty$. The $C^{k}$ and $C_{b}^{k}$ functions employ the classical notion of a derivative in their definitions. If we employ the closed unit interval, $\overline{\boldsymbol{\Omega}}=[0,1]$, instead of $\boldsymbol{\Omega}=] 0,1[$, the difference between $C^{k}$ and $C_{b}^{k}$ disappears. This is because if $f$ is $C^{k}([0,1]), f(0)$ and $f(1)$ are real numbers and are not allowed to be $\infty$. Thus unboundedness, as in the example above, is precluded. Very often, we think of $C^{k}$ functions in this light. However, in some situations the differences between $C^{k}(\Omega)$ and $C_{b}^{k}(\Omega)$ must be kept in mind. Generally, finite element functions are smooth on element interiors (there are exceptions, however) but possess only low-order continuity across element boundaries. One might be tempted to characterize them as locally smooth but globally ""rough."" The piecewise linear finite element functions discussed in Sec. 1.8 are of class $C_{b}^{0}$. The Hermite cubics employed in Sec. 1.16 are $C_{b}^{1}$. To calculate derivatives of such functions we need to employ the notion of a ""generalized derivative,"" as was used in solving the Green's function problem of Sec. 1.10. For example, the first derivative of a piecewise linear finite element function is a generalized step function; the second derivative is a generalized Dirac delta function (i.e., delta functions, of various amplitudes, acting at the nodes). In the case of the Hermite cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate} \setcounter{enumi}{9} \item M. J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp, ""Stiffness and deflection analysis of complex structures,"" Journal of Aeronautical Sciences, 23 (1956), 805-823. \end{enumerate} \section*{Section 1.15} \begin{enumerate} \setcounter{enumi}{10} \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item P. J. Davis, Interpolation and Approximation. New York: Blaisdell, 1963. \end{enumerate} \section*{Appendix 1.1} \begin{enumerate} \setcounter{enumi}{12} \item P. G. Ciarlet, The Finite Element Method for Elliptic Problems. New York: NorthHolland, 1978. \item J. T. Oden and J. N. Reddy, An Introduction to the Mathematical Theory of Finite Elements. New York: Academic Press, 1978. \item J. T. Oden, Applied Functional Analysis. Englewood Cliffs, N. J.: Prentice-Hall, 1979. \end{enumerate}","

 Additional context 0: cubics, the first derivative is continuous, the second a generalized step function, and so on. We have seen in Sec. 1.16 that other generalized functions also arise in the analysis of finite element behavior (namely, the dipole). The useful examples of generalized functions are by no means exhausted by what we have seen thus far. However, the ones we have introduced are perhaps the most basic. In the mathematical analysis of boundary-value problems, and consequently in finite element analysis, we need to introduce classes of functions that possess generalized derivatives and, in addition, certain integrability properties. We have encountered such functions in the statements of weak formulations in Sec. 1.3 and 1.16. These are particular examples of Sobolev spaces of functions defined as follows: \begin{equation*} H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\} \tag{1.I.3} \end{equation*} where \begin{equation*} L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\} \tag{1.I.4} \end{equation*} In words, the Sobolev space of degree $k$, denoted by $\boldsymbol{H}^{\boldsymbol{k}}$, consists of functions that possess square-integrable generalized derivatives through order $k$. A square-integrable function is called an $L_{2}$-function, by virtue of (1.I.4). From (1.I.3), we see that $H^{0}=L_{2}$ and that $H^{k+1} \subset H^{k}$. The Sobolev spaces are the most important for studying elliptic boundary-value problems. The question naturally arises as to the relation between Sobolev spaces and the classical spaces of differentiable functions introduced previously. In particular, when is an $H^{k}$-function smooth in the classical sense? The answer is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is

 Additional context 1: is provided by Sobolev's theorem, which states that, in one dimension, $H^{k+1} \subset C_{b}^{k}$. That is, if a function is of class $H^{k+1}$, then it is actually a $C_{b}^{k}$ function. For example, in Sec. 1.3 we required $H^{1}$ functions. By Sobolev's theorem, such functions are, additionally, continuous and bounded. In Sec 1.16, we employed $H^{2}$ functions. These are $C_{b}^{1}$ by Sobolev's theorem and thus possess bounded, continuous, classical derivatives. Certain ""singularities"" are precluded by square-integrability. For example, $x^{-1 / 4}$ is in $L_{2}$, but $x^{-1 / 2}$ is not. (Verify!) Such considerations become important in many physical circumstances (e.g., in fracture mechanics). The number of other types of function spaces that arise in mathematical analysis is large, and many are difficult to comprehend without serious training in ""functional analysis."" These topics are outside the scope of this book. The reader who wishes to delve further may consult $[13,14,15]$ and references therein. \section*{REFERENCES} \section*{Section 1.2} \begin{enumerate} \item A. R. Mitchell and D. F. Griffiths, The Finite Difference Method in Partial Differential Equations. New York: John Wiley, 1980. \end{enumerate} \section*{Section 1.5} \begin{enumerate} \setcounter{enumi}{1} \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item B. A. Finlayson, The Method of Weighted Residuals and Variational Principles. New York: Academic Press, 1972. \item B. A. Finlayson and L. E. Scriven, ""The Method of Weighted Residuals-A Review,"" Applied Mechanics Reviews, 19, (1966), 735-738. \end{enumerate} \section*{Section 1.10} \begin{enumerate} \setcounter{enumi}{4} \item I. Stakgold, Boundary-Value Problems of Mathematical Physics. Vols. I and II, New York: Macmillan, 1968. \item G. Strang and G. J. Fix, An Analysis of the Finite Element Method. Englewood Cliffs, N. J.: Prentice-Hall, 1973. \item J. E. Marsden, Elementary Classical Analysis. San Francisco: W. H. Freeman, 1974. \item J. Barlow, ""Optimal Stress Locations in Finite Element Models,"" International Journal for Numerical Methods in Engineering, 10 (1976), 243-251. \item E. Popov, Introduction to Mechanics of Solids. Englewood Cliffs, N. J.: Prentice-Hall, 1968. \end{enumerate} \section*{Section 1.13} \begin{enumerate}

 Additional context 2: \text{(prescribed shear)} \end{align*} \end{minipage} \right. \] where $E$ is Young's modulus and $I$ is the moment of inertia, both of which are assumed to be constant.\\ The setup is shown in Fig. 1.16.1.\\ \includegraphics[max width=\textwidth, center]{2024_10_04_fba7dc36d090c246379ag-48} Figure 1.16.1\\ Let $\mathfrak{f}=\mathcal{U}=\left\{w \mid w \in H^{2}(\Omega), w(1)=w_{x}(1)=0\right\}^{7}$. Then a corresponding weak form of the problem is:\\ \[ \text{(W)} \quad \left\{ \begin{minipage}{0.8\textwidth} \text{Given} $f, M$, \text{, and} $Q$, find $u \in \mathfrak{f}$ \text{such that for all} $w \in \mathcal{U}$\\ \begin{align*} a(w, u) = (wmf) -w_{,x} (0) M +w(0) Q \end{align*} \end{minipage} \right. \] where $$ \begin{aligned} a(w, u) & =\int_{0}^{1} w_{, x x} E I u_{, x x} d x \\ (w, f) & =\int_{0}^{1} w f d x \end{aligned} $$ \footnotetext{${ }^{7} w \in H^{2}(\Omega)$ essentially means that $w_{, x x}$ is square-integrable (i.e., $\left.f_{0}^{1}\left(w_{, x x}\right)^{2} d x<\infty\right)$. }The collection of functions, $\mathcal{U}$, may be thought of as the space of finite strain-energy configurations of the beam, satisfying the kinematic (essential) boundary conditions at $x=1$. It is a consequence of Sobolev's theorem that each $w \in \mathcal{U}$ is continuously differentiable. For reasonable l, these problems possess unique solutions. Let $\mathfrak{d}^{h}=\mathcal{U}^{h}$ be a finite-dimensional approximation of $\mathfrak{f}$. In particular, we assume $w^{h} \in \mathcal{U}^{h}$ satisfies $w^{h}(1)=w_{, x}^{h}(1)=0$. The Galerkin statement of the problem goes as follows:\\ (G) $\left\{\begin{array}{c}\text { Given } f, M, \text { and } Q, \text { find } u^{h} \in \delta^{h} \text { such that for all } w^{h} \in \mathcal{U}^{h} \\ a\left(w^{h}, u^{h}\right)=\left(w^{h}, f\right)-w_{, x}^{h}(0) M+w^{h}(0) Q\end{array}\right.$\\ a. Assuming all functions are smooth and bounded, show that the solutions of $(S)$ and (W) are identical. What are the natural boundary conditions?\\ b. Assume $0=x_{1}<x_{2}<\cdots<x_{n+1}=1$ and $\mathcal{U}^{h}=\left\{w^{h} \mid w^{h} \in C^{1}(\bar{\Omega})\right.$, $w^{h}(1)=w_{, x}^{h}(1)=0$, and $w^{h}$ restricted to $\left[x_{A}, x_{A+1}\right]$ is a cubic polynomial (i.e., consists of a linear combination of $\left.\left.1, x, x^{2}, x^{3}\right)\right\}^{8}$.",70,"How do the properties of square-integrability affect the classification of functions in Sobolev spaces, and what are the implications for physical applications such as fracture mechanics?","The properties of square-integrability significantly affect the classification of functions in Sobolev spaces. Specifically, a function $w$ belongs to the Sobolev space $H^{k}(\Omega)$ if it possesses square-integrable generalized derivatives up to order $k$. This is formally defined as:

$$
H^{k}=H^{k}(\Omega)=\left\{w \mid w \in L_{2} ; w_{, x} \in L_{2} ; \ldots ; \underbrace{w_{x \ldots x}}_{\text{k times}} \in L_{2}\right\}
$$

where $L_{2}$ consists of functions for which the integral of the square is finite:

$$
L_{2}=L_{2}(\Omega)=\left\{w \mid \int_{0}^{1} w^{2} d x<\infty\right\}
$$

The implications of square-integrability are crucial in various physical applications, including fracture mechanics. For instance, certain singularities in functions are precluded by the requirement of square-integrability. An example provided in the context is that while $x^{-1/4}$ is in $L_{2}$, $x^{-1/2}$ is not. This distinction is important because it ensures that functions used in modeling physical phenomena do not exhibit behaviors that could lead to non-physical results, such as infinite stresses or displacements.

In fracture mechanics, where the behavior of materials under stress is analyzed, ensuring that the functions representing stress and strain are square-integrable helps to avoid unrealistic predictions of material failure. The Sobolev spaces, particularly $H^{1}$ and $H^{2}$, are essential in formulating boundary-value problems, as they guarantee that the functions involved are not only square-integrable but also possess the necessary smoothness and continuity properties required for physical realism.

Thus, the classification of functions in Sobolev spaces, governed by the properties of square-integrability, plays a vital role in ensuring that mathematical models accurately reflect physical behaviors, particularly in complex scenarios like fracture mechanics."

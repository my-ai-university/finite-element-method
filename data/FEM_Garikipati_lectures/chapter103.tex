\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}

\section*{ID: u8TtqJHfhbw}
In this segment we'll move on to looking at the actual declaration, or the definition rather, of the functions that we looked at previously. All right, so let's move over to the code, and the first function that we're looking at is the constructor. Remember the 2 inputs are an unsigned int called order and an unsigned int called problem. Now here, I actually have something going on here between this declaration of the function name and its inputs, and the {. And this is so that we can call the constructor of some of these data objects that were declared earlier as class objects. So if I scroll up here, you can see that here's fe, which is an FESystem and dof-handler. Now when we're declaring objects in the class declaration, we can't call their constructor. But normally, we actually call the constructor when we declare an object, right? So in order to get around that difficulty, we can call the constructor here with, the constructor for these class objects we can call within the constructor for the class itself, okay? So in the constructor for fe, we input this FE-Q, which again is a deal.II object, but notice the input is order. So FE-Q will keep track of what Lagrangian basis function order we're using. And the reason it needs to know that is so that it can keep track of how many nodes there are, right? Because obviously if we're using a higher order of basis function, then there are more nodes. We have midside nodes. All right, so we tell it the order, and we also give it the dim, which again is the dimension. Now, again, this is that template parameter I was talking about in main.cc, and we input a 1, all right? So anytime we see dim in this template, it will actually be 1 because that's what we've declared it as in main.cc. All right, dof-handler now will take in triangulation. Triangulation again was the mesh, or holds the information about the mesh. And dof-handler holds information about the degrees of freedom, and obviously they're related, okay? Now we'll get into the constructor function itself, and here I'm just passing variables. So order is the input, and now I'm storing it in the global integer basisFunctionOrder. And the same thing for problem. Problem is an input to the constructor, and I'm storing it in the global integer of prob. Now you'll notice here I have an if statement, if(problem == 1 | | problem == 2){prob = problem;}. Otherwise, I output an Error saying that the problem number should be 1 or 2. So if you try to input problem part 3, it'll give you an error. Why, because obviously there is no part 3. You could have a similar sort of if statement and check on the basisFunctionOrder. However, some of you may want to use the generalized formula for the Lagrangian basis functions. So, orders higher than 3 may be valid, so that's why he didn't put that check in there. But, if you'd like to, you're free to go ahead and do that. All right, the second part of the constructor, again, has to do with the solution names for outputting results. So I won't look too much at that. The destructor is short, but there is something in it, dof-handler.clear(). Again, that's something going on with the deal.II data object, so I won't go into that too much either, all right? So that's the constructor and destructor. Let's move down now to xi node. Now here, I want to explain to you a little bit about how deal.II actually does its node numbering. And we'll look at it for linear, quadratic, and cubic basis functions, okay? So for linear basis functions, it's exactly the same as in the class. So here's an element, sorry, it's not exactly the same. In the class, or in the lectures, I should say, The node numbers started at 1 and went to 2. For deal.II, The node numbers start at 0, okay? Now globally, so this would be for an element. And for the system, globally It would be very similar. So, here I'll just do a 3 element mesh, and in deal.II the numbering just goes sequentially up, 0,1,2,3. Whereas in the lectures, it was started at 1, of course, but that's the only difference there. For quadratic, it starts to get a little bit more different here. Okay, so now we have our element. We have 1 midside node now, all right? In the lectures we again just started 1, 2, 3, right? In deal.II, there's a significant difference here in that the left node is always 0, and the right node is always 1, and then we go to the midside nodes. Okay, we actually follow that same pattern, or a similar pattern, here on the global scale. Here each element has 1 midside node. Again, in the lectures we just went straight across. In deal II, globally, we sort of go element by element following the same pattern. So, we do 0, 1 then our midside node, 2. Then the right node of the next element, so 3, 4,5,6, okay? And then cubic Follows that same pattern. All right, so we have our 2 midside nodes now, again in the lectures. We went sequentially 1, 2, 3, 4 in deal.II. Again the left node is 1, the right node is, or sorry, the left node is 0, the right node is 1, and then, 2, 3. All right, and again, we'll do it globally as well. With 2 midside nodes, Or 2 internal nodes per element. And I'm not going to bother with the lecture in method. It's just sequential as before, but for deal.II at 0, 1, and then 2,3 on the midside, 4,5,6,7,8,9, okay? This will come into play when we're defining our basis functions. Now, getting back to the code, why do I have this xi node? This will take your deal.II node number, and it will output the value of xi. So for example here, xi of 0 is = -1. Xi of 1 = 1, xi of 2 = -1/3, and xi of 3 = 1/3, okay? And so what this function does if you input a 0, and you're working with cubic bases functions, if you input a 0, go get -1. If you input a 1, you'll also get 1. So this is your input. And your output. All right? Actually, for any basis function if you put in a 0 you will get -1 and put in a 1, you'll get a 1 because of the way they're numbered. However, if it's a cubic basis function and you input a 2, Then you'll output a -3, or a -1/3, excuse me. Or if you input a 3, you'll receive a 1/3 as your output. All right, obviously that would be different than for quadratic, if you input a 2, you would get 0 here, right? See 2 = 0 in this case, all right? So that will come into play as you're defining your basis functions as you'll see in a minute. All right, so you actually don't need 2 of them. There's nothing you need to define within this object's xi node, it's just there in case you want to use it. You actually don't need to use it if you don't want to. All right, let's move on to the next functions, and these are our basis-function and the basis-gradient. All right, so, for the basis-function, We have input the node number, and you're inputting the value xi, xi again is in the bi-unit domain so it's from, anywhere from -1 to 1. Generally, you'll be using the basis-function in your quadrature when you're numerically integrating. And so usually the value of xi will be add the value of the quadrature point that you're at. Okay, and again, basisFunctionOrder is a global value, so you can access that here. They're a couple of ways you can define your basis functions, you can just use if statements. For example, you would say, if basisFunctionOrder = 1 and if node = 0, then you would define your linear basis-function for node 0 evaluated at xi, or you could use your generalized basis-function. Now that's in the lectures, but I'll write it again here just to point out some things, okay? So here's our basis-function at node A evaluated at xi, so you can see it's the same inputs. We have input of xi. In the function, and we have the input here. Of the node. Okay? Now, I'm going to change this formula just slightly from the lectures. Because our basis functions or our node numbering, rather, in deal.II start at 0 instead of 1. So, remember this, big pi is a product. It's similar to a summation with sigma, only now, we multiply each term instead of adding each term. All right, so we start at B=0, and we'll skip it. We'll skip B=A, and as long as $<$ number of  nodes el. Okay, so of course that upper bound will change depending on the basisFunctionOrder. Okay, that's actually what this is. So the number of nodes in the element is basisFunctionOrder + 1. Okay, and then within this product we take xi, which is the same as this xi here,- xiB/xiA- xiB. And again to get this value, you could use the function xi-at-node, and you would input, for example in this case, (B). Okay, so that's an example of where you might use that function. Of course, you don't have to, you can use if statements as well if you like, okay? Now you'll store whatever that value is, you'll store that value within this double value, and you see that's what we return. And it is a double, okay? Now again, in the code we move on to basis-gradient. In general, a gradient would be a vector. All right, but this is 1 d, so there's just 1 component, so I'm only returning a double. But we have the same inputs, node and xi. All right, and remember this is just the derivative with respect to xi, it's not the derivative with respect to x in the real domain. Again, we're dealing strictly with a bi-unit domain here. And again, you'll store that value within this variable value which is then returned. All right, so those are the basis functions and basis gradients. You need to be sure to use deal.II's node numbering here, all right? So that's very important for you to remember that. So if you're doing your if statements, remember to do 0 is always at the left, 1 is always at the right. And then you count up on your midside nodes, as we showed on the previous slide, okay? If you use the function xi node in this generalized formula, it's already taken into account the deal.II node numbering, okay? So let's wrap up this segment here, and in the next segment we'll move on to generating the mesh and the boundary conditions.

\section*{ID: iCkgh-vzvv4}
Welcome back. What we worked out in the previous segment was the form of the stiffness matrix for a general element. We proceed now with getting together the other parts of our matrix vector version of the finite dimensional weak form. And the first thing we have to do is carry out a similar equation for the forcing function. That is, the, the, the distributed body force. All right? So let's start with that. So, we will next. Consider. The following term. Integral over omega e W,h,f,A,d,x, all right. No gradients in here so this actually becomes somewhat simpler integral to calculate. So integral over omical e. For wh we have sum over a. Na, Cae. And we'll put parenthesis around here to remind ourselves. This is what wh is represented as. We have f, A dx now. Because we know that N A ultimately is represented in terms of C, that is the coordinate in the by unit pairing domain. And because we know that we have availability, we have available to us this map. Right? For the geometry, isoparametric map. What this tells us is we can very well write this as an integral now by changing variables as an integral omega X C. Sum over A, C A E. The reason we can do that is, what? Reason we can pull C A E out of the integral because of course it is just a degree of a freedom that is used to build our Representation of the waiting function, it does not depend upon C, right, or does not depend upon position, so we are able to pull it out, just as like, just as we've been doing all along. All right and remember that this sum runs A going from one to three, right, for three nodes in the element. All right. We have C A e. N, A, f, A. Now, the integral over dx can now be written as dx, dz, dz. And of course, we remember, from before, something we calculated a couple of units ago. dx, dc is he over 2. All right? We're going to go ahead and build this integral. And in order to build it, let us just, just in order to be able to fix ideas, and get something that we can compute. Let us consider the situation where f and A are uniform, right? They're uniform over Omega E right? Over the element of interest. What this allows us to do is to pull f and A as well out of the integral. And h e is of course the element width, which is independent of. Right? So what that tells us is that what we're trying to compute from up here, right? I've represented it in here in dots with ellipses. This is sum A equals one to three CA e Times FAHE over 2, integral minus 1 to 1, NA, DZ. Right. Now, just as we did before, in the case of linear basis functions, and just as we did in the previous segment in order to compute the left-hand side integral. Right? We go now to matrix factor notation. All right. So, what we're trying to compute on the left hand side, right? The integral that comes from the top of the slide is now represented as c1e, c2e, c3e, using vector notation for representing our degrees of freedom of our waiting function over the element. Times F A H E over 2. Now, we get a vector here. Right? And this is integral. The first component here is integral minus 1 to 1. N 1, right? And, 1 however, we already know, is one half C times C minus 1 d C. And, 2, the, the integral from N 2 is integral minus 1 to 1. Or the integral from, ga. The integral for component 2 is the integral minus 1 to 1. Of n 2. Which is one minus xi squared d xi. All right? And the third component is integral minus 1 to 1 one-half C times C plus 1 dC. All right? Okay. We go ahead now and compute those integrals. They are relatively straight forward to compute. Let's do that. We get our C vector for the element. We have F A h, e over two because f and a, we are assuming are uniformed and for our integrals we get here first, we get one half of c cubed over 3 limits minus 1 to 1. For the second one we get c Minus Z cubed, over 3, minus 1, 2, 1. And for the third, we get one-half, Z cubed, over 3, also minus 1 to 1. Right. When we compute result, we see that we are left with c1e, c2e, c3e, fAhe over 2 and here, we get 1 over 3, 4 over 3, 1 over 3. Right. Okay. So this is our general representation of the contribution from the right-hand side forcing function term, for a general element, okay? What we're going to do now is go to assembly, right? So, assembly. Of global matrix vector equations All right. Now, it's worthwhile just in order to position ourselves to recall that what we're attempting to do here is actually carry out an integral over the entire domain. Right, over our entire 1D domain. And, we've used this partition into elements to write that as a sum over element integrals. Of that term, that element integral. Equal to the sum over elements of that element integral. Plus a term which arises from the fact that we have a, what kind of a boundary value problem do we have? We have a, there  boundary value problem. Right? So we are WHL times T which is the attraction times A. Okay? So, this is our, finite dimensional weak form expressed as a sum over element integrals. And we've just, over the previous segment and over the first few minutes of this segment learned how to compute this integral, in the last segment, and that integral, in this segment. For general elements. Okay? Okay, now, in order to carry out the assembly, we just need to recall something. Okay? We need to recall. That for the Dirichlet Neumann problem. Right. Because we know that u h add 0 equals some given value which actually is equal to 0 in this case. Right? What that implies for us is that w h at 0 is also equal to 0. All right, because we have a tertiary boundary condition at x equals 0. We also have a homogenous tertiary condition on the waiting function. Of course, it turns out that our tertiary condition on the left hand side, on x equals 0 is itself homogenous, meaning we are saying that the trial solution there has to be 0. Okay? Even if it were something other than 0 not, if it were non-zero, we would still have a homogeneous Dirichlet condition on the waiting function, right? And that's just the nature in which we built our waiting function, okay, the function's base, in fact. All right, so what does this mean? What this implies is that for element 1, okay? Because w h at 0 is equal to 0, we construct w h e equals 1 as sum A, not starting with 1, but starting with 2, to number of nodes in the element, which is 3. Okay, so we have this. N A C A e. Okay. And in fact, going from local to global numbering of degrees of freedom, we will get here here is what we get. Okay? Here is what we get, because of the fact that we can go from local to global numbering of nodes, here's what we get. When we right out our matrices, right, matrices and vectors coming from our weak form, here's what we will get. We will use the fact that this thing can be written explicitly as N2 times c, 2e plus N3, c 2e plus 1 for e equals 1. Okay, we're going to use this. Where do we use it? Here's what we get. Right, our global equations for the matrix vector weak form. Okay? They take on the following form. For element e equals 1, we have something a little special, right? We have here c 2 e, okay? C 2 e, plus 1. Okay? This is for e equals 1. Okay. In fact, let me do this. Let me explicitly use the fact that e equals 1 here. When I do that, c 2 e becomes c 2, and c 2 e plus 1 becomes c 3. Okay, we have 2EA over h1. Okay? We're assuming here, of course, that E and A are uniform or not only over each element, but also between elements, right? Effectively over the entire domain. There is nothing preventing us from developing the more complicated formulation where E and A are allowed to vary with position. Okay? All right. Now, what we get for the matrix contribution here is a matrix that is not square but has only two rows. Okay, and in fact, that matrix has the form minus 4 over 3, 8 over 3, minus 4 over 3. 1 over 6, minus 4 over 3, 7 over 6, okay? Multiplying it here, however, is the full d vector for that element. Okay? That would be d1 in terms of global, no numbers, d2 and d3. Okay. That's all for element one. Okay, and note that I called that h, h1 or e equals 1. Thus, plus now, sum E equals 2 to number of elements. Here things look the same. 2e minus 1, the same as for general element, c2e minus 1, c2e, c2e plus 1. 2EA over he. Our matrix, which we developed in the previous segment which is 7 over 6 minus 4 over 3, 1 over 6, 8 over 3 minus 4 over 3, 7 over 6. Completing bisymmetry. All right, all of this multiplying here, d2e minus 1, d2e, d2e plus 1, right? Global numbering of degrees of freedom and nodes. Okay, so those terms together, that's this one, for the first element and this for a generic other element give us our left-hand side. This is now equal to, okay? The right-hand side. Again, the same thing happens with the forcing function contribution, okay? So we get for the c vector, for the forcing function contribution, again, we get c2, c3. Okay? The contribution here is fAh1 over 2. And it multiplies a vector which is just 4 over 3 and 1 over 3, 'kay? The first element, the first entry, the first component from the general form of the factor that arises from the forcing function is absent. Because in element one, we start with that global degree of freedom. Because the weighting function satisfies the homogenous Dirichlet boundary condition. All right, we have this plus, sum over e equals 2 to number of elements. C2e minus 1, c2e, c2e plus 1, fAhe over 2. Now we get the full form of this vector 1 over 3, 4 over 3, 1 over 3. In addition, let's never forget the Neumann boundary, too. Right. Which is wh at l. But that is simply c2, nel plus 1, times tA, okay? That's it. That's really our weak form written out fully in matrix-vector notation.

\section*{ID: 9N2qXvjmNO4}
All right. Let's now write this out entirely in terms of matrices, getting rid of this summation over elements. All right? Okay. In order to that, what we get is our big vector c2, c3, c4, c5, c6. So on, until we come to the last element, c2nel minus 1, c2nel, c2nel plus 1. Okay? Now in order to write this, allow me to assume that every element. Or allow me to consider the case, where every element has the same length, okay? So, if he equals h for all e, okay? That's a, that's the case I'm considering. It just makes it a little more makes, makes what I have to write a little more, a little less cumbersome, right? In particular, what it does is that it allows us right here 2EA over he, okay? Sorry, over h, if you just agreed,  right? Now, we are going to write out our big matrix. Giving myself plenty of room. Okay. For the very first element, we have contributions only from c2, c3. All right? And if you go back and look at what we have on the previous slide, let me just pull it up for a second, all right. Okay, look at the contributions of c2 and c3 on the left-hand side here, okay? We get here, minus 4 over 3, 8 over 3, minus 4 over 3. And here we get 1 over 6, minus 4 over 3, 7 over 6, okay? We move on to the next element. Element two has contributions from c3, c4, c5, right? Or alternatively from global nodes 3, 4, 5. Global degrees of freedom 3, 4, 5. Okay. It is going to get slotted in to these positions here. Okay. Right, that's where the contribution from c, from c3, c4, c5 will come. All right, now the contributions on the side of columns will come in here, right? Which will come all the way down here. Right. It come all the way up to this, this column and to the next two. Okay. So, what that means is that when we come here, we add on 7 over 6, right? From the one, one component of the stiffness matrix of element two. And because I need a little more room, here let me move these arrows across a little. Okay? So. We get 7 over 6, and then the rest of that, of this stiffness matrix of element two just fills itself here. Minus four-thirds, 1 over 6. Right? We get here minus 4 over 3, 8 over 3, minus 4 over 3. And here we get 1 over 6, we get minus 4 over 3. And here we get 7 over 6, okay? The important thing is that, that is where the, the, the contribution from the common node or the common degree of freedom of elements one and elements two comes about. And that is from that degree of freedom, okay? The process continues of course until we come, we arrive at our very last element. Okay? And for it, we observe that it's going, it's going to form this little block at the very bottom of the bottom right of our matrix. Also observe that as we go onto element three and so on, we are going to add some more terms here, right. That's where terms are going to get added, right. That's where components are going to get added into this stiffness matrix. When we come down here, at our very last element, nel, we see that it will give us a contribution, 7 over 6. But that contribution itself will be added on to a contribution that was already put in there from element nel minus one, okay? So, we get seven-sixths here, minus 4 over 3. 1 over 6 minus 4 over 3, 8 over 3 minus 4 over 3, and here we get 1 over 6 minus 4 over 3, and 7 over 6. 'Kay? The d vector here is full, right? It starts at a d1 d2, d3, d4, d5. And d5 is the last trial solution degree of freedom for element two. 'Kay. We carry on until we come here to d 2neL minus 1, d2neL, and d2neL plus 1. And all of this, then, gets closed. Okay? So that is our stiffness matrix. Right? 'Kay, that or, or those are the contributions from the left-hand side of the weak, of the weak form. All of these now equal to, on the right-hand side, again our c vector just as before. C2, c3, c4, c5. We go on until we come to c2nel minus 1, c2nel, and c2nel plus 1. Right. Now there are two types of terms which need, which need to go in here, right? The contributions from the forcing function as well as the contribution from the traction. Okay? So, I will open a parenthesis here, since I've allowed us to assume that we have a fixed element length, we don't have to use he, we use h instead, okay? Right. Which is uniform for all the elements, the same element length for each element. So, we get a contribution fAh over 2, and now we get our vector from the forcing function. For element one, because those are the only degrees of freedom from the weighting function we go back and look at the way we wrote out our factor from the forcing function. We get four codes, okay? We get 1 over 3 from element one. We come to element two and those are the contributions. Okay, from the weighting function degrees of freedom. All right, and if we go back and look at how we wrote the sum over elements of the contributions from the forcing function, we see that we do get indeed a one, and a one-third being added here. We get four-thirds from the c4 degree of freedom and we get a 1 over 3. But this 1 over 3 will then be joined by another one-third from element three. Right? And this process continues until we come all the way down to the log, to the to the contributions from the last element. Okay, the contributions from the last element will have the first node from the last element will give us a contribution of one-third where the first degree of freedom will. But it will be added on to an existing contribution from n element, nel minus 1. The second degree of freedom gives us four-third and the very last degree of freedom gives us one-third. Okay? This vector then contains the contributions from the forcing function. For the traction, remember, if you go back and look at the way we wrote out the traction vector contribution down here, it's the very last term on this slide. The only contribution comes from c2nel plus 1, okay? Since we have the entire c vector sitting here, what that tells us is that in order to write the traction contribution as a vector also, we fill it with zeros all the way down except for the very last component, which is tA. All right. So that when, and oh, we need to close the parenthesis. Okay, so this is you know, now if we go back and look at the matrix that we have on the left-hand side here, we sum over, you know, we, we add up the terms that needed to be added. We likewise, we added, add up terms that need, need to be added here. We end up with a matrix vector problem, okay? One thing I want you to note, and this is what we're going to pick up when we enter the, when we start the next segment, is that the length of this vector is, what, how many components on that vector? 2 times nel. Whereas that vector has 2 times nel plus 1. Consequently, this matrix that we have here is 2nel times 2nel, plus 1. Right? This vector again here is 2nel. Each of those vectors here, right. Each of those vectors is however, 'kay, of the same length, right. Because they're forming a sort of dock right up with the, with the c vector. All right? So we're going to use this when we return to the next segment.

\section*{ID: oEulZlPMkpk}
Welcome back. At the end of the last segment, we had essentially carried out an assembly of our matrix vector equations for the finite element weak form. Let's pick up there and see and just make some more observations about the form that we've arrived at for quadratic basis functions. Right. So where we are is the following, right. So we've got the global matrix vector equations. All right? And the form that we had was the following. c2, c3, all the way up to c2nel plus 1. Multiplying 2EA over h times a matrix which is minus 4 over 3, 8 over 3, minus 4 over 3, one sixth, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6 minus four thirds, eight thirds minus four thirds, 1 over 6 minus 4 over 3. We'd get, again, 7 over 3 here, and then the process would go on. Right. And at the very bottom of this matrix, the bottom right of this matrix, say somewhere down here, we'd get 7 over 3 minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3, 1 over 6, minus 4 over 3, 7 over 6. And because this matrix has grown a little bigger than I had anticipated, let me make these bounding brackets bigger. All right. The d vector that we get here is d1, d2, d3, all the way down to d2nel plus 1. Right. Now, this is equal to a vector which here is c2, c3, up to c2nel plus 1. 2, sorry. Right now, here we get the two contributions to the forcing, fAh over 2. Now, the way this adds up is the following. We get 4 thirds, 2 thirds, 4 over 3, and so on until for the, in the very last element, the contributions are 2 third, 4 over 3, and 1 over 3. Right? And here we get a vector which is the contribution from the traction. It's filled with zeros all the way down to the very last component, which is t times A. All right? I'd like to point out a few things which is observe that here I've explicitly included the addition of components coming from the same global degree of freedom, but on different elements, right? Likewise over here. I've also done the same thing with the vector rising from the forcing function, right? Here and there. Okay? Also, at the end of the last segment, I made the point that the dimension of this vector is 2 nel, whereas this vector is 2 nel plus 1. Consequently, the matrix that we have here is 2 nel times 2 nel plus 1. It is not a square matrix, okay. This thing being 2 nel, so are these vectors, right, the vectors that are involved here are also 2 nel. Okay? So, at this point, we are essentially, we have everything in place. The only thing we have to do yet is account for the Dirichlet boundary conditions. And the way that shows up is, when we look at our D vector, right, our vector of degrees of freedom for the trial solution, we note that D1 here is our given displacement at x equals 0. I think at one point, I refer to it as u g, but we're actually calling it u0, right? This is the known Dirichlet boundary condition. All right. BC is short for boundary condition. Since it's known, what we can afford to move it to the right hand side, right, and really have it derive the problem. And the way we go about doing that is to observe that that degree of freedom, d1, which is equal to u0, is the one that multiplies out this column of our matrix. Okay? And also note that because of the sparseness of this global matrix that's formed, we have zeros here, okay? Zeros all the way down. Okay? So, we can considerably simplify our, problem here, by essentially moving this column that I've just marked out in the big matrix to the right-hand side. All right? And when we do that, we get c2, c3, up to c2nel plus 1 multiplying 2EA over h. Big matrix here. I'm going to write it yet again. It's 8 thirds. Sorry. 7 over 3 minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3. And here I get 1 over 6, minus 4 over 3, 7 over 3. Continuing on, I'll get terms of this type. Okay. I'm not going to write the rest of this matrix below here, but essentially indicate that it continues until we end up here at 7 over 3 minus 4 over 3, 1 over 6 minus 4 over 3, 8 over 3 minus 4 over 3, 1 over 6 minus 4 over 3, 7 over 6. Okay? This is the big matrix that we have, right? Now, because we've gotten this by moving our column over we're going to note that this is a square matrix. We'll come back to that in just a little bit, okay? The, the degree of freedom vector for d is now starting with d2, d2, d3, going all the way down, up to 2dnel plus 1. Okay? This is equal to, on the right hand side, again, we have our c vector, c2, c3, c 2nel plus 1. Okay? Multiplying. Now, fAh over 2. Here we have 4 over 3, 2 over 3, 4 over 3, going all the way down until the very last entry here is 1 over 3. We have the contribution from traction, which is full of zeros, except for the last component, which is t times A. Sorry, sort of messed that up. t times A. Okay, now, that column that we moved over from the left-hand side shows up here. It is minus 2EA over h times d1, which we know to be u0. All right? That's our specified Dirichlet value before the displacement, right? And then the column that, that we obtained from the left-hand side is this, minus 4 thirds, 1 sixth, and zeros. Okay. Let's look at the size of our vectors. This remains a 2 nel. This has now become 2 nel from 2 nel plus 1 on the previous slide. That's because we've lost the first component by moving it over to the right-hand side. Consequently, we now have a matrix here, which is 2nel times 2 nel, it's a square matrix. Okay? Here again, we have 2 nel, it's just the c vector, and all the vectors here are all 2 nel. Okay? All right. So, what we're going to do is when we go to the next slide, we're going to call this the global c vector, this will be the global d vector, so I'm going to call this c transpose. This will be denoted as d, okay? This is, of course, c transpose again. And the sum of the three vectors that I have in the parentheses on the right-hand side is going to be our f vector. Okay? Our matrix here, of course, is going to be our k matrix. Right, our stiffness matrix. Okay? All right. We have all of this, let's write it out. Okay, we have this, and I'd like to make a few remarks about it first, all right. And in order to make those remarks, I am actually going to go back to this slide and annotate this further. And because I've already annotated it a bit, I'm going to annotate it now using color. Okay, the first thing I want to point out is that if you look at the entries in the stiffness matrix, you will see that they're really quite different from the entries that we had in the case of the stiffness matrix for the linear basis functions, right? They're not just twos and plus or minus one, right, they're a little more complicated. All right? Okay, so let's make that observation first. Remarks. One. K matrix components. Are different from linear case, okay? Remark two is the following. If you observe the, bandwidth of our matrix, of our K matrix, observe that the bandwidth is larger. Okay? The bandwidth is 5, okay? So that is a second remark to make. Okay? The bandwidth of K is 5. Okay, so what we're getting here is that because of the use of quadratic basis functions, right, we are observing that the degrees of freedom have much more interaction between them due to use of quadratic basis functions. All right? The third remark to make, again, moving back here look at the part of the forcing vector on the right-hand side that arises from the force, from the forcing function terms. Okay? And in particular, observe that the midside nodes have a larger contribution than the end nodes, right, of an element. Okay? So, midside nodes have a larger contribution to F vector, okay? Also due to the use of quadratic basis functions. Okay, those three remarks are important to understand and appreciate some of the differences that arise from the use of different basis functions. Okay?

\section*{ID: ajelt6TEsHs}
In order to proceed now, what I'd like to do is fairly quickly go over how things change if instead of looking at a Dirichlet-Neumann problem, we were to look at another type of boundary value problem that I have sometimes spoken of, for this prob, for, for this case right. Let's look at the Dirichlet Dirichlet problem. Okay, and, and fairly quickly set that one, set that up. All right, so now what we're going to do is consider. The Dirichlet problem. Okay. And we distinguish this from our Dirichlet-Neumann problem by noting that when I say it's just a Dirichlet problem, without any mention of Neumann, what I mean is that all the boundary conditions are Dirichlet boundary conditions. Okay? So, what we would be attempting to do for this sort of problem is the following. We'd say find belonging to Sh, subset of S, where Sh consist of all belonging to, H1 functions on such that at 0 equals u0, as we've been saying. And let's suppose that at L, equals u sub L, okay? So we have Dirichlet boundary conditions at both ends. We do not have a Neumann boundary condition at the right end, right, there are no Neumann conditions in this problem, okay? All right. Find of this sort such that. For all wh belonging to vh, sorry. For all wh belonging to vh. Subset of 3 where vh equals the collection of waiting functions belonging to h1 on omega, such that wh and 0 equals 0, and wh at L also equals 0. Okay? All right. Find belonging to Sh such that for wh of this form, right, where you note that Sh and vh are different looking spaces because they have Dirichlet boundary conditions at both ends. Right? Okay. What do we want to solve here? Okay? What has to hold is the following: integral over Omega e Wh comma x, sigma h, A dx equals integral over omega. Sorry, I've been saying integral over omega e, it's actually integral over the entire domain omega. Integral over omega. Wh fA dx plus, plus nothing. All right? Because we don't have a Neumann condition. That's it. Okay? That is our weak form. Now because of the nature of our waiting functions okay, here's what we have. If this is our domain omega. Okay, and let's suppose that were working with quadratic basis functions. So those are our nodes for element one, and those are our nodes for element NEL, right? So this is element omega one, and that is element omega NEL. Okay? Right? We know that the sort of basis functions we'd be considering inside here are that and. That. On that, on element omega one. Here, we will have that basis function for the mid-side node. And, that basis function, right? For the first node of that element. Observe that in both cases for element omega one there is no use of our basis function from the very first node right, from node A equals 1, global node equals 1. That node does not contribute a basis function to our waiting function. Likewise for the waiting function here, our node A equals 2nel. Plus 1 has no contribution. Righ, okay? However, node a equals 1 and node 2nel plus 1 do have contributions for it's only for wh that they don't have contribution, okay. All right. When we work things through, here's what we will get. The matrix. Vector. Weak form. For this problem? Global. Here it is. We get here, c2 c3. Everything carries on until we come to the last element. We get c2nel minus 1, c2nel. 'Kay, that's it, we don't get c2nel plus 1, because we're simply not using that basis function there. Okay. Here we pick up 2E. We, we are now coming up to the, to work with eventually give rise to our stiffness matrix right? So here we have 2EA over h. Right? We have our matrix here. Okay. Now this matrix is going to have contributions when we add up all the contributions coming from the different elements and accounting for the common degrees of freedom across elements. Right? We are going to have a, contribution that, takes on the following form. It will have a form, 8 over 3 minus 4 over 3, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6. Minus 4 over 3, 8 over 3 minus 4 over 3, 1 over 6 minus 4 over 3 7 over 3. And so on. Of course here we know that we get some more terms. Right? And so on, right? Except, that when we come all the way down here for the last element we get 7 over 3 minus 4 over 3. 1 over 6, okay? And here, we get minus 4 over 3. 8 over 3. Okay, and 1 over 6. Sorry not 1 over 6 it's, minus 4 over 3. Right. We get this, that's the last row we get, okay? In particular, what I want to point out is that if compare this with the corresponding metrics at this stage, for the Durtally Noin problem, there is an extra row, for the Durtally Noin problem, which is missing here. Okay? This would now multiply the following matrix, right? It will multiply D 1, D 2, so on, all the way down to d 2 n e l plus 1. Okay, actually I realize that I do need another. Column here. Sorry, let me just put it in here. I do need another column here which is Minus 4 over 3. 1 over 6. That's what I did. Okay. Right. We have this equal to, on the right hand side, we have our c2, c3 vector. We're going all the way now to c 2 n e l c, 2 n e l. Minus 1, and 2 c n e l. Okay? This multiplying. Alright, vector here which is filled with our multiplied by f a h, over 2. Okay. And here we have 4 over 3. 2 over 3, 4 over 3. So on, until the last entry here is one third. All right, actually no. We don't get the one third entry here. Because we have only up to c2 n e l degrees of freedom. The C vector. Right? As a result, the last entry here would also be 4 over 3, okay? 4 over 3 would be the entry from the mid-side node of the last element, okay? The contribution from the last node of the last element, which is the one that I've marked out here Is missing also in the vector that arises in the forcing function. Okay? We have here also then the contribution from traction. Well we don't have a contribution from traction, because there is no traction in this problem. Right? That's it then. Okay, there's actually, I don't even need this parenthesis. Okay, that's it. That's what our right hand side looks like, okay. I can point out something about dimensions. This is matrix is, this vector is 2nel minus 1. This vector as before is 2NEL plus 1. This vector is 2NEL minus 1 just like the C vector on the left hand side because it is the same. The forcing vector now rises only from the uniformity, from the distributed forcing. Which we are resuming to be uniformly distributed and therefore has a constant F. Okay so this is also 2NEL minus 1 in dimension. Okay. This matrix that will eventually give rise to our stiffness matrix, we know has therefore, dimensions 2nel minus 1 times 2nel plus 1 in the d vector here. D1 is known. It is U0. Right. It is this condition. This is also known. It is U sub L. That known condition. We will do what we did in the case of the Dirichlet Riemann Problem. We will note that the d1 degree of freedom multiplies out that column. Whereas, a d. 2 N E L plus 1. Degree of freedom multiplies out, back column. Since D1 and D2NEL plus 1 are known, because U0 and Ul are given to us, we are going to move close to columns that are marked over to the right-hand side, just as we did in the case of the problem, except  problem, we did that only to this column, all right? Okay, let's go ahead and do this now. Right? Redo this. We get C2 C3 up to C2nel-1 C2nel multiplying 2EA over h times this matrix now which is 8 over 3 minus 4 over 3, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3. 1 over 6 minus 4 over 3, 7 over 3, and so on. Now, we come down here and it ends in 7 over 3 minus 4 over 3 in the last element, minus 4 over 3, 8 over 3. Okay? All right? Multiplying In the d vector, we don't have d2 because, sorry, we don't have d1 because we moved it to the to the right-hand side. We come all the way down and we end here at d2nl. Okay? Equal to here, c2, c3, up to c2nel, okay. Multiplying fAh over 2. The vector here from the distributed forcing is four-third, two-third, all the way down until it ends in 4 over 3, at a mid side node of, at the mid side node of the last element. The last two contributions are 2EA over h times d1, right. Multiplying the first column, right, of our matrix on the previous slide, which is minus four-third, one-sixth, zero. All right, zero's all the way down And minus 2EA over h d2nel plus 1, multiplying 0, 0, zeroes all the way down except 'til we come to the last two entries, right, and here we get one-sixth and minus 4 over 3. Okay? We close parentheses, here. And note that d1 is known, because from a Dirichlet condition, that is U not. And d2nel plus 1 is known because from our Dirichlet condition, that is UL, okay? We have here C transpose. That matrix is K. This vector is D. Okay? C transpose has dimensions 2nel minus 1. D also has dimensions 2nel minus 1. K therefore is 2nel minus 1, squared. It is a square matrix. C again is 2nel minus 1, the entire vector here has dimensions 2nel minus 1. Okay? This is our f vector. Everything sitting inside these parentheses is our f vector. This, of course, is C transpose. Okay? Putting it all together. Again, for this Dirichlet-Dirichlet problem we end up with C transpose, K d equals C transpose F, just as we did for the Dirichlet-Neumann problem. Okay? So, for the Dirichlet problem also we have this. Okay? So whether we have the Dirichlet-Neumann problem or the Dirichlet problem we see that C transposed K d minus F equals 0. Now, remember that when we write out our weak form, right, such as in this case at the top of this slide I've written out our weak form fairly briefly and I made a statement on how it has to hold for all WH belonging to the space VH. My question is when we come down to this final version of the weak form, whether for the Dirichlet problem or the Dirichlet-Neumann problem. Where can we see a term that represents the weak the form? Right, it is c transports right? Or, or the c vector. Because that contains the degrees of freedom which are going to be interpolated to form the actual reading function field. All right? They have been interpreted in some basis functions. So if our weak form has to hold for OWH what can we say about this final equation, with regard to C? All right. We can say that this equation must hold for all C belonging to R2nel. This would be for the Dirichlet-Neumann problem. Or, for all C belonging to R2nel minus 1. This would be for the Dirichlet problem. Right. Well, if that is the case, one can show from stand, standard arguments that this implies therefore that K d minus F has to be equal to 0. Okay? Or Kd equals F is our standards form of the finite element equations for linear problems. Okay. And as we saw before, as we did for the case of the linear problem, we say from here d is now equal to K inverse F. How we actually solve this is a different matter which we won't get into in this series of lectures, but nevertheless, this is the definition of our nodal solution. Right? Once we have that, we can go back and regain our fields by using the basis functions. All right, this is a great place to end this segment.

\section*{ID: 7EauMTn5Zgg}
What I'd like to do in this segment is talk about how we actually carry out integration of the different terms that show up in our weak forms. And I'm referring specifically to the fact that when we carried out the formulation for linear and quadratic basis functions. We explicitly carried out certain integrals which we could do by the way because we assumed that our coefficients of the problem, our, our coefficient e if we were talking about elasticity, or it would be like, it would be ended something like a conductivity, if we were talking about some other problem. That coefficient was uniform over the domain. We also did the same thing for the forcing function, right? The distributed forcing function f we say was uniform over the domain. It made our integrations fairly easy, okay. However, that's not always the case. So we will need to develop the ability to carry out numerical integration. Okay. It's needed if the coefficient in our problem, such as E our forcing function f. And other quantities, maybe the area a, and so on are complec or let, let me see, complicated functions, right, we're not really talking about getting into the comple, into a complex space here, but, but, but we need them, however, of e and f and so on, are complicated functions. All right, functions of position x. Okay? All right we could also need them if the sort of basis functions we need are very high order. Right. And sometimes integrating them or, or if all of those basis functions are something more complicated than simpler, simple sort of polynomials we're using here. Okay, all right also a complicated Basis functions are used, okay? Now, so develop numerical integration for this. In particular, the approach that we will take is what is called Gaussian quadrature. Okay? We will take, we will consider Gaussian quadrature, which can be shown to be optimal for polynomials. Okay by optimal we mean that it's possible to integrate certain types of polynomials. Polynomials exert an order exactly, okay?  There's a systematic rule about how we go about doing this. All right, so let's see what we need. If you look at to any of our integrals either, either the integrals going to the stiffness matrix or into the for, into the distributed forcing function, you will see that we always are faced with the task of integrating over minus 1 and 1, a function of the form g of z dz. Okay. The whole business with any sort of numerical integration, any sort of quadrature is the following. We replace it with a sum, okay? Over L. Going from 1 to mint, where n int simply means number of integration points, okay? We replace it with the sum where every of term in that sum, in that series if you like is in that finite series, is g evaluated at a certain value of c indexed by L. Multiply it by what we call a weight for L. Okay? This is the general form of a quadrature rule. Alternately and numerical integration rule. Let me say a few things here. N sub int is the number of integration. Points. Okay? See L is an integration point. Okay. Wl is the corresponding weight. Weight ascribed. To the integration point. Okay here of course L equals 1 to an int. And here, too. Okay? Now, essentially the way to understand this perhaps is to say that, well, if this is what we're trying to do, if this is c, and this is our function g, and maybe g looks like that and we are here between minus 1 and 1, right? What we're trying to do here is pick certain points z, that is zL, one of those zL's, we look at the value of g, right? And that value of z, right? So this is g at z sub L. Okay? And we are basically forming that sum, like I wrote on the previous slide. Okay. We're giving some certain, certain weight to the value of G, X, E, L. All right? Okay. And then there are just rules, right? Before we get to those rules let's just see one more thing. The weight wL are such that some over L going from 1 to n int, wL has to be equal to 2. Can you think of why this might be the case? Why do we have this requirement? It's because If g of z is a constant, we know that integral minus 1 to 1 gz, dz, is equal to twice of that constant, right? Right? Well, this is satisfied automatically by saying that what, when we sum over L g of zL, wL, if that is a constant, and this rule for wL is satisfied, right? For the wL to satisfied we get back twice of the constant. Okay, so we ensure that at the very least we can integrate constants exactly. All right, so Okay. All right, here are the integration rules, okay. So n int equals 1, okay? For this rule, z1 equals 0, right? And w1 equals 2. For n int equals 2. We have z1 equals minus 1 over root 3. W1 equals 1. Z2 equals 1 divided by root 3. W2 equals 1. Going on, n int equals 3 as z1 equals, minus 3 over 5 square root. W1 equals 5 over 9, z2 equals 0. W2 equals 8 over 9. Z3 equals 3 over 5, square root w3 equals 5 over 9. You can check that for all three rules I put down the sum of the w's is always equal to 2. Okay, these are all rules from Gaussian quadrature. Right, and it goes on, right. We can right out rules for any order of n int. Okay, I mentioned that Gaussian quadrature is optimal. It is optimal in the sense that a Gaussian quadrature rule. With n int points, right, and n integration points. Exactly integrates. All polynomials of order. Order lesser than or equal to twice n int minus 1. Okay. So, if we go back and look at n int equals 1 integrates linears exactly. N int equals 2 integrates octo cubics exactly. N int equals three integrates octo pentics. All right. And so on. All right. That's what we need to save for numerical integration and we'll end this segment here.

\section*{ID: --5tvLVBTtE}
Hi. In this segment, we'll start looking at the function for generating the mesh, and for defining the boundary conditions. All right, so let's look at the code. Generating a mesh, there's actually not a lot for you to do. I have this global variable L, you remember I declared above, it's a class object, a class variable. So you simply have to define the length of the domain. I'm assuming that the value of x at the left of the domain is zero. And then of course, the maximum value of x would be L. From there, I take the x-min and x-max and I put them I create these DL2 points. A point in DL2 is, it's essentially a location vector. A position vector. The dimension or the number of components in the point is defined by dim in this case, and it holds doubles. I have actually, I don't use point a lot in this template, you won't actually need to use it at all yourself. But that is the input that DL2 requires to create the mesh. All right, so I take the limits of the domain, which you define here with L. And remember the input is an unsigned int called numberOfElements. In main.cc, remember, I had put in 15 as an example. But that just gives the number of elements in our mesh. All right. And then DL2 will use that number to create a mesh. It stores that again in triangulation, which again is a class object. All right. So not a lot for you to do there, but an important step. A lot of it is, again, DL2's functions. All right. Now, in define-boundary-conds, there actually isn't anything that you need to do in this function for this assignment. However, for the future assignments, you will be writing the defined-boundary-conds function yourself. It will be very similar to this function, but again, you have to change anything here, but I want to step through with you now because again, you will be using in future assignments. All right, so here we're using that boundary values map that we talked about, and we already talked a little bit about how the map itself works. However because of the way DL2 does their node numbering, instead of simply saying okay, we know how many nodes there are, so I'll just do boundary values at zero, is equal to the fixed boundary condition of zero. And if I have a Dirichlet boundary condition on the right, then I'll just take the last node. But again, since they aren't numbered sequentially, we can't just take the highest node number. Because that's not necessarily the last node in the domain. Right? So again, as an example, let me take Let's create a domain here. And I will, for simplicity, I'll create four elements. And on the top, I'll do the x coordinate. So, this isn't the real domain. We're going zero. And I'll go from zero to one. That would make these 0.25, 0.5, and 0.75. And just to make my point about the node numbering, I'll make this quadratic, quadratic basis functions. And so we have one mid side node. Down here, I'll put the node number. Of course in DL2 node numbering. Okay, so it's 0, 1, 2, 3, 4, 5, 6, 7, 8. Again so, this node, assuming we're doing two Dirichlet conditions, we want to fix this node. And this, let's say we want to apply a displacement of 0.1, that's a pretty big displacement here actually. Since it's linear elasticity, I'll do 0.01. So let's say, g2 is equal to 0.01. All right, so what we want to do is boundary-values of node zero is equal to zero. And boundary values of the last node, which in this case is actually node 7, not node 8, is equal to 0.01. These are semicolons, and so since we don't necessarily, okay, for the one d case, we can keep track of the way DL2's numbering their nodes. However, once we get to 2D and 3D, it gets more complicated from there. So instead what we're doing and what I'm doing in the code, is I just have a for loop. I'm looping over all of the nodes, so you can see here I have unsigned int globalNode=zero, and I'm looping up through nodeLocation.size. Remember, nodeLocation.size is a vector of the locations of the nodes. All right, so let me write out what nodeLocation, what this vector would look like, in our case, the actual data that it's storing. Okay, so we have nodeLocation. And this is what it's holding. Okay, so its hold for node zero, again looking at the indices are these node numbers down here. So for node zero, we're holding the value at zero. For node 1, the x coordinate is 0.25. For node 2, it would be 0.125. All right, and that goes on, until node 7, which has a value of 1. And then node 8, which would have a value of 0.875. Right. Okay. So those are the entries in nodeLocation, okay? And so the length, you can see it has a size of 9, which is of course equal to the number of nodes in this system, okay? So, here in the function define-boundary-conds, we're simply looping over all of the nodes using this nodeLocation vector. And then we have an if statement. If the value stored in the node location at the current node that we're on is equal to zero, in other words, if we're at the left end of our domain, then we set boundary values of whatever the node index is equal to g1. And actually, I can do here an else if, if I wanted to, else if, or just if nodelocation at our current node, wherever we happen to be, is equal to L. In other words, if we're at the right side of our domain. And if problem equals 1. So problem 1 is where we have two Dirichlet boundary conditions. So if we have a Dirichlet boundary condition in our problem, and we are at the right hand side of our domain, then we will store the Dirichlet value of g2 in our domain. So that's this one right here. Okay, so again, there's nothing you have to edit here. But it's good for you to understand what's happening so that you can use it on the next homework assignments. All right. Let's move on, and in this segment, let's still look at set up system. Okay. So in set up system, first we have to define. Again, we were using g1 and g2 in the previous function, then here's where we actually define what those values are. And you'll have to look in your homework assignment to see what those values will be. Of course in problem 1, we use both g1 and g2, in problem 2, g2 won't actually be used, because its a Dirichlet Neumann problem. But you can still use the same value of g2, right. Okay. This next line is just DL2 distributing the degrees of freedom. It's just keeping track of degrees of freedom. Now here, there are several lines going on here, but essentially all I'm doing is creating this vector node location which holds the x coordinates for each global node, again by its global index. Now that those structures are defined, now we will call this function define-boundary-conds. Okay. Notice that since we are still within another class function itself, we don't have to create like, problemObject. We don't have a class name, since we're not dealing with an actual class object yet. So we don't have this problemObject.define-boundary-conds(). We can simply use the function name itself, because it is also an FEM class function, okay? All right. So we call that function, which is the one we were just talking about. From there on, this next section just has to deal with resizing the matrices and vectors. You can see rather than using the function.resize, which is what standard vectors use, the DL2 vectors and matrices use .reinit, short for reinitialization or reinitialize. And we use this dof-handler.n-dofs. That gives us the total number of degrees of freedom in our domain. Since we have one degree of freedom per node, it's the same as the total number of nodes in this problem. Now, here we define the quadrature rule. I set up a quadrature rule of two quadrature points per element. However, that's not going to be enough. Remember that for the quadrature rule, let's say you have a quadrule, whoops. Let's say you have a quadrule of 2. This is actually good for up to, for an integrand of up to third order. So, remember that Good for and integrand of third order or third order polynomial, right? So remember that this Gaussian quadrature is exact for polynomials, Of order equal to 2 times the quadrature rule, minus 1. So in this case if you have a quadrature rule of 2, it's good for a third order Because 2 times 2 is 4 minus 1. So if you have an integrand of order 3, with polynomials of order 3, then the Gaussian quadrature will integrate exactly for you. Okay. But remember, you'll be using not only linear, but also quadratic and cubic basis functions. And you'll be doing an L2 norm. Okay? So you have to, in each of these cases, don't look actually just at the order of your basis function. Look at the whole order of the integrand that you're integrating. And once you know that polynomial order, then you can decide what quadrature rule order you need. Now having said that, you don't have to necessarily change your quadrule for each problem. You can decide what's the maximum quadrature rule that you need. And that actually will come from the L2 norm using cubic basis functions. You can decide what quadrature rule that is. Is it four, is it five, is it six? Decide what that number is, and then you can use that quadrature rule for all of your problems on homework one. Okay, for whatever basis function ordering. All right. So again, you will define that here. So you would change quadRule to whatever value you want. And then quad-points and quad-weights are, again, just vectors. You put in the, so quad-points stores the values of c for each quadrature point. For two point, it's minus squared one third and positive squared one third. And the corresponding weights are one. Okay. If you go to a higher quadrature rule then you will obviously create a longer vector. You can see that here I do quad-points.resize, and I resize it according to the quadrature rule that you define up here. Okay, so it will automatically go to the correct length. You have to define the correct values. And those can be found online, or in a book on finite elements or on numerical integration, all right. Okay, so let's stop this segment here. And in the next segment, we'll move on to the assemble system function, the assembly function, which involves creating f local and k local.

\section*{ID: W-vbVPBeNK0}
In this segment, we'll start looking at the main function in the template file and that's assemble system. So, let's move over and look at the code for a second. The assemble system is only called once, of course. So here we zero out K and F. I've defined several constants and objects up here. So I don't think I've used this before. I have const unsigned int. So what that constant does is, it allows me to define this variable only once, so it's actually not even a variable. It can't vary, it's a constant. So this dofs-per-elem is degrees of freedom per element. And of course that changes with your basis function order. Same as number of notes per element in our case. Then I declare a full matrix. Again, this is another DL2 object, and it stores doubles and as I'm initializing it here, Klocal is dofs-per-elem by dofs-per-elem. Okay, it just defines the size of the matrix again. Flocal is also a DL2 vector. With length dofs-per-elem or nodes per element. Now here I'm creating this standard vector of unsigned ints called local-dof-indices. It also has a length of dofs-per-elem. And this vector will actually update for each element. It's different in the values,and will be different for each element. But what it does is it relates our our local node numbering to the global node numbering. So let's look at that for a minute. Again, I'll create another sample domain. I'll do let's, I guess in this case three elements. For quadratic. So we have one mid side node. And let me do the global node numbering on top. And then on bottom I'll do the local or the element node numbering. Again, both of these are in DL2's syntax. All right, so we have 0, 1, 2, 3, 4, 5, and 6. And now I'll break this up into element 1, it's called it e1, e2, And e3. Okay, so those are our three elements. So locally, these nodes would be labeled 0, 1, 2. Same for all of them, 0, 1, 2, and 0, 1, 2. So local-dof-indices will look different for each element. So local-dof-indices. For element 1, this is what it will look like, local node 0 corresponds to global node 0. Local node 1 corresponds to global node 1. Local node 2 corresponds to global node 2. Okay, so they all match up there, and let me specify here that these are the global nodes or local nodes. All right let me change that. So on the top this is the index, is the local node, and the actual value stored is the global. So the index is local. And the value is the global node. So for e2, again we have 0, 1, 2. But globally now we look at element 2. Local node 0 corresponds to global node 1. Local node 1 corresponds to global node 3. Local node 2 corresponds to global node 4. Now, of course, DO2 is handling all this for us. It will populate this local- dof-indices vector. But I just wanted you to understand exactly what's what's going on here. So, for element 3. Now we look at local node 0. Here corresponds to global node 3. Local node 1 is global node 5. Local node 2 is global node 6. So if we, let's say we're working with element 2. We've initialized local-dof-indices for element 2 and so now using the C++ syntax, if I put in 1 as my index it would give me 3. If I put in a local node of 0, it would return 1, a global node number of 1. So this vector local-dof-indices will relate our local node number to the global node number. The index to the vector is the local node number. The value stored is the global node number. And we'll need that especially when we're going to assembly. So now let's go and let's see I also have these variables stored here they're all doubles, h-e is the element length, x we'll use to find the value of x at quadrant points. Which we'll need to define our forcing function. And then f is the value of that forcing and function. Now we have a loop, a loop over elements. This is where, remember in our review of C++, in our introduction to C++, we talked about iterators and they're similar to pointers in some ways. And that we can use these iterators and four loops. In this loop over elements, we're actually using an iterator. You can see here, it's an active cell iterator. By the way, in the DL2 name, nomenclature, they call elements cells. So when you're seeing a name that DL2s set up, it usually says cell for the element. So, here, we're actually looping over elements, they're called cells. And now all of this I could have put, this line elem = dof-handler.begin-active() I could have put here in this declaration portion of the for loop. But since it's so long I can declare it ahead of time and that's okay to have an empty spot there in the for loop, declaration. So I've declared two objects here, I have elem, which is my iterator and it's And I'm setting it to the beginning element. I also have this endc, which is the last element. Or really it's a marker after the last element, okay. And so my for loop, I'm already starting at, element equals the first iterator. For the iterator is equal to the first element. And my condition is, as long as I'm not at that marker past the last element, then I'll go through the for loop. And again, this ++elem, it just increments to the next element. Again, even though elem is not an integer, it's an iterator. The ++ still iterates forward, okay? So now that we're within a particular element, we have to update the local-dof-indices vector, okay? So we do it this way, elem->get-dof-indices. Remember this arrow is the way we, for either a pointer or in this case an iterator, this is how we access a function. Okay, so since elem is an iterator, instead of doing elem.get-dof-indices, we have to use this arrow essentially, okay. And so that will populate the correct local-dof-indices with the correct, global, node numbers, okay? All right, now, I'm going to find for you the element length. As I told you before, the way we've defined our mesh, each element will have the same element length. So, you could actually have just taken the length of the whole domain and divided by the number of elements to get element length. This is just a little bit more general in case you ever set up your mesh to have elements of different lengths. What we're doing here is I'm saying h-e = nodeLocation at local node i minus the nodeLocation at local node 0. Okay. Remember, local node, or sorry, local node 1. So local node 1 again is always on the right. You can look back here on our schematic here. The local node 1 is always on the right of the node. Local node 0 is always on the left. If we take the x coordinates of those two and subtract them then that gives us the element length for whatever element we happen to be in. Again I use local-dof-indices though Because I may not be in element 1, I may be in element 2. And I really want the, and remember, nodeLocation, the vector nodeLocation, is indexed according to the global index. And so I would want, in this case, if I wanted the x values at local node 1 and 0, I would need the global node numbers 3 and 1 in order to access the correct x values within my nodeLocation vector. So that local-dof-indices will come up again as well as nodeLocation. But now we have h-e which is our element length. Now, first let's create our local force vector, Flocal. Now there are really two parts to this. There is the portion due to the forcing function, or the body force, and then if you have a Neumann, condition, a non-zero Neumann condition like we do on problem two, then you'll have to add in the effect of the traction at that boundary, okay? So first we will do the portion of Flocal due to the body force. Okay, so here I've set Flocal = 0. Again, we zero it out for each new element. And now in looping over A, so each component of Flocal, so I'm at Flocal. And now I'm looping over my quadrature rule. Okay, I'm looping over each quadrature point, because we will need, remember in Flocal and in Klocal we're performing integrals. All right, so let me actually write out this general form for this portion for Flocal, and again this is in the lectures. Let me write this out. So Flocal is equal to the area the cross-sectional area times the element length over two. Remember, we use that element length over two to convert from bi unit domain to the real domain. We're integrating from negative one to one. Basis function a corresponds to our index here evaluated at node xi, times our forcing function or our body force, f. f we're given as a function of x, but since we're integrating over xi, we need to evaluate everything in terms of xi. Okay, and then we integrate it over our d xi domain. Okay. Now since we're using quadrature, let me write this out using quadrature points. Ahe/2 is still on the outside. However, now, we are going to loop over our quadrature points. And so, I'll just do q equals 0 as long as we're less than quad rule. Okay, and now we're going to evaluate the integrand at each quadrature point. And remember you get the quadrature point from that quadPoints vector. I'm going to distinguish it here just by saying it's xi q. So xi q is the quadrature point. Okay? And f is a function of x, but again we evaluate, you can get x, a value of x for that particular quadrature point. Now it's no longer an integral. It's a summation. And so we also multiply it by the weight. So that's short for the quadrature weight at that particular quadrature point. And again, we also have a vector of quadWeights. Now, this term should be easy enough to evaluate because we already have our basic function that has as an input the node and xi. Now here, you're given a forcing function or a body force. How do we do this term here? We have a value of xi. We need a value of x. So this is also something from the lecture, but just as a reminder, we can interpolate. Over the values of x at our local nodes to find the values of x at xi. Let me specify here that this is the value of x at xi q for example. Okay, so what will we do? We'll start at A = 0, and as long as A is less than the number of nodes, the element, we will take x, A times our basis function evaluated at xi q. And what do I mean by x a? This is the value of x at node A. In other words, which we can get from nodeLocations, okay? NodeLocations, but remember, A is a local element, or a local node number, nodeLocations requires a global node number. And so, to convert we use this local-dof-indices of node A. I've actually written all of this up for you in the code, but I wanted to explain it to you in this one case. Because you will be performing other integrals in this homework, and in other homework assignments, so I want you to understand what's going on here, okay? So, going back to the code you can see, again, we're looping over B=0, B < dofs-per-elem, in other words, less than number of nodes in the element. And x +=, so we're doing this summation, nodeLocation(local-dof-indices) * basis-function(B, quad-points(q). Okay, again I'm just using the next B instead of A but it's the same thing right? Okay, so now you have the value of x here, this is where you have to start doing it yourself. So now that you have x, you'll need to evaluate f, evaluate N A, and essentially just create this integrand, well what was an integrand and is now this inner portion of this summation. And you will use that to define Flocal at A, okay? So you'll need to do that. Now that takes care of the body force. However, in problem two, or part two of the this homework, you also have a Neumann condition. Of course that only applies at the right hand node. And so what I've done here, is if were at problem two, then it's checking if your node location, if the node that your at is on the right hand side, then that's where you'll have to add in the traction times the area which is H, okay? So you're really just adding an h to the appropriate component of Flocal. And again you can review your lectures if that process is a little bit fuzzy. So that will take care of your Flocal. So we've taken care of the body force and when applicable we've taken care of the Neumann boundary condition. Now the next part is to define Klocal. And let me write out the general form for Klocal as well. Now Klocal is of course a matrix so let's look at component A, B of Klocal. We have 2EA, E is the Young's modulus, A is the cross-sectional area over h e where h e is the element length again. And here we've already converted to an integral over the bi-unit domain. That's why you have the h e in there. Now we're dealing with gradients. So this is the partial, I guess the derivative of basis function A with respect to the xi, times the derivative of basis function B with respect to the xi, okay? And so again, these two terms you'll get from your basis gradient functions that we've defined previously, all right? And again, here you'll convert this integral into a summation over quadrature points, multiplied by the appropriate quadrature weight. Okay, so I've set up the for loops for you. We have a loop over A and a loop over B, and a loop over your quadrature points. Now you simply have to write out what is Klocal A B equal to? And of course, since it's a summation, you'll be doing a += just like you did with Flocal and like we did when evaluating the value of x at the quadrature point, right? Okay, so once you've filled in those lines, you'll have your Flocal and you'll have your Klocal, okay? Flocal will already have any Neumann boundary conditions that are involved in the problem. Now you need to go to assemble system, you have to actually assemble them into the global matrix in the global force vector. Now again this was done in the lectures. You can go back and review that there. The real trick of it here is to remember to use local-dof-indices and it makes everything very simple. So for example if for F We have the global nodenumber. So essentially you'll, for each element, you'll be looping over each node in each element, okay? And you'll put that value of Flocal into the corresponding value, that should be a +=, because we don't want to delete any values that were already in F, right? We just want to add on to them. And so you add on the Flocal for the local node number. Okay and the local node number will of course be going from zero up to the number of nodes in the element. And then how do you go from the local node number to the global node number?. Again its just local-dof-indices, okay? All right so, it's actually pretty straight forward. You will do that with F and you'll also do that with K. Of course, K has two indices, so we have two loops here. We have a loop over A and a loop over B. Now, the one thing that you need to be aware of, really, is that since K is a sparse matrix, we can't just do K  just as a short hand and  You can't just do += like this. And of course A and B here are the local. All right, you can't do it this way. We instead do K.add. And it's because it's a sparse matrix and we're not dealing with all of the, not all of the indices are actually available to us, okay. So we do this K.add, you have your A global, B global. So these are the indices, these correspond here. And then this term is the actual value that you're adding. So it would be this here. Okay? So those are equivalent as far as the thinking goes behind it. This is, of course, the actual writing that you'll do. Okay, now this assembly process is really just two for loops, just a few lines. It's good for you to understand what is going on here because you'll be doing this exact same process in all of the remaining homework assignments. It's actually the exact same lines of code. Okay. And so it's good for you to understand what's going on here in this first homework assignment. Now the last function that's called in the symbol system is this apply boundary values. Now we're not going to go through all the mechanics that we learned in the lectures of applying the Dirichlet boundary conditions. You remember in the class we had K, D equals F, right? If we happen to know that boundary conditions, if we had Dirichlet boundary conditions, and say these, excuse me. If we had Dirichlet boundary conditions in these two, on these two nodes, then what would we do? We would actually remove the corresponding rows and columns. Okay, we'd also remove the corresponding entries in F, and that would give us a smaller matrix vector system, right? I think we set these as bar, and then these were actually K, D, and F, or something like that, right? The way deal two does it a little bit differently. Instead of actually deleting these rows, it just modifies the entries. And KM modifies the entries in F to give you the correct values of D, or the correct displacement values, okay? So you can actually look into that a little bit more if you'd like to, but the main point is that the deal two will be taking care of the application of Dirichlet boundary conditions, and it actually does it without modifying the size of K, D, or F. Okay, so that is what's happening here. You can see we're inputting boundary values, which is the map that we created earlier that holds the values for the Dirichlet boundary values, K, D, and F. This false has to do with whether or not deal two is modifying entries in the row only, or if it also modifies entries in the column as well. False means it's only modifying entries in the row. Again, that's not something you'd need to modify, but it's helpful to see what's happening there. Okay? All right, let's stop that segment here. And in the next segments, we'll look at solve and output results, which are really short functions, but then, we'll look mostly at calculating the LT norm of the error.

\section*{ID: l86hK6L-4BY}
Welcome back. At this point, what we've managed to do with our formulation of the finite element method for these 1d linear elliptic PDEs is actually state the problem in completeness, right? We've derived the matrix vector equations for linear and quadratic basis functions explicitly. And we've also laid out the parts to be followed when we go to higher autobasis functions. And, while, in the formulations we develop, we carried out analytic integrations of all the terms that were needed. We've also looked at how we would go about carrying out numerical right? Numerical integration. And numerical integration is what would actually be done in a code, which before too long you will also be writing. What I'd like to do now is well actually observe that we are ready to move on to other problems, right? We are now ready to move on to multidimensional problems. Before we do that however, it's important to understand a little more, just a little more about the mathematical basis of the finite element method. And there are at least two things we need to do. What we will start doing with this segment, and it'll take three or four segments, or maybe a little longer. What we will do is to understand why the finite element method works, what its special properties are. And also, get a a fairly high-level view of the way finite element conversions works. Right? And we'd see the mathematical basis just to see a few of simple proofs for why the method works. Okay? All right. So that's the plan. We are going to start by talking of norms. Okay. And that, and by this, we mean mathematical norms, right? How do we estimate the, so to, so to speak, the magnitude of certain functions. 'Kay. In a manner, that makes sense for what we have to do here. Okay. Let me give you a little background before we really plunge into the subject and let us start by considering right. Consider the the finite dimensional trial solution that we are working with, okay. So consider the finite dimensional  trial solution. Okay? One thing I'm going to start doing from now on is perhaps stop referring to this as the finite dimensional trial solution, though it is that but actually start calling it the finite element solution. Okay. And so, this is the field that we would get by carrying out the finite element solution, and maybe going back and re-interpolating from the trial solution degrees of freedom. Okay. So let me also state that. Also called The finite element solution. Okay? That's probably how we, are we refer to it from as, as we move on. All right we consider it in the context of the problem we're solving. So we have something like that, and let's suppose we have some number of elements here. Okay, what I'm going to do is consider to show that there is some generality, complete generality with what I want to state. Let's consider that we are using here quadratic basis functions. Okay? So, that would mean what are intended to be elements here, each of them has midside norms, okay? So this now is omega 1, right? That represents some general element omega e, right. I'm going to now sketch out the quadratic basis functions. All right. So we have that. We have N2. And I'm sorry, sort of straight over a little. Let me try to do better.  We have N2. And we have N3. All right, for this element. Okay, I'm not going, well let me label them just for this one, maybe. N1, N2, and N3. Okay. For element two, let me perhaps use a different color so we can highlight a certain point. Okay. So for element two, my shape func, my basis functions are going to be, that. Okay, can, can I do better than that. Sorry. Okay, that's N1 for element two. That is N2 for element two, and that is N3. Okay? So, again, I will say N1, N2, N3. Okay? Now, of course, we also know that associated with each of these nodal points, we get trial solution, degree of freedom values, right? So let me write those. And here, I'm going to write them using global numbering, okay? So here I have d1. I have d2 here. I've realized I switched from sub to superscript in the first one. Okay. D1, d2, I'm going to write d3 here. D4, d5, and so on, okay? Now, the point I'm trying to make here is that the way you would, we would construct our feed now, right, within each element e, would be obtained by that process. Right? DAe, and we now how to go between local and global no degree of freedom numbers. Okay, observe the nature of our basis functions here. I'm going to ask you a question. What I've sketched here in red and green are the basis functions, respectively, for elements one and two. Is the solution continuous? So, the solution, the finite element that one would generate by re-interpolating from the trial solution degrees of freedom. Would that be continuous? 'Kay. Think about it. The answer is yes, it is continuous. Okay. Mathematical notation for a continuous function is C0 on omega. Okay? When we see a function of C0 on omega we're seeing, C0 simply means the function itself is continuous, right? Over the domain omega. Okay what about its derivatives? Are the function, or its derivatives continuous over omega? Think about that. Well let me ask you in step stone comma x. It turns out that it is continuous within each element. Right? So if I look at e comma x, right? Restricted to a single element, right? Within that element, e, the derivative is also continuous. Okay? So we would say that this function is C0 on omega e. Right? So the derivative is continuous within each element. Right? What about over the entire domain? Is the derivative continuous across the domain? In general, well not in general. The, the derivative indeed is not continuous across the domain. In particular, there are points of discontinuity on the, in the derivatives, right? At element edges like there. And there, right. What we will see is that comma x is discontinuous. Okay, all right. So, so comma x is discontinuous. In all of omega. Right? So now here what I am specifying is that we are not considering restricted to a single level. Right? It is across elements. Okay? So it's discontinuous in omega, and so what we say, however, is that comma x, the, the derivative is not in C0. When we take the entire domain omega into consideration, okay? So this is an important thing to notice, right? And where does this come from? Why is it that the solution right, the final element solution as itself continues, but its derivatives when considered over the entire domain omega, are discontinues. Where does that, where does that come from? Right, it comes from the basis functions, right? So what's important to notice here is that the Lagrange polynomial basis functions have been constructed. To be only C0 on omega, right. We have basis functions, which are only continuous, and that's immediately observable if you just look at this sketch that I made of the basis functions, right. Clearly the basis function that continues, but of course if you look at the inter-element nodes, it's clear that, that the derivatives are not going to be continuous. And of course, once you have that, any function that you then construct using that basis to represent the function is also going to be only continuous but not be continuous in its derivatives. Okay? So, we say that it's only C0. We say that it is not the function itself now, the function eh, or referring to basic functions themselves are not Cn on omega for n greater than 1. Sorry, n greater than 0. Okay? All right, the basis functions themselves have been constructed to only be continuous. There you see 0, okay? Their derivatives are not continuous, okay? Right, if the derivatives were continuous, we would say the functions would be Cn, right? Where n would be the number of derivatives that were continuous. Okay? So we're saying that the basis functions are not Cn, they're only C0. Okay? All right. So let me just give you an additional statement here to give you the more general case. In general, a function is in Cn omega if its derivatives Up to order n are continuous on omega. All right, our basis functions themselves are continuous. None of their derivatives are continuous. Therefore, they're only C0. 'Kay, the zero indicates no derivative continuity. 'Kay? However, remember when we talked about spaces. We introduce the spaces L2 and H1. Okay. So however, does right, the finite element solution belong to H1 on omega? Okay. All right. Remember, so what we're asking is, what this means is, if you recall, is the following. If we integrate over omega squared plus 1 over measure of omega to the power 1 over number of spatial dimensions. Okay. Sorry, this should be 2 over number of spatial dimensions.  Okay? Times comma x squared, dx. Right. If we do this, okay, what we are asking is, is this quantity bounded? Think about it. Is not continuous, okay? So what we will find is that this term, right? The derivative term is discontinuous. However, if we integrate it, right? Discontinuities are not unintegrable, all right? They can't be integrated. Okay so when we integrate it, we will see that this quantity that we put down here on the left-hand side is indeed bounded. Okay? So the answer is yes. Okay? All right, so even these continuities themselves don't pose a problem. Okay? All right. Now, okay, so does belong in H1, okay. So I'm going to use this background to say that and, and of course you know we, of course belongs in H1, because that's how we constructed right. Remember when we defined the Sh, right. Okay. So recall, belongs to Sh, which consists of, we, we said that it consists of all functions to each belonging to H1, right? On omega. And then we put in boundary conditions and so on, right? So, so by construction, it does belong to H1 and I'm, I'm just pointing out that yes, it does indeed belong to H1. Okay, with this as background, I'm going to start defining some norms.

\section*{ID: NzqUwHWhooQ}
There remained an error on board work on the very last slide of this segment and the error came in here. The term that I've circled there, which is a factor multiplying the gradient of the function square, should be replaced with just, the following, all right? So we, the term in the denominator is really all we need. So we need measure of omega to the power 2 over n s d. That's it. We do not need that term which I have just written in green to be in the denominator. It is in the numerator and that's all we need to have as the factor. All right that fixes that slide and the rest of it or what came before it of course is correct.

\section*{ID: KahsBm-Ve-Q}
HI, in this segment, we'll be finishing up the template for the first coding assignment. We'll look at the solve and output results functions, and especially we'll be looking at the L2 norm of error function. Okay, so let's move over here to the code. Solve you can see is a very short function. We first create this SparseDirectUMFPACK object A. Again it's also like a sparse matrix, but it's set up for a particular solver which is the UMF pack solver, okay? Now this A we initialize with our global matrix K. Again we've already applied the under conditions, we've already populated it from K local. And now we're going to do A.vmult D and F. What that is doing is D is = to K inverse times F, okay. I just want to point out here in the future when we are doing some matrix spectrum multiplications we will use dot vmult with DL2's full matrix data type. Just want to point out that here with SparseDirectUMFPACK.vmult does a matrix inverse times a vector. But in the future one we're dealing with just the DL2 full matrix and DL2 vector dot vmult simply does a matrix vector multiplication without doing any inversion. So again, in this case, with SparseDirectUMFPACK, it does a matrix inversion with DL2 full matrix it does no matrix inversion, okay. But again there's nothing here in solve that you need to change. Help results again is nothing you need to change. It creates a .vtk file, which is actually a text file, but that can be read through visualization software. Okay, in 1D there's actually not a lot to see because it's just a straight line, but in 2D and 3D it's really helpful to see what's happening with your temperature plots or your displacement plots as the case may be. All right, so we'll move on from there down to the L2 norm of the error. All right, Now here I'll write it out on the screen what this is. So we have out L2 norm of the error. Again, this is written on your homework assignment, and we talk about L2 norms in the lectures. Right here it's the square root of the integral over the entire domain of your exact solution, minus your finite element solution. Put dV just for generalization of course, in this case it's just across the x component. Alright, so this U is the exact solution. And is your approximate, or finite element solution. Okay, so let's look at how we'll do that. All right, so first I define this double L2 norm is equal to zero and eventually That's the value we'll be returning really the square root. So L2 norm will be that integral and then we square root it as we return it, okay? We have again a constant unsigned int of dofs-per-elem, which is again, just a number of notes per element. Our local-dof-indices vector and I've created these variables u-exact or just which is our approximate solution, our finite element solution. X, which we will use to find the value of X at each point. Again, X is the value in the real domain, the coordinate in the real domain. And HE, the element length. All right, we have the same loop over elements. Let's go back to the board for a little bit. We're going to be integrating this through Gaussian quadrature. The reason why we have this loop over elements is because we're breaking up this integral. First we break it up into loop over elements. And so now instead of performing the integral of sigma over full domain, now we're adding up individual integrals over each element's domain. Still have u minus dV. All right, now, this integral. Remember, we're using Gaussian quadrature. So, inside this element loop, we will now have another loop. Actually, first, let me, let's convert to our bi-unit domain. So we're going from minus one to one. U. Again, now these are being evaluated like c. And to convert, we multiply by he over two. Remember, we had that same he over two factor in our f local from doing that conversion as well, right? And now it's in the d c domain, okay. And now from there it's easy to go to our quadrature. Then I'll keep our square root we still have our loop over at the elements. So that would be element equal to zero less than the total number of elements. Although actually we will be using our iterator again to loop over elements. And now we're going to loop over quadreger points. I'm just going to put a q here but understand that's quadreger points. We'd be going from q equal to zero, less than quadral. Now we will be evaluating u at that quadrature point, and at the quadrature point. And then we will. Oh, I forgot to put my square there. C minus squared from the beginning, right? So that quantity is squared times he over 2. And now we have to remember to multiply by our quadrature weight. And all this is on the inside of the square root still because this is all involved with the integral. So this is the weight for cq, okay? Now again, u is actually a function of x, similar to our forcing function or our body force. It's a function of x and x, itself, is a function of cq. And so you would evaluate c the same way we did with f. Our solution vector D has the values of At the nodes, right. But here we want the values of At the quadrager points, so you will also do similar to finding the value of x, you will interpolate to find the values. So Of CQ is equal to the sum over A, with the same bounds as before. It will be DA, so that's your solution vector, at node A., times NA CQ. And again this D will come from D, we use local d ref indices again to convert from our local node number of A. To the global node number because d, of course, is a global vector. And so we need a global index. All right, and so now with this, you should be able to create that integrand, okay. Let's go back to the code I've actually again I found for you, just as before. I found x for you again. Just the same way as before. No location, again, is how we find xa, or in this case xb times basis function b at the particular quad point. Now remember, when we're doing a summation we always need to zero out x equals zero, equals zero before we start adding to it. Otherwise the numbers will be, if we don't zero out X and Beforehand, then they'll be including previous values of X and And they'll be getting much larger than they actually should be. Okay, so we zero those out before you perform the summation. And Again, here is D, local dof indices of B times our basis function. All right, so now we get to the line that you have to do. And essentially, this is what your doing. Adding it up with an L2 norm okay. And then we gout out of loop and it returns the square root. All right, so if you remember, why do we even do this L2 norm error? It's so that you can verify that your finite ultimate solution is converging to the correct solution as you increase either the basis function order or as you increase the number of elements. Now something to watch out for there's something to think about is what happens when your basis function order is same as the polynomial order of your exact solution. What does that do to your L2 norm? Think about that. Also, what happens as you increase, once you have a really high number of elements in your mesh. And look at what happens the L2 norm. Is it still decreasing? Is it still converging at an optimal rate as you increase element sizes? If not, think about why that might be the case, okay. And think about what relation the computer, the machine error has to do with that, all right? Okay, so that concludes the first coding assignment and our discussion of the template.

\section*{ID: rB9e0NweTVA}
Hi, in this segment, we'll be looking at two visualization tools that you can use to look at the results from your finite element program. All right, so the two that we commonly used, although there are others are Paraview and VisIt. Okay, so let's look at their websites Paraview's just at paraview.org, and VisIt is at visit.llnl, that's Lawrence Livermore National Labs, .gov. And that's where you can download them, although if you're using the deal to virtual machine ParaView will already be installed there, and I think VisIt too. Okay? So if we go over to the website, here at paraview.org. It's really simple, they have a download tab here. And for the ParaView installer, you just choose your operating system, Windows, Linux, or Mac. And it's pretty straightforward from there to download that and install it. If you are using, or using Linux, you can open up the file, and actually within this bin folder, you can open up the executable. You can run the executable here. Okay? So you actually don't need to install anything when you're running it in Linux. You will install it with Windows, and with Mac it's also pretty easy to just download and run. Okay? So for an example here, I'll open up file. And let's go to our FEM templates. Here's an example of an elasticity file. So I open the .dtk file and pressed apply. Now it has this blank or this grayed out box. Okay, this box is a domain. But I don't want this solid color, I can come up here and change it to U, which is our displacement vectors. Okay, now it's giving me the legend with the magnitude of displacement as the color. Okay, I can rotate that if I want to. Since this is an elasticity problem, you'd probably be interested in seeing the displacements. Not just as a color but the actual displacements themselves. And so we go here, press on WarpByVector. And I'm going to scale it by 50, because I know that the displacements on this problem are actually pretty small. So I'll press Apply. And now you can see the displacements there and the magnitudes. You can also come up here and change for Magnitude of displacement to the X displacement, Y and Z and so on. Okay? All right. So that takes care of it pretty well. You could also. Look at the outlying surface with edges, so you can see your mesh. Wire frame just shows the mesh itself. Okay? So you can play around with some of the other options if you want, but this should cover you for the problems we'll be working with. All right, I want to show you one other thing here, I'll open up some solutions to the transient heat conduction problem. So here I can actually open up a group of vtk files that have the same, it's solution, dash, and then a number. So it groups them together, automatically. Okay? I can just press Apply here, and it automatically has the temperature profile, the temperature distribution. But since there are more than one, I can come up to the top of the screen, and I can select next frame, and watch the evolution. You can actually press play, but since there are only four files here, it goes through them pretty quickly. But if you're outputting more files, you could watch the evolution of your model as time steps on. Okay? So, this is a pretty powerful tool in looking at your finite element results and kind of getting a feel for what's happening in your solution, all right. We'll go quickly over to the VisIt website too. Again, that was at visit.llnl.gov. And it's similar. You go to the downloads tab. Here, the Executables. And then you scroll down and you can download for Windows, Mac, or Linux. All right, let me open up VisIt for you, I run mine from the terminal on this computer. It can do essentially the same things but it has a little different format. Now we can open up the elasticity problem. FEM templates. Homework 4. Solution .vtk. All right, so, now the file is open, I want to add a few different things. I'll add the mesh and press draw, so you can see the mesh there. I can see the mesh. Scroll out or in. Then I'll add sudocolor, a view magnitude. Again the displacement vector as hue, magnitude just again, gives me the magnitude. In VisIt, it's not quite as easy to just get the X, Y, or Z components of displacement. All right, here I can rotate it around if I want to. And to look at to deform it according to the displacement, I have to come up here to Operator, Attributes, down toTransforms, and Displace. All right? I need to select which variable I want to be my displacement variable to vector. And it's the vector u. And again, I'm going to change the multiplier. That to scale the displacement by 50, okay? Apply and draw, can dismiss that file, or that box. Okay, so again, you have the displaced domain, all right? And again, it's very similar if you want to look at your temperature distribution on the heat connection problem. Okay, so that should be enough to get you started using VisIt or Paraview in order to visualize your finite element results.

\section*{ID: XIvPxWjti-w}
What we are going to do, is to use this as background to define to define norms. Okay? We are going to restrict ourselves to defining a, the sort, only the sort of norm we, we, we want, okay? So what we will do is define the H1 norm, okay to be the following, okay? So supposing we're taking a func, we have a function, v, okay. We're going to define the H1 norm of v, and we will write it as that. Okay? The double lines are, are a kind of bracket. They're they're usually used to designate a norm, and that's subscript 1 indicates that we're talking to H1 norm. This is defined as the following 1 over measure of omega to the power of 1 over number of spatial dimensions. Integral over omega, v squared plus measure of omega to the power 2 over number of spatial dimensions, 'kay? Of v comma x squared dx. All of this raised to 1 over 2. Okay. Just a couple of things to notice here. We have divided through by measure of omega raised to 1 over number of spatial dimensions to get rid of the effect of having integrated, the function and its derivative squared over the domain. Okay? Defined as this, we have what is called the H1 Hilbert norm of our function v. Okay? Hilbert norms are, are a special case of a more general type of norm that's called a Sobolev norm. Okay. This is a this is an example of a, a, this is an example of more general norms called Sobolev norms. Okay? The idea of Hilbert and Sobolev norms come from functional analysis in Hilbert and Sobolev spaces. The fact that we are raising the function to the 2nd power as well as its derivative to the 2nd power, and then taking the one-half power of the whole thing is what makes it from a Sobolev norm in general to a Hilbert norm. Okay. 'Kay, for Sobolev norms, you would be raising to other powers, and also sort of taking that, the root of that corresponding power, right? So that, that, that leads to Sobolev spaces. Okay, so, but, but this is what we need to work with here, and so we work just with just H1 Hilbert norms, okay? Or we will just recorded them as H1 norms. Now note that you get a, a special case. Not, this indeed is actually a special, special case of a more general way of defining even a Hilbert norm as one can extend this to, to the Hn Hilbert norm. Okay? By, by just taking up to n derivatives instead of a single derivative as, as I have done here. Okay? So we can extend this to define the Hn norm, 'kay? By including the first n derivatives. Okay, instead of taking just one derivative here. Okay, now, the fact that we can do this also allows us to define what is called the H, what we may choose to call the H0 norm. Okay, by just saying well, I don't, I don't consider any derivatives, I just consider the function itself. Okay? So the H0 norm would then be denoted by either 0 here or it's more common not to put anything. Okay, so when I just write this without any, without, without the subscript 1 as I have in the first equation of the slide, we have what is called the H0 norm. Okay, which is just now 1 over measure of omega 1 over nsd integral over omega v squared dx to the power 1 over 2. Okay. This is called the H0 norm. Now, you note that the way we first talked about H1 functions. At that time, we also talked about another class of functions called L2 functions, which were simply squared in the. Right? Okay? And, and in that case, we just simply multiplied the function, we squared the function, and integrated over the domain. Right? And, we said that if that, this quantity in fact is integral that we've written on the right-hand side, we said that if it were bounded, the function is an L2 function. Okay so this is entirely equivalent to also the L2 norm. Okay. So in fact what we have here on the, at the bottom of the slide is more commonly referred to as the L2 norm rather than the H1 norm. Okay, that is more common terminology. But, but you see that the L2 norm is basically the H0 norm. Okay, so this is all background. What do we care? Where does any of this help us? Here is a result I'm going to put down that we will use today. Okay. The result is the following. We can also sort of leading up from this idea define what we will call the energy norm. Okay. You can also define the energy norm of v, okay, which is the following. We will denote the, the energy norm as I'll tell you what it is first. It is integral over omega, okay, of v comma x, E v comma x, dx. Okay? All of this to the power one-half. Okay? This is the energy norm of v. Why is it called the energy norm? This is a throwback to the time when structural mechanics was considered problem that finite element methods were applied to. In that setting, if v were your, your, if v were your displacement field, then the quantity I've written out here will be related to the strain energy, okay. So this comes from the notion of the strain energy of v. Okay, you would tend to think of v as a modulus, and therefore, you would conclude that this is a strain energy. Okay? So this is the energy norm. Okay? All right. We have a result which is a result on equivalence, equivalence of norms. Okay? And that result is the following. Okay? It says that if we take v, we form it H1 norm, and multiplied by some constant, say c1. Okay? We're seeing that this H1 norm of v, bounds from below, the energy norm of v. Okay? Okay? It can be a lower bound of the energy norm of v, which itself can be an upper bound of the H1 norm, sorry, which, which itself can, can be another load bound also of, of the H1 norm. Okay? Alternatively, what the statement says is that essentially what this thing is saying is that we have the H1 norm of v and all we have in these two terms that I put arrows on. The only difference between them is the fact that they have different constants. Okay? What we're saying is that just by multiplying it by different constants, it is possible to bound the energy norm of v from below and above. Okay? All right. So, the only way this is possible is if, fundamentally the H1 norm and the energy norm behave in the same manner. When one goes up, the other goes up too. When one goes down, the other goes down too. Okay, if that is the case, then we can always find these constants c1 and c2, such that you can bound the energy norm from below, and below like here, or from above like there, using the H1 norm. Okay. So essentially this, what this thing is saying is that one can bound the energy norm from below and above by the H1 norm. Okay, that really the only difference is a constant of multiplication, okay? So it's in this sense that we, that we say that the energy norm and, and this H1 norm are, are fundamentally, you know, they're, they're fundamentally equivalent. Okay. To end the segment, I just want to pre, present a little more notation, okay. It is the following. We will define what we will call inner product notation. Okay? So. We will define w comma f as written like that to be the integral over omega of w times f dx. Okay? Right. We will define as well the following. We will see that, so this is set to be the inner product of w and f. Okay? In particular, this is what is called the L2 inner product. Okay, and you just take two functions, multiply them together, and integrate them over the domain, over which both functions are defined. We get the L2 in our product. Okay? We also have what we will call a bilinear form notation. Okay? And for our focuses, we will define the bilinear form of w and u. As this to be the integral over omega of w comma x, E u comma x dx, okay? All right. You observed where this is coming from. Where does the integral on the right come from? Why are we defining it in this form? What is motivating it? Right, what is motivating it, is that is the fact that this integral on the right is, in fact, the integral that shows up in our weak form, right, this is what leads to the stiffness matrix. Okay, and in fact the inner product that I defined up here is exactly the sort of inner product that shows up on the right-hand side, right, the forcing function. Okay? All right, so this is motivation for defining them. Why do you think this form is said to be bilinear? Okay? Right. The reason it's bilinear is that it is linear in w, in the function w. As well as in the function, u. Okay? So it is bilinear in w and u. Right? Or, or alternatively, there's linear in each of w and u. Okay. Therefore, we say this case is bilinear. Okay, so that is perhaps the bi here is superfluous but I use it just to re-emphasize what's going on. It's individually linear in w and u. 'Kay? Last question for this segment. Do you see a relation between what I've just written between this bilinear form and the energy norm, that we defined in the previous slide? Right. Note that a of u comma u, right? If we use the same function in both slots of the bilinear form, we essentially recover our energy norm. Okay. And what comes ahead of us, the analysis, we will tend to use this notation, a of u comma u, for our energy norm of u. Okay? Okay, we'll end this segment here.

\section*{ID: CkUdAi3ehbI}
so there was a question on the equivalence of the H1 and energy Norms uh essentially asking whether one can always establish this equivalence the short answer is yes and the more detailed answer is that yes one can and the reason one can do this is um draws from from from from two facts okay um let me go over those facts right now so um does the equivalence of the energy Norm sorry the H1 norm and the energy norm and now let me use our new notation for the energy Norm right let me use this right does this equivalence um always hold okay and the short answer is yes now to understand why I'm not going to give you the entire proof but uh here's the basis of the proof the basis for why this works first of all observe that in order to speak of either of these Norms they have to exist okay okay right um note that uh there exists that's the symbol for there exists right there exists the H1 Norm of v and uh the energy Norm right both of these exist uh if and only if iff is for if and only if which means it's a necessary and sufficient condition okay so if and only if uh this Norm the the H1 Norm is less than infinity and so is the energy Norm okay to even speak of them for these Norms to be worked with they have to exist and what the sorry that's than infinity what this means is and the reason we can say this is that um Infinity is not a real number right the Norms both have to be real numbers for us to work with them which means they have to be finite numbers okay those are the the real numbers only include finite numbers okay um first of all so this works and then what what we can also use is the fact that Omega okay the measure of Omega uh is also um less than infinity what this means is that that our domain Omega is finite okay what this means is that Omega is finite okay then it's a little technical but then one can actually use the fact that since Omega is finite uh one can always represent the effect of the function itself over the domain right the function squared and integrated over the domain right one can always represent the effect of of from that contribution uh by just scaling the energy Norm with uh by by a constant okay so what this means is that if you now look at the H1 Norm right and I'll write it just in this form so if you look at it as $b^2$ plus uh measure of Omega to the square / NSD uh V comma X the whole squar DX right and yes of course there is also the one over measure of Omega 1 / NSD here and this whole thing raised to the half power right this is the H1 Norm right now if you are comparing this on the side with the energy Norm which is um which is of this form right um the energy Norm is integral over Omega V comma x e v comma X DX uh to the half power right you're comparing these two what is missing on the right hand side is this term right using the fact that both the Norms have to be bounded for them to be defined and the fact that the domain Omega is finite right one can essentially demonstrate that the difference between these two Norms brought about from the fact that the H1 Norm does have the effect of V squ while the energy Norm does not have it that difference can essentially be compensated for through constants right through finite constants so this is why when we multiply this by C1 we can indeed show that it's lesser than or equal to the energy norm and with just using a different constant one can demonstrate that the energy Norm itself is lesser than or equal to to some other constant multiplying the uh the H1 Norm okay so this equivalence holds in general it it's based upon upon this fact and that one okay

\section*{ID: Pz7YMpWeXLc}
Okay, so we move on with our analysis of the more mathematical properties of the finite element method. What we did in the last segment was establish some facts about norms. In this segment, we are going to look at two properties of the finite element method, okay? Those properties are consistency and the best approximation property. All right. In order to do that, there is, just one more thing I want to do here. Okay? So, let's start by recalling. Recall, the finite dimension, the, let's just recall the weak form. Okay? Right? Recall the infinite dimensional weak form if you'd like, okay? Very quickly let me right this out. Integral over omega w, x and then now rather than write sigma, I'm going to write E u, x. We add an A here. Okay? This is equal to we said integral over omega w f A d x plus w(L)tA, okay? Now, note that, this, okay? Is essentially like our, this in fact our definition of a w,u. When we define a w,u, we didn't include the area in that definition which could very well include in the definition. That's a w,u. Likewise, this is w, f, all right? The way we defined it in the last segment. And, in this case, you know, I defined it earlier in the last segment for general mathematical functions. Here, we have an Adx. But then we can just say that A is sort of a part of dx and you know, sort of lump it into that into the definition. What we will do now is also define this quantity to be w, t, okay? All right? At L, okay? Note that this last quantity we've just defined W,t on L is also an inner product. Okay? All right. It is also it's linear in w and it is linear in t. Okay? All right. So essentially what we're saying is that using this notation we can rewrite the weak form in what is called abstract notation. Okay? So in abstract notation we have A w, u equals w, f plus w, t at L. Okay, this is just abstract notation for our weak form. Now. In exactly the same sort of manner note that we can rewrite we can also write our finite dimension in weak form. Okay? All right. The finite dimensional weak form let me go to the next slide just so that I have enough room for everything there. integral over omega w h, x E, u h, x A dx, okay? Again everything's sigma a just E u h, x. All right? Integral over omega, w h f A d x plus w h at L t A, using our notation, using our abstract notation, note that this is a w h, u h, equals w h, f, plus w, h, t at m, okay? Abstract notation for our finite dimensional weak form, okay? So we have these two, weak forms in abstract notation. Okay? Now, let's do the following. Okay? So let's look at the abstract weak form in the infinite dimensional case. Okay? So, consider a w, u equals w, f plus w, t at L. Okay? So this is the, abstract form of the infinite dimensional weak form, okay? Now, remember what we say here for, the waiting functions, okay? We say that this weak form holds for all w belongs to v. Okay? All right. But then, also recall that wh, okay? Which is our finite dimensional weighting function, all right, belongs to Vh, which is a subset of V. Right? Right? What this says is that when we say here that the infinite dimensional weak form must hold for all W belong to V, it also holds for finite dimensional weighting functions, right? because finite dimensional weighting functions also belong to the same space, okay? So, what this means, is that so, so this relation, let me call this relation A, 'kay? Right? That equation the infinite dimensional weak form, 'kay? So, A also holds for w h because w h itself, belongs to V, okay? What this implies is that we are able to write a w h, u equals w h, f plus w h, t at L, okay? I'm going to call this relation B, okay? All right, and this just comes from the fact, that our finite dimensional weighting function, also belongs to the same space V, as our infinite dimensional weighting function. All right? Okay. Now, let me go back, and label the proper finite dimensional weak form that we worked with, as C, okay? All right, and now, what I'm doing to say, is subtract. B from C, okay? We're going to go to the next slide, and actually do this, all right? So, just start at B and C, and let's subtract them from each other. Let's subtract, sorry, let's subtract B from C. So here is C first. A, w h, u h, going to leave, leave myself enough room here, okay? Equals w h, f plus w h, t at L. This is just equation C, which is nothing but our finite dimensional weak form. I'm going to subtract from this B, which we caught by observing that the finite dimensional weighting function, also satisfies the infinite dimensional weak form, okay? All right? So, what do you get here? Okay? What you see on the right-hand side is 0, 'kay? Because these are, these, these forcing terms exactly cancel out. Now, recall that the functional, that, that, that A is a bilinear form, right? Which means it is linear in each of its arguments. It's linear in the first argument, as well as the second argument, right? Likewise for this. What this lets us do, is just simply write out, what we have on the left-hand side as a w h, u h minus u equals 0, okay? All right. So, we've got as far as here, 'kay? Now, look carefully at that term, u h minus u. It gives us the difference between the finite element solution and u, which is the solution to the infinite dimensional weak form. The solution to the infinite dimensional weak form would be the exact solution, okay? So, u h minus u is a very special, very important measure of our numerical method. What would you call it? It is the error, e, 'kay? It's the error in the finite element solution, and why is this? Because u, which is the solution to the infinite dimensional weak form, has to be the exact solution, right? The infinite dimen, dimensional weak form is equivalent to the strong form. The solution to the infinite dimensional weak form has to be the solution of the strong form, which has to be the exact solution, all right? Right, u is also. The exact solution, okay? All right, so, what we've just demonstrated here is a consequence of the condition that we had on the previous slide. This condition, right, the condition that we've marked as B, okay? So, what we've seen here is a consequence of this condition, okay. So, let me state this here. This results. From B, okay? Which is called the consistency condition, okay? All right. Let's look at what we have here first, and then try to understand the consistency condition better, okay? What we have here is a statement saying, that a of w h, e equals 0, okay? Now, this bilinear form that we have, a, can be viewed as a mathematical generalization of the idea of taking a function, w h in this case, and projecting it along a different function, e, in this case, okay? So, effectively, what this is saying, is that suitably defined, the projection of the error on our space of functions w h, right, or space of functions V h, that projection is 0, okay? What this is saying, is that the, that the projection. Of The error on the space v h is zero. Right? Viewed differently, the saying that any error that exists in the finite element solution, actually lies outside the space that we're even considering for our finite dimensional solution. Okay? So, already the saying that in some way we are actually doing very well in having chosen a space v h in order to get our solutions, in order to look for solutions. We're doing very well, because any error that we get is not even in that space. So within that space we're doing as best as we can. Okay? All right, so it, it's, it's saying that in some sense it is saying the error is orthogonal. To v h. Okay? Right? One can define, formally and rigorously, a notion of orthogonality for functions, using our, bi-linear form. And what it is saying is that with that def, definition of orthogonality, our error really lies outside of our space v h, because it has no projection left in v h, and that's why we're seeing this result that this projection is effectively zero. So in a sense, it's saying that we're doing very well with, with having chosen the space v h. Okay? That's one interpretation. Now I said that it comes from this condition b. Right? So let me rewrite the condition b. Okay. The condition b states this, right? It says that a of w h comma u equals w h comma f plus w h comma t at l. Okay? This is what we have. Compare this with, compare this with the equation that we actually solve to get our finite element solution. So, and that equation which you recall is the finite dimensional weak form. In this abstract notation, the finite dimensional weak form is this. Okay? Right? This is the, this is the equation we're actually solving to get u h. If you look at condition b, we can regard condition b as being obtained, if we simply went to our finite dimension week form and replaced u h with u. Okay? If we simply went to our finite dimensional weak form and replaced u h with u, we would get condition b. Right? So what is this saying? It is saying that if somehow we were to come upon the exact solution, okay? U, that exact solution also would satisfy the equation we are trying to solve. Right? So, what this is saying is the exact solution u satisfies the finite dimensional weak form. Why is this important? The equation we are actually solving in our numerical is the finite dimensional weak form. What we just demonstrated is that if our method were such, or, or, you know, by chance, were were to well not by chance, if we would actually pick the exact solution, right? Maybe our space was such to include the exact solution. Then, if we were to take that exact solution and plug it into our solution, into our, into the equation we are working with, we would find, we would find satisfaction of the equation. Right? So, we would, our method admits the exact solution as one of the solutions it can find, that it will accept. This is not the case with certain other numerical methods, right? Most prominently, the Finite Difference method. With the Finite Difference method equation, the underlying equation itself is altered so that, if you take the finite difference equations and substitute the exact solution, you are not guaranteed satisfaction of the finite difference equations. So in that case, you are actually working with a set of equations which does not any longer admit even the exact solution as an acceptable solution. The finite element method allows the exact solution, a. Okay? And this is why this condition, b is referred to as the consistency condition. Okay. The exact solution is consistent with the finite dimensional equation we are solving. Okay, the, the exact solution is consistent with the finite element method. Okay. So this is the consistency of condition. Okay. And I should also extend this node to saying, well we've all ready said the exact solution u satisfies the finite dimension of weak form. Right? So this means two things. What it means is the finite element method can recover the exact solution. Exactly! Right? Right? And then we also to mention here, this is not the case for other numerical methods. Right? Of course, other numerical methods unless they inherit this particular property wouldn't do it. Most prominently, finite difference methods do not have this property, okay? So this is, this is important to recognize. Okay? And a great deal of the strength of the finite element method comes from this result. Okay? And this result, consistency, is what leads to the conclusion that we observed in the previous slide that the error in the solution that we obtained, the error in our finite element solution is, orthogonal. In, in, in a suitable mathematical, in a rigorous mathematical sense, it is orthogonal to the space in which we are looking for solutions, right, in, in the space v h. Okay? All right. Let's stop the segment for here.

\section*{ID: -ouSo-1TcIw}
Welcome back. In this segment we are going to look at a at, at another key property of the finite element method. And this is called the best approximation property. It draws in a very important way from the main result that we had in the segment preceding this one. And that result is the consistency of the finite element method. Okay? So, let's get started here. We're going to look today, look at in this segment, the best approximation property. Okay. In order to build up to this result, let's consider the following. All right? Let us denote, as we've been doing so far. Let u h, which we will recall, belongs to s h, be the finite element solution. Okay? All right. And let w h as we've been doing also. It belongs to b h, be a weighting function Okay? And note that I'm saying it is a weighting function, and the idea is that we are going to consider any waiting function living in v h. Okay? Now I'm going to introduce a code field that we have not previously considered. Okay? I'm going to denote this as capital U, h. It too belongs to s h, okay. And however it is not the finite element solution, okay. So what does this mean? This means that when we say that U h belongs to s h. We are recalling that s h finally consists of functions, right? Since capital U h is meant to denote a general member of s h which is typically different from the finite element solution, right? What that means is that u h belongs to h one on omega. And u h satisfies the Dirichlet condition. Right? The Dirichlet boundary condition. Okay. And let us suppose that we're working with the Dirichlet problem. And you know how this would work if we were doing the pure Dirichlet problem. We would have the other boundary condition also in there, okay? So this is u h. All right, if we have this, let us note that, okay. Note the following, okay? Note that any u h, okay, can be written as the finite element solution, okay, plus w h, okay? Why is this? Note that as I made a point about, w h is any waiting function, u h is a very specific member of s h. It is in fact the finite element solution. All right. Also note that u h belongs to h one and so does w h. Okay. Therefore, their sum also belongs to h one. We haven't quite proven it, but you can probably. See why that works out. It does indeed work out rigorously. Therefore, when we add u h and w h, we recover a, we recover functions which live in h one, which re, which remember, or which we recall from just out here, is also a property of u h. Okay? So take. So adding little u h and w h maintains that property of capital U h. What about satisfaction of the Dirichlet boundary condition, which capital U h has? Well, remember that since little u h is the finite element solution, it does satisfy the Dirichlet boundary condition. What about w h? Does w h not mess up that result? It doesn't, because w h is zero on the Dirichlet boundary. Okay. Okay, so note that when we write u h in this form capital U h in this form, it does indeed satisfy all the requirements we have for this new set of functions capital U h. Okay. So let me just state that. U h equals little u h. Plus w h, right, where little u h is a finite element solution and w h is any waiting function, okay? This does indeed belong to h one, right? And I would just write and satisfies. The Dirichlet boundary condition. Okay? In what we are going to do today, the main result of this segment, we will use this result in a in an important manner. Okay, so with that in place let me state the main result of this segment. It is a, it's a theorem really. Right? And than theorem is the following. If we consider the energy norm of the error. And here I'm writing the energy norm using abstract notation. In one of the segments preceding this one, we demonstrated that with abstract notation for the bilinear form as we had defined it. We do indeed get the, the energy norm of the error, if we write it in this form. All right, so. The theorem is that the energy norm of the error is bounded from above. By the energy norm of what you may also consider the error, except it is capital U h, this new field that we've introduced, minus u. Okay this is the result that we are going to prove. Why is it of interest? Note that e, the error is the finite element solution that's little minus u, okay? So what we're saying here in this theorem statement is that the energy norm of the error, 'kay, is such that it, that, it is such that the finite element method has picked a member of S, H, okay, to be the finite element solution, which gives us the lowest energy of any possible other member of S, H, 'kay? Right, so let me state the, sort of give you a plain English statement for the relevance of this theorem, 'kay? So what we are saying here is that the finite element solution minimizes, minimizes the energy known. Okay? Of U h minus u over all members. Belonging to Sh, 'kay? The space Sh consists of a number of functions, okay? One of them is the finite element solution. That finite element solution is that member of Sh that minimizes the energy norm of the difference between that function and the exact solution, 'kay? In this sense, with the energy norm as a measure, the finite element method picks the best possible approximation among all members of Sh, okay? And this is why we call this segment the best approximation property, and in fact this theorem is called the best of the, this is, this, this statement that we have here represents the best approximation property, 'kay? And this holds for all Belonging to Sh, okay? All right, we're going to set about proving it. To prove it, let us first consider the energy norm of a slightly different function. I want to consider the energy norm of e plus wh. All right? Now we know how to expand this out, right? We get a result, which works just like the expansion of a perfect square, 'kay? It is that a, this is equal to a, e, e plus 2 times a. Well let me write it out more explicitly this, this is a subsequent step, plus a(e,wh) plus a(wh,e) plus a(wh,wh). All right, and this comes about just from the expansion. If you were to go back and look at what this abstract notation actually represents for the energy norm, that using the fact that it is bilinear in its two arguments, right, with these being the two arguments. You will get this result, right? It's just like, just like the expansion of a perfect square. And then, one recognizes that because the bilinear form also is symmetric, right, we note what we get for the result in the bilinear form is symmetric. Right, we see that these two terms, right, are equal, okay? So that lets us then write it out as energy norm of e plus the bilinear form a, let me write it as (wh,e). 'Kay, recognizing that the second and third terms in the line above are equal. Plus a of (wh,wh), all right? Okay, stare at this now and try to recall whether we can say something special about this term. Right, indeed we can. In the previous segment we demonstrated that this term is equal to 0. And this term is equal to 0 is a result that we've termed consistency of the finite element method. Right, it is the result that it, it, it, it, it, it comes from consistency of the finite element method, and consistency of the finite element method itself, you recall, is the fact that the finite dimensional weak form admits the exact solution as a solution, right, it recognize the exact, recognizes the exact solution. Okay, so what this says then, is that what we started out with at the top of the slide is a of e,e plus a of wh, wh. Now, each of the two terms on the right-hand side is a, is an energy norm, right? The first is the energy norm of the error, the second is the energy norm of the waiting function. There is a property of norms, right, a fundamental property of norm. Right, almost the definition of a norm. It is that each of these has to be greater than or equal to 0, and you can go back and check from our definition of what the energy norm is, that indeed this holds, okay? All right but if this is the case, what this implies for us is that let me now just pull down the term from the left-hand side here, a of e plus wh,e plus wh equals this. Okay, what this implies for us is that a of e,e, I'm just writing the energy norm of the error, right, this term. Okay? What it implies for us is that. Well, let me ask you. How is this term related to a of e plus w h? E plus w h? What's the relation here? Right, it is lesser than or equal to it, okay? All right, simply because the sum of two nonzero terms in the right-hand side is equal to the term in the left-hand side so we get rid of one of those nonzero terms. Well, what remains must be less than or equal to what we had in the left-hand side and then just flipped around their positions. Okay, and now we're getting somewhere. In order to proceed, let me go to the next slide. I'm going to rewrite this term differently, okay, the term on the right-hand side. Note that. A, e plus w h comma e, plus w h is equal to a. Now, the error. I'm going to rewrite the error in terms of the finite element solution and the exact solution. The way we define the error was simply the finite element solution, little u h, minus the exact solution, u. And we have w h there, the same in the second slot, u h minus u, plus w h. Okay? But now I can recombine this, right? I note that when I get u and w,h, u h and w h, this is from the result we proved at the beginning of the segment, just capital U h, right? The function that we introduced which we identified as being any arbitrary function in S h. Okay? That minus u, comma, same thing on the left-hand, on, for the second slot. All right? Now, we put this together with what we had at the bottom of the previous slide, right, which is this result, in the bottom of the slide. So, combining. With the inequality. Above, right. The inequality that we came upon just a few minutes ago, we see that that inequality which is a of e comma e is lesser than or equal to this term. 'Kay? Well, that means it's also, it's, it's lesser than or equal to U h minus u comma U h minus u. This is what we set to prove out, set out to prove, okay? We're done. What we've done here is proved the best approximation property, right. And I'll restate the significance of this result. It says that the other way to write this is instead of the, you expand out the error right, which is u h minus u. Okay, so the energy norm of the error where the error is defined as a difference between the finite element solution and the exact solution is bounded from above, right, by the energy norm of any other function living in S h, right, any other function, all right, for all U h belonging to S h. But little u h, the finite element solution also belongs to S h. So what the finite element method does is that it picks for us the solution, as the solution, as the finite element solution, that member of S h, that minimizes the energy norm. All right, of the difference between the function and, and the exact solution. So it's in this sense that we have this best approximation property. Okay, so re-emphasize that, let me just state that this is what we mean by the best approximation. Property. Okay? And just remember in all of this, that what we have is little u h, is indeed the finite element solution, all right? So, the finite element method does a very efficient job once we pick our space S h, does a very efficient job of choosing that member of S h that minimizes the norm of this of this difference. 'Kay, and says that well, that is your finite element solution. Okay. All right, good. We'll end the segment here.

\section*{ID: THsKroJbkVc}
All right. So, we've just seen this best approximation property of the finite element method. And hopefully, it gives you a little more, little insight into how the method works in picking solutions, from the space that we have chosen for it.  In this segment, I am going to derive another result that is sometimes called the, the equivalent of the, of the Pythagoras' theorem. For this for this particular problem that we are working with. And it has a nice consequence that is useful to note when one is actually looking at numerical solutions, and comparing with exact solutions. Okay? So so right, so, so the title of this segment is The Pythagorean, The Pythagorean Theorem. Right? And it's really in quotes because it's, it's similar to that, but it's not the original Pythagoras' theorem. Okay. Right, here is theorem first of all. This theorem is really a corollary of the fact of consistency, but but, okay, so let me state this as a corollary. Okay? And the corollary is this. It is, that right. If we consider the energy norm, of the exact solution, okay? This is equal to the energy norm of u h comma u h plus the energy norm of the error, okay? For, for a specific type of problem, okay? This holds for a problem where S h equals V h. Now, if you're wondering what that means, it simply means that our Dirichlet boundary conditions, for this particular problem, are homogenous ones. Okay? So, any place that will specify there is a Dirichlet boundary condition, we are saying that, that Dirichlet boundary condition is 0, okay? All right. So what this means is Dirichlet boundary conditions are Homogeneous. Okay? All right. And you note that this actually holds for the Dirichlet-Neumann problem. In fact, something that we often remarked upon during our development of the finite element method, for that particular problem was that, well, yes, if, if the, if we have the left-end of the bar fixed, then S e h and W e h are, are, the same, right? We just  S h, and, and V h aren't the same, right? Or alternately, little u h and w h satisfy the same conditions, right? The same  conditions. Okay, it is for this case, that we, that, that it holds. So, proof. Okay. So, let's in order to start looking at this, let us consider here right. Let us consider the following. For this setting we have u h Minus u equals e. Right? That's our definition of the error. Right? This is the error. The error in the finite element solution. And from this, we can therefore, write that U equals u h minus e. Therefore, when we consider on the left hand side, the energy norm of the exact solution. We see. Of course, we see this. But now, just as we did in the previous segment, we can expand out the term on the right hand side. By just using the fact of bi-linearity. All right. And this gives us a u h comma u h Minus a U h comma e minus a e comma u h Minus, sorry plus a e comma e. All right? Okay. And then as before, we recognize that the second, and third terms are the same because of the fact of fact of what? Fact of symmetry, right? The symmetry of the bi-linear form, right? So, that let's us write it as a. U h comma u h Minus twice a. U h comma e. Plus a e comma e. Okay? But now, if you look at the condition in the statement of the corollary, you should be able to see something about the second term there. It should be able to say something about the second term. What can you say? This is equal to twice a of w h comma e because the spaces S h, and V h are the same. Right? So, u h is the same as w h, right? U h is a w h, it is indeed a weighting function. Okay? And this therefore, is equal to 0, by consistency. Okay? And there we have it, right? We, what we are left with is, is the result we set out to proof. Okay? This is a, the energy norm of the finite element solution plus the energy norm of the error. Okay? This holds of course, for the problems with the Dirichlet boundary conditions are homogeneous, right? So, it's a useful to think to note in some sense this looks just like the Pythagoras' Theorem. Recognizing that the energy norm involves a square. So, it's like saying that w, w, if we use the this bi-linear form to define a more generalized version of an inner product, right? It is, it is indeed an inner product, but we can use it to use to define a more generalized notion of a square, right, of two functions. Right? Then this generalized notion of the square of functions tells us that in as much as we admit, this to represent a square. We are saying something to the affect of u square equals u h square plus e square, okay? All right. So, that is why, we call it the Pythagorean theorem. What good is this? Okay so a corollary of back Okay? So, let's call this corollary Maybe, I should call this corollary 2, and I call that corollary 1. So, let me do that. Let me go back now, and call this one that's corollary 1, okay? Okay, corollary 2. It says that, I mean, corollary 2 is actually let me state it. The corollary 2 says the finite elements solution. Okay? Underestimates The energy norm Of the solution. Well, let me  the solution, let me see the energy norm of the problem. Proof is a single step from corollary 1. We see that a U comma u equals a U h comma u h plus a e comma e. Now, because of the nature of this bi-linear form and the fact, that it leads to an energy norm, when we use the same function, both slots. We know, that this is greater than, or equal to 0. And so, is this, right? This is a fact that we've remarked, upon the previous segment. All right. What this implies for us then, is that if we drop the last stone, what we get for the right-hand side is that a of u h comma u h, the energy norm of the finite elements solution. Is lesser than, or equal to, the energy norm of the exact solution. Okay? And this what we mea,n by saying that the finite element solution, u h, underestimates the energy norm, all right? Of the problem. The exact energy norm of the problem, is what we have on the right-hand side here. Okay? When we get the finite element solution, and we go back and compute the energy norm, which is what we have on the left-hand side. We will be computing an energy norm, which is less than that of the exact solution. Why does this matter? Well, it is, as all norms do the, this is a particular way in which to get some sense of another sense of control over the solution. What is the solution doling, how big is it? Well, you can compute it's energy norm, it turns out that the energy norm that we get from finite element solution is not as big as the energy norm of the exact solution. Why might, where else, when might it matter in a more physical sense? If you are solving a problem in linear elasticity, for instance. Where the ene, the energy norm as, as written out here actually, has physical meaning in it. And in fact, corresponds to, what corresponds to the strain energy, right? What it says is that, if you had the exact solution available to you, and you were to go ahead, and compute the strain energy of the exact solution. And compare the strain energy of your finite element solution, what you, what you ought to, to see is that the finite element solution actually, underestimates that strain energy, right? It gives you a smaller strain energy. Okay? So, that's a particular result of this, of this, of this method. Okay. We are actually going to end this segment here. It's a shorter segment, but we are, we are getting prepared for the main results of, of our, analysis of the finite element method.

\section*{ID: JPmvH5xkGas}
That was a question on my use of the of the term homogeneous when I described Dirichlet boundary conditions so let me explain that. So the question would be something like what are. Homogeneous. Dirichlet boundary conditions? Here the term, the views of the term homogeneous simply means 0. Okay, so, when I say that a problem has homogeneous Dirichlet boundary conditions, what we mean is. Homogeneous, sorry, Dirichlet. Boundary conditions implies that u h at 0 for instance. This is the setting in which I used it. Sorry, that's a little too close. U h at 0 = u 0. So that's the form in which we write our Dirichlet boundary condition in the Dirichlet-Neumann problem, for instance. This is equal to 0, right, this is what makes it homogeneous. That's what we mean by homogenous that's all. So once we have that, the point that I was making in the previous segment is that once that we have that condition. We also have always that since we have a Dirichlet boundary condition at 0, we always know that w h(0) = 0 okay? But now with this fact that both u h and w h vanish on the Dirichlet boundary, right? This, plus the fact that u h and w h both belong to, H1 on omega essentially implies that the spaces are identical, Sh = Vh okay? That was the condition under which we derived our last two correlates, okay? All right. I should probably state here that the use of the term homogenous comes from at least in my usage of it, comes from basic theory of differential equations. Where differential equations where the, with the forcing functions that is 0 are referred to as homogeneous differential equations. And in fact, the solution to those differential equations are called homogeneous solutions, right? Okay, we'll stop here.

\section*{ID: huxL37pFyRg}
Okay. So we're now shaping up to get to the main result related to finite element analysis, right? An, an analysis of the conversions of the method. To get there, to, to actually get to that result, there is one more class of results that we need. So the title of this segment is going to be Sobolev estimates. Sobolev of course, refers to the type of space that we are working with and that I'll tell you what we're trying to estimate here. In order to say what we are trying to estimate let us sort of reuse our symbol U, capital U of h that we introduced before. It continues to represent a general member of the space Sh, which means it satisfies the, the rationally boundary condition and it lives in H1. Okay? For, for our particular problem, it, it lives in H1. But in the context of Sobolev estimates, which are applicable to spaces with which, which are applicable in general to Hn spaces. Let me just redefine this for now. Okay. Let me say that Belongs to Sh, which now consists of all functions. That for our, for our purposes now, which actually hold in more general settings. Okay? Consist of Hn functions. And additionally, of course, they need to satisfy the Dirichlet boundary condition. Right? And so, if we were doing the, the Dirichlet-Neumann problem, we would have At 0 equals 0. Okay. So we're re-invoking this, this same function capital. Remember, U, capital Does not need to be the, the finite element solution. It could be, right? But it does not need to be, right? So let me state that. Does not necessarily represent little Right? Which is the finite element solution. It is any other solution in Sh, right? Where Sh now has been defined to include all functions that belong, that live in Hn. Okay? All right. I want now to consider a very special member a member of this set of functions. Okay? Consider Such that. At any nodal point let me say, this as X let me just say, x sub A. Okay? So At x sub A equals little at x sub A. Right? Okay. But if A here just denotes our global numbering of degrees of freedom, then this remember, using our finite element vector notation is just the degree of freedom, d sub A. Okay? All right. All right. So what this, what this says is that A is a global degree of freedom, right? And this, this then implies that xA is a globally numbered node and dA is a globally numbered Trial solution degree of freedom. Right. Or finite element solution degree of freedom. Okay? All of that should be clear. So what we're saying is that we want to consider a special member of this class, where Is equal to the finite element solution at the node, the degrees of freedom. Right? At the nodes, okay? But you know that if we do this, we would have capital Actually equals to little everywhere, right? Because of the fact of the finite element representation. Okay? Right? So this is not, not exactly the solution we want. Not exactly the the function we want. Consider another function, U tilde h which is such, that U tilde h. Okay? At xA, now equals the exact solution evaluated at that point. Okay? The idea is that if you had the exact solution and you could evaluated it at a nodal point, right? Take that value and let's suppose that u tilde h is such that it hits the exact solution right there on the nodes. So what we're seeing here is that u tilde h, right? Expressed now as a function of parametrize by little x, right? By position. U tilde h is what is said to be nodally exact, okay? I'm going to sketch it out for us, suppose this is our domain. And for simplicity, let's just suppose we have linear basis functions. Okay? And suppose our exact solution is the following. Let me go to a different color here. Our exact solution I'm going to plot in green, okay? Let's suppose this is our exact solution, okay? All right. This is our exact solution and what we're talking of doing here is following. We are talking of we're talking of looking at these nodal points. And identifying the value of the exact solution at the nodes. Okay? And then we're talking of so, so this, so this is u, the exact solution, okay? All right, and what we are, what we want to do now is identify the u tilde h. Okay? U tilde h, remember, is a member of S h, right, which means it inherits the representation that we get from our basis functions. If we are working with linears here, u tilde h can at, can at best be linear for each element. However ,we want it to be such that actually hits the exact solution at the nodes. Okay? For many of the other elements it's, it's not going to be very different from the exact solution because perhaps the exact solution is not too different a linear, sorry, it's not too different from a linear solution elsewhere. Okay. You see that there is a difference, right? See, this is our u tilde h. And this is what we mean by saying that u tilde h is nodally exact, right, it's exactly the nodes. It hits the exact solution in the nodes, but it is a member of S h, so over an element in this particular case it is as, at, at, it is, it is, it is linear. Okay? And let me go back to my red here, okay. So this, this is just plotting up a solution up here. All right? Okay, so this is what we mean by saying u tilde h is nodally exact. Such a function, u tilde h, is what is often called an interpolate of the exact solution. Because what we are doing is taking the exact solution at the nodes, and then using the basis functions to essentially interpolate between those exact nodal values to get u tilde h, okay? U tilde h is nodally exact and is also called the interpolate. Okay? All right. Okay, and just remember that u tilde h belongs to S h, right, and that's re, reflected in the fact that it is linear over elements. In this case we are considering linears, okay? So we're considering, in this particular example only linear basis functions. All right, okay. So this is what we have. Now, here is a result. The theory of Sobolev spaces, right, or, or mathematics of Sobolev spaces, functional analysis on Sobolev spaces, gives us the following so called interpolation Error estimate. In Sobolev spaces. Okay, the interpolation error estimate is for the following quantity. It holds for the m norm, the h m norm of the difference between this interpolate and the exact solution. Okay? It's important to note why this is called the interpolation error, right? So the term on the left is my interpolation error. Interpolation error is this, okay? U tilde h minus u. The reason it's called interpolation error should be pretty obvious, right? And what is that? Think about it. What it is saying is that, suppose you had the exact solution, right, and at the nodes you were to hit the exact solution, which is what U tilde h does, right? But then because of your choice of a finite dimensional basis over the element, you are now interpolating from that exact solution. But since you are interpolating with a finite dimensional basis, you fail to hit the exact solution over an element, right, most prominently here. Therefore, this difference that we are looking at is indeed the interpolation error, okay? All right. So the Sobolev, estimate here, and, and I'm, I'm just going to state it, we're not going to prove it, is the following. It is that this interpolation error estimate is bounded from above by c as a constant. h e is our element size. Okay, and in these error estimates the we assume, or the error estimates have been derived for the case in which we have a uniform element size, okay? So the c h e to the power of a number alpha, which I will define in a little bit, okay, c h e to the power alpha times the h r norm of u, okay? So what do we have here? U tilde h minus u is the interpolation error. It's the error incurred by the fact that we're using finite dimensional basis functions even if we had the exact solution at the nodes, okay. H e, as before, is the element size. c is a constant. r is what is called the regularity of the exact solution. Okay? So note that when we're talking about u r the r norm of u, we're seeing that yes, if we were to square integrate the exact solution and also square integrate its derivatives up to r, right, up to its rth derivative, we would get a quantity which is bounded, right, and that is the r norm. Okay? All right, so let's just recall that this quantity, the r notm of our u Is a measure of smoothness, of regularity, really. r comes from regularity. Right, which is really a measure of smoothness Of u, all right? If you have the r norm of u, it means that you can take up to r derivatives of u and still have that quantity bounded, which gives you some sense of how small the u is, okay? All right, the next thing I need to tell you is about what alpha is. Right, alpha is the exponent in to which h e is raised, right, the power of h e. Alpha satisfies it, it's an exponent, obviously. And it satisfies. The following condition. Alpha is equal to the minimum of right, k plus 1 minus m, and r minus m. Well, what is k now? k, k is the order of the polynomial order of our finite dimensional basis. k is the polynomial order of the finite element, sorry, finite dimensional basis. Okay? All right. Let me do just well, let, let, let me rewrite the result here, okay. So that is h minus u interpolation error. We are computing the m norm of it, right? And this is lesser than or equal to c h e to the power alpha. Okay, so what this is telling us is that supposing in most cases, let us suppose that we have a solution just to fix ideas. Let's suppose that our exact solution is such that we can actually take derivatives of it up to a very high order and you know, we're, we're able to take derivatives of it to a very high order, which means it's very smooth, okay? What it means is that in this definition of alpha, r is going to be very big, okay? So even though we want to take, so, so, we, we are going to take m derivatives of the solution and we are interested in knowing what the m norm of it is. If r is very big, what it says is that the order of our of our exponent here, the size of our exponent is controlled by the polynomial order of the basis functions that we have. Okay? Now why does this matter? The question is, what happens with our interpolation error as we refine the mesh? Okay, if r is big enough, then alpha essentially reduces to k plus 1 minus m, right? So if r is large, okay, which means the exact solution is very smooth. Right, we have a nice, well-behaved problem. Okay? In this setting what we will see is that alpha being the minimum of those two quantities, is indeed going to come down to k plus 1 minus m, okay? And therefore, what this thing is saying, then, is that our result is that the, the interpolation error. The m norm of the interpolation error is lesser than or equal to c h e to the power k plus 1 minus m times this r norm of u. But then if u is very smooth, we expect that the r norm is not very big, right? We won't get a very large number when we integrate the functions, to integrate the function and its r derivatives up to r, okay? But that tells us then that now as as h e tends to 0, right, if our quantity k plus 1 minus m is greater than 0, what happens with the interpolation error? What happens with the norm of the interpolation error? It vanishes, right? Okay? It vanishes at the rate k plus 1 minus m, right? So it tells us that any nor, that, that this h, sorry, this m norm that we want to compute of the interpolation error also tends to 0 at the rate k plus 1 minus m. Okay? So as we refine the mesh, as we make h e smaller, right, remember h e going to 0 means we are looking at mesh refinement. We are going to smaller and smaller elements. Okay? So what the Sobolev estimate tells us is that yeah, as you refine the mesh eh, if you were to look at this interpolation error it will vanish, provided, provided what? Provided that number is greater than 0. How can we make that number greater than 0? We can make the number greater than 0 by taking a higher polynomial order, right, or making sure that we are not taking too many derivatives in computing the norm. So we don't, if we are not looking for very high m norms, right, we're not looking for taking many derivatives when we compute the m norm, right, then this holds. Right, and this is an important thing to know. This, this property, note we should note that this property comes to us directly from the Sobolev space. Right, we're not saying anything yet about the finite element solution. This result is a property of our Sobolev space S h. Okay? How is it a property of our Sobolev space S h? Well, that's what determines the polynomial order k. Right? That is determined by, by the space we have picked for S h, right? So it says as long as you pick that to be high enough, and as long as you're not looking for too many derivatives in this interpolation error it does converge. Okay? No talk yet about the finite element solution. Okay? That will come next. And that will come in the next segment.

\section*{ID: wBZ1sUdG7fM}
Okay, we are here now and at the cusp of what I hope will be our final result of this analysis of the finite element method. So this is, best titled, Convergence of the Finite Element Solution. Okay. Before I state the result and prove it there's one result that I need to recall from the, from a few segments ago and generalize it. Okay? So recall that when we actually started talking about the analysis of the method. We used this or I stated the fact that the H 1 norm is equivalent to the energy norm, okay? So recall equivalence of H1 and energy normals, Right? And the way we stated that result was the following. We said that If we look at c1 times the one norm of v, right? Some function v. This is lesser than or equal to the energy norm of v. Well, this is actually the energy norm squared, so we do that. Okay? And it's also less, it can be bounded from above by the energy norm. And the energy norm itself can be bounded from above by some other constant multiplying the v 1 norm. All right, and I believe there was also a question about what, what exactly we meant by this equivalence and how it held why it might hold. And I said that well, it really, it really rests crucially upon the fact that when we speak of these norms, we are talking of quantities that are, real and therefore bounded, and not infinite. And then using the fact that our domains are, finite, we can actually prove this result. Okay? So so we have this. Now it, it, it emerges that this equivalence between the h norm and the energy norm extends in fact to the general Hn norm also, okay? So this extends to the H n norm. Right? So this says that if we just look at c1 times the n norm of v, it is bounded from above by the energy norm. And that itself can be bounded from above by some other constant multiplying the n norm of v, okay? And the re, and the reasons why this holds are pretty much the same with the H1 normal. Why, where the result holds for the H1 norm. Okay? So this is what we're going to use. All right, so here is our statement. Again, I'll state it as, as, a theorem. The theorem is 1 on the, um,n norm of the finite element error. Okay? And it states that it can be bounded from above by some constant, whatever constant, some constant. Right? So let's denote this constant as c bar. h e, again, raised to the power of alpha, where alpha is the same exponent that we came up on in our sublib interpolation error estimate, okay? This times, the r norm of u. Okay? So, this looks very similar to what we saw for the sublib error estimate except that now we are indeed talking about the finite element error estimate. Okay? We are indeed talk of what happens with the finite element error, right? And we just remember that this is u h minus u. Okay? All right, proof. And let me just start, all right, I can start the proof here. And to start the proof, let us actually start with this s, result I just stated at the beginning of the segment. The result just above theorem. Right? And let us start by writing c1 times energy norm, sorry, n norm of, finite element error. Using the result we just stated above, it is clear that that can be written as being bounded by the energy norm of the error. Okay? All right now, let's move on. Okay. I'm actually going to reproduce this, this last statement on the now slide here. c 1 n norm of the finite element error is lesser than or equal to energy norm of the error. Okay, now the energy norm of the error itself you recall from a couple of segments ago, is bounded by a quantity. Which is also the energy norm of u h minus u. Right? We call this the best approximation property. Right? This is the best approximation property. Okay? Now I want to state something about notation here when one is working with inequalities. When one is working with inequalities and I write inequalities running in this fashion. The way to read it is that, what we have here, the energy norm of e is greater than or equal to C1 times the N norm of E. When we come down the second line what we're seeing is that what we wrote on the first line is itself less than what we wrote upon, on the second line. So it's very similar to actually taking what we have on the second line here and, moving it back here, okay? That's how we understand these, these, that's, that's the notation that's used when no, one is writing on multiple lines, okay? So what, really what this is stating is in fact, the best approximation property, right? Because there we did indeed prove that the energy norm of the error is lesser than or equal to the energy norm of this particular quantity, okay? But now note that when we choose those functions Okay, included in those functions was this interpolate U tilde because After all was just functions living in Sh, right. And satisfying that there is a boundary condition. Since U tilde h also lives in s h and is in fact equal to the exact solution and the nodes it also does satisfy the conditions, right. So it is in, in fact one of these members, right. So, the, the reason that we get this result is the best approximation property and the fact that U tilda h also belongs to Sh, okay? All right, but now, one can again invoke the result as stated at the, before we started this theory, right, on the equivalence of thee energy norm and the end norm. So we can see that that energy norm that we have on the right itself is lesser than or equal to, let me see this. Right, I could directly write this as some other constant C2, right? Times the N norm of U tilde h minus U. Okay? Just from the fact of our statement on equivalence of norms, okay? All right, so, we have this, and, what we can do now is invoke, so, this, this, this result follows from equivalence of Hn and energy norms. Okay, now continuing to work. We recall our sublet interpolation error estimate. Okay, so That interpolation error estimate is one that is applied to U to the H minus U, right? because that's interpolation error, okay? And that was the U to the H minus U is, lesser than or equal to, some constant CHE to the power alpha times the R norm of our exact solution. Okay so this is the interrelation error Estimate. Okay? All right, but now look at where we've got. We started out with C1 times the N norm of our finite element error and through this process, right, of repeatedly invoke, of invoking various results that we've accumulated. We have arrived at the fact that it is bounded from above by the term on the right hand side. Let me now just collect these results to say that, in the N norm of the finite element error is lesser than or equal to C2, which is a constant, times C, which is a constant, and divided by C1 which is yet another constant, times He to the power alpha Okay, now, this is our new constant C bar. Okay, so, our final result is that the, the finite element error in the N norm is lesser than or equal to c bar, it's a constant He to the power alpha times the r norm of u. Okay, that's our result. Now, let's let's just spend a couple of minutes examining this result, okay? What it tells us is that again, you know if we look at cases where the solution the exact solution is very smooth for U sufficiently smooth.  What you're seeing is that alpha that is defined as the minimum of K plus one minus N and O. Okay, in this case, we're working with the n norm, right? So we have k plus one minus n. I'm sorry. Okay? And, r minus n. Okay? Alpha is a minimum of those two quantities, right? So for u being sufficiently smooth what we see is that alpha is indeed equal to K plus one minus n. right? Where k is the, polynomial order of our baseless functions. Right? And n Is really the number of derivatives we are taking in calculating the norm of the error. Okay? Alright, to get a little more insight, lets consider. Let's consider what happens with let's consider special cases, right. Consider n equals 1. So we're considering the, the 1 norm, right, the h 1 norm, and Well let's just consider n equals one. Okay? In this case, our result becomes that the h 1 norm of the error, what is, what do we mean by the h 1 norm of the error? What we're seeing is we're looking at the error in the finite element solution. Square integrating it and also square integrating the derivative of the error. So we're trying to gain control of not only over the, over the error but also it's derivative. The demanding that the error is somewhat smooth, because we want to say that even its derivatives should not get very big. Okay? So we're not saying that the error shouldn't be too much, but it also should be smooth. All right? Okay. So this thing now is lesser than or equal to our constant c, r, he, okay. To the power k plus 1 minus 1 times the r norm of U. Okay. So, it is saying that if we are looking at the H1 norm of the error, basically we are trying to the error itself and its derivative. Right? It converges at the rate of k, right? Our polynomial order, right? So the linear baseless functions, right? For k equals 1, okay? We get the error in the, in the one norm is essentially proportional. Right? To, well, I may not say proportional. The error in the, the the one normal error is bounded by C bar he. So, indeed as we refine our mesh, he goes to zero, right? Because he tends to zero. The one norm of the error will also tend to zero. Okay? But note now, that if we were to take, take, if we were to go to higher polynomials. Right? The same one norm of the error would now converge at a higher rate. Okay? All right. Okay. Now so this is the main sort of result that we want to establish. And of course, as we go to higher and higher order polynomials, our, our error converges more rapidly. Now, if we want to compute the error in the L2 norm, right? Which is equivalent to the H0 norm. There is an additional technique that needs to be invoked. It's called the Aubin-Nitsche. Aubin-Nitsche method, which we wont do, because that takes a lot more work. Okay? But essentially in this setting, the result is the following. If we consider the L2 error, right? And we'll make it explicit that we're talking about the L2 error. Now we're just talking about square integrating the error, right? No derivatives. Then it, the form is essentially the same. Okay? In this case, we get h to the power of k plus 1. No derivatives, right? So we don't lose any powers here. We don't have k plus 1 minus n. Okay? Times the r norm of the exact solution. Note that of course, we are assuming that the exact solution's sufficiently smooth, so that exponent alpha always reduces to k plus 1. Right? So this is the condition for the L2 error. So what this means now is that if k equals 1, the L2 error of our finite element solution converges quadratically. All right? It converges at he to the power of two. Right? And so on, right? So, if you had k equals 2, we would converge at the rate he to the power 3. And so on. Okay? So, essentially this summarizes the the key result that is used in conventional finite element error analysis. It says that as we refine the mesh, our solution converges. Right? And depending upon the measure of error we want to use either just the L2 norm of the error, which would be just square integrating the error and making sure that that quantity is decreasing. Okay? Or whether we wanted to include higher order derivatives of the error. We can, we can, we can address all those problems. Okay. We can address all those questions. In all of this at least in the, in, in the way I try to, to, to bring out to, to give you greater insight into what is happening we're assuming that the exact solution itself is smooth enough. And note that this is a requirement. If you start out with a, with trying to solve a problem whose exact solution itself is very badly is, it's very irregular, you can't hope to converge to, to solutions easily. Right? It just becomes much more difficult. There are methods that, that, that, that can be used to address such problems and also to carry out the error analysis of these, of these methods. The other thing I should note is that the interest in, in compute and always looking at errors, which involve an integral over the domain is precisely to make sure that we are actually controlling the error all over the domain. Okay? And this is why we always take into the square integral of the, sorry, the, the, the error itself squared and integrated or the or we compute it a further derivative on it and square and integrate that. Right? The whole reason for integrating it is to make sure we have control over the error over, over the entire thing. Okay. This is a good place to end the segment.

\section*{ID: oKn3AFD2wx0}
Welcome back. At this point we've completed our derivation of the finite element method for one d linear elliptic p d e's. And we've also spent a little time in understanding the basic mathematical properties of the finite element method. And especially with application to this problem. As well as how that pans out into studies of the consistency and conversions of the method and therefore to error analysis really, right. We looked at how the, the error of the finite element solution in the, the error in the finite element solution converges with element size tending to zero.  What I'd like to do in this segment and maybe the next one or two, is look at an alternate way in which the weak form can be obtained. And this approach is a a subset of a type of calculus that may be sometimes called variational calculus or more broadly. Various known methods. Okay? So the topic of this segment is going to be we're, we're working towards variation of methods so uh,so let me start out with talking about. Variational methods, okay? Before we can get there, however, we, of course, have to motivate it, right? And in order to motivate let us consider the following, all right. Consider the following integral. Okay. Consider, let me call this, i func, i depending on u. Okay? Written in this form. And I'm going to write it as, I'm going to define it here. It is the integral, over, the domain of the following quantity. One half E A, u comma x. The whole squared. D x. All right? Now, when you stare at this integral, if you, remind yourself, of, one dimensional linearized elasticity, you will recognize E to be the modulus. Right? And you will recognize u comma x. Right? That quantity to be the strain. Okay? And then, when you stare at this integral, you would recognize it to be the, the strain energy, right? The strain energy in. Linearized elasticity. The strain energy in linearized elasticity of course in one d. Right. So, so this is all well and good. Now what one can do is, is the following. So we, we, we, we make this recognition first and then. The, you know, once, once we recognize that there is this notion of a strain energy, we can ask ourselves, well, what does it do for elasticity problems, right? Of course it gives us a notion of, of the, of the energy stored in the elasticity problem. But it also allows us to construct a different kind of quantity, okay? It allows us. To construct. The following integral. Okay. Now, now I'm going to start putting down the kind of notation that we need for the development ahead of us. All right? It allows us to construct the following in a, integral. Or to construct the following integral which I'm going to denote as pi. It also depends upon you. Right? And in the context of elasticity, this is the, well, this is just a displacement field. Right? U is the displacement field. And pi of u. I'm now going to define by a few terms, by using a few terms, the first is going to be the same, term, that we wrote above, the integral that we wrote above, the, strain energy. Okay? Now, recalling the other data of the strong form of the problem, all right? I am going to add on a few more integrals, I'm going to add on integral over omega f, which you recall is our distributed forcing. Right? Times u, multiplied by A for the area, d x minus I am also going to consider the traction that we apply in the context of a Dirichlet Neumann problem. Okay? So t A would be the force, right? The, that, that makes up the traction. That, multiplied by u at the position l. Okay? So I want us to consider this. And consider this integral where we're seeing as we did for the strong form of the problem that u belongs to the space s, right? Our usual space of functions for the for the exact solution, right? The, the space's function, functions from, from which we draw the exact solution, right? So u belongs to s, which now consists of, all functions, u, such that, we're thinking, we're thinking of this in the context of our Dirichlet problem. Right? So we're thinking of this in the context of the Dirichlet condition. U at zero equals u nought. Okay. All right. And f t and the constitutive relation that we're familiar with. Sigma equals E u comma x. Right, all of these are given. All right. Consider the following, consider the setting. Now what I want you to do is con is focus your attention upon this integral that I've defined that I've written as pi. And ask yourself what it is, right? What does it tell us physically at least for this problem if we're thinking of an elasticity problem. Right? Take a few seconds to look at it. Note that the first term was the strain energy. This term, I'll point out to you, the gives you the total work done by the force f on the displacement. And this recognizing that t A is also force. Is the work done by the traction specified for the Neumann boundary condition under displacement at that end. All right? So, in the context of elasticity, if you've studied that problem, this is something that you probably recognize as a potential energy. All right? More broadly, pi is pi of u. Is something we actually can call the Gibbs free energy. Okay? When we restrict our attention to mechanics only. Right? The Gibbs free ener, energy for purely mechanical systems. Or, purely mechanical problems. All right. Also called, like I said, earlier, the potential energy in the context of mechanics. It turns out, as many of you may know, that the Gibbs free energy is actually applicable to general processes and physics. It's typical to have a contribution from chemistry. Maybe from, from, from temperature and so on. Also in the Gibbs free energy. However if we restrict ourselves to only mechanical problems as we're are trying, as we're doing in this case, then pi of u would be recognized to be the Gibbs free energy restricted to that. Okay. All right, so that, that, that's something to note. Now I want you to also observe that as a matter of notation. I've written u in rectangular brackets there, right? What I'm trying to point out here is that there is the, the, the reason I'm writing u in in rectangular brackets is that I want you to think of this quantity pi. As not just a function. Okay? So I'm going to state that here. Pi of u is not a function. Okay? The reason it's not a function is because a function, properly defined, mathematically defined, takes on a, point value of it's argument. And returns to us another point value. Okay. Or more typically, a function is a mapping from, typically, real numbers into real numbers. Okay. Let me just, just, just state this. So, supposing we do indeed have a function. G of x, okay? This may typically be a mapping from the space of real numbers, right? From which you may take x. On to real numbers, right. And graphically this can be represented as follows. So on this axis I have x, I have g. And in order to get a particular value of g, you would go on to the horizontal axis, choose a particular value of x, and say, all right, what is the value of g? Maybe it is that one. You take another value of x, you pick your value of g, that's that. All right, maybe this is this, and. The process goes on. Right? Now the function that we have in mind is what we get by connecting all of these dots. Right? And maybe it does something else out here. Okay? Nevertheless, the important idea is that when we take a particular value of x. Right? We get back a, well, actually, what I've drawn is yeah, it's, it's okay. We get back a particular value of g. All right? Okay? So, in this sense, a function takes a point value of its argument and returns a point value. Or it takes, in this case, a real number and, and returns a real number. Pi, however, is a little different. Okay? And I want you to think for a few seconds, or maybe a little longer, in what manner pi, as defined in the previous slide, is different. Is it, in what manner is pi different from this function g that I've specified here. Right? So, how is pi of u different? You'd know the answer immediately if you've gone through this sort of exercise and you may still know the answer. You may have figured it out. But the response is that in order to evaluate pi, we need more than just a point value of u. Because pi is an integral, we actually need the entire field u in order to evaluate pi. And then, in fact, when we supply the entire field u defined on our domain omega, we get back a real number for pi. Okay? So, let me state that here. Pi of u is a mapping of a field, right? The field u, which is a, u is properly a function of x, right? Because you, you pick a particular val, value on your domain x, and you get a particular value for the displacement, right? You get back a point value for the displacement, right? You get back a real number for the displacement, right? So that's the nature u, u is a function. However pi is a mapping of that entire field u, okay? To, the real numbers, All right? And, why to real numbers? Because we have recognized pi to be an energy of sorts. It's a free energy or potential energy. Which after all is a scalar. Right? It's a number. Right? It gives us a real number. Mathematically stated, we have the following. Pi of u is a mapping from something that tells us where we draw u from. Where do we draw u from. Right? We draw it from our function space s. Okay? So prize and mapping s, to the real numbers. Okay?

\section*{ID: AhazUtXxjes}
All right. So, what this means is that In order to use this notion of pi, we need to think of things a little differently. Okay? We need to think of We can think of a function, we can start out by thinking of a function, and indeed we do need to think of a function, because a function is involved in here. Right? We do have u, which is drawn from a certain function space, right? So, this is x, right? And our domain is 0 to L, right? This is our domain omega. Right? This is our domain omega. On this domain, we do indeed have a displacement field, right? It's a function. Right? And recognizing the sort of boundary condition we have at x equals 0. Let us set u equal to 0 there, and then, we may have Something like that. Right? Okay. So, that could be our displacement field, right? In one dimension. Now, when we have a displacement field of this type, okay? We get a particular value for pi. Okay? So from here, you may think of now constructing this mapping to a pi of u, right? Note, however, that in order to calculate pi of u with this value, with, with this particular displacement field. You need to know the displacement field over the entire domain, because we need to integrate, right? Now, if instead of this displacement field we were to have a different field, which I will use green, all right? So, you may have some other field, right? Also, satisfying the same boundary condition. But, perhaps, our field is this one, Right? And, from here we would get a different pi. Or field, maybe I should call this u2, and the original field u1. Okay? That is u2 maybe, and this original field is perhaps u1. Okay? So, it's critical to understand, that in order to calculate each value of pi, right? A single scalar value of pi, you need the entire field. All right? So, in this sense, this is fundamentally different, from a function which you can evaluate with just a point value of the argument, okay? All right. So, what we see is that pi is in this way, it is fundamentally different, from a function. It is what gets called a functional. Okay? And this because anything that takes as its argument an entire field, is a functional. And clearly, it, a, an, an integral is, is a functional. Right? There's another object, a very familiar object, that is also properly defined a functional. Can you think of what that might be? It is closely related to integral, actually. A derivative, right? Strictly speaking, a derivative is also functional, because we cannot evaluate a derivative, with just a point value. Right? Properly defined, a derivative needs a field description, all right, you may have a formula for a derivative, right? But in order to obtain that formula, right? In or, in order to even define a derivative, you actually need a field, okay? The pi so, all right, so pi is a functional, all right? And this is the function that we're going to work with. Okay, so, so this is the setting, and let's, let's proceed from here. So, so pi, pi is functional, in particular it is a, it can be called, and this is the term I will use for it. It is in fact, our free energy functional. Right? The Gibbs free energy functional. Okay? All right. Okay. So, what can we do with a free energy? Right? Or, or an energy, an ener, an energy in general, but particularly, a free energy? There is a certain commonly used technique, or principle, if you want to call it that that. That is often invoked, when we have a free energy. Can you think of what that is? Yes, it's the ideal of minimizing, or extremizing free energies. Right? So, extrema of free energies. 'Kay? Characterize, and why do we bother with extremizing free energies? It's because extrema of free energies are characterize states of equilibrium of systems. Okay? And therefore, there is this interesting calculating extrema of free energies, okay? And I'm going to draw the very common picture, which I am sure you've all seen, and either in high school, or early on in your college careers. It's the idea that well, if you have a If you have a free energy. Right? With respect to some with respect to some variable and, and just not to confuse things with anything else. I'm going to denote the variable as y, let's suppose our free energy here is f. Right? Where our energy is f we know these kinds of notions, right? That at extrema, right? There are our system is at equilibrium. Right? So, we may have that. We may have a different extremum there. And then, we may have that type of extremum, as well. Okay, so at each of them, we know that, a, f prime is equal to 0. All right, as well as here. All right? And we know that when f prime is equal to 0, we are at an extremum. Right? Each of these are extrema they are also  these points of equilibrium. All right? So if so, these are, so f prime equals 0, are  Okay? And then, of course, we can go on to the notion of second order extremum, conditions. And make statements about which of these extrema are which of these extrema have equilibria that are stable, or unstable, or metastable, or, or neutrally sta, stable and so on, right? We, we, we can go on, and have that discussion, right? So, so, let me just put that down, right? So, typically, if if a double prime let me see. So, if f double prime is greater than 0, we have a stable equilibrium. Right? If double prime is less than 0, we have unstable equilibrium. And if it's equal to 0, we have a metastable, or probably more widely called a neutrally stable equilibrium. Right, and they are characterized by these three.   This is all standard, fairly straight forward stuff. So well, this is all standard straightforward stuff but the question obviously rises can we do the same thing for our free energy functional pi? Right? So, can we find extrema Of, I'm sorry I have a slightly recalcitrant pen, today. Can we find extrema of pi? Right, and clearly once we, if we can find extrema of pi, we can recall these statements about the system being at equilibrium, stable, unstable, and so on. All right? Ask us can we find extrema of pi? The answer is yes, yeah, we can of course. But is it straightforward? Do you know, directly how to do it? Well, sort of, yes, if you think about it. We know if, that if, if, if a function is sufficiently smooth, we can define it's derivative. And it's derivative is what gives us notions of extrema, right? Where, where the derivative vanishes is what gives us an extremum, and that's what we saw in the previous slide. The same sort of thing would apply to pi as well, if, if pi could be sufficiently smooth as a function out of u we can, we can indeed find extrema. Right? Assume that it is smooth, right? The question then remains, what do we mean by a derivative, right? And, and the challenge here is the following. What is, a, an, oh, wait let me see, an appropriate notion. An appropriate Notion of, I'm going to write it out in words, right? Of a derivative Of pi with respect to w r t with respect to u. Okay? Question mark. What is the appropriate notion for this? Why is this not obvious? Why is this not ob, Why is this not something that we can directly inherit, from our conventional notion of derivatives of functions? Well, because we really need to say what it means to differentiate a functional, which is a set of real numbers, right? It, it maps into the space of real numbers. We need to talk of what it means to differentiate it, with respect to an entire field, okay? And this is not that straightforward, if you think about it, right? In fact, in fact, unless you are already know how to do it, it, it, it should not obvious to you, how to do it. Okay? So so, we are really talking here of differentiation. With respect to a field. Right? A field being u, in this case. Right? We need to appropriately definite this notion. Okay? If we can define this notion we can actually make progress towards talking about  states of equilibria and so on. Okay, we are going to do that but this is a good place to end this segment. We'll come back and define this notion of a functional derivative.

\section*{ID: Ka7UcnAKdz4}
Okay, so in, in the last segment we observed the the existence of a free energy functional that has relevance to our problem. At least if, if viewed as an, an elasticity problem, all right. And then we motivated, used that as motivation to talk of begin talking of states of equilibrium of, of, of the extremer of this free energy functional. And made the observation that, well, we really need to see what it means to take a derivative of a functional, right? Of the free energy functional with respect to an entire field, right? Which is the displacement field. What we will do in the second is actually work through that work through that question, okay? The method we are going to use here is call a a variational method. Okay? And the reason it's called a variational method is actually very straightforward. It's simply because we're actually going to consider variations, and the kinds of variations we are going to consider are the following. I'm going to once again draw our field u. That's x u, 0, l, all right. And let's suppose that this is the field that we have. Okay? This is u. Now, in order to carry out the kind of mathematical operations that we require, we are going to vary this field. And in particular, the way we are going to vary it is that's right, I am going to say that the, after the variation, the field u, will look something like this. Okay? This is the varied field u and I will also now use yet another color to mark out what we sometimes call the variation, okay? So the variation that we're talking about here is this, right? We took the field u, varied it as I'm showing you with these blue arrows to get the varied or sometimes called a perturbed field, right? Which shows up in green, okay? So right. Now, I am going to label the, the green field, here. The green field is what I am going to label as u sub epsilon, okay? Where that subscript epsilon is, it comes for a parameter that I'm going to introduce that we used to sort of control the amount of variation we have here, okay? Okay, so here are the definitions, right?  So what we are doing here is consider. A variation. Okay? That I am going to label w, okay? Such that. The quote unquote varied. Or perturbed field Is the following. U sub epsilon which is now defined as the original field u plus epsilon times w, okay? Now, I will introduce two new quantities here, right? I'll introduce w as well as epsilon, all right? I need to see something about that. Epsilon is just a number, okay? So epsilon belongs to the real numbers, it's just a number. So, if I state this and as you can imagine now the, the w's of variation I'm calling u sub epsilon the varied field. It implies that w is also a field, right? Right, because u is a field. I've given u epsilon is, u sub epsilon is also a field and clear and, and epsilon is just a real number. Obviously, w has to be a field. That's right and if you stare at this figure I've drawn here, the graph, what you should be able to see is that let's switch colors, here. The blue arrows essentially represent the field epsilon w, right? So let me try and draw epsilon w here. It varies something like that. Gets a little smaller here. Bigger again. Okay? Something like that, right? So this is epsilon times w, okay? Now, what do you. Now remember, epsilon is a real number, right? What is that telling you about w? Okay. Note, definitely the way I've drawn it, at least. W of 0 equals 0, okay? With all these leading hints, you should know and, and given the fact that w itself is a field. It's a field which has, which satisfies w at 0 equals 0. It should tell you something about the space that w belongs to, right? Observe that as drawn w belongs to the space that we've been denoting as v, right? Where h consists of all w, such that w at 0 equals 0. Right? Indeed, that's the case. Now there is a reason, a very strong physical and mathematical reason, why w is always chosen to be to vanish at at 0. In fact, more importantly w always vanishes at the Dirichlet boundary, okay? So, w vanishes. At. Okay? And this would be principle and variational methods. The, in order to see why w vanishes at the Dirichlet boundary it is useful to recognize that really the field that we care about is u. W and in fact Epsilon as well are things that we have introduced, objects that we have introduced into the formulation. In order to serve our purpose and in order to take us where we want to go, okay? In particular as you can given the fact that I've called u, epsilon the, the perturbed field, the very perturbed field. You may imagine that now we're, and the fact is we're going to use it to it to construct our notion of a derivative. In computing derivatives of our free energy functional you will appreciate that it is if critical importance that we do not violate the Dirichlet boundary condition, right? Because if we do, we are actually changing the problem. And so it is that w vanishes at the Dirichlet boundary in order that the use of excellent does not violate the Dirichlet boundary condition, okay? This is, so that u sub epsilon obeys or satisfies. The Dirichlet boundary condition. If we were to take a variation of, if we were to construct a perturbed field that violated the Dirichlet bound condition, we would no longer even be talking about the same problem, okay? The nature of the problem would have been changed, all right? It is for this reason that w must vanish where ever we have a Dirichlet boundary condition, okay? Of course by now you are recognizing w as our waiting function, right? What we call our waiting function in our formulation or at least in the weak form leading up to our finite formulation. Indeed that's correct and it's no, it's no accident that I labeled this w as well, okay? All right, how does all of this help? Here's how it helps. What we're going to do is consider. Consider the perturbed functional. Okay? Consider the perturbed functional by, but now evaluated with u sub epsilon, okay? Right? It is essentially the, what we would compute for our free energy functional by using the green curve from the previous slide. Okay, the use of epsilon. All right, now is where our epsilon comes into the play, comes into the picture, all right? All right, consider we consider the functional pi sub epsilon and in fact consider the following, right? Supposing we take pi sub epsilon. Sorry, pi of u sub epsilon, all right? Now, the role of epsilon here, if you look at how we used it in constructing use of epsilon. The role of epsilon is to sort of control the amount of variation we're applying to u, okay? Think about it, and actually for that, let me go just back to this slide. Think about it, look at this slide. We've defined what w is and having defined w, it already gives us a, variation. In fact, given that epsilon is just a real number it should be clear to you that if I change epsilon, if we change epsilon, that blue curve on this graph will simply get scaled, okay? But not fundamentally change its character, all right? It will simply get scaled, so really the role of epsilon is to control the amount of variation. W controls the nature of the variation, epsilon controls the amount of the variation, okay? So what we want to do is having looked at how the free energy functional changes and gets perturbed with our variation. We've also, we've already decided about the shape of the variation by choosing w, right? Or by taking various w's. In order to control the amount of that variation, we are going to look at how the perturbed functional varies with respect to epsilon, right? As we look at its derivative with respect to epsilon, we get a sense of well, okay, I'm going to add a variation of a certain shape. As I now control the amount of variation I know how much my functional varies, right? So you want to consider this, right? So this gives us the amount. Of. I'm sorry. Variation. In pi. Having sorry. The amount of variation in pi for variation. In u, having chosen, shall we say, the form of the variation? And the form of the variation comes from w. Okay, so already if you'll think about it, we're now getting close to this notion of well, how much is pi changing with, with variations in our field, u? Okay, now however, what we want is a variation in pi right about u. Okay? All right? So if we further consider now, d d epsilon of pi u sub epsilon. We construct this derivative with respect to a scale of epsilon, which is easy to do, right? Because one thing you will note is that with respect to epsilon, pi will be a function. Okay, and we'll see this clearly. All right, maybe I should just state his here. Pi is a simple function of epsilon. Well, it's a functional. All right, so let's get back to it. We want to know how pi varies with u, but right around the actual value of u. Well, that can be delivered for us if after computing this derivative of pi with respect to epsilon, we simply set epsilon equals zero. Because after all, epsilon equals zero makes use of epsilon same as u, so here we have it, right? So, what this tells us is the derivative well, let's simply call this a derivative. This is the variation in pi with respect to u at u itself, right? For that, for that field, okay? And this indeed, if you think about it now, this indeed is our notion of the functional derivative. Okay, so now we've, we've constructed a clear notion of what it means to take a functional and look at how it varies with respect to a function like field u. Okay? All right, this is a good point to end this segment as well. When we come back, we will apply this machinery to the particular functional that we have created.



\end{document}
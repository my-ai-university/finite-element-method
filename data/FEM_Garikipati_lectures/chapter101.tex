\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}

\section*{ID: U65GK1vVw4o}
Hello, and welcome to the series of lectures on finite element methods. What we will be doing here is developing the basic introductory finite element methods applied to a certain number of problems in physics. For those of you who are going to be taking the series of lectures as a MOOC from somewhere in cyberspace hopefully this experiment will be an interesting and more importantly useful one. For the others of you who also have access to real lectures in class, this should all serve as a as an enhancement to the in class experience. Okay, so let's get started and what I'm going to do is give you some introduction to what we will be doing during these lectures. The point here is to introduce you to the development of finite element methods, or the mathematical background to them. And to, how to code them up, right? And eventually, and, and then use those, use that code to solve problems. This series of lectures is not about a specific software, whether that it's a commercial software or something that's open source. We will be using something like that, but this is not the, the point of these lectures is not so much learn the software, as to learn the mathematics and the, the computation algorithms that are required to develop finite element methods. As far as background is concerned, I would expect that this should be accessible. These classes should be accessible to the advanced undergraduate student with an understanding of differential equations, but perhaps more importantly, a grasp of linear algebra. So, it's expected that you know what matrices and vectors are and how to multiply matrices and vectors, maybe compute inverses of them, and so on. With regard to differential equations we will use the terminology of differential equations right through this series of lectures, but again, you're not expected to know classical solution methods for differential equations. You're not expected to recall things like methods of characteristics or separation of variables, asymptotic methods or anything like that, okay? We will refer to differential equations. We will refer to some of the, some of the machinery that goes with them, but we will mostly be developing everything that we need. Let's see, and yes, this is going to be a series of lectures with units, and the units are, are, are, are, are all ready laid out, I will be filling them in with segments. And we'll see how it goes. We'll start right away now and one thing I want to point out is that we are not going to develop these lectures as addressed to specific problems only. And in order to do that, we have to go back and recall what the underlying differential equations are for each particular phenomenon that we want to address, okay? I will focus more upon the nature of the differential equation for which we're developing a particular finite element method. And along the way, wherever appropriate and perhaps also often for the development, I will say that while this particular set of methods that we are developing has a particular phenomenon as a canonical example. So we will often refer to elasticity, or linear elasticity in one dimension or multiple dimensions. We will refer to transport problems like the heat transport problem, and, and so on. And right, the other important thing to note is that because this is meant to be an introductory level class and to finite element methods, it will focus on linear problems only, 'kay? I will try to state that as often as possible, but occasionally I may forget to do so. But you, it will, it will be pretty clear that we're not looking at non-linear problems, okay? I think that's about it and we will just get started now. So, to begin, we are going to  consider a particular differential equation, and this is the set of we, we, we're, we're going to start with things in 1D. And we're going to look at a type of differential equation that we will, that I will refer to as linear elliptical differential equations in one dimension. There are at least a couple of examples of phenomena that are governed by this particular differential equation. And, let me straightaway put those down, so that it gives us something to something more concrete to think about as we're developing these methods. Probably the most common one is 1D heat conduction. At steady state. So when we talk of the 1D heat conduction equation at steady state it is actually the same mathematical equation as one-dimensional diffusion at steady state. Okay, so this would be mass diffusion. So we also have, one-dimensional mass diffusion. At steady state. So, whe, when we talk of this, what we have in mind is the following. So, this is my little prop for a one-dimensional domain, all right? So, we're talking of how heat is conducted along this in this case, set of Lego blocks. Or, alternately, if, if this were a one-dimension domain and we were talking of mass transport or mass diffusion along this deme, domain, we may consider maybe introducing a drop of dye at one end and watch as it makes its way by diffusion through the bar or, or through the, through this one-dimensional domain, okay? So those are the two, the two types of problems that I've written down here. In addition, there is also the problem of one-dimensional elasticity, all right? And depending upon where you come from, you may think that is more canonical than either one-dimensional heat conduction or one-dimensional mass diffusion. So, in the case of one-dimensional elasticity, we make, we may look at this as a, as, as a, as representing a bar. And talk of maybe holding it at one end keeping the displacement fixed equal to zero at one end or at where I'm holding it with my right hand. And either apply a load at the other end or specify the displacement of the other end. Okay, and then we would have the problem of solving for the displacement field over the bar, okay? In both cases so whether, whether it's heat conduction or mass diffusion or, or, or elasticity there are other fields that we also need to talk about and which we will here as, as we start developing the, the material. So let me write down also one-dimensional, one-dimensional elasticity, also at steady state. Okay, and as I said just a few minutes ago, all these problems are, that we will consider will be linear problems. In fact, in the series of lectures, except perhaps at the very end, we are not truly going to consider non-linear problems at all. Okay let's actually dive into it and lay down our differential equation, and begin thinking about what it takes to solve it, okay? So. All right, so as, as I develop it just in order to have something to, to, to, to hold on to that's a little more concrete, let's just think of ma, maybe you want to think of one of these three problems in your in your mind, as, as we develop it. I, and, and I will try to refer to these problems as we develop the equations and see what each of these you know, as, at, see what the quantities would work out to in each of these cases. Okay, so let's let's start with this, and, and do the following. Okay, I'm going to develop this first by thinking of a problem in one dimension elasticity for my own purposes, 'kay? So let's suppose we have a bar, okay? And let's say that this is our x-direction, okay? At this end we have 0 and the bar extends up to the point L, okay? Let us suppose that because the way I've drawn it here you know immediately that at the left end I am seeing that the bar is fixed. All right, it's built into some sort of wall here. So, getting back to this, at, at, at well the right end for me, the left end for you, the, the, the bar is fixed, okay? So, so this, this, this what we mean by the support. At the right end, we may either specify the displacement, and I will denote that as U sub g, okay, for given displacement. Or we may do something else with it. We may specify the force at this end. And for various reasons that have to do with how we're going to write things out as we go deeper into this into the, into this series of lectures, I'm going to denote it as t. Okay, so what we have here is U sub g is the specified. Displacement. At x equals L, sorry, x equals capital L or t is the specified. We, we could call it the specified force. But, again, for reasons of generality, with what we're, we will develop as we go deeper into the series of lectures, I'm going to call it the specified traction, 'kay, at x equals L, okay? In addition, what we may also have is a distributed force over this, over this bar. And I will denote that force by just a series of arrows here. And I'm going to label that as f. So what we want to think about here is the foll, is, is, the following idea. As far as this distributed force is concerned we have our bar in one dimension. And what the force is doing is, effect, is, is, it's a force acting on every little volume or mass element of the, of the bar, okay? So, let me write that down as well. F is a distributed Body force as we often call it. Okay, all right, so, so this is the setting and I, I, I'm now going to state our problem in more mathematical terms.

\section*{ID: kkD65zDSBE}
Okay, our problem is the following. We want to find the displacement field which we are going to denote as u. Find u. And we expect it to be a function of position. We do not expect it to be a function of time, force, rather obvious reasons, right? And what are those reasons? Think about it for a couple of seconds. Or you probably, see it already, right? We're talking about steady state drop, that's right. So things are not a function of time. They're only a function of position. Okay, so, we want to find u as a function of x. And now, I'm going to use, introduce some more mathematical notation that will be useful for us. We want to think of u as function, as being a mapping, right, from a certain domain in one dimensional space, right? We're thinking of it as being in mapping from the open interval 0, L, okay? Into a certain range, right? Now, the displacement field, because we're doing a problem in one dimension is for our purposes just a scalar. Okay? So, it's, so, when we find the displacement field, we expect it to be nothing more than a real number, right? Apart from that, we really don't want to say anything more about it at this point. So, I'm going to denote that as R, where R denotes real numbers, okay? Okay? Also to prepare us for what comes later, I'm going to denote this as R1, although in one dimension, specifying that one is really superfluous because if even if we wrote just R, we would know that it is was one dimensional space, okay? So, the way you would read this is, that user mapping from the do, from the domain 0, L, it's an open interval onto one dimensional space of real numbers, okay? It's important to note that we are talking of an open interval here, not a closed interval. The reasons for this will become clear as, as, as we develop that, I'll, I'll come back to this, and to say, why it is that we said, it's an open interval. Okay, so, if we want to find u of this type given, 'kay? Given u at x equals 0, equals u0, okay? We're also given ug, right? The, the quantity that I talked about on the previous slide, where I talked about, when, when I mentioned that we may have a displacement specified at the right end, or t, okay? So, we know u naught, we know either ug, or t, okay? We are also given f, this distributed body force as a function of position, okay? In addition, we are going to say that we have what is called a constitutive relation, okay? So, we also have the constitutive relation, 'kay? Which we are going to write now as sigma equals E times u, x, okay? So, we're given all these quantities, and, and this, this last constitutive relation, okay? And we want to find u, such that. The following holds. The derivative of sigma respect to x, plus f, equals 0, okay? In the open interval 0, L, okay? So, this, of course, is our differential equation, right? So, this is our differential equation. 'Kay? But this is not all. We need to specify other details about, about our problem, okay? And in particular in this case, we need to specify also the boundary conditions. So, we want this differential equation to be satisfied with. The boundary conditions. U at 0, that's u at x equals 0, is equal to u naught, right? And really here, we're just recalling what we already wrote in further up in the problem definition, okay? So, we have this condition, and either u at x equals L, equals u given, or. Sigma evaluated at L equals t, okay? Where, what do we mean by sigma? Well, we need for, in order to get sigma, we go back to our constitutive relation, which we have back here, and evaluate it at x equals 0, okay? Right? So, so these are the boundary conditions. And, and it's important to note that for the way we've set up the problem this boundary condition at x equals 0, always holds. However, at x equals L, we have either this condition, or that one, okay? So, let me say, just one, or two more things about these boundary conditions. So, of the boundary conditions. U at x equals 0 equals u0. And if we have the displacement boundary condition at x equals L, these together constitute what we call Dirichlet boundary conditions. Okay? Dirichlet boundary conditions are boundary conditions which are applied to the primal field that we are solving for, in a partial differential equation, okay? So, let me state that, as well. These are boundary conditions. On the primal field, 'kay? And by primal, we simply mean sort of, you know, the primary field, right? What is the pre, what is the field in terms of which we are posing the problem? Well, we are posing the problem in terms of our displacement field u. And we apply boundary conditions on that field, we call them typically Dirichlet boundary conditions. And this terminology is quite uniform throughout mathematics, right? Through, throughout the field of partial differential equations. Okay. There's another type of boundary condition which is the following, right? In this case, we have sigma at x equals L is equal to t. This is what we call a Neumann boundary condition. All right. Now, if you look at where, what we've written out for sigma. In this particular case, we have, using our constitutive relation for, for, for, for, for sigma. We have E u, x, the whole thing evaluated at x equals L, equals t, right? Just from our constitutive relations, right? So, this thing would follow from. Right? So, we have this from our constitutive relation, and typically, in a fairly vast range of problems, whenever you have a boundary condition that is applied to a derivative, a spatial derivative of your primal field, tends to get called a Neumann boundary condition. That's not uniformly true, for certain high order problems, the situation gets a little more complex, which we won't get into in these series of lectures at least, okay? Alternately, in the context of of the displacement problem, the, the, the Dirichlet boundary condition is often also called the, the displacement boundary condition for obvious reasons. Whereas the Neumann boundary condition often gets called the traction boundary condition, okay? So, let me just state that for elasticity. The Dirichlet. Boundary condition, is called the displacement boundary condition. Okay? And. The Neumann boundary condition for obvious reasons. Is called the traction boundary condition. Okay. At this point, let's stop here from this segment. We will pick it up when we come back.

\section*{ID: 9GFtoFRXwPw}
>> Welcome back. We'll continue with our development of, our one dimensional elliptic partial differential equation and we have been doing it so far using elasticity as a canonical physical problem, right, to guide us through this process. So we had got as far as writing out the differential equation and we were talking about boundary conditions. So let's continue talking about, about boundary conditions, okay? So let me actually draw our domain once again. We have the bar. We have our coordinate x. We have x equals 0 and we have x equals L. Now, At the left end, x equals 0. We have u 0 equals u naught. Right? Specified as a condition. And at the right end, we have either u at L equals u sub g for given, or we talked about how we have sigma at L equals t, okay? And at the very end of the previous segment, we had talked about you know, the nomenclature that we employ for these different boundary conditions. Now I want to point out something that's actually quite important both from a mathematical and a physical standpoint for this problem. Which is the following, observe that we always have this the Dirichlet boundary condition at x equals 0. However, at x equals L, we have either a Dirichlet boundary condition or a Neumann boundary condition. Okay? In, in particular we do not have a situation where at both ends we may have Neumann boundary conditions. Okay. So let me state that. Okay. So in terms of boundary conditions >> Okay? We have either >> case a, being Dirichlet boundary conditions. As we go on, I'll be offering you as BCS short for boundary conditions. So either way, we have Dirichlet boundary conditions at x equals 0 and x equals L, right? Or b. Dirichlet boundary condition at x equals 0 and Neumann boundary condition at x equals L. It turns out that in case b we, we could actually of flipped things around. Which is that we could have had Dirichlet boundary condition x equals l, and a Neumann boundary condition at x equals zero that would not pose a problem. Okay, what's important is to note that there is a particular combination that we are not considering. Okay? It's very obvious but I'll let you think about it for a second or two, to make sure that you are indeed with me. This. Okay. The condition we are not considering is Neumann at both ends. Okay? So, we do not consider Neumann boundary conditions at x equals zero and x equals L. All right? There's a very good reason for this. Okay? Let me try and motivate the reason first from a, from a physical standpoint, okay? What will, why this is important is the following, is, is, is the following. For the kind of problem we are considering, we're saying that we always need to have the displacement specified in one end or the other, at least at one end, okay? So we may have the dis, the displacement specified at this end of where I'm holding it and on, on the other end we may either specify the displacement or a force. That's okay. Right? Or attraction, sorry. That's okay. Okay? Or what we are not allowed to do is to say that we have the traction specified at one end as well at, as, as at the other. Okay? So we can't have Neumann boundary conditions on both sides, okay? And now you may wonder why this is the case. In particular you may ask the question, but surely that leaves out a very important class of problems. In particular, that does leave, it seems to leave out the class of problems where I mean I'll take this bar, right? Remember it's after all an, an elasticity problem, right. You may say that it, we're leaving out the pro, the class of problems where we take this bar and sort of throw it one way. Remember we're doing it one way, so we're assuming that it moves only one direction along the x direction. If we, if we, if we toss this bar and it moves along the x direction, we may say well, we've ruled out the ability to solve that problem because we have stated, so far in our development, that we always have the displacement specified at one end or the other. Okay? If we were to toss it and just watch it, watch it evolve in time, we would not be specifying the displacement at one end, right? It would just be, we would be throwing it and let it evolve in, in space and time, but we would not have specified the displacement at one end. Okay? The reason is that, the reason we, we, we have this restriction is because we are considering steady state problems. The problem where we actually throw the bar and watch it evolve in, in, in space and time would be a time dependent problem. Okay? And in particular in the context of elasticity it would no longer be an elliptic problem, it would be a hyperbolic problem. There are other ways in which that problem is treated. In particular, we would need to specify an additional set of conditions. Can you think of what those may be? We would need initial conditions as well. Okay? What does that mean mathematically, 'kay? What it means mathematically is the following, okay? The reason we do not consider Neuman boundary conditions in, in this description of the problem is because of the following, okay? Supposing you consider, let, let's suppose we do try to specify non boundary conditions at both ends. Okay? So consider, Neumann boundary conditions at x equals 0 and x equals L, okay? So what this may mean is that we are saying that sigma at 0 which we know to be E u,x evaluated at x equals 0 equals let's say. T, 0 right for traction at zero? And sigma at L which is what we have indeed considered is E times u, x evaluated at x equals L now equal to T sub L. Okay? I'm distinguishing T0, T not and T L just by the positions, okay? Now you observe that this poses a certain problem because supposing so, so if you had these two conditions Our, problem would be one of finding this basement field which, in addition to satisfying these two boundary conditions also, of course, needs to satisfy the differential equation. Right? So we had this and now we say consider U of X satisfying these Neumann boundary conditions and the differential equation. Right and the differential equation remember is the sigma d x plus f equals 0. Right? In 0, L. Okay. Now, if you had this sort of situation, making our substitution come from the constitutive relation tells us that now d dx of E times u,x, where I've simply substituted our constitutive relation for this stress, plus f equals 0, okay? Now. This is the problem we would be solving, right? We, we would be solving a problem with these two boundary conditions and this, differential equation. Now observe that any solution we get to this problem would be non unique up to a constant displacement field. Okay? Because supposing you did have a u that satisfied these conditions. Right? Alright. What we could do is that u of x plus some other u which I will denote as u bar. Okay? Which is a constant. With respect to wrt being short for with respect to. X, okay? This field is also a solution. Right? Clearly if u, if u bar is a constant with respect to x, it would satisfy our two Neumann boundary conditions. It would satisfy that. And it would satisfy that because derivatives with respect to u, sorry, derivatives with respect to x of u bar would be zero. It would not contribute to that boundary condition. Likewise to the differential equation, right? Because the differential equation actually involves two derivatives on the displacement field. Right? If, if, if E were a constant. Or even if E were not a constant it would involve at least one derivative on, on, on the displacement field. Right? So therefore adding on U bar equals constant would still satisfy that, that equation as well. Okay? So what we see is that If we had Neumann the boundary conditions only on our problem the solution u of x is non unique.  Up to a constant displacement field and in the context of elasticity, a constant displacement field is also called a rigid body motion. Right? Because remember, this is our one dimensional domain over which we're trying to solve this problem. What is a displacement field that is constant with respect to position? It's one in which we seeing, we're seeing that the entire bar has a translation, right? Every point in the bar has the same displacement, u bar, added on to any solution we specify to it. Okay? So we say that the solution u x is non-unique up to a rigid body motion. U bar, okay, which is a constant. Okay? In this particular say, case, saying a rigid body motion and saying u bar is a constant are actually superfluous. They both mean the same thing. Okay? So it is that for these kinds of problems that we are looking at, in particular elliptic differential equations in one dimension, okay? We cannot have Neumann boundary conditions, only. We do need a traditionally boundary condition and it is exactly to make sure that our solution is unique with respect to the solution, with respect to the primary field. Okay? And also remember that I said that, that, we did talk about how the problem of actually tossing this bar and watching it move in space and time is a different problem. Okay it's no longer an elliptic problem. In particular, it's no longer a steady state problem. Because we would be looking at how a solution evolves in time by not assuming that things are constant In time. Okay. So maybe I should state that as well. Neumann boundary conditions alone can be specified. For the time-dependent. Elasticity problem. Okay? And I'll just state here that though we won't be coming back to this time-dependent elasticity problem. I will just state for now that this would then become what is called a hyperbolic differential equation. The hyperbolic partial differential equation. Okay? We are not doing that right now. We are looking at elliptic partial differential equations, which for the kinds of things we are looking at also means that we have steady state problems, okay? So those are the important things we need to say about the boundary conditions. What I will also do now is spend a few minutes talking about what I've been calling our body force, okay? So let us now recall. The body force that we've written as f has a function of x. Okay, and the role it plays in our problem is the following. It gives us d sigma dx plus f, and here I'll make it explicit that it depends upon position, is equal to 0 in. Okay? In this case, what I'm calling the body force, of course is a term that is applicable only when we're doing elasticity. Okay? In more general problems, in more general PTEs, this would just be called the forcing function. Okay? So this is the. Is the force and function in other pdes probably not other pdes. Maybe I should just call this in general pdes, all right, simply because the term body force may not have any significance in other pdes. All right. It's just, it's just the forcing function. Nevertheless, since we are talking in the context of, we are talking of elasticity let me state what this forcing function is. Okay. Give you, give you an example of the forcing function. So, as an example, let's suppose that we have again this sort of situation. We have the bar. Kay. It extends along the x direction. 0 and l, and that, I had this sort of picture that I drew early on, right? Just say that that is our distribution of the body force, right? F. So a specific example of this would be the following. Supposing that although I've drawn this body as extending in the horizontal direction at least relative to the, this particular screen let's also suppose that, interestingly enough in this situation, this was the direction of the action of the acceleration due to gravity. Okay? I just happened to turn my coordinate system around so that it turned out to be horizontal but that's the, the, the direction of gravity. Okay? So G is D. Acceleration- Due to- Gravity, okay? In that situation, f of x would be the mass density of the body, which, you know, could be a function of position, right? Maybe this bar does not have a constant density with respect to position times g. So is this a specific example of the body force. And know, perhaps make it a little more physical maybe we should say that this bar, you know, is actually turned around this way. So, downward is the x direction, and now you can see very clearly how that would be the body force, right? It could simply be the the distributed force. Distributed because it's distributed at every point in the, every, every, every, every little mass element of our bar. And it's the force that's distributed in every mass element of our bar due to gravity. Okay, so that's another example of it. Okay. So, I think we can stop this segment here. When we come back, we will continue with, saying a little more about this, differentiation.

\section*{ID: JfdcpgVTrco}
Okay, we will continue with laying out the elements of our linear elliptic differential equation in one dimension. Okay? Let's, let's look at our differential equation again, so. We've written it in the form d sigma dx plus f function of x as we discussed, is equal to 0 in 0, L. Right. Well, one thing you note of course is that I'm using total derivatives here. I'm not using partial derivatives, that's because when we're doing one dimension, doing things in one dimension, we don't even have time. There are, there are no other variables, right? So we're free do to this. Of course when we go into multi, multi-dimensional problems we will have to use proper partial derivatives. Right, so though I've been calling it a partial differential equation sometimes and a differential equation, really they are one and the same thing for this one dimensional problem in a single variable. Okay? Just a few more things I need to state here. One thing you will note is that I've been careful to say that we have an open interval over which the differential equation is specified. Okay? So, that is an open interval. Right, and you recall what this means? It says that we are interested in looking at this differential equation over the range 0 to L, right. So, we have our domain here. That is 0, that is L. This is the domain over which we are interested in looking at this differential equation. But importantly we are seeing that the differential equation does not apply at the point x equals 0 and x equals f, right. So open interval remember that this implies the domain between. X equals 0 and x equals L, but importantly excluding the end points. X equals 0 and x equals L themselves, right? This is important. And what I want you to do is think of why that maybe, why is it that we specify the differential equation, the partial differential equation or just the differential equation, in this case excluding the point 0 and L. And to help you think about the answer, recall that we do need to know something about 0 and L. If we don't have the differential equation specified at those points we must have something else. All right? Well, you probably get the answer. The an, the, the answer is that we do already have boundary conditions specified at x equals 0 and x equals L. If we were to insist that the differential equation also held at the boundary points, we would actually have a, an over-constrained system. Okay and this will often manifest itself in the mathematical or numerical solution of problems where you by, by, by essentially developing instabilities in your solution mattered or, or, or situations where you just can't find a solution, okay? So let me just state that. This is because we have boundary conditions. Right. And those boundary conditions are either on the primal field itself, or on effectively on u, x, okay, at. The boundary points. Okay? And this is why the partial differential equation is always specified over an open interval. This, by the way, is the case for any differential equation, right? And any, any, any, any partial differential equation, is not specified at its boundary points. Likewise, if you had an ordinary differential equation it would not be specified at the end points of the interval of interest. Okay? This is an important thing to note and, and to recognize that it's not purely, matter of pace. It really means something. Okay? It means something mathematically and physically as well. Okay. That's one thing I wanted to state. The other thing I also want to state is I also, I also want to talk about is the constitutive relations, right. Constitutive relation. And in this problem, we specify the constitutive relation as being sigma equals E times u, x. All right, in constitutive relation as, the term suggests, it tells us something more about the constitution of the domain of the problem, okay? In this particular case, we started out with a primal field u, but then we, introduced another field sigma. Okay? And, what we are seeing, is that we know something more about the domain of interest, and in our case the domain of interest is this one. Sorry. That is our domain of interest, right? We're seeing that, sorry, it's not x here, it's 0, that is x, okay. We are saying we know something more about the domain of this bar, over the domain 0 and L, by specifying this particular relation. Now we are trying to set this problem up using elasticity as a, as an economical physical phenomenon, right? In that context sigma is the stress okay and. And we rewrite it here. Sigma is the stress. And it is related, not to the displacement itself, but to the gradient of the displacement, which in this setting of linear or linearized elasticity is the strain, okay? The strain is sometime given its own symbol. And sometimes, or I guess pretty often is denoted by epsilon, which is u comma x. Okay, so in the context of this particular problem that we're looking at, the elasticity problem, we are seeing that there is a relation between the stress and this which is this field signal and the strain, which is the displacement reading. Okay? And that relation as you can see here is a linear one. Okay, so when we see sigma equals E times u comma x what we have here is linear, linearized elasticity. Right, and it's obvious why we say it's linear or linearized because that's a linear relation between the stress and strain. In this setting, E is a physical constant, that often gets called the Young's modulus, or you may simply call it the modulus in a one dimensional setting. Right, in, in a one dimensional setting, there is only one modulus, might as well call it the Young's modulus. Okay? So this is the other bit that I wanted to state about this, about the physical phenomenon that we're using for now as canonical setting for the problem we're considering. The probably very last thing I will do is restate essentially the same problem but in the context of a different physical phenomenon. Okay? So let me state it here the 1D scalar, because remember we said that in 1D everything is essentially scalar. So the 1D scalar 1D scalar linear elliptic problem. Also, models are heat or diffusive mass transport. Okay? Let's suppose we are talking about heat transport, okay? In that case we would be, we would state the problem as following. We would say find u function of X, which is a mapping, again, from this domain 0, L, open interval to the one dimensional real space. Okay? Given, I, I will attempt to use different symbols here, okay? And I, I, I'm going to use different symbols. But because we're talking about the heat condition of, of the heat transport problem we, I, I will also state here that what we're talking about when we talk about u, the primary field, we, we have the temperature in mind, okay, for, for heat transport. Okay, so define the temperature U, which is mapping from 0,L open interval to R1, given the following, okay, given, u0 and ug or. Now, I want to reserve t for traction. So for the other sort of boundary condition that we could have in the context of heat conduction, I'm going to use j, j bar. Okay? We're given these. We have our forcing function, f, and the constitutive relation. Now in this case the constitutive relation that I want to use is going to be for the heat flux. And want to use a different symbol instead of sigma here. Okay. I'm going to use the, the symbol j for the heat flux. J equals minus kappa u comma x. Okay, all right? Such that. We have the following, okay? Minus d, dx of j equals f of x in (0,L), okay? All right. Now let, let me put down the boundary conditions. And the boundary conditions are. 'Kay, u at zero equals u naught and either u at L equals ug. Right? Now both these, u at 0 being u naught, and u at L being ug would be given temperature conditions, all right? Or at X equals L, we may also have the condition that j at L, okay, equals minus j bar. Okay? All right. Right, so this is how we may specify the heat conduction problem. Now in doing this observe that what we have here is the, is to be be thought of as the divergence of, of, of the heat flux. Okay, and in fact G. In this case, would be the heat flux. Okay? What I've circled here would be the divergence of the heat flux. Okay? In the con, in the context of heat conduction, what we would be seeing would be that the, so, sorry this would be the negative diversions of the heat flux. Okay. So the minus diversions will be heat flux. What we would be seeing is that the negative of the diversion of the heat flux in the context of heat conduction is the amount of heat you're supplying to a particular little volume element in, in this domain of interest. Okay? I already said that you, you, that the conditions on you would be, would be temperature boundary conditions. So let me specify that also, okay? So the Dirichlet boundary conditions would imply in this case, temperature on great conditions. Okay? And this, and finally, the Neumann boundary condition would imply a heat flux on recondition. Okay? In particular, this condition that j at L equals minus j bar. Okay? This would have the following significance that j bar, as I've written it here would be the heat influx at x equals L. Okay? So, if you look at this problem that we've just described here rather quickly and you just make substitutions, right? You just make substitutions instead of you know, instead of j, you go back to sigma instead of kappa, you go back to e. And instead of j bar, you go back to t. Okay? You will see that the problems are in fact very similar. Mathematically, it is exactly the same problem. But we know that this is a problem of steady state heat transport or diffusive mass transport. So, so, so this is important to, to note that there is a multitude of problems that are described by the same differential equation or the same partial differential equation. It's just a matter of redefining our fields of interest boundary conditions of interest and giving them different ascribing to them different physical interpretations. Okay? And because we're interested in developing in this class finite element methods that apply to a range of problems. We will tend to focus more upon the underlying differential equation of interest. And, of, we will repeatedly make connections with the physical problems but we will also bear in mind that rather than thinking of the method as a problem to be developed as a problem applicable. Only to structure mechanics and only to heat conduction, it truly is a lot more general than that. All right? Okay. So this is where we were and this is where we are going to stop for this segment.

\section*{ID: APWKnOzxX4}
Welcome back, we continue with our preamble into the finite element method. And you will recall that in the previous segment, we had started working with this linear, one dimensional, partial differential equation, which was of elliptic type. Okay? And, we, we, we'd written out this differential equation. We had, also motivated it in the context of elasticity. And also looked at how essentially the same partial differential equation arises in other problems. Right? We'd looked at heat transport as well as mass diffusion. Okay? We'll pick up now, but we'll focus on the PDE itself. All right? So, the topic of this segment is what we call the strong form of the p, of the partial differential equation. Okay? So. So the strong form of a linear PDE, or partial differential equation, of elliptic type. Right? In one dimension. Okay? Now let me write the, the PDE and this is the same PDE that we were working with in the previous segment, but let me put it down now. Okay? So what you want to do is the following. Right? We've set ourselves the task of finding U, function of X, given certain data. Right? We're given Ug, U not. We're given Ug, or we're given t, right? And remember how we qualify the difference between Ug and tUg would be the Dirichlet boundary data, and t would be the Neumann boundary data. Okay? We're given these. We're also given the function f, function of x. And the constitutive relation. And that constitutive relation is sigma equals E. U, X. Right? Given all of this, what we want to do is sorry, we already said that we wanted to find U. So we want to find U given all of this information, such that the spatial gradient of sigma plus f equals 0 in the open interval 0, L. Okay? With boundary conditions, Boundary conditions being u(0) equals U not. And, either U at L equals U given or sigma at L equals T. Okay? This is our specification of of the linear partial differential equation, of elliptic type in 1D. Now, we call this a strong form of the equation, right? That's new terminology that I will introduce. We call it the strong form for two reasons, one is that if you look at the PDE itself, you will observe that the PDE has been specified to hold essentially at every point in the interval of interest, right? When we say that d segment dx plus f equals 0 in the open interval 0,L, we're specifying that that PDE has to hold at every point. Right? So, in a sense, that is a strong condition, we've seen that at every point we want it to hold. Additionally, you observe that when we account for this, constitutive relation, right? What this implies is that, if we substitute that considerative relationship for sigma in the partial differential equation, what we get is the following form. Right? We get d, d, x of sigma. But sigma itself is E du, dx plus f equals 0 in the open integral. Now, if you observe that term, what we see is that there are two derivatives effectively on the field of interest, right? There are two derivatives implied here on the prime of field U. And that is a slightly strong condition, because really for this equation to make sense, classically, we do require that U should be a function of a type that allows us to take two derivatives okay. As you may imagine if U is not a terribly smooth function, if it has discontinuities and so on. If it, if, if even worse, right, if it has delta functions, right? If there are delta functions in it. There are certain difficulties we are going to encounter with taking two derivatives of it. Okay? All right? And so, what, what we're doing here is w, w, we are, we, we require so called strong conditions of what I will loosely call smoothness. Okay? On U. Right? And why is this? Because the strong form, and I will capitalize it here to emphasize that, for now, it's something special to us, right? We require strong conditions of smoothness on U effects, because the strong form has to Spatial derivatives. 'Kay, and there's also this idea that we require the whole point wise, 'kay, so we require The partial differential equation to hold pointwise In the interval of interest. Okay, these are the reasons for which we call this form of the equation the strong form. All right, and this is of course is the classical form in which you have seen partial differential equations posed up to this point, assuming of course that you do not already know something about the mathematical background to finite element methods. Okay, all right, so, so this is the strong form with a PDE of interest and, and, of course, we observed in the previous segment that it, it models elasticity, it models heat transport, mass diffusion and so on. What I'm going to do now in the next few minutes is sketch out for us the approach to an analytic solution, 'kay? And I say just sketch out because I'm not actually going to produce an analytical an, an analytic solution. But I will point us in the direction of of how we would proceed, okay? Of course I'll be using the fact that this is all in 1D, so it makes life a little easier for me, okay? So so let's like now look at an analytic, analytic solution. 'Kay? The approach is very straightforward and you probably already know how I'm going to approach it. It is to simply integrate them, okay? So, we have d/dx of sigma equals minus f, all right, and I did that by just moving f over to the other side, it's just integrate, okay? So we integrate this now. I'm going to integrate this from 0 not all the way up to L, 'kay, but I'm going to integrate this from 0 to say y, okay? And here the the dx, of course. And here I have integral of f. Also over the same limits, right? 0 to y, dx, right? Where y belongs to the open integral, sorry, it belongs now to the closed integral. Right, y belongs to the closed interval, 0 to L. Importantly, we do not necessarily, we do not actually in so, in solving this PDE, we do not want to integrate all the way up to L, right, because we want to maintain a general solution applicable at any point, right, and that's why we don't take the upper limit all the way up to L. Okay this again is completely straight forward, right, so what we, what we know we get from here is we get sigma at y minus sigma at 0 equals minus integral of fdx over the limit 0 to y. F in general is at this point it's, it's not quite unknown. It's data, which we would know if we set out to actually solve the problem. But I want to leave it general. I do not want to assume a particular form of f or anything, right? All right. So we have this. But then we realize that what this implies is, of course this, right, it is that E u, x, evaluated at y minus E, u comma x evaluated at 0 equals minus integral 0 to y, fdx, okay? because that's what it implies for us, okay? And you, and you see how I get this last line by simply making the substitution for the constitutive relation for sigma, all right, at the two limit points. Now in in sa, in what is actually some what of an abuse of notation, let me simplify this to write as let me simplify and write this as E du/dy equals minus integral 0 to y fdx plus E du/dx evaluated at x equals 0, okay? Right? Okay, so and now you know what, what the next step is. Well, we need to do one more integration, right? So what that does for us then is the forage now we'll integrate this again now let's integrate from 0 to z, right? Or 0 to zed depending on which part of the world you come from. Okay, so that becomes E that the, the integral here is E, du/dy dy equals minus integral 0 to y of fdx, and then we're doing another integration, right, 0 to z dy. Okay, and just to make it clear, let's do this, okay? And then we have another term here, which is plus E du/dx evaluated at 0, okay? But this term also gets integrated from 0 to z, dy, all right? Okay again, straightforward enough what, what we get here, now, now note that of course E could be a function of position. Right, so, so, you're, you're allowed to make E a function of position. So E could also depend upon y. All right, so now when you do that integral we know what we get, we, we essentially get to make things simpler, let me forget about that particular dependent, okay? Let me, let me leave things as they are. Okay, so let's suppose E's a constant and we go ahead with the integration what we get here is now E. U at z minus E. Again, u at zero equals everything that we have here on the right hand side, correct? So we have integral zero two z, integral zero two y f d x, d y plus integral zero to z. Well actually this last integral can be made very simple now, right? Right, because it is just E. D u, d x evaluated at zero, right? Times z. Okay. So this is the final form that we get, and since we want to write this all as a, as a result for u of z. We write it as one over e. Of integral zero two z. Integral zero two y, f, d x, d y, plus e, d u, d x. Evaluated at zero times z + E multiplying u at zero and close the brackets, ok? So this is the final, the general form of our equation. Of course I made things even a little simpler for us by saying that lets just assume that E is not a function of position. Okay, and if E were a function of position, of course, would involve a little more. But just for the point of fixing ideas, I want to take this approach. Okay. Now, you observed that we have some unknowns here. Would, we know how to apply them, them, right. We would apply the boundary conditioning, we would apply the differential boundary condition here for u of zero. That is equal to u not, okay? That leaves us however with one more unknown, which is this okay? And this would be obtained by applying the boundary condition that we have at l. Okay? This term which is determined. By applying the given boundary condition. BC for boundary condition at X. Equals L. Right? X equals l or in this case z equals l because we are finally expressing this as, a function of z. Okay? Alright. Now. If the bound recondition that we have z equals l is a displacement boundary condition, if its a directional boundary condition it could be straight forward to have to apply it. What if it were a Neuman boundary condition? Think about it. Straight forward of course its strictly almost if it were a Neuman boundary condition we would simply carry out a derivative. A few right and thereby allow us. That would allow us to apply the boundary condition right. So if boundary condition at x equals l is a Neuman boundary condition, we first calculate, calculate or go to u comma z. Using that form of the function, okay? And that will then allow us to apply the Neuman boundary condition, right? And then apply The boundary condition, e u comma z at l. Okay? That will then allow us to determine this unknown. Okay? I also note now, that I have just 1 sign off in this, slide. I have a minus sign here, which I forgot to carry over. So let me just put it in here. Okay? That takes care of things. So, this would be the. General approach to getting an analytic solution of the strong form of the pde that we are working with. Okay. The trouble with this of course is that we can do this only for very special forms, or for the forcing function f. All right? And furthermore, we did observe during the process that I simplified things by saying that E is a constant. Okay? I'm assuming that E is a constant. It does not always have to be the case. I assumed E were a constant to go through the process. If E is not a constant it makes the process a little more complicated right? And likewise if F were some sort of complicated function. Furthermore this sort of simple minded approach or simplistic approach works only in one dimension right? As we go up to, to higher dimensions integrating up the equations of, of this form. This, this particular linear elliptic PDE. It is not at all a trivial matter, alright? Exact solutions or analytic solutions exist only for very special cases. And what we are trying develop here is really an approach to solve general boundary value problems. All right? With general data, general data in the form of F and E. So that's what we're going to go on to. Shortly, but we'll do that in the next segment, and we'll end this segment here.

\section*{ID: qPjJoKwvYEk}
All right, welcome back. We'll continue, where we left off with the previous segment, was in identifying the strong form of the linear elliptic PDE, in one dimension. We sort of sketched out, an approach to generating analytic solutions to the strong form. And then, I made the point that this is not a very general approach, simply because it's not a very useful approach I should say. Simply because as, as you know, the given data, and as boundary conditions get more complex, in particular in multiple dimensions, as you can imagine. Domains of interest in the context of many problems are not just a simple regular domains, right? And that introduces a further element of complexity, to solving partial differential equations, in strong form. Okay? So, analytic solutions are limited, and one tries to find approximate solutions. Okay? The first obvious way to approximate any differential equation is to go to something like, a finite difference method, right? Where a finite difference methods simply take any derivative, and replace it with a difference. Okay, and this has spawned a whole class a whole field, of finite difference methods. What we try to do with finite element methods is, is a little different. Okay, we take if a mathematical approach that is fundamentally, quite different. And the basis of that difference in approaches, is to go from the strong form to you guessed it, the weak form of the PDE. Okay? So, that is the topic of this segment. All right? The weak form of a linear Elliptic. PDE in One dimension. All right. What I'm going to do is, first give you the weak form, and then, develop more ideas about it. Okay, so, the first part of this segment, the next few minutes may seem a little formal, but don't worry, by, by the end of before too long actually, you will be masters of it. Okay, so, here is the weak form. The weak form is the following. It is to find U, okay? U of x, belonging to S. Now, S for us is a space of functions. We talked about finding a function u. And when we see that it belongs to S we are thinking of S as some sort of collection of functions, right? Some sort of space of functions, from which we expect to draw our actual solution u. Okay? When I say space of functions, you may think of you know, any class of function just to fix the ideas of polynomials, or, you know, specific types of polynomials. Legendre polynomials, or Lagrange polynomials, or you may not want polynomials. You may want to have a harmonic functions, right? Or, or exponential functions, or something. Okay? This is the sort of thing we, we have in mind when we say, that u belongs to a space of functions S. Okay? All right. We want to see a little more about, what the space of function is? Okay? And we say that the space of functions u,  S, okay? Consists of all u, right? Such that u at 0 equals u not. Okay? All right. And. So, what I'm doing here is sort of building in the Dirichlet boundary condition, okay? Into this sub-space of functions. What we're saying here is that, we're only interested in solutions you would satisfy the Dirichlet boundary condition. Now of course, there is a possibility of having two Dirichlet boundary conditions. When we saw the strong form, we observed that on the right-end of, of the domain, at x equals L. You could have either another Dirichlet boundary condition, or a Neumann boundary condition. It turns out to be a little cumbersome to develop the weak form, for both cases. And so, I'm going to develop the weak form, for a single Dirichlet boundary condition. Okay? At x equals 0. Later on, we will see what happens, when we have Dirichlet boundary conditions at x equals 0 and x equals f, okay? All right. If, if we did have that, we would build that condition also into the space S, okay? For now, we're just saying, that we have a single Dirichlet boundary condition, okay? So what, so, let me write that. We're assuming, we're, we're considering a case where there Dirichlet boundary condition at holds, at x equals 0 only. Okay? Let me see actually, let's consider this case. Okay? Just, just, because otherwise, it just gets a little cumbersome to develop, in most general form. We'll come back to it, all right. So, this is what  we want to do. We want to find u, belonging to this particular space S. Which for now, is completely general. All we're saying is that, it needs to satisfy the Dirichlet boundary conditions. Okay? So, find u, belonging to space S given all the other data that we have, right? Given u 0, t because we're developing it, with the single boundary condition, right? So, we are not considering u g, now. Okay. So, we're given u 0 t. We're given the function f, right? Which is our forcing function, our body force in the context of the elasticity problem. And, the constitutive relation. Sigma equals E U comma x, okay? So, we're given all this data. Which, which was the same as, as the case, for the strong form. Okay? However, there is more, okay? Such that. For all the w belonging to V. Okay? Now, this is new. All right? This symbol is for all, okay? Right? So now, we've done something new. We've introduced a new function w, which was not in the mix at all. We are saying that it, is a function belonging to some space V. Okay, think of V as the same sort of concept our S is, right? If you think of S being some plane kind of polynomials, maybe V is the same sort of class of functions. But we want to say a little more about V, okay? So V, consists of all functions w such that, w And 0 equals 0. Okay? So w, also satisfies Dirichlet boundary condition except, that it is homogenous. Okay? It is a homogeneous Dirichlet boundary condition. Okay? If we had a we, if we were considering a boundary value problem, with two Dirichlet boundary conditions on u, at x equals 0 and x equals L. We would likewise, have w at L also, equal to 0, okay? Note that so far, w is something we've cooked up, we've just, we've just conjured it up. So, we are allowed to say, what we want about w. Okay? All right. So, let me read what we have, so far. We're not yet done. Find u belonging to S, where S is that, given u not t f of x and, and the constitutive relation, sigma equals E u comma x such that, for all w belonging to V, for V is as specified the following holds now.  What holds? Integral 0 to L. W comma x, sigma d x Equals integral 0 to L w f d x plus W at L. T, all right? Essentially, what I've done is, go to an integral form. This is our weak form, okay? I want to do, just one more thing here. Observe that I'm integrating over x, right? And x really x I mean, integrating over x going from 0 to L. Which is effectively our volume in one dimension, right? Now, recognizing that though we're working in one d, we're really thinking of problems that you know, the canonical problem that we are considering here is that of elasticity. Which actually has some cross sectional area, right? A, right? Which could potentially be the function of x, okay? In order to make connection, with what we're were going to do when we go to multiple dimensions, I want you to take the step of simply multiplying everything here, through by A, okay? Okay? Because what that does is A d x then, becomes A volume element, right? Okay? Likewise here. All right. And what we get here, is like a boundary force. 'Kay? This one made the, the ultimate transition to three dimensions completely seamless. Okay? All right. And, and, A is a lot to be a function of x. Okay, so, this really is our weak form of the equation. Okay? It's considerably different, if you don't have experience with this type of equation, it may look as though it bears no relation at all, to what we started with. Right? To the strong form. It does bear a relation, right? In fact, we're going to show in a short time that it is completely equivalent to the strong form. This weak form by the way, is the basis for approximations that leads to the finite element method. It's a, it is, it is it is a fundamental aspect of a class of approximation, or a, or a class of methods, which are called variational methods

\section*{ID: WKdrBI8Lw9M}
Okay? So let me, let me just tape that. The, the weak form is the basis of the finite element method. And because this is sort of the first time we're writing the term, I will capitalize it. Okay? It's the basis of the Finite Element Method and of other variationally based numerical methods. The term variational may be new to you. You will, before too long know what that means. Okay? Just take it from now for now. Okay? So this is doing the basis of the finite element method. Okay? The very first thing I'm going to do is demonstrate to you, the equivalence between the strong form and the weak form. Okay? So let me state that here. The claim is that the strong form, the strong form and weak form are equivalent. Okay? What that means is that one implies the other. So the strong form implies the weak form, and importantly the weak form implies the strong form. Okay? So let me, I'll write that out, strong form implies the weak form and importantly, the weak form also implies the strong form. Okay? We're going to set out to show this and it turns out that the first step is actually quite easy to show. The first step being the strong form implying the weak form. Okay? So we'll take that approach. Okay? So, consider the strong form. All right? We know the strong forms, I'm not going to write it out in great detail. I'm only going to star write out the essential ingredients of it, right? And the most essential ingredient is the PDE itself, right? So that is d dx of sigma plus f equals 0 in 0,L with the boundary conditions, B.C. for short. Now, the boundary conditions we have are the following, U at 0 equals U not, right? Now because we've, we've written out the weak form for a single Dirichlet boundary condition, only at x equals zero, that's what we have to assume for the strong form as well, right? We need to consider that strong from, we can't have different boundary conditions in the strong form and expect it to lead to the weak form that we showed. Okay? So the boundary condition at x equals L is sigma at L equals t. Right? And we need to remind ourselves that we also have the constitutive relation, right? Sigma equals EU, x. Okay? So, this is where we start out. What we are going to demonstrate now is that this leads to the weak form. And to do that, we will proceed as follows. What we are going to do, is introduce W, just like we did back there, right? W belonging to V, right? The space V, which consists of all functions W, such that W at 0 equals 0. All right? As we discussed previously, W satisfies a homogeneous Dirichlet boundary condition, right? Any place we have a Dirichlet boundary condition, W must satisfy a homogeneous Dirichlet boundary condition. Now, what we will do is the following, we're going to treat W as a waiting function, right? In fact W does get called In the context that we are developing here, W is indeed called the waiting function. Okay? And I'm going to use it indeed like a weighting function. I'm going to multiply W in to our strong form. Okay? So what I'm doing here is multiply strong form by W, and now integrate. Right? And integrate over the interval 0 to L. Okay? So integrate 0 to L, 0 to L. The right hand side, of course, stays equal to 0. Okay? Also, because, we have this we introduce the area, right? So I'm going to sneak the area also into this, multiplication here. Okay? And then I integrated it over the domain of interest, okay? Because the right hand side of zero, of course, it stays at 0. Okay? So, we have this form and now this would work. Okay? But really we haven't done anything special. What we are going to do next is use a very common technique in calculus, in integral and differential calculus, which is called integration by parts. Okay? So I'm going to say that here, integrate, by, parts. Now, in integrate by parts, what we're going to do is act upon that derivative. All right? When we do this, we get the following form. Right? When we apply integration by parts we get minus integral 0 to L, W, x sigma Adx plus w sigma A evaluated at 0 and L plus integral 0 to L, WAfdx equals 0. Okay? And integrating by parts, if it's a step that's probably familiar to all of you, we have done that. Okay? That is the common integration by parts step. Okay? And if this is something that doesn't seem terribly familiar to you, recall that integration by parts involves two steps, right? It involves, the product rule of differentiation. Right? And what is referred to in higher dimensions as the Gals divergent theorem, but in lower dimensions, it is simply the fundamental theorem of calculus. Right? All right, it's not a difficult exercise to do, it's actually quite trivial. Okay, so, we have this form now. Let's focus attention upon that term. Okay? All right? Let me re-write this, this follows let me write it now as let me do two things. Let me move this stone to the other side of the equation. Okay, right? But then let me write the other side of the equation first. So effectively what I'm getting here is the following. I get integral 0 to L W, x sigma A dx equals integral 0 to L, WfAdx plus W sigma A at 0 and L. Okay? All right? Okay, let me go to the next slide and simplify it. For doing that, let me just go back here for a second. If you look at the very last term of the last equation on this slide, we observe that it involves that term being evaluated at two limits, 0 and l. Okay? And so that gives us integral 0 to l, W, x sigma Adx equals integral 0 to L WfAdx plus W at L sigma at L A minus W at 0 sigma at 0 A. I simply expanded out that last term. Okay? Now, if we stare at this very carefully, one should be able to say something special about this very last term here. What can we say? Think about it for a few seconds. In particular what do we know about W at 0? Remember we have a homogeneous Dirichlet boundary condition on the waiting function. Right? So that factor is equal to 0. Okay? Which gives us then our final weak form, which when we also observe that in addition, we know something about sigma at L. Okay? We choose that sigma at L is equal to t. Okay? The fact that w and zero is equal to zero comes from the homogeneous Dirichlet boundary recondition on W. Okay? D-I-R is short Dirichlet. The fact that equals t comes from the Neumann boundary condition. Right? On the strong form. Okay? So, when we do all of this we get our final weak form. Integral 0 to L, W, x sigma Adx equals integral 0 to L WfAdx plus w at L t times a. Okay? This is our weak form of the linear partial differential equation of elliptic form in one dimension. Okay? And you observed that we started with the strong form and essentially obtained this weak form. Okay? It is indeed the weak form we wrote out because W, with this waiting function that we'd introduced in the weak form, does indeed satisfy all the conditions we'd assumed of it when we specified the weak form. In particular we have this homogeneous boundary condition of W. Right? And the fact that we started from the strong form, allowed us to bring in the Neumann boundary condition on sigma. Right? Okay. We'll stop the segment here.

\section*{ID: 7SUuQQxW2y8}
Okay, we'll continue. In the previous segment, we did two things. We introduced the weak form of the partial differential equation for linear elliptic problems in one dimension. And we also did took the important step of demonstrating that the strong form of the same equation actually leads to this weak form, okay? So we demonstrated the equivalence between the strong and weak forms in one direction by first considering the form, strong form in demonstrating that it does, indeed lead, lead to the weak form that we'd specified. We are going to complete that exercise now by starting with the weak form and demonstrating that it too does lead to the strong form. Okay, so, we do the following, right so. What we're attempting to do here is show that the weak form Implies the strong form. Okay? Well, we start with the weak form then. And again, I would not write the weak form in complete gory detail except to except for the important components, okay? So the, so the weak form is the following, right. It's find u belonging to S which satisfies our Dirichlet boundary condition. Such that. You know, given all the data, and I'm not going to specify the data again in this in, in writing it here, okay? So assume that we have all the data. We have our knowledge about the boundary conditions, and constitutive relations, okay. So find u belonging to S such that for all w belonging to V, right, all waiting functions, w belonging to the space V, which consists of functions w satisfying the homogeneous Dirichlet boundary condition. Okay? The following equation holds. Integral 0 to L, w, x sigma A dx equals integral 0 to L, w A f dx plus w at L, sigma at L A. That is our weak form. All right, so we start from here and we want to get to the strong form. The approach that we take is essentially to step backwards, relative to the approach we took in the previous segment, okay? So, that means we apply integration by parts to this term. Okay? And the reason that you may already see that that should be the approach to take is that you recall that integration by parts essentially transfers derivatives from one field to another, right? And so we want to transfer the derivative from this, from w comma x onto sigma. Okay? That's the approach that we want to take here. All right, so, let's get going. Let's do that now. When apply integration by parts to that term, recalling again that integration by parts is just a combination of the product rule of differentiation and the fundamental theorem of calculus. When one is working in 1D, we get the following. We get minus integral 0 to L, w sigma comma x, which you remember this as d sigma dx, right. A dx plus w comma X, sorry it's not w comma x here. Plus w sigma A evaluated at the limits 0 and L equals, everything on the right-hand side remains the same. Because there are no, essentially because there are no derivatives on the right-hand side that we want to transfer on to other, other functions. Okay. In the next step, we are going to split out this term. Okay. We're going to split out this term, okay, into the two limits that it implies. So we get minus integral 0 to L, w sigma comma x, A dx plus w at L, sigma at L A minus w at 0, sigma at 0, A. All right, and you observe that that is essentially what we've done here, right? We've split out w sigma A, evaluated at 0 and L into the two limits. All of this is equal to, integral 0 to L w A f dx plus w at L, sigma at L, A. Okay? And now since you are already getting to be quite expert at invoking our conditions on the problem, especially our boundary conditions, it should be pretty obvious what to do in the next step, right. Think about it for a couple of seconds if you don't already know it. Paying particular focus upon the term that, I've applied an upper brace, bracket to. Right. So, what we do here is to first observe that, that goes to zero, right? And that is so because w does indeed belong to the space V, right? And we have that homogeneous Dirichlet boundary condition upon it. So that last term disappears, okay. And what I'm going to do now, is to essentially combine what remains into integrals, right, here and there, and to terms multiplying w of L here and there, okay? When I do this, I'm giving the following, I, I arrive at the following form. When I combine the integrals, right, because those integrals are over the same domain I have the following, I have w times minus sigma, x minus f. Because what I've done here, is to bring f, which was on the right-hand side in the previous equation, right? In the last line of the previous slide, right? The term, the integral involving f, I've brought to the left-hand side, okay? So, that flips its sign. I have A d x. And I'm simply going to bring everything over to the left-hand side here, okay? So, I have w at L, let me keep A here. Multiplying sigma at L, this was all the stuff on the left-hand side, and the term that I need to bring over from the right-hand side is the minus t, okay? All of this equals 0, okay? Let me just remind ourselves of what we had here. Oh, I, I noticed that when I, when I wrote this Weak Form I actually, sort of, jumped ahead of myself by writing something that I ought to have done a little differently. The sigma at L in the Weak Form, actually appears as t, right? So, I need to correct all the sigma at Ls, and replace them with t, right? So, and, that is the Weak Form. I was jumping ahead of myself by already implying the Neumann boundary conditions, but that is not how we write the Weak Form. That is how we write the Weak Form, okay? So, now, things are, things, everything's all right. Now, when I come back here, I do indeed, have, in this very last te, term sigma minus t, sig my tail minus t, okay? And this is equal to 0. Now, let's recall the conditions under which this holds, okay? This holds For all w belonging to our space V, which consists of functions w such that, w and 0 equals 0, okay. So, the statement here, the mathematical statement is that, that equation in the first line of this slide holds for all w, long as w vanishes at x equals 0, okay? Now, in particular, I'm going to claim that if this holds, the, the condition I'm going to use is that if this holds, I can consider a particular, right? So, what this implies is that it also holds for w of x equals phi of x times minus sigma, x minus f, 'kay? Where, phi of x is greater than 0 for x belonging to the open interval 0, L, okay? And phi of x equals 0 at x equals the points 0 and L, okay? So, really what we're talking about is a function of this type. If this is 0, and that is L, right? And this is our x-axis. The function phi is one that, that looks something like this, right? It may be as smooth as this, or it may be something else, 'kay? Doesn't matter, right? It's positive, right? And it right, it's positive, and it vanishes at 0 and L, okay? What this does then, is two things, right? What this implies for us is that then, it ensures that therefore, w at L equals 0, right? It also ensures Dirichlet boundary condition that w needs to satisfy, right? The fact that w has to vanish at 0 comes from the space d, right? That's, that's the condition that we're assuming on the waiting function. In addition, we've assumed a very special one, right? It's not like we've assumed them, we're saying that because this condition has to hold from all w belonging to phi. It also holds that this particular form of w, right? This one, where phi has the form that we've assumed here, okay? Well, what is that do for us now? Because we've taken a form that makes sure that w at L goes to 0, the second term in this expression here for the Weak Form vanishes, okay? That leaves us with integral 0 to L. Now, for w, we have phi of x times minus sigma, x minus f, right? All of this is our W. But then, this multiplied again, minus sigma, x minus f d x equals 0. The other term involving the boundary terms vanished by our choice of phi, okay? So, what we get from here, is now integral 0 to L phi of x minus, well, minus sigma, x minus f. The whole square d x equals 0, and I realize I'm missing an A here. Okay? Well, but if this is the case, what do we know here? We know that phi, x is greater than, or equal to 0, everywhere, right? Or 0 to L, right? In fact, its, in fact, over the, over the domain, over the interior of the domain, right? This term is indeed equal to 0. Sorry, over the interior of the domain, this term is indeed, created in 0, okay? This being the square of an expression, is also greater than, or equal to 0, okay? So, the only way this whole expression can be equal to 0, right? As an integral over the domain, is if, minus sigma, x minus f is itself equal to 0, right? In the open integral of interest 0, L, okay? But this, of course, is what? This is nothing but our, our, our the p d e, which goes into a strong form, okay? This is simply, the p d e in In the strong form. Okay? Let's continue then. So, so, what we demonstrated that having when we, when we considered the Weak form. It, since it holds for all of w, it also holds for a certain choice of w which reveals to us that particular choice of w reveals to us that the pde of the Strong form must hold. Okay? Let's continue with that exercise now. So now, let's return to this other form that we sort of massaged our weak form into, and that was integral from 0 to L, W minus sigma comma x minus f, Adx plus w with L A sigma at L minus t equals 0. Okay, for all w belonging to v. Okay, now what we've demonstrated just above in the slide, by a particular choice of w. What we've demonstrated by choosing a particular w? Is that this term actually vanishes. The pde of our Strong form must hold. That leaves us with only this last term to worry about. Okay. Okay, and this has to hold for all w belonging to v. So, in particular. It also holds. For a w of x, which is such that w at 0 equals 0, which is required by our specification of the space v, and w at L, not equal to 0, right? It also has to hold for this particular choice of w. Well, if that is the case, the only way the condition at the top of the slide can hold is, if sigma at L minus t equals 0. Okay? But after all this is nothing but the Neumann boundary condition of the Strong form. Okay so we've done two things here, we've demonstrated by starting from the Weak form that, since the Weak form has to hold for all waiting functions belong to our space v it also must hold for certain special choices of w. And from the special choices of w, we are able to demonstrate that one, the pde must hold, right? And secondly, the Neumann boundary condition must hold. Right? The pde of the strong form and the Neumann boundary condition of the strong form. It would appear that we left out one component of the Strong form here. What about the Dirichlet boundary condition on the Strong form? Where did that go? How do we make sure that that is satisfied? Think about it for a couple seconds. My claim is that it is already satisfied. Okay, so it is indeed already satisfied. That the Dirichlet boundary condition on the Strong form is already satisfied, simply from the fact that we have our Weak form here, right? On this slide, at the top of the slide, on the second line of the slide you will observe, that when I stated the Weak form, I said that we wanted to find u, belonging to S, where the space S consists of all functions that actually satisfy the specified Dirichlet boundary condition, right? Which, for this, boundary value problem that we're considering to fix ideas, and to develop our approach. The Dirichlet boundary condition in this case is that u vanishes at x equals 0. This condition, right? It's already in our Weak form. Okay? So our Weak form already has that condition, right? Let me just state that. Weak form already has the Dirichlet boundary condition on u. Okay. And that comes about because we say that u belongs to space S which of consists of all the functions u such that u at 0 equals u not. Okay the Dirichlet boundary condition so to speak is built into our Weak form. Okay? So, we've demonstrated that the Weak form implies our pde of the Strong form, it implies the Neumann boundary condition of the Strong form and it already contains the Dirichlet boundary condition of the Strong form, right? That essentially completes it, okay. So what this implies is that, the Weak form implies the pde, Neumann boundary conditions of the Strong form, and contains. Dirichlet boundary condition. Okay. So, indeed, the Weak form is completely equivalent to the Strong form. Okay, so what we demonstrated in the segment before this is that the Strong Form implies the Weak form. And we did this by starting out with the Strong form, multiplying it by the weighting function, and integrating by parts. What we've done here is start out with the Weak form, pose certain arguments makes, make the observation that since the Weak form holds for all weighting functions w belonging to the space v, it also hol, holds for certain specific forms. Right? And for it to hold those specific forms, what we've demonstrated is that certain conditions must hold. In particular, we demonstrated that the pde must hold. Right, the Strong form of the pde must hold. Our Neumann boundary must hold and the Dirichlet boundary condition is built in. So, we have that the Weak form implies the Strong form, okay? All right, so we'll stop here for this segment.

\section*{ID: EN3kzZKhctc}
Hi, I'm Greg Teichert. I'm a Ph.D student who's working on Finite Elements with Krishna Garikipati, and for the next few segments I'll be introducing you to programming in C ++. Now this isn't meant to be comprehensive overview of C++, it's really just to give you the basics, the basic understanding so that you're able to do the coding assignments that are associated with this course. All right, and so let's begin. I'll write down an overview of what we'll be talking about in this first segment. First, I will talk about how to run your code, your C++ code using CMake. Then we'll move onto talking about the basic structure of a C++ program. And from there, we will move on to talk about the basic number types, or a few of the basic number types. In C++. And then we'll move on and talk about C++ vector, a standard vector. All right, so when we're running the code using the deal.II finite element library, we'll be using a building system, a build system called CMake. And CMake takes your C++ code and it creates a Makefile which will then be used to run the code. So we'll move over here to the desktop screen. And you can see here I have two files. I have this CMakeLists.txt and main.cc. .cc is the file extension that specifies that this is a C++ source code file. The CMakeList.txt is what CMake uses to identify the source code. All right, so this is the file that we'll include with the coding templates, with your assignments. It's also in with, there's also a CMakeList.txt file included with all of the deal.II examples. All right. So here on line number 6, it says set target and a high of, in parentheses, main. So that specifies the name of the file of this first code that we'll be looking at. That's main, of course. main.cc. The .cc is specified here in line 10 as the file extension. All right? If your source code had another file name, you'd simply change that here. For example, for the examples in deal.II, they're all labeled step-1 or step-2 or so on. But I'll leave it as main for now. And from there, in my terminal, I'll go to the directory where those files are held. So they're on my Desktop in a file called Example, okay? If I list of contents of that file, you can see there are CMakeLists.txt as well as main.cc. Okay? Now, the very first time we want to run the code, we need to run CMake. And CMake will create the Makefile that we need. So, I type in CMake and then CMakeLists.txt. And there, if I open up the folder again, you can see it's created several different files, one of which is this Makefile. Okay, now we only have to do this once. The only time we need to run CMake again is if you needed a new Makefile. All right, so from here on we'll refer to the Makefile. And the Makefile will link to the source code. And we type in make. Make builds the executable. If we want to run it as well, we type in make run. Okay, and there you can see it. Built the target and ran the target. Okay? So that's the way that we are going to be writing our C++ code both in these tutorials in the beginning and also for the templates for your assignments. All right? Now we'll move on to discussing the basic structure of the C++ program. So I'll open up this main.cc file, and you can see it's pretty sparse, there's not much in it. And these are the basics of what you actually need in your C++ program. And this actually is in the same structure as a C++ function. And we'll talk more about functions later on in one of the following segments. You can see here each function has a name, in this case the name is main. It has, or can have, inputs. The inputs would come here between the parentheses. As you can see, this function main doesn't have any inputs. The outputs are described right before the function name. In this case it's int. Int stands for integer. So that means this function main, doesn't need any inputs but it has one output, an integer, Now down at the bottom it says return 0. Here we're specifying what specific integer we'll be returning. And we return 0 to let the computer know that the program ran correctly. For example, if you had come across an error you might do return 1 and so the operating system would know that you had come across a processing error and that was why you were terminating the program. Okay? So that's the basic structure of the C ++ program. Of course, it's not doing anything. And in order to add some more functionality, we add header files. Header files include additional functions and data types that we can use. So let's add one of those and we add header files using \#include. And I'm going to add a file called iostream and iostream allows you to print statements to the terminal, to the screen. It also allows you to input statements from the terminal. So for now, I will just be printing a statement to the screen. One of the functions included in IOstream is cout. I do this standard std::cout to specify that cout is a standard C++ function that we're using. Okay. And so we want to print out to this screen. I'll print out the canonical example, Hello, world! Okay, the quotes specify that it's a string that we're printing out words. And standard endline, endline, just ends the line on the screen. Get carriage return, okay. So, we save that. Again, we don't need to run CMake again, because. We are still using the same make file even though the source code itself has changed. All right, so I'll type in make run, and there you can see on the screen we have, hello, world, okay. Now most likely we want our program to do more than just print words to the screen, right. We will be using numbers of various types, and so let me talk about what some of those types are. I won't be discussing all of them, but just a few. Okay. So there are actually mostly just three types of numbers that we'll be using in the templates. The first is int and we solved that already in the function. We were returning an int. An int is an integer. Okay. So that zero one two minus one minus two and so on. The second number type that we use quite a bit is an unsigned int. And an unsigned int is just what you'd expect. It's an integer without a sign, so it's non-negative, we can't store sin with that. The third number type that we use quite frequently, is called a double. A double stores real numbers, and I'm not going to get into the specifics of how it's storing these numbers. I will explain though, that it's called a double because there's actually another number type called a float. Which also stores real numbers. A float uses 32 bits per real number. And can you guess how much a double uses? 64 bits. OK, double is short for double precision. We use more memory to store the number so we can get a greater precision with those numbers, all right? So, let me show you here in the source code how we would declare these integers and real numbers, okay? So for an int, I type int. We have to let the program know how much memory to store for each of these variables that we're declaring here, okay? So I can declare int a, and you notice I put a semicolon at the end there. I can also declare multiple ints at the same time, separated by a comma. I can specify the value while I'm declaring it. So, I could do d equals 1, e equals 3, and maybe f without actually specifying anything there. Okay, so I can do all of that. Similar with an unsigned int. Specify a number there, and same with the doubles. 1.3, for example, i. Okay, so all of those are valid declarations of an integer, unsigned int or a double. Now there's one thing to be careful of, and this is a bug that can show up in, that will probably show up in your code at one point or another, it's certainly shown up in my code. And that's the fact that, okay we understand that an integer plus an integer equals an integer. Or an int times an int equals an int. Int minus and int equals an integer. In C++ an integer divided by an integer is also an integer. And so for example, if we print to the screen, 3 divided by 4. What would you expect it to print to the screen? We would or I would have expected it to print 0.75. Right? But if you look here. It printed to zero. That's because three is an integer, four is an integer. When you divide it, C++ wants an integer, and so it truncates any decimal values and takes only the integer in front of the decimal. Okay. So if you want C++ to recognize that three divided by four is .75, you need to put a decimal on either the three or the four or both. So no if we run this, you can see it prints to screen .75. Alright. So that is something to watch out for, too. So if you want your division, your quotient to give you a double, make sure that the numbers involved are also doubles. Okay. All right. So for the last topic in this segment, I want to talk about standard C++ vector. And these are very useful objects in C++. A vector, as you might expect from mathematics, is an array or a range of elements. However, as opposed to in mathematics where we talk about vectors and a vector would be an array of numbers. In C++ you can have a vector of any other C++ object, so you can have the vector of doubles, you can have the vector of integers, a vector of unsignance. You can have a vector of characters, of letters. Or you could even do a vector of vectors. So they're really quite handy, it's not just a mathematical object, but you can also think of it as a storage device. Now, over here on the, in our code, we'll look, but to use vectors, I actually have to include another header file. Okay. So I pound include vector. And when we declare a vector, we use standard vector, again because it is a standard C++ object. We have to declare what this vector will be storing in it. It can only store one data type, all right. So for the example here, I will store a vector of doubles. And again, as with integers and doubles and so on, there's more than one way to initialize a vector. So we can declare its name. Without saying anything about it. When we declare simply the name, then the vector has size of zero, there are no entries, there are no spots for any numbers, it has a length of zero. Okay. Let's declare another vector here. Make it a vector doubles as well. And here, I want to specify the size, and so in parenthesis I'll put three, for example. So now, we've declared a vector of size three, of length three. However, the doubles that are stored inside of it have a default value of zero. Now, if I want to make a vector of length three where all the entries are one, for example. I would again put in a three for the first entry and separated by a comma, I would put one. Okay. And so that will make every entry initialize as one. And as before, we can initialize more than one vector at a time Just like with doubles or integers. Okay? Now, a vector differs from the integers and the doubles in one way, in that a vector has several functions associated with it. And, okay, now I'm saying function but, again, function in C++ doesn't mean the same thing as a function in mathematics. We're not necessarily taking a number or a range of numbers, performing some mathematical operation to produce another number. A function in C++ performs some operation, but it's not necessarily a numerical operation, okay. So, one common function for vectors is this resize function. So, when I declared vec1, it was empty. I want to resize it, so it now can contain five elements. Okay? So now let me demonstrate another function. If you want to seal it as the length of the vector, you can do vec1.size. And I'll print that to screen, and we'll see that. It should print out five, which is the length of vec1, okay? Now we can access any component of the vector using square brackets. So let's look at the first component. Notice indices in C++ start at zero, all right? So vec1 should print out the first element of the vector vec1, okay? Okay, so here on our screen, we printed out the size, which is five and the first element, which is zero, which is what we'd expect. Because when we resized the vector, we didn't specify the values, and so the default value was zero. We can also resize vec2 and vec3. Even though they've already been initialized, we can resize those. Another function that is Is handy is the push\-back function. So right now, vec1 has a size of five, and all the elements in vec1 are zero. Now let's say I want to add another component to the end of vec1. And I don't want it to be zero, I want it to be, for example, three, maybe 3.2. Okay. So this adds another element on to the end of vec1. And so what that does. Is it increases the size by one, and we'll be able to see that here. As I print this to screen, okay? So the original size was five. The first component is 0. The size after column pushback is now six. We've added another element onto that vector. And element contains the value 3.2. All right, now there are many other functions related to vector and one valuable resource for looking at these functions, looking at other header files in C++ and even a very informative tutorial series is found on the website, cplusplus.com. I'll write that here. Plus is spelled out. And so if you have further questions on something we've talked about it in this segment or any of the future segments You can refer to cplusplus.com and, in their search, you can search, for example, vector or double or any of these other data objects or functions that we're talking about. All right, in the next segment, we'll move on to talking about conditional statements and for loops and what the scope in C++ means.

\section*{ID: tDeMawGbov4}
Hi, in this segment we will be talking about using pointers in C++ and functions. All right. So for pointers, I'm going to talk a little bit, briefly, about how C++ is storing the values of these numbers that we're working with. Okay, I won't get into a lot of detail but I'll draw a quick picture here. So, for each, each number is stored in memory, all right? And there are two components, two aspects to each number. One is the value of the number, whether it's an integer or a double. And that's as we'd expect. However, the operating system needs to know where to look to retrieve that value when we are referring to that variable. And so we have an address as well. So each variable has an address, and it has a value. Okay, and just for simplicity I'm going to label the addresses 1, 2, 3, 4 and so on. Okay, so let's say that we have declared an integer a. And we're going to give it a value of 3. Okay? We can also declare what's called a pointer. And a pointer doesn't actually store a value. It stores an address. So here, I'm going to declare a pointer. We declare it with the asterisk. And I'll call it b. So the value of a pointer is the actually the address of another variable. Okay, so here I might store as the value of my pointer, the address of int a okay. So let's look at how that works in the code. I create an integer a, and I'm also going to create a pointer b. Now, b by itself at this point is uninitialized. It doesn't have a value. The way we initialize the value of a pointer Is through a none pointer element of the same type. Okay, so I will say that b is equal to the address of a. So if we want to get the address of a, we actually use the ampersand. So let me write that here. So the address of a = \&a. Okay? So I'll do that here. B is equal to \&a. So now we were to print to the screen, actually let me give me a a value. I'll say a = 4. Okay, now one other thing before I print to screen. If we want the value of a pointer or rather the value that a pointer points to, the value pointed to. We can use the asterisk. So *b, would give you in this case, that I've shown here, 3. Okay? And so here I've simply set b to store the address of a and yet when I print out the value of b, it will give me the value stored at the address that it contains. So it should print out three, exactly. Now, what happens if I change the value of a? Say a is now equal to 4. What will that do to b? Will b still be three or will it now be four? Let's look at that. You can see that the value pointed to by b has changed to four. Why? Because b is only storing the address. And so here, when we've changed the value of 3 to 4, b is still pointing to that address in the memory, okay? Now, while I've shown this demonstration with integers, you can actually have pointers of any other object in C++. You can have pointers of vectors, okay? So let me do that. Let's create a vector, standard vector. Of type double and I'll create a regular vector here, vec1 and I will create a pointer to a vector, vec2. And, let me initialize vec1 to have length 2 and values of 3.1 as an example. And in order to initialize vec2, again as before say vec2 is equal to the address using ampersand of vec1. All right. Now, vectors have functions, we've talked about before. All right, we can look at the size of the function. So how does that work with a pointer to a vector? Now, with a regular vector, with as is the case with deck one, we would just do .size. Right? And. As we can see here. Okay, gave me an error. Oh, this is important for you to recognize. Okay, I forgot my semicolon. And it gives a helpful hint here, the expected semicolon before return zero, okay. So now we should be able to run it. And again, I forgot to save it. So now we should be able to run it. Okay. So, it's given us the value of 2, the size of vec1. Now in order to do that same thing with the pointer, we actually don't use the dot anymore. We de-reference the vector. So, we, instead of looking at the reference that the pointer's pointing to, we look at the value. And we do that through this error, this arrow, excuse me, through the arrow. And then we can use the same function name, size. Okay, and now, as we run it, you can see it's printed the same size. We can, as before, if we change something with vec1. So I'm going to resize vec1 and maybe make it 3. And even though we only changed it on vec1, the same change goes throughout in vec2, because, again, it points to that address. And I have saved my program and so now I can run it. You can see they were both 2 2 in the beginning when we resized Vec 1. Both Vec 1 and Vec 2 have changed too, a size of 3. Okay. Now, I want to change gears a little bit and talk about iterators. So, with the for loop that we talked about before, we used an integer to increment from one element to another or to iterate the for loop. There's another way to do it and iterators are related to pointers, and so that's why I waited to talk about it until now. In the case of a vector, there's what's called an iterator, and an iterator is simply an object that points to a single element in an array of elements, or a range of elements. And it also knows where either the next element in that range is, or where the previous element in the range is, depending on which direction the iterator is going. Okay, and the reason I'm brining this up is because we do use an iterator In the coding assignments, and I'll point that out to you as we go over the template file. All right, and so I wanted you to understand what's going on there. Okay, so in this case I will create a vector, and I'll iterate over this vector with an a for loop, okay? So I'll create a standard vector of doubles, call it vec. And I'll do a for loop. So I will declare the iterator here. So it's a standard vector of doubles. So it's an iterator over a standard vector of doubles. All right. And I will call it, IT and so a vector, the vector class has an object that is a beginning iterator and an ending iterator. And so I'll set the sequel to the beginning iterator. And it's the function to retrieve that iterator is begin. Okay, and the conditional in this case won't be a less than, it'll actually be not equal to. So as long as the iterator is not equal to, so the not equal condition you'll use with the exclamation point and the equal sign. So if the iterator is not equal to vec.end. Then we will go through the for loop into iterate. We actually have the same syntax here, it++. Of course, in this case it's not mathematically adding one to the iterator, because the iterator isn't a number. It's just iterating from the current element in this range of elements to the next element. All right. And so just to demonstrate what's going on, I'll print to the screen. I'll print to the screen the values at each point. Okay. Actually I'll print to the screen the values of the index of the iterator. And so you can see this is similar to the pointer in that we're using this star, or *it to sort of de-reference the iterator, it gets some information out of the iterator, essentially. Okay, and so now if I run that. It gives us the value of the vector at each of those points. So we only had two components and so it gave us the first and second components. Just to illustrate what's actually going on I will change the value of the second component so vec I'll set equal to 4.3. So now as you run it you'll see that it's, it outputs. So it* points to the first element in the vector and it prints out 3.1. And then on the second loop, *it points to the second element in the vector, which has a value of 4.3. Okay. So, again, it's pointers and iterators aren't always the same thing, but they are related. And a pointer can be used as an iterator at times, and an iterator can act like a pointer. Okay, so let's end this segment here, and in the next segment we'll move on to discussing functions.

\section*{ID: n-7XJK26n5I}
Well hello, and welcome back. We'll continue with our development of finite element methods, and we're working, as you remember, with a one-dimensional linear elliptic PDE, right? And this zeta, eta, xi model for elasticity, for heat conduction, mass diffusion and so on. So where we are is that we've written out the strong form of this PDE. We've we then introduced the weak form and importantly we demonstrated that the, the strong form and the weak form are essentially completely equivalent. Okay, that's where, as far as we got in the previous segment. And that's where we are going to continue from today. And to get us started I am just going to write out the strong form and the weak form and use that as a point of departure, okay? So. So the strong and weak forms of 1D linear elliptic. PDEs, okay? That's where we are going to start off today. Okay, so I'm going to try and write them in a somewhat unified manner, so I'm going to first write out the data, okay? And that is, given u0 we are working with Neumann conditions, remember? Just in order to not keep things too general while we're developing things right now. Okay, so given u0, t, which is going to be our Neumann boundary precondition data we're given our force in data, f as a function of x and the constitutive relation. Sigma equals Eu,x, 'kay? This is what we're given. All right, so now, let's write out the strong form. Write out the strong form here. And I'm going to use the other side of this the other half of this screen to write out the weak form. Okay, and let me also do the following, okay, do that and that, okay, all right. So the strong form is find u such that d sigma/dx plus f equals 0 in the open integral, and we have the boundary conditions, u. So, u and 0 equals u naught. And, sigma at L equals t. That is our strong form. The weak form is again, given the data find u belonging to our space S, which as you recall from our discussion, the previous segment incorporates the Dirichlet boundary condition. Okay, find u belonging to S such that for all waiting functions w belonging to another space, V, which is characterized by the requirement that all functions in V must satisfy the homogeneous Dirichlet boundary condition, okay? So find u belong to S such that for all w belonging to V, the following holds, right? And, and the important thing is that the weak form is an integral form, right? Integral 0 to L, w,x sigma dx. And in order to be able to make connections later on with multiple dimensions, I'm introducing an area of that, right? A, this is equal to integral 0 to L, wfAdx plus W at LtA, all right, so there we have it. Our strong form and weak form. Now, when we look at this, you remember also that importantly these two are completely equivalent, right? So let me write that here actually. I don't need to use all of this, so let me make this line a little shorter so I can write. I can write at the bottom of it, okay? And recall, importantly, that we demonstrated last time. Recall that the strong form. Is completely equivalent to the weak form. Okay? Now, if the strong form and weak form are completely equivalent you realize that in this sort of presentation, we have not yet done anything that may help us solve the PDE in an, in an easier manner, right? In, in, in, in essence, both these forms, the strong form and weak form are, so to speak, exact statements of the problem. And if you were to try and solve the problem in, in its exact form, there is no reason why either of these forms the strong form and the weak form, should present an easier task. A task can become easier. However, if we resort to approximations, if you choose to work with approximations of the strong form, you would be headed into something like a finite difference method where the obvious approach, would be to represent that derivative, and any other derivative that appears with the difference formulas, okay, and therefore finite differences. With finite elements we take a different approach, we will work off the weak form and introduce approximations there, okay? So the way we do this is is the following. So. Let me state here that the finite element method. Is based on an approximate version Of the weak form. Okay? And in order to understand what sort of approximations we have let me state that when we say that u belongs to s and w belongs to v. What we have in mind here are function spaces that we describe as being infinite dimensional function spaces. Okay? These are infinite-dimensional function spaces. And I'm going to tell you what that means. Okay. Infinite-dimensional is important. What this means is the following. If you are thinking of these spaces s and v as being maybe polynomials, right? We say they're infinite-dimensional in the sense that at this point, we are considering polynomials of all orders. Okay? So, you'd be considering the, constant, linears, quadratics, cubics, quartex, and so on, right? All the way up to infinity, right? What, and, and therefore infinite-dimensional, because we think of each order of polynomial as being a dimension of the space of polynomials. So let me give that to you as an example. So example, if s comma v, are. Let me put it this way. Let me use mathematical notation. So, s comma v belong to the space of polynomials of order n on x, right, where v denotes polynomials. We have in mind that n equals 0,1,2, and so on, okay. Where P denotes Pn denotes polynomials of order n. So it's in the sense that they're infinite-dimensional. Now the problem with infinite-dimensional spaces is that we're really looking for solutions in, in, in a huge space. And it is, it is the fact that we're looking for solutions in such a large space, that makes our task difficult, when we restrict ourselves to the exact statement of the problem. The idea of developing approximation methods based upon the weak form is to say that well, let's make our lives a little easier by restricting the dimensionality of the space in which we are looking for solutions. Okay? So what we're going to do here is construct approximations. In what we would call, you know, instead of infinite-dimensional, we're actually going to look at finite-dimensional spaces. Construct approximations in finite-dimensional. Function spaces. Okay, we construct approximations in finite-dimensional function spaces. All right, and example. Of course an example would be, would be to say that we construct for approximations in Pn of x, where n equals, maybe we want to say maybe we just have zero and one. Ok, and this would mean that we are considering approximations in polynomials of up to first order, right, up to linear polynomials. Okay? So this is the approach we take and, and as you can imagine now this, this is, this is likely to make our life a little easier. Because we're saying straight away that, yes I know that this the problem of interest as driven by the data that we have, is, you know, is likely to have polynomial solutions of some arbitrary order. But I'm going to try and approximate that those polynomials of arbitrary order with linear polynomials. Okay? And because it, it, because of the properties of these function spaces those approximations would be good or bad. And actually it is, it is it is addressing that question that financial element error analysis is occupied with. Okay but, we'll get to that later. All right, so, so, the way we formalize this is to do the following, okay. So, we restrict, okay, we restrict the solution space. All right, and importantly we need to restrict the space of weighting functions as well. Restrict the solutions space and weighting function space. And the solution space and the weighting function space as well. Okay, this is what we're aiming to do. And the way we write this formally is the following. We say now, now we're working with the weak form only. Okay, we say the following, we say want to find, first of all, we want to find not u itself any longer. Because we've already given up on the prospect of finding the exact solution. And u denotes the exact solution for us. Okay? So we want to say that we want to find an approximate solution, and we are going to denote the approximate solution by u sup h. That's not u, u raised to the power of h but u sup h. Okay, so you want to find u h function of x, which now belongs to a function space. But this function space is a finite dimension of function space, okay it's also an approximation. And that too is denoted by h, I'll tell you in a little bit why h. Anyway, we do the sketch. All right. So we want to find belonging to Sh. However, we want to allow Sh to lie only within the larger space of, of solutions, which also contains the exact solution. Okay? So we want Sh to be a subset of S, okay? Now, apart from that, we say that Sh consists of all function. Now we want to say something more about right? And actually, about hs itself and one of typical ways in which to do this in the context of finite elements is to say something to make a statement about the so, so to speak the regularity of our function space. Okay? I'm going to use certain notation and we'll come back and clarify this notation in a little bit. Okay? The way we'd introduce this notion of regularity is to say that belongs to a space that I am going to denote as H1 on 0, L. Okay? Apart from the fact that it belongs to the space H1, I'm going to tell you what H1 is in just a little bit. Part from the fact that belongs to H1 we do require that it satisfies the Dirichlet boundary condition. Okay? Right? So straight away, we've gone to this finite dimensional space Sh. And not only have we done that, but we've also said something more about we've, we've set ourselves up to say something more about what the space Sh is like. Okay? All right. So you want to find living in, in this finite dimensional space Sh such that. For all wh, okay? Again, the weighting functions, we are not, we are no longer pulling out of this big space V. We're taking this also from a reduced dimensional space, a finite dimensional space. Wherever we like for S, we do want Vh to be a subset of our original space V. Okay? And Vh, we will tend to say consists of all functions wh. Also belonging to the space, H1 on 0, L. Such that wh at the richly boundary Vanishes, okay? All right. So, at this point, this is purely cosmetic. Right? All all we've really done is to, is just to say that okay, we're thinking of finite dimensional spaces. I've introduced some cryptic notation by talking of H1 and I haven't yet told you what H1 is, but I will. And furthermore, I'm denoting all our approximations, all our finite dimensional functions and finite dimensional spaces by this sop H. Okay. So we're doing this, what do we want? We, we want that the weak form still be satisfied, except that now the weak form is an approximate weak form. It is a finite dimensional weak form. Okay? And it's finite dimensional, because it is computed with these finite dimensional functions. Okay? The first stone looks just as before, except that all functions are evaluated with wh, sorry, not with wh, but with the finite dimensional law versions. Okay. On the right-hand side, we have wh times f A dx plus wh at L t A. Okay? This is it. This constitutes our finite dimensional, our finite dimensional weak form. Now because in the mathematical setting this manner of introducing finite dimensional spaces is ascribed to a Russian mathematician called Galerkin. This is also called the Galerkin weak form. Okay? I'll, I will tell you in a little bit, what H is. Not right away in this segment, but in a little bit. H, H as you can ima, as you may imagine or as some of you may know bears a relation to to our finite elements when we introduced it. Okay? So we'll talk about that in a little bit. Let me see. Is there anything else I need to say about this right now? Oh, I believe not. So, so this is essentially the form in which we are this is essentially how we're going to proceed. All we've done here to see that is to observe is that when we wrote out the weak form originally the one that's completely equivalent to the strong form. We had in the mind that and we had in mind that infinite dimension of function spaces. And we recognized that can prove to be that can prove to be challenging to find solutions in, because we're looking for solutions in such large spaces. Instead, we now restrict ourselves to finite dimensional space, then saying that, that right there lies our approximation. Okay? However, we are still going to solve the weak form as, as written. Importantly as well, our finite dimensions spaces that we are, that we are setting ourselves up to use are subsets of our original larger spaces. Okay? Okay? These are subsets of our original, larger spaces. Okay. This is a good point to stop for this segment. When we come back, we will expand upon these ideas. All right?

\section*{ID: NGFDgAV86uo}
So there was a question about use of the finite dimensional weak form. And specifically, the question was now that we're talking about using a finite dimensional weak form is it still equivalent to the strong form. The short answer is no and if that causes panic, don't panic. Because after all, we are talking of approximations. All right? So, if we were sticking with something that were completely equivalent to the strong form, we would not have moved on at all from an exact statement of the problem. Okay? And we know that an exact statement of the problem implies an exact solution, which is often difficult to find. The whole business of finite element methods or any other approximate method is to look for approximate solutions. And the way we approach the approximate solutions is by freeing ourselves so to speak, from the, from the tyranny of exact solutions. Okay? There, there, there is more and, and, and, and let, let me state it here. So let me start out by saying that the, the finite dimensional weak form is not equivalent. To the strong form in general. Okay? Now qualify that in general, as well. I will just say that there is more. In order to understand what more there is let's, like, recall the way we proved the equivalence. Okay? So recall, recall the infinite dimensional Weak form. Okay? And I'm not going to write it out in full detail, but, but essentially recall that the way it goes is that we say, find u belonging to S, where we know what S is, such that for all w belonging to v. The following condition holds right, w comma x sigma Adx integral zero to L equals integral zero to L wfA dx plus w at LA. Right? And then we proved that this was equivalent to the strong form. In proving that it was equivalent to the strong form, we very critically used the fact that the weak form holds for all w belonging to v. This was what allowed us to then, see that. Well, if it holds for all w belonging to v, it also holds for a very special w. Right? And if, and for it to hold for that very special w, we realized that the implication was that the strong form had to hold. This le, leads to weak form being equivalent to the strong form. All infinite dimensional, right? And both of these are infinite dimensional. Infinite dimensional. Okay? We've abandoned that now, right? In what manner have we abandoned it? First of all, by saying that now we are looking for solutions belonging to a smaller space, Sh. And furthermore, that the finite dimension the weak form holds for certain weighting functions. Wh belonging to smaller space vh. Okay? Now we're seeing, we're seeing that belongs to Sh and wh belongs to vh, which is a subset of V. Okay? As a result, we, we, we have lost the ability to invoke the argument that the weak form holds for a sufficiently large space of functions. Okay? And therefore, we cannot always make the argument that the strong form is implied by the weak form. Okay? But that is the nature of our approximation, right? So let me just say, therefore the finite. Dimensional weak form does not imply, okay? Does not imply the does not imply the infinite dimensional strong form. Okay? It does not imply the infinite dimensional strong form in general. It could, however, imply the infinite dimensional strong form. If it turns out that the problem that we were looking at in any case, because of the data supplied to us. It did, indeed have solutions that lived in the smaller space, Sh. Okay? In that case, we are, it would work. And in fact, we will see that the finite element method has a very powerful property to make that work. We will see more of that when we understand the, the convergence properties of finite element methods. But for now in general, yes, the, the whole point is that we have chosen to adopt approximations and these approximations imply that our equivalence between the weak and strong forms is lost. But we do need to give up that the, the, the, the insistence upon that equivalence. Because otherwise, we would only be locked into only finding exact solutions.

\section*{ID: x8zxnXLhW0}
Okay. Welcome back. We'll continue with our fleshing out of these ideas of the of the finite dimensional weak form. In particular, what we are going to do is say a little more about these function spaces from which we are drawing the finite dimensional functions. Okay, so what we're going to do here is talk a little bit just a little bit not too much and not at all in a formal way but hopefully in a rigorous manner. We will talk about function spaces, all right? And in particular, the kind of functions spaces we are going to consider are called Hilbert spaces. In the branch of mathematics that is usually referred to as functional analysis, okay. So recall, the form in which we specified And Vh. Right, we said that Belongs to a space which consists of functions which are drawn from a group of functions that I denoted as H1. And apart from that, we said that Just needs to satisfy the boundary condition. What we are going to do in this segment is focus upon Focus upon an introduction to the ideas that are behind these sorts of function cases H1, 'kay? And in order to do that let's do the following. Supposing we consider, The function u which actually, let me not use u just to avoid any confusion with the either the exact solution or approximate solutions. Let me just see, it's considered function v, okay. Consider a function v which is a mapping from the, from the open interval 0,L to- The real space, okay? And if you recall, when we first started talking about the exact solution u this is exacly how, this is indeed how we wrote it, okay? So, If we have a function v I will define the following quantity, 'kay? I will define. I will define the function v to be an L2 function. If the following holds, okay? Supposing we take the function v and we square it, okay? We square the function and integrate over the domain zero to f, okay? Now, if this integral is less than infinity, okay? Then, we say that v belongs to L2 on, the open interval 0,L. Okay? And I realize that what I wrote finally is just sort of restating the first line. As I said define the function v to b and L to function if that is less than infinity. And then, I said, the n v belongs to delta theta, okay? But essentially what we are saying now is that the function squared integrated over the domain is bounded, okay? An alternate way of saying is that the function is squared into , okay? V is L2, if it is square integral, okay? Now, we're working in 1d so things are a little easy in 1d. Let's look at what is allowed in L2, okay? So examples. Okay? V of x equals to a constant is obviously allowed. V of x equals some polynomial, right? So we could look at it as v of x is equal to sum over k. Maybe a sub k, x to the power k, where k is now summed over 0 to n, okay? This is also allowed, v of x equals h of x minus x not where h denotes the heavy side function. Okay, so h of x minus x not is the heavy side function located at x not, 'kay? So this is the Right, so this, this function looks like this. If this is x and this is v and that is the point x not then v is that function, right? Okay, this is also acceptable. And why is the, for, for each of these functions you observe that if you squared them and integrate them, you will essentially end up with a constant, okay? You'll end up with a number which is in fact, which is, which is it's a real number. It's, it's there for , it's less than infinity so it, so the function is L2, okay? Can you think of a function that is not L2? Right, think about it. The delta function, right? The the dirac delta function is not L2. So all of these are L2, okay? If you have the dirac delta function v of x equals delta at x minus x not, okay? This, this function does not belong to L2 on 0,L, okay? Okay, that does not belong to L2. And why does it not belong to L2? For it to belong to L2, you need to be able to square v and integrate it but the square of a delta function is is vacuous, right? It's it's, it's it's meaningless, it's, it's not even defined so the function does not belong in L2, okay? So these are, this gives us some idea of what is implied by, by L2 essentially it is telling us that the function is square integrable 'kay? And it gives us a basic notion of the boundedness of a function importantly it tells us not just how the function behaves at, at in that a particular points over its domain. But it gives us a an, an integrated sense of the behavior of the function over the entire domain, 'kay? And this is the important property that the integral delivers for us, okay? Now in general, one can define Lp functions where p is some arbitrary real number actually, okay? But that gets a lot more complicated and, and involved, 'kay? But let me just state it, one can can in general define Lp functions. Right, where p itself well, p, p could be integers of and actually p could be real numbers. In fact, there even is a, a definition of an L infinity function, 'kay? But we, we don't need those details for our basic introduction to finite elements, 'kay? So, so that is, is one type of function that will be of use to us. And you observed in defining L to functions we just talked about the function itself. However, when I introduced the, the notion of finite dimensional function spaces and I refer to this space H1 which I'm going to define now. And that step will bring us to the notion of regularity of functions. And the notion of regularity again, is just a way to talk about how the function behaves with its derivatives, okay? So, let me just pose this question in a very colloquial manner, okay? So, how about control Over the derivatives of v, okay? And this is this whole notion of regularity. 'Kay? All right, and I, and I say control over, over the functions because now, you know, when, when we've said so far that the function itself is squared integrable, that's an L2 function. In a sense, we are saying that yeah, the function itself is sort of controlled in some manner, right? It doesn't, it's not, it doesn't get too big for instance, right? It doesn't get unboundedly big, okay? So we are controlling the function itself but we haven't said anything about it's derivatives. We're going to do that now, okay? So we say that v of x belongs to H1 on 0,L if our integral, 0 to L, v square plus v comma x the whole square. Now, we need to do something here. As you can imagine, when one gets into physical problems, right? Where v would mean something with physical significance, like it could be a displacement. It could be a temperature, it could be a charged field or something like that, right? A charged distribution or something like that. When we then take spatial gradients of such functions, we know that we, we're changing the units of the functions that we're dealing with, right? The physical units of these functions, so in order to count for that fact and make sure that both, both terms in this integral have the same units. For dimensional purposes, we multiply this by L square, where L is just the measure of the domain, okay? Okay, so we have this and now we're integrating this integrant over the domain zero to F, okay? So v belongs to H1 if the square of the function plus the square of the functions derivatives, 'kay? Or in this case the square of the functions first derivative when integrated over the domain remains bounded, okay? So let me introduce a remark and this just paraphrases what I said about introducing L. L has, or L squared in this case, has been introduced For dimensional purposes. Right? In general, If you don't want to write L squared or if you don't want to write L. It's common to write m for measure of 0,L this equals L, okay? Where m here is the measure of our domain, okay. In one dimension the measure of a domain is simply the length, okay? Well actually I, I need to say it instead of just being the measure it's gotta be the measure to the power one over d where d is the spacial dimension, 'kay? All right, so if you were doing, if you were in 1d, d would be equal to one. If we were in three dimensions, d would be three and then the measure would be actually the volume, right? But the volume to the power of 1 3rds would again give us the notion of it length.

\section*{ID: bdU1Bab05Po}
So if we were working in three dimensions, in all three, d equals 3. And then, the measure of our domain, okay. Our domain may be in gentle, be referred to as Omega, okay? Omega to the power of 1 over d would be measure of Omega to the power of 1 over 3. And, this would be a. Length. Okay. So this is how we would extend this sort of idea to multiple dimensions. Okay, but then let's get back to our H1 functions. So, v belongs to H1 on 0 comma L, if the integral of the function square plus now let me write it differently. Measure of 0 comma L square v comma x square, the whole thing, dx less than infinity. So we require that the function itself, and its first derivative squared integrated be bounded, and H1 there simply refers to the fact that we attempting here to control not only the function itself, but also its first derivative. Right. If we go on, and want to control also the second and higher order derivatives, then we would accordingly go to higher orders of H. Right, we go H2, H3, and so on. Okay, so now you observe that when we say that. Now, when we see that belongs to Sh, which where sh is equal to functions belonging to H1. When we say this, what we are saying is that is such that we expect that is such that the function itself be bounded, okay? Because it is square integrable, the function itself as well as its derivatives, okay? And as you can think, now as you can imagine now, we're saying something about, about the nature of the displacement field that we're expecting to find to, to find as our, our approximate solution. Okay? So, what we are saying here is that and comma x are. Square integrable. Okay, let's look at functions that are H1. Okay. Okay, so examples. Okay examples of H1 functions. Okay, so are the pcv of x equals constant, no problem, okay? If we say v of x equals sum k equals 0 to N, a sub k, x to the power k. Okay? This is also all right. There is a third type of function that I want to consider and I could write it out in form, int as it in, in the form that I've written out the first two functions here, but let me instead draw it. Okay? Let me instead sketch it, because it's, it, it, will still convey the same idea. Where as defining it would, would require us to define things that we will need only later. Okay, so let me suppose that v is of the following form linear, maybe quadratic, linear again, some other polynomial, linear again, constant, okay? This sort of function also belongs to H1 Okay. It belongs to each one because clearly, the function itself, if you square it and integrate it, is bounded. At points such as this, you observe that v comma x, okay? Is v comma x suffers a jump, okay. Those double brackets there indicate a jump in a function, okay? We know that there is a jump in the function there. Because if we look at the derivatives, right, right? We know that the derivatives are going to look like that or the derivatives are going to be, are essentially the tangents to the two sides of the curve, right? Those tangents are not the same right? And therefore we, we know that the function itself suffers a jump. However, these continuous functions can still be squared and integrated. Okay, in fact, we consider in exactly such a case in, in the case of L2 functions, right? We did consider the, the Discontinuous function itself, the Heaviside function. And, observe that, yes, to belongs to L2. So, Heaviside function, Discontinuous functions can be squared and integrated. That's what happens with b comma x when it comes to defining a 2v in H1, okay? Right. However, what about a function, which has this form? All right, let's do, let's do something that looks pretty much like what we have there. What about this function? Right, as we saw in the case above we see that yes here, the tangents are different, right? So there is a jump in v comma x. But out here, what we are seeing is that, v itself suffers a jump. Okay, and therefore, what happens when we consider v comma x? V comma x, we know goes as the a delta function, which would be called at this point x not, right. We know that d comma x is, is like e, delta of x minus x not. Okay? And that's where the square root of integ, integrability of the derivative of three breaks down. Okay? So in this case, we have v does not belong to H1. Okay. All right this is probably a good place to end this segment. We've essentially gone over the definition of our very basic function spaces, introduced these ideas that we, that we need here. Ideas of square integrability of the functions, and of their derivatives, and this idea that when we specify these functions to belong to certain spaces, these Hilbert spaces L2 and H1 by the way are Hilbert spaces. We gain control over the functions themselves, and their derivatives.

\section*{ID: 7N5YSuBZMZM}
Okay, welcome back. We continue now with working with the with the finite dimensional weak form or Galerkin weak form. What we saw in the last segment is was a digression to say more about what we meant by function spaces and to understand why this might be important for us, okay? Because and, and, and, you recall that we defined function spaces as being useful or, or we introduce function basis as being useful in order to give us a sense of control over the functions themselves and over their derivatives. Right? It's when we say that we have control over the functions we are saying that the functions are bounded. When we have control over the derivatives in addition to the functions we have this notion of regularity, right? That the functions are sort of smoothed and so on. And we saw that through examples. Okay, with that as background now we will actually launch into the finite element method for this 1D linear elliptic PDE. Okay? And this would be, an we are going to work off the Galerkin, or the finite dimensional weak form. In this segment, we are finally looking at the finite element method for linear elliptic PDE's in one dimension. >> Now, we're going to work off the Galerkin weak form. Okay? Recall the Galerkin of the finite dimensional weak form. One more time unto the breach. Okay, so one more time we're going to write this out. Find u h belonging to s h which is equal to s h is equal to functions u h. Now we're seeing that they belong to H1 on (0,L). Such that they satisfy our boundary condition. Now gratifyingly, we know what H 1 means. So, we already have an idea of the extent to which we've restricted our, search to solutions for this PDE. Okay? So recall all the functions we talked about in the previous segment. Those are the sorts of functions we are now, considering as candidates, okay? Okay. Actually, for this reason, u h is often also called the space of trial functions. Okay? So, often, u h is also called a trial function. Because we say we're going to try out all these functions living in s h, okay? So maybe that's a useful thing to say here. Okay. Find u h belonging to s h, such that, for all w h belonging to v h which is also drawn from H 1 Okay? Such that, for all w h belonging to v h, the following holds. Integral zero to L, w h, x sigma h A d x equals integral zero to L, w h f a d x plus w h at L, t a. Okay? That is our finite dimensional or Galerkin weak form. Now one thing I should point out here is the following. In our finite dimensional weak form, observe that we have sigma h, all right? In that finite dimensional weak form. What we imply is that sigma h is going to be obtained as E u h, x. Okay? However, f, our forcing function f, is given to us as data. So we assume, and in fact we, we, we will in fact use the fact that, in our implementation of the finite dimensional weak form. We are not going to attempt to, to approximate the, the data, right? We are going to represent the data exactly, exactly as given to us. So f, f of x is not, finite dimensional. Okay? f of x will actually be represented as it is given. Okay? In this sense, you may think of it as it's not exact but, but, but the fact that it is data is why we are not going to represent it as a finite dimensional approximation. All right? It's given to us. Okay? All right. And, and of course in this the case, does the question arise for t, the traction? Think about it. It doesn't in 1D because in 1D the traction is applied at the point. So the question of whether we approximate it, whether we write it as finite dimensional or not doesn't really arise. We go to higher dimensions and we go to three dimensional problems we will see that, there is a question to be answered for the traction. Okay? Because the traction will then be defined on the surface and so on. But, but we get to it when, when we do. Okay? So t, let me just state, is a point value. Okay? And I'm stating all these things for us to understand, why in our finite dimensional weak form, we have finite dimensional versions of w h. We also have a finite dimensional of sigma h, right? And that's this because sigma h is going to be obtained through our constitutive relation applied to the gradient of the finite dimension of trial solution, u h, okay? And on the right-hand side, again, the waiting function is finite dimensional, not so for the forcing function which is data. And it's actually not a relevant question for the traction in this one d setting, okay? So, that's what we have, all right? So that explains why we have final dimensional forms w h,and sigma h. All right, so, we can write out this finite dimensional, or Galerkin weak form, if we say, how we obtain our finite dimensional functions, okay? So, that's really the question. How do we obtain u h and w h, right? Alternately. Alternately, how do we obtain S h, and V h? We need to define those. All right, we need to say, what kinds of restrictions we're using on our finite dimensional functions, okay? In one sense, the finite element method could be thought of, as simply defining what these finite dimensional approximations are, okay? So, here, here's how we do it, okay? The way we do it is to I'll write the statement here, and then I will start sketching things. What we do is partition 0, L into finite elements, okay, which are disjoint. Sub domains of 0, L, all right? Okay. So, here's, here's how it goes. Let me just draw our bar, one more time. And here, we have our x-axis. And that is L, that is 0. Now, for brevity, as well as to make an easy transition to multi-dimensional problems, I am going to introduce notation here for this domain 0 to L. I am going to write omega is our open interval 0, L, okay? So, that is our general domain. Okay, so, the way finite elements proceeds is to partition our domain omega into sub domains, right, which are our elements. And this partitioning is done by nodes that I have, that I'm now marking here, okay? So, we look at these nodes as being defined as x we call this x sub 1, x sub 2, and so on, x sub N, right? Sorry I shouldn't call that N, I'm going to use that for another, for something else. I'm going to call this x sub, call this actually a K, big K, because I am going to call this very last node x sub N, okay? Or I think, I'll even call it, x sub number of nodes, okay? All right, now, each of these sub domains that we've thereby, defined with these nodes. These, these are going to be our elements with the finite element method are omega 1, omega 2. In general we have omega e, right, and so on, okay? Observe that what we've done here is, what the partition that we've introduced. Is the following. We've partitioned omega into sub domains omega e, all right? And the nature of this partition is such that, the total that our entire domain omega is the union of each of these sub domains omega e, all right? Furthermore, each of these sub domains omega e is an open sub domain. Okay? It's an open interval, right? So, the total domain omega is the union e equals 1 to n sub el, which for very obvious reasons is number of elements, okay? So, the domain omega is the union of each of these sub domains, omega sub e, and e here, runs from 1 to n el. Now, because omega e is open, okay, and because we have a very convenient one dimensional setting, what you will note is that each omega e is the open interval of x sub e to x sub e plus 1, right? Clearly, because omega 1 is the Omega 1, which is this one. Is the open interval, x 1 to x 2 and so on, okay? So, now, there's one more thing we need to do here for technicality. Because each of these omega es is an open interval and because we would miss out the, intervening the inter-element nodes, if we just went with this union, we need to say also the following. We need to apply closure to this union. Okay? And say that once we close that union, we also have the closure of our open interval. Okay? So this way we are making sure that we, that we are not missing any points. This is for purely technical purposes. When it comes to computing, it makes no difference because each of those points is what we call a set of zero measure. Okay? So let me just state the, the notation here. So omega bar is the closure of omega. All right? So, it is basically saying that omega bar equals, omega union the boundary points of omega which introduce even more notation is written as that, okay, partial forming, reverse the boundary forming. These are purely technical points, but it's useful to state these now so that there's no confusion later on about what we are doing with nodes, and so on. Okay? All right. So. We have, this, partition. Let me introduce, terminology which have been using already, but let me define the terminology. So. The points x e are the nodes of the partition. Right? Omega e is an element. Okay? Obvious notation, obvious, nomenclature which I'm sure you all either knew, or figured out, or anticipated or whatever, but here it is. Okay? All right, so how do we first, use this partition, how do we begin using it? As the very first step we observe that our weak form has, is stated as an integral. Okay? And what that lets us do is to write that integral as a sum over the elements, right. Over the sub domains of the partition. Okay? So, the weak form or the, the  weak form, what this actually holds for the, for the infinite dimension weak form, as well. All right, the weak form. We will now write as follows right? Instead of writing it as a sum over zero to L, we will first write it as a sum over omega, okay? W h comma x sigma h A d x equals integral over omega, W h f A d x plus W h at, L t A. Okay? So we've gone from writing it as an integral for over zero to L to an integral over omega. Purely notation. The next thing we can do now is to write this as an integral over each element subdomain, okay? Like that, okay? But note that we need to get to all of omega, right? So here we take a sum over e, okay, where e runs over all the elements, one, two and e l. One to number of elements. Okay? And here, too, we have sum e equals one to a number of elements. Integral over omega, omega e W h f A d x plus W h at L t A. Okay? So this is the first that our, partition of the domain into element subdomains lets us . What that, the next step that it also makes possible for us is to now think in terms of defining our, finite dimensional functions. But observe that having reduced those integrals to integrals over smaller subdomains, really, all we need to worry about is how to represent these finite dimensional functions over the subdomains omega e. Okay, so let me just write that here. So, represent. S h and v h over. Each omega sub e. Okay? So we can really afford now to focus on what's happening within each element within each subdomain. Right. With the idea being that the union of all the subdomains are gives us the, the, the complete, the entire domain that we're interested in. And thereby we can also define our finite dimension functions over the entire domain, okay, by going to these sub domains. Okay, let me, it's useful to stop here. When we return, we will think in terms of how we define these functions over the subdomains.

\section*{ID: lMj8hv0HIOo}
there was a question on apply closure to our full domain Omega and to power up to the union of our subdomains so let me revisit that little bit and clarify something so let's recall let's recall the finite element partition okay so we know that we have these subdomains Omega sub e and each subdomain Omega sub e is the open interval X e 2x e plus 1 ok and here's the picture we have here we have the point 0 that is the point L write x equals 0 and x equals L we denoted this point as X 1 and we have other nodes here ok that is X 2 and so on for a general subdomain which is an element Omega E we have X E and we have X e plus 1 ok and as the manner in which I've written Omega e it is clear that it's an open interval now recall as well that Omega the domain of interest is this open interval okay so where is all mcgurk does contain the nodal points except for X 1 and X believe I call that X n but we know yeah that's X okay what while on the girl contains all the nodal points except the first and the last okay each Omega E does not contain its own nodal points all right so Omega contains the points X 2 X 3 so on up to X N - one and I note that we since we're using any L for a number of elements we also have the following relation right we also know that the total number of nodes is equal to number of elements plus one okay let's just remember that because we made later on we will want to use maybe any l2 number everything ok so this is the important point Omega does contain these nodal points right but it clearly does not contain X 1 and X n okay so as we can see there is you know Omega and the union of all the Omega eases are closely related but they're not quite the same okay in particular what we can see clearly is that Omega itself is not just the Union over e of the Omega is okay why is this because each Omega E does not contain its own nodal points whereas Omega does contain the interior nodes alright the only way we can connect them we the only way we can relate our meager to a union of the Omega YZ is to make sure that we pick up all the nodal points okay the nodal points by the way are more can are otherwise referred to as the limit points of the corresponding open sets okay so the way we can do this however is to see now if we apply closure to Omega what we make sure is that it does indeed pick up the nodes X 1 and X n okay and in order to make sure that it is equal to some other closure what we do is to go back to our union of the element subdomains since each of those subdomains is an open set and these are disjoint open sets we know that this Union on the right hand right of the blast equation this is a secret equation by the way is not yet complete we know that what I have on the right hand side is still missing all the nodal points now when I apply a closure there I make sure that I pick up all the nodal points including X 1 and xn okay so in this form so this contains all nodal points X 1 all the way up to X n okay it's really a technical requirement because it turns out that when you do the integrations the fact that you may be missing just one point does not really make a difference to the type of integrals we will consider in the problems of interest it's a technical point but then it's it's an important one to to recognize just in case it leads to confusion later on ok

\section*{ID: m-DqA1jsMUw}
Okay, so we are going to continue with our development of the Galerkin weak form.  For this  one-dimensional linear elliptic PDE. Let's begin by recalling where we were. So we've got as far as here. If this is our domain of interest, or this is the physical problem we're looking at, we have this bar we have our x-axis, positions L, 0, 'kay? And this is our domain omega. Without those endpoints. What we did already is to partition this by the use of points that we're calling nodes. We've partitioned it into subdomains, each of which is denoted as omega sub omega e, 'kay, indicating an element. The nodes that we have used are labeled x1, x2 and so on. In general, this would be the point xe and that could be the point xe plus 1. The point with x-coordinate, x equals L is node, N number of nodes, N nodes and that indicating number of nodes. And we also observe that the number of nodes is equal to our number of elements plus 1 in one dimension. Okay, in one dimension, also, for the very specific manner in which we are partitioning omega for this simplest of finite element examples, 'kay, simplest of finite element cases. Okay, this is as far as we we've got, and what this allows us to do is to write the Galerkin weak form. As follows. We can now write it as a sum over e, going from 1 to a number of elements open integral over omega e of Wh,x sigma hAdx equals sum over e going from 1 to number of elements, integral over omega e WhfAdx plus Wh at L, which of course is our last node, times t, which is the traction times area. So this is our Galerkin weak form. Now observe that each integral really is redefined in this expression as an integral over the relevant element subdomain. What we need to do to proceed is to talk of how we're going to write out e representation for our trial solution. Right, our trial solution of x and the waiting function. Okay? This is what we need to do. Now, in doing this observe that because of the fact that we're able to use this decomposition of our integrals over these element subdomains, we really only need to worry about how to carry out this representation over an element subdomain. Okay? So, let me state that. Need to represent and wh over omega e, right, where, of course, e goes from 1 to number of elements, right? So we have the advantage of needing to focus only upon what often gets called a local representation here, right, local being local to the element, okay, all right? Okay, so let's look at how we can go ahead and do this now. So let's then focus upon a single element. That is xe, that is xe plus 1. And omega e. Okay, all right, so it's all with this subdomain that we need to write out our trial solution and waiting function. The way we can do this now is to define what we will call local basis functions. We define local basis functions on omega e, okay? All right. And what you are going to observe is as you may expect there is going to be a finite number of these local basis functions, okay? So we define local basis functions on omega e. And let me also state that we will define a finite number of basis functions.  Because we have the advantage of working just locally over the element's subdomain, we're going to focus on this element's subdomain, define a finite number of basis functions over that subdomain. But then, if there is a finite number of basis functions over a single subdomain, and there is a finite number of subdomains, what we are going to observe, of course, is that there is a finite number of basis functions over the domain omega. Okay? So, we will have a finite number of basis functions over omega e and therefore. Over omega itself. So, we have a finite number of basis functions. Naturally, of course, our weak form is going to be finite dimensional, right, so that, that's the connection, right? So when it comes down to defining a finite number of basis functions we, we are assured of having a finite dimensional weak form, the Galerkin weak form, 'kay? This is how we want to think about. All right, so let's talk about what these basis functions are going to be. We are going to take in this initial entry-level, very first presentation of a finite element method, we are going to take the simplest possible basis functions, 'kay? We have an element, and it is by choice, by the way, that I have chosen an element to have only two nodes on it. We are going to use those two nodes to write out to expand fields of interest over the element, in terms of just two basis functions, okay? Okay, so what we are going to do is the following. We will define two basis functions. Over omega e, 'kay? These basis functions are going to be polynomials. Okay? And since we are in one dimension, if we have two basis functions and the basis is complete, it follows that we are talking of representing functions as what type of polynomials? Think about that. If you have a complete basis, which you're going to use just two basis functions in one dimension with polynomials, what kind of polynomials are we able to represent? 'Kay, let me just add on that bit about complete basis. Well, you probably got the answer. What this implies is that we are essentially going to be representing functions here as linear polynomials. On omega e, okay? This is the simplest sort of polynomial basis function we can a, adapt, in the most standard type of finite element method. There are finite element method in which we can even use constant polynomials they need a little more work than we need to do, than we should be doing at this early stage of developing the finite method, so, so we go with linear polynomials, 'kay? All right let's actually write, write these down now. So the way this works is, is the follows graphically, this what we will do. We have omega e and I'm going to write the noted coordinates here. Our basis functions are the following. They're linears, right? That one and that one. This is N1 for obvious reasons, that we're going to call N2, okay? So, N1 is a function of position and N2 is also a function of position, okay? With these, what we will be able to do now is to write out our finite dimensional trial solution, okay? And for the very first, just, just for this case, I'm going to specify that this is the finite dimensional trial solution over element e. 'Kay, restricted to element e, and obviously it is restricted to element e, because it is over element e that we've defined our basis functions. Okay? So, the way we write it out in this case is sum A going from 1 to number of nodes in the element. N sub n sub e, is for number of, the capital N is for number, the little n is for nodes and e is elements, okay. So we have two nodes in the element, so n and e is equal to 2, 'kay? So the way that we write this is then NA functional position times some degree of freedom that has been interpolated. And we will write that degree of freedom as d A sub e. Okay, which in detail is simply N1, functional position times a degree of freedom, sorry, that degree of freedom should be 1. Degree of freedom 1 corresponding to element e, plus the basis function 2, function of position times degree of freedom 2 corresponding to element e, okay? So in this what we have is that NA of x is the basis function, all right, and dA of sub e is the degree of freedom, 'kay? 'kay. Also this is NA is basis function A, where A equals 1,2. And dA sub e is degree of freedom A equals 1,2 on element. E, okay?

\section*{ID: 96IDHPmiBN4}
All right. So, again, on this light, u h, u h over element e of x is sum A going from 1 to number of nodes in the element. Na functional position d to the a e. Now, because of the way we do this, you are observing that the degrees of freedom and the basis functions which I'm drawing are fundamentally associated with nodes. Right? Right, and then here also we know that we have a degree of freedom, right, corresponding to each node. We have d1e corresponding to that degr, to that node, and we have d2e, okay. And for that reason we have often also call Na x as being the nodal basis functions. Alright and d e as being the nodal. Degrees of freedom. All right. So we've got this far with talking about how we are going to write out this expansion of our finite de, finite-dimensional trial solution over a single element. We do the same thing also for the weighting function. Right? So, similarly. For wh function of position also restrict to element e. Okay, restricted to element e because that is where we are defining it so we do the same thing. Wh of e, x equals summation e going from one to number of nodes in the element. Which in this case, because we are looking at the simplest possible finite element formulation, that number of nodes in that element is two. We have NA function of position. Now, the degrees of freedom for the weighting function will be obviously different from the degrees of freedom for the trial solution, right, these are different functions. So, though we're using the same basis functions for the weighting function and the trial solution, we have to use different degrees of freedom, if not they would exactly the same function, right? so we have c a e, okay. We are in this case again, this is the degree of freedom, dof for short of the waiting function All right, okay, lemme see. There are many things to be said about this, but first of all, this is our most elementary sort of approach to writing out a Galerkin weak form. When we have the same sort of basis function for the weighting function, as well as for the trial solution, right, this is called a Bubnov-Galerkin method. Okay, and what this means, is that, that same basis function. For Of e and Wh of e. There are other, more advanced finite element methods that for good reason, because of certain things to do with the mathematics of the particular equations being solved, do not use the same basis functions for the trial solution and the weighting function. And they're called Petrov Galerkin and we won't get to them in this, series of lectures. Okay, so all right. Let's move on here. Now, now you will note that all I've done so far is to sketch what these functions are going to be. I, I've called them linear function and so you can image what they are. You can very well construct them in terms of position over the, in terms of the x coordinate over the element. But it it turns out that we can do better, we can do something much more systematic. Okay? And here's how we do it. We first take this approach where we say that this element of interest which has coordinates Xe and Xe plus one. We regard this as being the physical representation of the element, right? Or this is the element in the physical domain. Okay? So we call this the physical domain. In order to construct our basis functions we always think of any element of open arbitrarily element has been constructed from a mapping, right, from a different domain. And in this domain the element is going to look similar, okay, except it is that. I'm not going to use X for position along this right, where, whereas I have X for positional in the physical domain, I'm not going to use X in the, in this domain. In this domain I'm going to use z. Right, as a coordinate. And this point here, the node on the right is z equals one. The node on the left is z equals -1. And the mid point is z equals 0. It's important to note that I have not introduced three nodes. The nodes that I think of are still z equals -1 and then z equals 1, okay? I just marked the midpoint, the midpoint is not a node, okay? So these are the nodes. Right in this representation. Now this is called various things. Some people may call it the mathematical domain, I call it the bi-unit domain. Bi-unit because I'm saying the length of my element in this domain is, goes to minus one to one is two, right? So it's two times one, so it's a bi-unit to me. Now we always think of our actual physical element as being constructed from a mapping, as in mapping from our bi-unit domain. okay. Why do we do this? We do this because it becomes very convenient to define things like our basis functions and later on to carry out integrations if we have this idea of the bi-unit domain. Okay, and in fact, here is what we'll do. Our basis functions, N1 of x. We will write as N1 of x, but really x, remembering that x is a mapping from this bi-unit domain, right, x is a function of z. And likewise N2 x equals N2 x mapped from the domain, from the bi-unit domain. Okay? Now, strictly speaking, there is an, an abuse of notation going on here because I cannot have a function parametrized in terms of x and then use the same symbol for the function and parametrize it in terms of z, okay? But I will nevertheless follow that abuse of notation because otherwise our notation just gets much too unwieldy. Okay. Alright, so now that we are in this bi-unit domain, it becomes very convenient to write out our basis functions. Here they are. N1 of z equals 1 minus z divided by 2 and N2 of z equals 1 plus z divided by 2. Okay? Completely clear what's going on here. We have z equals -1, z equals 1. That is our point z equals 0. If you just evaluate 1 minus z over 2 what we, you will observe is that at z equals -1, h takes on the value 1, okay? So, this is the value 1. Okay? And it goes to 0, x equals 1. Okay? This is our function and 1, sorry. That is N1. For N2, let me try and use the ability here to go to a different color. For N2 I have that. Okay? That is N2, that is again the point one, and you observed that when, where at xz equals -1, and 2 drops to 0. Okay, so I can just those facts here, okay. What we observed is that N1 at z equals -1 equals 1. N1xz equals 1 is 0. And 2 at z equals -1 is equal to 0. And, N2 at z equals 1 is 1. Okay? Now if we write these points z equals-1 and z equals 1 as let me call this point z1, and this point is z2, okay? Just corresponding to the idea that the node on the left in this bi-unit domain is node 1 for the element. And the node on the right in the bi-unit domain, this one, is node 2, okay? What that then lets me say, is that Na okay, whichever shape function, whichever basis function you want to use. By the way, basis functions are often called shape function. Okay? I just prefer the term basis function. Okay. If you cha, if you take basis function a and you evaluate it at zb, this, is, the Kronecker delta. Okay? And you know what the Kronecker delta does, right? Kronecker delta is equal to 1 if A equals B, and it's equal to 0 if A is not equal to B. Okay, the basis functions that we have described here, have the so called Kronecker delta. Property. All right? Okay so we've got as far as describing our simplest one of, you know, indeed our simplest basis functions for this finite element formulation. The simplest of finite element formulations. We will continue with this, but this is also a good place to end this segment.

\section*{ID: b8cYZqVUjCk}
All right, let's continue. So what we did in the previous segment was introduce the notion of the bi-unit domain and use that to define our basis functions, right? Specific-, and specifically, we are working with linear basis functions in this simplest of, finite element formulations. Right? And we've, we've gone ahead and defined those basis functions. And at the end of the segment, we also talked about. We observed that these basis functions have this delta property. Okay? So, these basis functions also have another property which is useful to note. Also. Right? So, let's do the following. Let's consider N1 at some arbitrary point z in the Bi-Unit domain. Plus N two at Very trivially or very simply, this is one minus xi over two plus one plus xi over two. Which is one. Ok? So at any point in the domain these basis functions add up to one. This is important because it, makes sure that we can at the very least represent constants. Okay? So this, this, this is an important property. The, the other thing I should mention now is that the way we've written these functions can be generalized and we will do this, later. We can generalize these to a higher order Polynomials generalize, generalizable, because I'm not quite sure that's a word. Anyway, it's generalizable to, higher order polynomials Okay. And, in generalizing them to higher order polynomials we are going to do it. The, the time has not come to do it. We'll do it later after quite a few more, segments. But when, when we do, do that. We will do so by observing that these are linear basis functions of a class of polynomials called polynomials, okay. So the generalize-able to higher-order polynomial, and they are drawn from a family of polynomials that are called Lagrange polynomials. Okay? We do that later, not right now. Yet another thing, I want to point out about these basis functions. Let me see. That's a bullet point and this is another bullet point. Okay? Here's another bullet point. If we look at, these basis functions. And for just a minute, I'm going to go back to our physical domain, okay? So, let's suppose we have element here. This has, this is element e. And next to it we have element omega E plus one. Omega E plus two next to it. Right? And so on Now, we focused upon a specific, or and arbitrary element e in order to construct these, basis functions. And we, and we did that by going into the bi-unit domain. First of all, we need to, recognize that each of these elements in the physical domain is hooked in from this by unit domain, c equals minus one c equals one, and in this, this bayou domain we are going to denote as omega c. Okay, for, for obvious reasons. So note that each or, an arbitrary element, omega e or omega e plus one or whichever one we want, is always constructed as a mapping from the same Element in the bi-unit domain, right? The same parent element, as it is sometimes called, in the bi-unit domain, okay? Once we do this, we get our basis functions in the physical domain, and I want to draw them. which i want to do in a different color here. Ok, so in the physical domain, if we look at omega, element omega e, we've had this basis function, right? And that one. Ok, if we look at omega, at at that element omega e plus one We have also similar looking basis functions. But what happens here is that they are defined over element omega e plus one. Right, and so on. Right? Now If we look at, how this basis functions appear together, in, in the physical domain, let's focus on the fact that here we have no dome, x e plus one. We may choose to look at these basis functions in the physical domain as being located at global node X E plus one. When I say global node X E plus one, what I mean is that I'm looking at this node as being pa, one of the nodes in the overall domain omega. Right? So, I may look at x e plus 1 as a global node. Okay? And in fact it has global node number. E plus 1. Right? If you were to look at that same nodal point, in the context of element E. Okay? You would call it, local node number two For element E. Okay? Okay. It is also the local node number 2, 4. Omega E. Okay? And it is local node number one for element. >> Omega E plus one. Right, this is nomenclature that we'll use later on when we, when we, as we advance with our finite element formulation. Nevertheless, the point I wanted to make here is now when we look this collection of basis functions and we focus upon the individual nodes and view them as global no-, nodes on a sort of scale, right? But global node numbers What you observe is that we have for element sorry, for node x e plus 1, we have a basis function which I'm going to draw slightly off. I'm going to sort of emphasize it and I'm, I'm drawing it in red, but it's slightly off the actual line describing the basis functions, right? And, and I'm doing that just so you can see the underlying element. Level basis functions which when put together give us what may be called a, a basis function corresponding to node x e plus one. Okay? Now, the basis function corresponding to node x e plus one, which I will label like this, is the, basis function. For global node e plus 1. Observe that basis function corresponding to global node e plus one has a very local sort of sphere of influence. It is nonzero only on the elements that are immediately adjoining node number e plus one, global node number e plus one. Ok? So basis function for global node e equals one is nonzero. Only in elements, omega e And Omega e plus one. We may choose to regard the basis function correspond to global node number e plus 1, this red hat function I have, outlined here. We may choose to regard it as a globally defined basis function. However, its support is very local. Its support is local to the two elements in this case adjoining the node of interest, okay? So this property is referred to as local support. Sorry not local it's called compact support properly, sorry. Compact support. Compact support of Global basis functions. Okay? The idea here is that our In, in constructing our final element formulation, it is convenient to focus upon our element's subdomains. And by invoking this idea of the bi-unit parent domain, it gets to be very conve, very convenient and very clean to define basis functions over each element. However, we can step back and take the global picture, right? And recognize that, that by putting together these local functions, local basis functions from the elements adjoining each node, we can actually construct these global shape functions. Global basis functions, right? If we do that, what we observe is that the finance, that, that the type of basis functions we are using here are ones with compact support

\section*{ID: gucGXA6qhDw}
All right, so that's word making as a remark. And that remark is that the local definition of basis functions. Leads to global basis functions. Okay? Global basis functions associated With each node Say xe, all right? So, if we look at the global basis function associated with node e, that global basis function has compact support. Right? In elements omega e-1 and omega e, sorry, not omega e+1 it should be just make a e in this case. Okay? Now the fact that this global basis function is associated with the elements, adjoining a given node will be observed in higher dimensions as well. Okay? Because in higher dimensions, it won't be just to the right and left. There would be neighbors in other directions as well. Okay? All right. Fine. So this is another property of the types of basic functions we are defining, so now with all of this in place we have a clean way to write out our trial solution as well as our waiting function, okay. And it's useful now to go back and recall from the types of integrals that we need to evaluate in the Galerkin weak form. Just what we need to do with these finite dimension functions. Okay? So let's recall. So the kinds of integrals we need to assemble come from the fact that we have an integral, sorry, we have a sum over e of an integral over omega e of a function of this type. Right? The gradient of our waiting function x sigma h Adx equals integral, sorry, sum over e, integral over omega e, wh f. It's worthwhile remembering that f is a function of as defined with respect to x, which turned can be parametrized by c, right, using our bi unit dimension. Okay, this x Adx + wh L tA, I'm sorry, there is no dx there. Okay? So these are the integrals that we need to compute. Using these finite dimension functions. Now, one more thing I need to state as we go on, okay. And that is the following, an important note to make is this, okay? Consider element one and omega e, for e=1. Okay. If we have that as our domain and let's suppose that this is omega 1, okay? The very first element. We know that this point here is x1, which in this case, because of the way we are setting up the problem, is the point 0. Right? And this is element, this is global node number x2. Okay. Recall that here we have a Dirichlet boundary condition. Right, at x=0. So out here we know that uh=0. Right? It's equal to u nought but then actually let's just leave it as u nought. I'm sorry, I'm getting ahead of myself by seeing that 0. That's u nought. Okay? And yes we know that if we're thinking of this bar, we're thinking of it as an elastic bar. And if it is fixed on the left that u nought will indeed be 0. But for now let's just leave it as u nought. Okay. So that part is straightforward. What about wh there? What should happen with the waiting function at that point? Because it corresponds to the Dirichlet boundary condition. Right, there's something special that happens to it. Think about it. Yeah, wh goes to 0, okay? Always. Now it does not matter what the actual value of u naught is. I know I got ahead of myself a couple of minutes ago and said that u nought is equal to 0. It often is. Even if it were nought equal to 0, the fact that you have a Dirichlet bonded condition there implies that wh has to be equal to 0. Okay, that's how we've constructed our rating function space. What this tells us is that as far as the field wh is concerned within this element, we only truly need that basis function. Okay. Because we are already going to make sure thus in fact, choosing just a single basis function allows us to ensure that wh does indeed vanish at x=0, okay. And in particular we choose only N2 to make the basis function there. Okay? So, what this says is that now if I just pull out that element, okay, now we go on because we know that at wh at 0, and x=0, has to be equal to 0. What it tells us is that the only basis function we need in that element is N2. Okay, N2 of x is only basis function, fn, short for function for representing the waiting function, okay? It's not the case for the trial solution. Only for the waiting function, right? Do we need a single basis function, so N2 of x is the only basis function needed for wh of x for e=1. Okay? That's something to observe here. All right? However, for any other element, e=2, and so on, right up to number of elements in the problem. We have properly wh of x sub e equals sum A going from 1 to number of nodes in the element, NA (x) dAe. Okay, for wh of x where e=1, we don't need a sum, right, we have a single basis function, we just have that, and I realized that I use d here. We are using c to write out the degrees of freedom, corresponding to the waiting function. Okay? So there's a difference in the way we choose to interpolate, or to represent our waiting function, depending upon the elements. Okay? As something to think about, what if we had another Dirichlet boundary condition in our problem, what if we had a Dirichlet boundary condition on the right end, as well. Then for that element also we would do the same sort of thing right, we would use only a single basis function for that one. Okay, so that's just something to keep in mind, I won't develop that idea right now, we come back to it when we need it. Keep it, that's just something to keep in mind. Okay, what I want to do as well is look at what we need for our finite dimensional functions, right? Let me just do this for you. Just go back to this previous slide and recall that when we look at this finite dimensional weak form, that's written just below the word recall. You observe that we have To compute a gradient of our shape function. Furthermore, you observe that sigma h is defined as e times x. Okay? So we see the need to compute gradients of our functions. Here, however, we do not need anything. We do not need the compute gradients, right? And there, we just need to evaluate the function at L, okay? At any rate, what this tells us is that we need to compute. Compute or calculate wh, x for the weak form. And we also need to compute x and in particular x is needed, for computing the stress sigma h. Okay? So, how do we go about doing this? Easy enough. Let's just recall that, again, taking advantage of the fact that we have a local representation, right? So, we can look at our trial solution in element e, in a general element e. And, so I will write it as, sorry, not a summation over e, but a summation over a. Summation over a, 1 to a number of nodes in the element, NA, I will now write it as it is a function of c, okay? We know that we originally defined it as a function of x, but then, by going to our bi unit domain, we observed that it's more convenient to write it as parametrized by c. Okay, we have this, dAe, and we also have wh of e. Function of c is also = to sum over E. NAc, ce. Right. Well, we also need to compute those components. Right. And the way we can do that is just by computing NA, x in both those expressions. Right, because d, the dAe's and cAe's are just degrees of freedom, right? They do not have any dependence upon the position, right? So we observe that in order to compute these gradients of the trial solution and of the waiting function, we just go to our representation for the corresponding functions. We essentially need compute gradients of our basis functions, okay? So how do we do that? Well, we just invoke the change rule, right, so sum over A Right. So now we write. NA, x as NA,c, c,x dAe, okay? And this quantity is done in exactly the same way sum over A, NA,c, which is easy to compute, because we are indeed actually defining the Na's as being parametrized by c, okay? This time, c,x, cAe, okay? Now this is useful, right, because it tells us how we can compute our gradients of required fields, except for the fact that we may say well, how do I go around actually computing that quantity? Hardware, write the derivative of my position in the bi-unit domain with respect to my physical coordinate x, okay? Think about it. We'll come to it, but we will do it in the next segment.

\section*{ID: Wlpa7OvTauc}
Okay, let's continue now. You recall that at the end of the last segment we've reached as far as observing that in order to proceed with our with, with computing the terms needed in our weak form, we needed to figure out a way to write out uh,x, which is sum over A. NA,xi, xi,x, dAe, and the same for wh,x. Okay? And, and in particular the question is how do we do this term, right? Simply because this one is easy, right? This this one is easy. Right, you already know how to compute the derivatives with respect to xi of the basis functions, 'kay? So let's look at how we compute xi,x. And in order to do that, let's just recall that what we've set up for ourselves is a description where omega e, right, and some arbitrary point x in omega e, where xe and xe plus 1 are the nodal positions, right? Some arbitrary point in x is always thought of as being obtained as a mapping from this by unit domain. Okay, now if we could figure out this mapping we may have a chance of figuring out what xi,x is. And what we're going to do now is do exactly that. We're going to define this mapping, okay? So the, we are going to define the mapping. To omega e from omega xi. It's an extremely straightforward manner in which we are going to do it, 'kay? What we are going to do here is the following. We're going to say that any point, x paramaterized by xi over element e is obtained as a, is obtained by representing it in a basis, okay? And I'm not going to complete the limits on that summation sign just yet, 'kay? We're going to write it as some basis function. Right, here, and I'm going to leave myself a blank spot for it times the nodal values, okay? So we're going to write the nodal values here as a let me think, okay, so I'm going to write these nodal values as xeA, okay? Now, we're going to have the sum running over the number of nodes in each element, right? So that is A equals 1 to Nne. When, when I write xAe, right, what I mean here is for A equals 1 I'll get x1e and I get x2e, okay? These are equal to my global nodes, xe and xe plus 1, okay? The coordinate x1e is the same as the coordinate xe. Right, but when I write xe, I'm viewing that as a global node, right, the coordinate is the same. Likewise x2e simply is the second node of element e. Well, globally that is just global node number e plus 1, okay? All right, because of the fact that we need to write this as a summation over the nodes in each element and there are, in this case, two nodes in each element which we're writing generally as N, sub n sub e, 'kay, we need to go to this other sort of numbering for local nodes, okay? So what I have in the brace brackets here are local nodes, and here, the same things appear as global nodes, okay? Those coordinates are the same. It was just that in one case, when we're just viewing a single element, we are regarding those nodes as local nodes, nodes local to that element and numbered 1 and 2 in that element, okay? When we look at them globally, we recognize that they're one of the many nodes in the global domain, and we number them as xe and xe plus 1, okay? So just below this, let me also write the local nodes. So that would be x1 for element e, that would x2 for element e, okay? So the idea there, is that we just want to take our nodes, our two local nodes, describe to them the actual coordinate values, right, that they each possess and essentially interpolate. Now, this blank spot that I've left here in the parenthesis, right, the, the, the parenthesis that I've left unfilled is where we're going to put in our basis function in order to do this interpolation of the geometry. Now, we're free choose whatever basis functions we would want here. It would just mean that we're interpolating the geometry in some way. What is the most straightforward way to do it, the simplest way to do it, one that would probably make our lives the most easy? Yeah, it is to choose the same basis functions as we chose to inter, to, to represent our finite dimensional functions, right? So we'll just use NA, a, A being 1 and 2, right, the corresponding two in fact the same linear Lagrange polynomials that we already know, okay? So, what we are doing here, using the same basis functions. As for representing. And wh, okay? When we do this, when we use the same basis functions to represent our finite dimensional functions as well as to interpolate our geometry, we have what is called an isoparametric formulation, 'kay, isoparametric meaning same sort of parameterization. Okay, so we have here an what we have here is an isoparametric formulation, okay? Isoparametic, but because we're using the same parameterization for our finite dimensional functions as well as for the geometry, okay? Again, it's not necessary, it is just a convenient way to do it. There may be special cases where we don't actually do the same thing, okay, where we don't have an isoparam, parametric formulation. One can design methods of this kind also, right? Finite element methods. Okay, but we will be taking simple approach here, which is to go isoparametric, okay? All right, what that lets us do then, is the following, it lets us say that x,xi is easy to compute, right? x,xi for element e is, now, sum over A, Na,xi xAe, all right? And explicitly, this is just N1,xi, x1 for element e, where x1e is the global node number, right, it's sorry, it's, it's, it's so, I'm sorry, x1e is the local node, right? But the coordinate x1e is the same as xe viewed globally, okay, plus N2,xi x2e, 'kay? And just to make that clear let me state that, that that is just equal to xe viewed globally. That is just equal to xe plus 1, viewed globally. Right, so the coordinates are the same. Okay again, that's easy, easy enough to do. In this particular case, we know exactly what N1 and N2 are. That's easy to compute. So that is just d/d xi of N1, which is 1 minus xi divided by 2 times x1e plus d/d xi of 1 plus xi divided by 2 x to e, okay? Carrying it out, we basically get x 2e minus x1e divided by 2, which is also xe plus 1 minus xe divided by 2, okay? Now, what we will do is observe that xe plus 1 minus xe. For element e is simply the length of that element, right? So we will write this as he over 2, okay? Where 'kay, this is to be viewed as the element link. Importantly what this allows us to do from the ground up, from the, even from the very simplest of finite element formulations is to have elements of unequal lengths, right? So in our partition of the domain omega into subdomains omega e, there was no requirement and indeed there is no requirement that the elements all have to be of the same length, okay? So he can be different for each omega e, 'kay? We don't need a uniform discretization, 'kay? So this is, allows us to have a non-uniform discretization. Okay? And the straight at, at the very outset, it's an important property of finite element methods. Okay let's see how all of this works now. Now so, so we've done very well. We have gone ahead and observed that well where did we start this. Remember we started this calculation all the way up here, okay? So what we've seen is that the derivative of x with respect to xi, which can be thought of as the tangent of this mapping, right, of a tangent of the map that gives us the physical element from this by-unit domain.  Right, the essentially what we've figured out is that we found for this mapping, we know that you know, every little well, well, we know that the scaling of the, the element of the physical domain, relative to the scaling in the parent domain is essentially he over 2, okay? It's a constant because, why, why is it a constant? Because for our geometry we're using linear interpolations, right? We're using linear basis functions, okay? That's what makes it a constant, that's what makes this, this tangent map a cons, a constant.

\section*{ID: cjahRaZ-7HQ}
Okay what we also have here is that we have a map that is invertible, okay? This isoparametric map. Is invertible. 'Kay? And the reasons for it being invertible, if you, if you really care about it is because we have a you know, or, or just a different way to state it is that it's a one-to-one unknown to man, okay? Therefore, it's invertible, 'kay? Because it's invertible what this means is that xi,x is just 1 over x,xi. 'Kay, first of all it means that xi,x exists and that can be written as just 1 over x,xi. This stat, this fact, this statement, will become a little more meaningful and significant when we go to multiple dimensions, 'kay? We'll have to have actual denser maps and those will have to be properly inverted and all that. All of that will be assured to, 'kay, but let's just lay the groundwork for, for that discussion when we get to it. Okay, so why do I care about xi,x? Because now what this lets us do is to go back and write uh,x as sum over A, NA,xi, xi,x, dAe, and wh,x, sum over A. NA,xi, xi,x, xi Ae, right? It lets us say that this is just 2 over he. Okay? All right, as the very last thing to do in this segment, I want to just begin assembling some of these integrals that we need to worry about, okay? So, consider The integral Or consider the, the following integrals, okay, which come from our finite dimensional weak form. First, consider this one. Integral over omega e, wh,x sigma h Adx, okay? Putting everything together, we have this as being writ, writ, written as integral over omega e wh,x. Now I'm going to invoke our constitutive equation, 'kay? And I'm actually going to move the, the area into this and write it as, okay, see what I've done? E times uh,x is sigma h from our constitutive relation, okay? And this I'm now going to write by bringing in our expressions on just the line above for uh,x and wh,x, okay? I'm first going to write wh,x. It is sum over A. I won't write the limits on A because it ju, gets too clunky, 'kay? We know now from having looked at it over the past couple of segments that A runs over the limits 1 to number of nodes in the element, which in the simplest of cases is 2, okay, so we, I have that. I just want to write the upper and lower limits. I'll just say that to sum over A. Okay in a,xi times 2 over he, cAe, and we're going to put parentheses here. I have EA, okay, open parenthesis, and here I have another sum but I need to be careful here and use a different index for the sums so I will use the index B, and not index A, 'kay? The sum over B, NB,xi 2 over he, dBe, close parenthesis. And observe that I shouldn't forget the fact that I have a dx here, okay? In writing this, this first set of parentheses is my representation for wh,x. The second parentheses, second set of parentheses give me uh,x, all right? And then I have the other integral, which is actually a somewhat simpler one, integral over omega e whf, a function of x, which could be written as, which could be parameterized by xi, Adx. This is somewhat simple to write. It is just integral over omega e, summation over A. And I will stop writing the limits on that, on those sums NA, no derivatives here, okay? NA CAe, I'll put parentheses on it, f perhaps a function of x through xi Adx, okay? This is what we have, right? As a very last thing, let me just do one more thing here. Observe that essentially everything that in, in these last integrals is parameterized in terms of xi, right, the NA is pa, parameterized in terms of xi and we can also compute the derivatives, right? They turn out to be constants we know, but we can compute the derivatives, can view everything as being parameterized by xi, except that we have this as the of a length element, right? Well, but then we can use the change rule there, right? So we're going to write that as dx/d xi, d xi, all right, and obviously the same there. Okay, what are for, what is dx, d xi? On the previous slide, and actually we have it in some form up here too, right? We see here, that we saw xi,x is 1 over x,xi. Right, and here we have what xi,x is, right? So what we see is we've already figured out that dx d xi is he over 2, okay? The same thing there, okay? What that will let us do is to rewrite these integrals as integrals over the bi-unit domain omega xi, all right? As a very last step that we're going to take in this segment, I'm going to take, I'm going to write that, 'kay? What that lets us do then is write integral over omega e wh,x, EA uh,x. And, and I'm writing this already in the form that we will, where, where we implemented our constitutive relation, okay? This is now, we can write this as an integral over omega xi. We have sum over A NA,xi 2 over he cAe, close those parentheses. EA, open another set of parentheses. This is the sum over B, NB,xi 2 over he, dAe. Now we have d xi, dx, but that we know is he over 2, d xi, okay? And of course what we're going to do when we come back is to observe that these two cancel out. And likewise we have integral over omega e whfAdx, can also be written now as an integral over the bi-unit domain of sum over A, NA cAe, f which we may eventually want to write as a function of xi A, now, we have dx there, but we're going to write that as dx/d xi, which we know again is he over 2 d xi. Okay, that's what we have for the, for the, for the main integrals that go into our finite dimensional weak form. So this is a good place to end the segment. When we come back, we will focus upon evaluating these two integrals.

\section*{ID: PJA5F7MVAWg}
Hi, in this segment we will be talking about functions in C++. Now the basic structure. Of a function we actually looked at earlier when we talked about the main function. Okay, but we'll go back to that now. Now, each function has a name. And. There are inputs. Possible inputs I should say. And possible outputs. And then there's the body. And if you do have an output, then you would return. The output at the end of the function. Okay. So let's look at some examples of functions here in the code. Let's say I want a function that will calculate, for example, the area of a triangle. And so, I would have an output, and it would be a double, which would be the area. I will name it $triangle-Area$. And I'd actually have two inputs that I would need. I would need the base, which is a double. And I'd need the height, which is also a double. Notice I've separated the inputs with a comma. And notice that I've given a name to the inputs, but the output doesn't have a name, okay? It will know, C++ knows the value output because at the end of my function I will again say return whatever value I want it to return. Notice with this format I can only return one output, okay? I have my curly braces which define the body of the function, and as we talked about before, it also defines the scope of any variables that I define within the function itself, okay? So I will create a double, a real number area. And the value of area is one-half. I could do 0.5 or I could do 1./ 2. Remember to put the decimal. Otherwise, 1/2 will give me 0. Right? So watch out for that. So I'll make that $1./ 2*base*height$, okay? And then, I just return area. Okay, so now I'll go down into the main function. And. I'll print that to screen. Actually, I'll create a double here. Area. Notice, I've declared area twice, I have a double here in my main function, and I have a double area within my triangle area function. But again, they're both within their own respective scopes, so it's okay. They don't see each other, all right? I've created double area there. I'll say $area = triangle-area$, and I'll put in, just for an example, 2 as the base and 3 as the height. And I'll print that to screen. So we can see what it gives us, okay? My source code is saved, and so I will make run. And it prints us screen three. One-half the base which is two times height which is three, okay? Gives us three. All right, we can actually. Save some space in this function. Instead of creating the double area, defining what the value of area is in the function, and then returning the value of area, I can do that all within one line. If I want to simply say return, $1./2*base*height$. And if we run that, again you see it gives us the same value of three. Now, looking back here at the top, I'm passing in values for base and height. You could consider these as being readonly values, readonly inputs, you could say. What's actually happened is in the function I'm copying the values of my inputs base and height and using that within the function to perform some calculation. Okay, there's another way. That's actually called passing by value, and passing them the value of the variable but not the variable itself. I can't modify based on height outside of the function itself. However, sometimes it's nice to be able to do that. For example, if you have multiple outputs that you want, then you can pass in an input by reference which allows you to change that input, and you can change multiple inputs at the same time. So let me create another function here to demonstrate that what passing by reference involves. So here, instead of returning a double, I'm not going to return anything. So I will put void, and I'll pass in double base, double height. Both of those I'll still keep as readonly, but I will also pass in a double area, and I'll pass up by value. Remember from our discussion on pointers that the ampersand refers to the address or the reference of a variable, okay? So I'm passing this in by reference. And so instead of saying return 1./2*base*height, I'll just say $area = 1./2*base*height$, okay? And now the value of that input has actually changed within the main function itself. And now instead of saying $area = triangle-area$, oh I also need to change the functioning. So I'll call the first one $triangle-area1$. This one's $triangle-area2$. So for $triangle-area2$, I'll still pass in 2 and 3 for base and height. But I'll also pass in a double area. And notice I haven't set anything equal to triangle area because that function is void. It won't return anything. But it will modify the value of area that I'm passing in, okay? So now I can run that. And I still get the same value of three. Okay, now what if, I mentioned before you might want multiple outputs. So. What if, for example. For a square. I just want some generic information. I want to pass in this length. But I want to get out of it both the area and the perimeter. You notice I couldn't do this. As easily. I mean I could maybe do it as an output of vector, a vector of doubles. And the first component of the vector could be the area. The second component of the output could be the perimeter. So that would be one way. This is another way to do it, which I think is a little bit cleaner. Okay, and so here I would say that the $area = length*length$, and the perimeter = 4*length. Okay. So now I need a double perimeter. Now notice I've, just to make a point, I'll change these names here. They don't have to be the same. My inputs within main don't have to have the same variable name as what's defined in my function inputs, okay? So I'll change these names just to make that point. So I'll input those into $square-info$. I'll pass in a length of 3, for example. Pass in Area1 and Perimeter1. And print those to screen for us to see. Okay, so my file is saved, and so I can run it. And it gives me an error here because I misspelled length. So these. These error statements are really helpful. And usually they can tell you exactly where you need to go, where the mistake is in your code. Okay, so now that is saved, and. It looks like I also misspelled Perimeter there. There, okay. Now we'll run it again. All right, 9 and 12, so 3 times 3 is 9, and 3 times 4 is 12, giving us the area and the perimeter. So that's an example of what you might use a function for. Now here my functions have all been numerical. But remember, functions in C++ aren't necessarily a mathematical function.  So let's say I could create a function to, that would print a vector, the components of a vector, to my screen. And it would be a void. And I'll just call it $print-vector$, or $print-vec$. And I'll take as an input, a standard vector. Of doubles. And I'll call it output, or out. All right, and then within this function I just want to do a for loop, for(int i = 0; i<out.size(); i++. And then I would then print to the screen out. Std::endl;. Okay, so that would be an example. Of a function that's not a mathematical statement. It's just something to make the main body of my function a little bit cleaner, or the body of my main function to be a little cleaner. So down here if I have a vector declared, a vector of doubles. going to notice in this case because the input to my function is a vector of doubles. That's, it won't work on a vector of ints, for example. And I'll do a length 3, components 1.2, okay? And now I just call this function $print-Vec$. I input. Uses an input test. And as I run it. There you can see it. Use the function to print all the components of the vector. Okay, so that is just to show you that a function doesn't have to be a mathematical statement. We use functions usually to make the body of the main function a little bit cleaner, a little bit easier to read. And, often, if there's something that you're repeatedly doing within your body, you'd probably want to make a function, so that you've only to find the operation once, and you can just refer to it again and again. Okay, so that will conclude our discussion on functions. And in the next segment we'll move on to talking about classes.



\end{document}
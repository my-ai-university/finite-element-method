\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}

\section*{ID: oCm0rxmWrxs}
Okay, so we have an isoparametric mapping. For geometry. All right. So that says, as we know very well now, x sub i sub e, all right, As a function of, c vector is again obtained as a sum over the nodes, n a. Right? And now, na we know is a function of, is parametrized by the coordinates, and this by unit of mean. And we have here, xA ie, right? this is exactly the same as before. Okay, and once we have this, we know what other use we can put it to, right? Do you remember what else we do with this? All right. We use it to write out our gradients, right? So, for gradients. And what are the gradients we need when we look back at our finite dimensional weak form, we, we already see this, that sort of gradient. Right? Whi comma j. Right? This now over the element is sum over A. NA comma j, cAie, all right? I'm, I'd probably add an e in there, as well, okay, just to keep directing it. All right. And, of course, the same thing for uhi comma j. All right. Uhi comma j, is over element e. Is again, sum over A. NA comma j, dAie. Okay, And we know straightaway where we expect to use this right of course this shows up directly in the weak form, when we look at it, right, just recall. Recall the term on left-hand side in our weak form, integral over omega, whi comma j. We have sigma h, but we know of course that sigma h is just Cijkl. Okay. Sigma ijCijkl epsilon hkl. Okay? But here is where I'm going to use the, one of those properties of our, elasticity tensor. And the property I'm going to use is the fact that where as the string is properly defined as the symmetric bar to the displacement radiant. Right? That's the stream. However, because we've talked about how the elasticity tensor is Is possessed of minor symmetry in those two indices, we're actually completely free, to not worry about the symmetrization of the displacement gradient. In, in using this constitutive relation. Okay, so what I'm saying is that we are completely correct, long as we make sure that c has minor symmetry, you're completely correct in writing this as Cijkl, u,k, comma l. Okay. And, the reason for this is minor symmetry. Okay? All right? And, and, if you're wondering, yes, it is indeed of course true that, for that term also, whereas, I've just written it as a gradient of the weighting function, we would be right if we want to go the other way there, and say that it's the, it's the symmetric part of the gradient of the weighting function. All right? But we just don't need it. We can leave it this way, a minor symmetry with respect to the ij indices, which we also have, takes care of things for us. Okay. But any, but at any rate, we see clearly what gradience we need to compute, right? You should remember that k and l are of course just dummy indices because they are being summed away, right? Or even otherwise the would be dummy indices. And so when, when I say I am going to compute i comma j here it's, it's really the same. All right, so, we do need to compute gradients. But once we have our isoparametric mapping, we know how to go about that as well, right? And what we are seeing here now is that NA comma j is just NA comma xi capital I, right? Using as before, the uppercase index for coordinates in the biunit domain. You have this times the derivative of xi I. With respect to X, J. Right, for all J. As before. Alright, that's one, two, three. Okay. And of course, we know how to compute this now. Right, using the Jacobian of the mapping. All right? So, what we do here is use the Jacobian of the mapping. All right, and that, when we write it out as a matrix, is the following. Right, J. Right, depending upon the coordinates and the by-unit to me, is You know this all very well now. X 1 comma c 1, x 1 comma c 2, x 1 comma c 3 equal 1. Until we come down here and we get x of 3 comma xi 3. Right? But then of course, we also know that J inverse of C. Is. C, one, comma, X, one. Under C, three, sorry. One comma X3, and we go on. Until we come to C3 comma X3. Okay? And once again because we have only a three by three to invert here, it's actually not that hard to do it analytically. Right? So in our element formulations, as I suggested before for the steerer problems, we can do this analytically. So we thereby get each of these derivatives that we need. Right, and therefore we know how to computer the gradience. Okay, well, if we know how to compute the gradience, we can go ahead and, assemble our problem. Right, our, weak form. We do that in the next segment. We'll end this segment here.

\section*{ID: 034gY2jXGnY}
All right. So, we'll move on. We've established our finite dimensional weak form. We've looked at our basis functions. We made the somewhat important observation. That for this problem, because we have a vector problem, the number of nodes is not identical to the number of degrees of freedom. Right? But then we made the identification and we know how to relate them. All right. Let's go on. We'll get into assembling the integrals in our weak form, in our finite dimensional weak form. So, we will call this segment the element integrals. And in order to get to the element integrals, we first need to make the observation that our finite dimensional weak form. Because it is an integral over the entire domain, right? Or the boundary of the relevant Neumann boundaries also allows us under this partition to write it as a sum over a, the, the individual sub-elements, right? Or the subelement boundaries. Okay, so the finite dimensional weak form can also be written as. Sum over e, integral over omega e, w h i comma j, sigma h i comma j, dv equals sum over e, integral over omega e, w, h, i, f, i, d v. Plus, now here, we need to be a little more careful, right? We are talking here of doing the following. We are talking here of doing a sum I equals 1 to Nsd. Okay? We're summing here over our spatial dimensions. And where as earlier we could directly put down the corresponding Neumann boundary, we are going to write that also as that integral also as a sum over sub-integrals, right? Each of them is a boundary. So we need to say here that we are going to take the sum, not over all the elements, but the elements belonging to let me see, right, En, right? Remember, E was a set of elements that had one of their boundaries coincide with a Neumann boundary. All very well except that that too needs to be indexed by the spatial dimension. All right? Because for each of the i's, i equals 1, 2, 3. We could have a different set of elements whose boundaries coincide with the Neumann boundary for that particular dimension. All right? Okay. So we have that and then the integral is over partial omega t bar, I, and I'll put the E up there. All right. And this, of course, is the same whi t, bar I, d of S. Right? And all right, so this is what we have. What we are going to do is just as before, assemble each of these integrals over the elements of domains, or in this case, over the special element boundary sub-domain. Right? And of course we are also going to use the fact that this is given to us by B, elasticity tensor, now multiplying U, K, comma F. All right. Okay. So, we're, we're going to proceed with this now. So, we've, we will as always start with the, with the left hand side integrals, consider first integral over omega e, w h i comma j I'm going to directly go to the representation including the elasticity tensor. Okay, so sigma h i g. Oh, if I'm going to use the sigma h, I need to have an h there, all right. Because that's where the finite dimensionality of this comes in. Okay, so we have here C, i, j, k, l, u, h, k,I dv, right? Now on a previous slide, belonging to the previous segment, we wrote out the, the expansions for w h, and sort of, for the gradient of w h and for the gradient of u h. We will now invoke both, right, and in doing so, I'm going to write it here. We have here again, allow me to skip, well, okay. Mm, I won't skip a step right away. going to write this as integral over omega e, the w h, w h i comma j, is written as sum over A, N A, comma j CA I E, okay? This is whi, j, and I put it in parentheses. We have here C, I, J, K, L. And again we have sum over B, N D. Now, the gradient is represented as A, using the coordinate of direction L. And of course it's, L runs over 1, 2, 3. So that means n b here has to take a comma l, right, that's where the gradient is computed. And we have d b k e d v, okay. Now if you were go back and stare at the way we did the corresponding step for our scalar 3D problems, you would see something very similar except the fact that we had no index on the degree of freedom, degrees of freedom for the reading function. Nor on the degrees of freedom for the trial solution for the displacement field, okay. The fact that we do have indices here, comes from the fact that these are actual vector degrees of freedom. And i and k here both run over one two three. And the fact that we have those indices also shows up in the fact that we have a couple of extra indices here, right. The coefficient there, if you want to think about it as that. The elasticity's tensor's actually coefficient of elasticity, is a full taut tensor unlike the conductivity tensor or the diffusivity tensor, which are second order indices, all right. So that's, that's really all there is. And once we take, once we understand that, and we are comfortable with that bookkeeping essentially, all is well, all right. So we proceed, and as you may imagine, what is the next step here? Or as, as you may recall, what is the next step? Right. It is to observe that the degrees of freedom, nevermind the fact that they're vectors now are still, after all, independent of position, all right. And so they can be pulled out of our integral. And in doing so, we can also because integration and summation in this case commute because of linearity and all that. We can pull those summations also outside the integral, right? Okay. What that gives us is. Now I am going to write this as a sum over A and B. And let's recall, let's remember just here that A and B run over 1 to number of nodes in the element, right? Integral, oh, sorry. We need to have here c A i e, okay? I have here, NA comma j, okay, C i j k L, right? And for our representation of the gradient of the displacement, from that representation of the gradient of the displacement, we get an NB comma L here. Okay? Our dV, all right, I'll put a parenthesis here, or pair of parentheses. And, all right. What we've done in the process is pull the d B k e out, okay. Right? Straightforward enough. Oh, this is an integral over omega e, right? To proceed, and remember that we know how to compute these gradients, right? We looked at that on a, at the end of the last segment, right? So we know how to compute these gradients, no problem, all right. Let's move on. Remember what the next step is, right? It is to convert it to, we, we carry out a change of variables and rewrite the integral here as an integral of the bi unit domain. So, once again, sum over A and B, and I'll forgo writing the limits of that sum. We get here c A i e integral over omega xi, all right. That's our parent domain. N A comma j, C i j k L, N B comma l, right. Now, just as before, d V can be written as determinant of the Jacobian of the mapping. D V sub, thing was putting it there, right? Okay, now, of course, I could rewrite this in another step by specifying that this integral over omega C really in, involves integrals of C1, C2, and C3. It's a triple integral, each one of them going between minus one and one, and the fact that this is essentially d xi, d xi2, d xi3, all right? And this integral basically becomes something like, it becomes actually a triple integral, all right? Minus 1 to 1, minus 1 to 1, minus 1 to 1, all right? So, that, that step is given, all right? So, so well we can go ahead and compute it. Now observe that, in general, depending upon the basis functions we've chosen, whether we've, you know, decided to stick with trilinears, or tri-quadratics, or, or, or rank higher order, these are functions of c 1 c 2 c 3. Now, the way I've constructed this, I've been thinking to myself, at least, that C is independent of position. It doesn't need to be, right. In general, it, too, can be a function of position, right. That, that would allow us to go to inhomogeneous materials. Okay, all of that will simply add a higher order dependence on position to this integrant. And of course, we know that in general, the determinant would also involve a dependence upon position, okay. But nevertheless, we not integrate this, right? We can be doing numerical quadrature. Right, so we do a numerical quadrature or numerical integration, which is quadrature. Right, and since we're working here with Lagrange polynomials defined on hexahedral elements, we have available to us the optimality of Gaussian quadrature. I ought to admit here though that if our elasticity tensor has some strange dependence upon position that is not just polynomial, we made, Gaussian quadrature may not remain optimum, all right? But, but, but still, it's, it's, it's an approximation, and that approximation is understood. Okay, so essentially we know how to compute this, right? Let's com, let's assume that we, we go ahead and compute this, right? If we go ahead and compute it. Let's, let's write it out, okay? Let's write it out. And let us write it out in this fashion, okay, so. All right? So, this thing is now equal to sum over A and B, c A i e. We've carried out the integral, okay? So we have something in there. So this entire integral here that we have here, this whole entire integral, Is done. Now we could realize now that I omitted to write my DBKE here. Okay, that integral with the raised bracket is done. So when that integral is done, we will be left here with a dBke, which is this one, right? Okay. I've left a, an amount of blank space there because I want to fill it in with something and I want you to think of what kind of object goes in there. In particular, is it a scalar, a vector, or a tensor? As a guide to getting to that answer, recall that the. The integral that we are trying to work with here is the one that I am now showing to you on this slide. Right? It's the one that is written next to the word consider. What kind of an object is that? Is that integral itself, once you valuate it, is that a scalar, a vector, or tensor? All right? It is a scalar. Because though the C has fu, free indices, I, J, K, L. Those are all being contracted out, right, and a sum over each of those dummy indices, I, J, K, L is indeed implied. Okay, so what we expect to see is that this thing is a scalar. But the way we've constructed it, we see that the c A, the, the, the c and d, right. By, by c here, I mean this c, not the elasticity tensor. Degrees of freedom have indices, free indices. With the final thing should be a scalar, there have to be other three free indices in whatever object is going to go into the black space. Which will be contracted out with INK. Okay. So, whatever object we have here, I'm going to denote it as K, of course. It will also have free indices I, K. Okay? Right. And it is essentially that integral. Okay? So the integral we have here is Kik. Okay? It is a particular component, the IK component of a tensor. Right? Thought of as a matrix, it is a 3 by 3 matrix, right? Because I and K run over. 1, 2 and 3, because we're operating in three dimensions here. All right. Now there's something else I haven't added there. And can you tell me what that well can you think of what it is? The indices A and B, right, because those indices are still here. So anything that we compute for the integral is um,specific to the combination of nodes A and B, which went into that integral. Right, that, that form that intergrand. So we have an object here which, which I'm going to write as KABIK. All right? Now there are indices all over the place here. And it's useful to think to just sort this out. So when I say I comma K go 1 over 1, 2, 3. I am going to write here the, the statement that's almost a contradiction in terms which is that a sum is implied.  Okay, over IK. So I do not have an explicit summation here, but over the A and B indices, I do have an explicit summation. All right, it's, it's just a matter of taste in terms of what I choose to write as an expressed summation and which I choose to invoke the Einstein summation convention on, okay? The fact that I'm invoking Einstein's summation convention on the spacial indices, spacial dimensions comes from a sort of hangover of continuum physics.

\section*{ID: nDDY9xAGcs8}
There was a minor error in board work that remained on the slide I have before me here, and that you have on your screen. It appears right about here. That term should not be sigma h with subscripts i comma j, it should just be sigma sup h, subscripts i j. The comma would suggest that there is a derivative in the j direction, which is not correct. Notably, however, if you were to just follow what was written up here, for sigma h i j, all would have been well. Right? But nevertheless it's important to make that fix if ever you come back and look at this slide. That's it. And with that everything is, consistent.

\section*{ID: JvFTOg-Csno}
All right, so this work we have. Let's just look at this object and understand a little more about it. Okay, so one way to understand it is to realize that we could also write that as follows. Sum over A and B of a vector, c A e, right? Transpose, right? K AB, 3 by 3 tensor, d B, e, okay? And for each of them what I have here is the following. C A e, and d B e, both of which we've encountered before, and we have had occasion to note that they belong to R3. Okay? Whereas this quantity K AB, right. It is a it is a matrix right? And it belongs to GL 3, right? It is a general tensor. It does not necessarily have to be symmetric. In some special cases, it's symmetric, okay. If you go through the process, you will, you will discover later that it is symmetric for A equals B. Okay? All right, but for now it is a general, it is a generally tensor in three dimensions. All right, so hopefully this, this helps. What I've done here is get rid of the implied summation over the indices that run over spatial dimensions, and replace those with vector tensor products. Okay? Now any, so each member of the sum is indexed by A and B. Okay? Fine, so this is what we have here. Let's go through and fairly quickly compute the one of the other integral. All right, so next consider The following integral over omega e W h i, f i dV. Okay, this is the one we want to integrate now. So, we have integral over omega e. Sum over A, N A no gradient on it, C A ie. That sum gives us our waiting function components. All right. This times fi, right or this actually multiplying fi and this is sum and glide over the i dV. Using our, using our usual tricks and here I will take the liberty of skipping several steps at once. All right, and still hope to get it right. Okay. So we get sum over A, c A ie, integral over omega C, N A, fi, determinant of J C dV c. All right, and of course now we know that dv c is dc1 dc2 dc3. The integral over omega C is just the triple integral minus 1 to 1 over the 3 coordinate dimensions. Right? In the bi-unit domain. Right, okay. As before, we are going to call this integral here. We are going to denote it as F internal. All right. Note that it has a an index A coming from the fact that it was computed with the basis function corresponding to node A. And it is each F internal A is a 3 vector. Right, and this also we could write as sum over A, using the notation we used to write out our left hand side, we have C A e, transpose F internal A Okay? All right, at this point we, we've assembled the two somewhat easy integrals. We will return in the next segment to finish up the the integral involving the traction terms.

\section*{ID: puYCOSoTMrk}
All right, time now to grapple with the traction integral. So let's recall what it looks like. It is the following. It is sum over spatial dimensions. So sum now over e belonging to each of these ones, right? Of okay now and we do an integral here partial omega e t bar i w h i t bar i dS. Okay? So what that tells us is that it's really this integral that we need to consider. Okay? So we have this one Remembering that we do not have here a, a sum implied. Okay, that's straightforward. We need to do that as integral over partial omega e t bar i. For w h i, we have sum over A, NA, CA, IE. That's whi. T bar I, DS. Which is sum over A, CA ie integral partial omega E t bar I. N a, t bar i, d S. All right? And now we recall the, the cases that we've actually considered before, right? So we just need to recall them. I don't think we need to remotivate and re sort of discuss them in great detail ago, again. So the the situation that we tend to have in general is this. That is omega E. And let us consider the, the general situation where, straightaway let's consider the general situation where this face is the one of interest, right, partial omega t bar I. Okay? So what this says is that traction component i is being controlled on that surface. All right. And I've drawn it, hopefully it appears to you that it is at an angle. If that is our basis set. Okay. All right. So that is the general situation we have. Recall of course, the something we know and we have looked at before. Which is that we are somewhat saved by the fact that As I have drawn it, perhaps that one can think of that face as coming from this one. Right? So for the way I'm drawing things here, I am implying that the face of interest, is a is this face, right? It is a xi 2, xi 3 face, right? So this is partial omega xi t bar i. Sorry, t bar subscript i, for this particular element. All right?  And so it is that we know where to con, to compute this, this integral except that we also know that when it comes to constructing our mapping, we may need to to construct local coordinates just in order to, to get the, the, the Jacobian of the mapping right. Right? So, let's suppose that in this setting, I'm calling that x delta 1. And the x delta 2 direction. Right, actually those vectors really are, e, should probably be written as e tilde 1 and e tilde 2. Anyway, those are the directions that we have, okay? And from this phase to that phase, we may think of there being a map that I denote as J sub S. Okay? C. All right? Okay. So we have all that and of course, I okay. So, so let, so, so I will, I will, I'm going to use that in a second. Before I do that, I just want to point that I also need to recall that the sum over nodes may not be over all, may not need to be over every single node, right? It may need to be only over that set of nodes. Right? And previously we've used the notation that this node, A, belongs to the set A sub n. Right? To say that that is one of the nodes corresponding to a surface with a Neumann boundary condition on it. Right? So really the sum is over A sub n, as is this one. Okay? All right, so using this map, the way we would construct this integral would be this. Now, the integral here would be over partial omega c t bar i. And for the way I've drawn it here, I am suggesting that that is C2 C3. Okay? All right. That is the, that is the C2 C3. Right? So then we have here our NA, t bar i, right? And now we're going to do the integral over dS sub C, right, which is the elemental area on the partial omega c d sub d bar i. You know, face in the bi-unit domain. And I've left some blank space here because I know that I need to construct, I know that I need to put in here determinant of js. Okay? Because js maps a surface to a surface. What is the order of js? What kind of a tensor or what kind of if you think of it as a matrix, what kind of matrix is it? Js is a 2 by 2 matrix, okay? So jS, using the notation that we've used is g l 2. Right? It maps two vectors to two vectors. Okay. It's probably useful for me to point out in addition that for the way I've constructed this particular example, dsc. It's something I stated already, but what is dsc? It is dc2, dc3. Okay? And therefore that integral would be over C2 and C3 going from minus 1 to 1, all right? Okay. So we have all of this and what we need to do is essentially go ahead and construct our vector representation of this. Okay? So in constructing the, the vector representation I'm going to write this as, now, I'm going to write this as follows. I'm going to write it as a sum A belonging to A, A sub n. Okay? Okay. And we write this as CA IE. Now that entire integral once I evaluate it, I am going to write as F. I'm going to write a T bar here,which just is reminding us that this particular type of forcing vector come to us, this particular forcing, comes to us from the traction. Okay. It is the it is an I component. Okay. And it also involved node A, okay? So that entire F with all kinds of decorations on it now with subscripts and superscripts galore, actually it has just three of them. Is the result of that integral that we carried out, right? That the integral over the surface. Okay? All right, so this is what we have. Now where does this go? Remember where this all came from. And in order to remind you of it, let me just go back to the previous slide. This is where it came from. We have at the very top of our slide here. The original integral from which this came. Right? And note, in doing this, that really there is a sum over the spatial dimensions, as the outermost sum here. Okay? And then there is the inner sum over the element integrals. Okay? So, what this is suggesting is that when it comes to assembly, okay. We will first assemble the contributions from each element. And then account for the sum over the spatial dimensions. Okay? But, but before we finally do that, there is one thing we can do which is going to get us rid of the restriction to the special set A sub N. Okay, right? And what that let's, what, what that involves is the fact that okay, so let me now say let me say the following. Mm, actually let me do one thing. Let me put a bar on this F first, okay, because what I am going to do now is just rewrite that as a sum over A, okay? Now I'm going to say that this sum runs over all the nodes in the element, CA ie, F, t bar, A, i, okay? And what I mean here is that F, t bar A i equals the F bar that I calculated on, on the line above. If A belongs to AN, right? Right, and it's equal to 0 otherwise. All right and, and you would probably recall that this was the, the approach we used when we constructed the the contribution from the Neumann boundary condition, also for the, for the scale of problem. Okay? All right. So, what have we here? We are now in a position to look at our total finite dimension you'll be performing with all these element integrals accounted for. Okay, so.  Right? It is the following. We have sum over e. C e, so let me see, sum over e oh right, we have yeah, one more thing, yeah. We have sum over e, we also have a sum over A comma B, c, A, e transpose, right? K, AB, d, B e, okay, this is the contribution from the left hand integral, equals sum over e, sum over A, c A e transpose F internal A, okay? And plus sum over spatial dimensions. Then the sum over, elements belonging to the Neumann boundary condition corresponding to that particular spatial dimension, right? And all right. And then we have yet another sum here over A. Right? And this will over all the nodes c, a, i, e, f, t-bar, a, i. All right? Okay. Now, in writing all of these, I did notice a few minutes ago that when I introduced this matrix K, it really corresponds to a particular element, because it arose from a specific element integral, right? And it arose from the element integral for element e. So strictly speaking, I need to have it, an E there. It's just to remind us that it is the contribution from that particular element. Okay? Likewise here and likewise if there's any room here for more indices, for more subscripts. There we go. Okay. So this is where things stand. The next step that I'm going to take is one in which I am going to collapse, well not collapse actually. Expand out these vectors, okay. And in order to do that let's just let's go to a new slide.

\section*{ID: 9PtK8-ByJbQ}
Recall what we have here. Okay. So we have a general element, right. Now, now we are, we are, we are rid of our bi-unit domain, right. We are rid of that domain. So I have an element So I think I may have distorted a little too much, but we'll see. There we go. Okay. So, let's suppose that this is the node A. Okay? Let me look at them all. Because I do need to know. There is a node in the back here. All right. And up here we have. Okay. So this is node. Equals 1, 2, 3, 4, 5. A equals 6, 7, 8. Okay, and the situation that we have is that C A E is C a 1, C a 2, C a 3, right, for element e, right, likewise d. So we would have, look I would say like, d a e. Or d B e, that's a dummy index of course. Anyway, let's write it as d B e is equal to d B 1, d B 2, d B 3, element e. All right. Essentially what I'm going to do is to observe that now, in the case of the trilinear element. How many total scalar degrees of freedom do I have? All right. We've actually done this calculation, right. Which is that we have, Nne times Nsd degrees of freedom per element. Right? Okay, so if this is the case, what it tells us is that we can actually write a, c e vector, which we can define as basically being C 1 e. Now, note, C 1 e is a vector. It's a three vector. Okay, it is the set of, it's this vector or A equals one, and so on. C 2 e to C number of nodes in the element e. Okay? Likewise, d e, it's just the d vector. Right? It's the collection of all the degrees of freedom that are used to build or interpolate, if you want to use that term, to interpolate our trial solution, our displacement field over the element, right? Okay, so there's a collection of three vectors, so it's d 1 e, right? And, and at that node 1, we have d 1, the one direction, d 1 and the two direction d 3 and d 1 in the three direction, right? Those are the three degrees of freedom at that node, right? Okay, so we have d 1 e, d 2 e, and so on all the way up to d n, n. E and d. Right? Note that c e and d e belong to r, NNE times NSD. Okay? They are big vectors. In particular if you are doing trilineals, we have a, how many degrees of freedom on a trilinear element here, for elasticity? twenty four. Okay so what we are seeing is that twenty four degrees of freedom on trilinear elements. Okay? And this 24 has got, as a, is the product of n n e. Times N sd. Okay? All right, well, what are we going to use that for? What we're going to use that for is the following, right? We are going to use that in order to write out this first integral here, and actually the second one, too. Given the first integral on the left, it's not the integral, sorry, the sum on the left and the sum on the, first sum on the right, okay? And in particular, what that's going to let us do is get rid of which of the two summation symbols on the left hand side. Right. It's going to let us get rid of the sum over A and B. All right? And in this case it's going to let us get rid of the sum over A. We're not going to do anything different with the, the code the code just yet. Okay? All right. Okay. So doing all this, what we get is, the following. Okay. So. Now, the finite. dimensional weak form. Really, this is already a matrix vector form Right? We have, we, we, we are already talking from matrix vector form. It's going to be the same following. It is sum over the elements c, e. Transpose. Now, because we're getting rid of the c, e, e, and the d, b, e, we also can, dispense with the subset the superscript in b under K e. All right? But what that means is we now get a larger matrix, K e. Right? D e equals sum over e. C. Transpose. Now we're going to call this the F internal for element e. And like I said, we're going to wait a little to get this last stub. Done, taken care of. Okay? So, for trilinears right, I'm going to stick with okay, let me talk about this in general. What are the dimensions of that matrix, K e? All right? It's got to accommodate the C transposing D e, in general. Right? So that is going to be N, N, E. Cross, times NSD. Squared. For the generally case right, we know that we still need to account for the Dirichlet boundary condition and we come back and do that later. Okay, so this is the generally case. All right? This is also, therefore number of nodes in the element times n s d. Okay? Just to point out, our K matrix, K e matrix now is a block. So, sorry, it's constructed of many three by three blocks. Right? Because each of the K a b, sorry, K a d, is was a three by three block, all right. So we have a three by three block. K 1 1. E. Another three by three block. K 1 2. E. Because, if we were bi-linear hold with trilinears we would have eight such blocks but let me call this K, one, number of nulls in the element E. Okay. You could come down all the way, until you came down, you, you would have, N N E rows, right. And this is K, E, N, N, E 1. Right? The one there is for the column. Until you come all the way here, you will get K N, N E, N N E. Right? For a trilinear element we would have eight by eight blocks, right. Eight blocks along the column direction, and eight blocks along the row direction. Right, each would be a three by three. Right, each of these is three by three. Right. Which, well it's actually by N S D times N S D. Okay? And likewise we would have FE That's a three vector. All right, I think this is a great place to end this segment. When we come back, we will pick up from here. And, completed

\section*{ID: jzXr4-vG0q4}
Welcome back. We are now at a, very close to an end game stage of our formulation for 3D elasticity. What we are going to try to do in this segment, and perhaps it will spill over into one more segment, is to assemble a global matrix factor equations. And talk about Dirichlet boundary conditions, and the final solution step, okay? So let's get started on that program. What we will do now, in this segment is, write out the global matrix vector equations. And in order to do this, what I'm going to do is, just write in the first line here the actually, the, the, the final matrix vector weak form, but retaining explicitly the sum over elements and then we work ahead from there, okay. So this is an equation we developed toward the end of the last segment. It's, sum over e, c e transpose, k e d e, right? And remember that your k e is our, elements difference matrix, right? This is equal to sum over e. Ce transpose fe internal plus sum over i, right? Over the spatial dimensions. Sum over the elements that have some part of their element boundary as coinciding with the alignment boundary of the problem, right? And then for such elements we have the sum over the nodes that actually over all the nodes, right? because we worked out the bit about nodes belonging only to the Dirichlet boundary over such elements, right? We got past that point. So we have the c is our vector of degrees of freedom for the weighting function. And we have this forcing vector, which we've been denoting as f t bar node a, spatial dimension i element e. All right from here we go on to the business of assembly. Okay? And in order to see how that works out, we are guided by our global definition of the c and d vectors, right. So let's start out with the global c vector, right. That is c and the way it is defined is the following. We have we start out from the, we start out by following the global nodes. Right, so we have for global node 1, we have degree of freedom 1. Right? Which would be in our case the it would correspond to the spatial dimension one. And then we would have the same node. Degree of freedom two corresponding to spatial dimension two, and spatial dimension three. Right? This would carry on, and for the general node A, we would have the same situation. Right, until it came down to the very last of our c degrees of freedom. Alright, you note that I'm not, locking us into a situation where the very last degree of freedom, right? Or the very last node which would be sitting here. Those degrees of freedom would be sitting there. Degrees of freedom would be sitting there. I'm not walking us into a situation where that has to be the last node in the problem, or the last numbered node. Because of course we know that the definition of Dirichlet boundary conditions may very well eliminate such nodes from having weighted functions being interpolated off. All right? So, I'm leaving open that flexibility here. So this is the global c vector, all right, and likewise the global d vector, all right. And I'm going to now call this a d bar vector, all right. And you may recall from our previous treatments of 1D problems as well as the 3D problems. But for scalar variables that we are doing, we are calling this d bar. Because we know that there are some of those degrees of freedom that we want to later on move over in order to impose Dirichlet conditions. Okay? So okay. So the same thing happens here. We have d one, one, d one, two, d one, three. All right. Carries on to da1 da2, da3, right? That's for some general note, right. dA1, dA2, dA3 simply represent the displacement, degrees of freedom In the respective directions. One, two, three. Along with respective coordinate directions, one, two, three. For node A. And then this carries through all of them. D down to number of nodes in the problem. One, two Okay? In general, the D bar vector will be. Will be what? Bigger or smaller than the c vector? Will d bar have more components, or fewer than c, or all the same? What do you think? All right. If there are any Dirichlet conditions at all, and there have to be Dirichlet conditions for this problem, the D bar vector will have more components than the C vector. I should also mention that in setting this up, I'm assuming that the very first node here does not have Dirichlet condition set up on it, okay? So let me just say that. No Dirichlet boundary condition on the very first node. But yes, there could be Dirichlet conditions on the very last node. So I haven't specified which node we're talking of at the, as the last component of the c vector. Okay, so we have these global, so these are our global. C and d bar vectors. Okay? And then once we have this, the, the degrees of freedom that we have are, are now to be viewed as simply those corresponding to these entries, right? Each of these, for the whole problem, right, viewed as a vector problem, each of these is a different degree of freedom, right? Never mind the fact that they come from the same node, right? As far the problem's concerned, they're different degrees of freedom, right? Likewise these, right? And of course for, for, for general nodes as well, okay? All right, so in this sense, one, one would say that d bar has number of nodes in the problem, times nsd degrees of freedom, right? All components, right? c has number of nodes in the problem times nsd minus ND where this now is the number of degrees of freedom with Dirichlet conditions on them. Okay, and in calculating ND, it is not necessary. Well let me ask you, do you think it, it is necessary that ND, right, ND, is it necessary that ND should be a multiple of the spatial dimension? Right, I'm asking, is, da, does it have to be a multiple of a number of spatial dimensions? And in, in particular I'm, the reason I'm asking this question is bec, is because I want you to think about whether Dirichlet boundary conditions have to be applied to all three degrees of freedom at each node. 'Kay, so the answer to this question is that is no, right? Because we, we've, we've, we know that we could apply Dirichlet conditions on a particular coordinate direction at a point and not on the others, okay? So in ge, so, so this answer in general is no, okay? Right, all right. Okay, so we have these things in hand, and now what we will do is to go ahead and essentially write out the global form, right, from that contribution. So what we have here is that sum over e ce transpose, Ke, de equals c transpose, K bar, d bar, right? Okay? Now we already know what our c vector and our d bar vector are. The K bar itself, is obtained by this assembly operation over the individual element's stiffness matrices. Right, and, and note, of course, that here, because we have confirmed indeed that we are doing 3D elasticity, the term stiffness matrix is is, is relevant, right? It's, it's, there, there, there, there's no confusion there. Okay now this proceed, this proceeds just as before, right? What we want to, to realize is that any single entry in the K bar matrix, which corresponds to degrees of freedom belonging to different elements, right, but the same global degree of freedom will have the corresponding terms added on, okay? So this thing works just as before when we realize that we just carry out the assembly over degrees of freedom over global degrees of freedom. 'Kay? You just have to carry out assembly of a globally numbered degrees of freedom and forming K bar, okay? And when we do this, we get that K bar as units, sorry, has, has dimensions of number of nodes in the problem times nsd minus ND, okay, times number of nodes in the problem times nsd, okay? In order to sort of further explain this process, let's try to do it for a pair of elements. Let's me try, let me try and draw something that's doesn't have very complicated surfaces. 'Kay I suppose that this thing goes a little longer. Okay, this is sa, somewhat similar to the si, sort of situation we'd looked at in the case of the 3D scalar problem. Right? So, let me label only, or let me draw only the common nodes here. Okay. And let us suppose that these nodes have global, global numbering. A, B, C, D. Okay, and furthermore these elements are omega, E1, and omega E2. Okay, all right. So, let's, look at what the, the Ke1 stiffness matrix may look like, okay? Now, let's suppose that for the Ke1 stiffness matrix we have a numbering, which comes from the, from the, from the local ordering of nodes, right? We, well, we know that's always the case, but let me just label the the local number in with nodes, right? For the local numbering of nodes in element omega e1 we know that what I've labeled here is global node c, supposing we say that, that, that is local node 2. Right, and element e1, right? According to that, D would be local node 3, right? A would be local node 6 and B would be local node 7. Okay? Now, for element omega e2 exactly those nodes. Suppose that they are local node 1. D is local node 4. Right? A here would be global node, A would be local node 5 for element e2. And B would be local node 8 for element e2. All right? So, with this background, all right? Let me try and write our Ke1. Okay? Ke1 would be a matrix where I'm not going to write out all of the components, because I'm just going to write the blocks, okay? So we know that this going to be Ke1, the 11 block up to the, up to Ke1, the 18 block, okay. Because, of course, we're working with bilineals here to fix ideas. So we have Ke1 88, all right? And down here we would have Ke1 81, okay? And Ke2. Is Ke2, 11. Ke2, 18. To Ke2, 88. Ke2, 81. All right.

\section*{ID: lcDP8zC6vrg}
Okay, with this in mind, I would like to go into our global stiffness matrix, and just talk about where the where the matrices are going to be added on, okay? I should, I should mention here, of course, that each of the entries in these K e 1 and K e 2 matrices is a is not a scalar, right? What is it? Each of these is a, that's right, a 3 by 3 matrix. Right? Okay? All right. Okay, right, and, and that's the same for K e 2. Right, so now let's look at our global K bar matrix, okay? And in setting it up. Okay, let me suppose that this is row A, row B, row C and row D. Okay. And in terms of columns also I have I may have A here, B, C and D. All right, with all of this in place let's look at which sub-matrices from the K e 1 and K e 2 which I wrote in previous slide, are going to make their appearance here, okay? So let's look at the A A contribution. Okay, for the A A contribution we will have, it should be here, right? We would have from K e 1, right, if you look back at the figure I drew of the two elements e 1 omega e 1 and omega e 2, it should be clear that from K e 1 we have the 6 6 component, right? And from K, and from omega e 2 we have the, which one, that's right, 5 5 component, okay? You know, if we filled out all the ABCDs here, we would have 16 entries, and I'm not going to try to do all of that, right? So let's look instead at what happens with the CC entries, okay? So the CC entry from G1 would be K e 1, 2 2 plus from e 2 which would be K e 2, 1 1, okay? And this would go on. Of course, neighboring omega e 1 and omega e 2 there may be other elements which also share our global nodes A, B, C, D, all right, and contributions from those elements, stiffness matrices would also be added in here and here. Right, so. Okay? Okay. Now look, let's look at what happens with a with maybe the AB combination of global nodes, okay? So if we look at AB, what we observe is that the, the contribution from element e 1 is going to be, will be from the matrix K e 1. What do you think? What, which particular block is going to contribute here? It would be the 6 7, okay? And from e 2. From e 2 it would be the, right, 5 8, okay? And so on. So let's do one more. Maybe we do the, what do I have room here for? Let me do the AD. Okay? So AD, from K e 1 is going to be 6 4, and from e 2 is going to be. Oh, I'm sorry. From A 1 it will be 6 5. Right, and from e 2 it will be I'm sorry. I can't, I can't read my own, own handwriting. It would be 6 3. Really sorry. 6 3 from e 1 and for, from e 2 it would be 5 4. Let me double check that. Okay, and of course, there could be other, there will be other contributions from other neighboring elements that share the same nodes. Okay? So this is our global K bar matrix. Right? With this in mind, let's also try to assemble our forcing vector, okay? I believe that with this in place, we can do this, we can assemble the forcing vectors fairly quickly, okay? So, what we see here is that when we look at our sum over elements, c e transpose F internal e. Okay? Now we have this written as c transpose F internal, right? Okay, where we already know what the c vector is we only need to say what the, what the F vector is, okay? And since I have some room here, let me try and write out the F vector here. Okay, and I'll try to do it for the same combination that we have, okay? So for the F internal vector. It's for the same, I'm, I'm going to try to do this for the same combination of nodes that I've written out for the, that I've used for the stiffness matrix. Okay, let's do this. Hopefully that gives me enough room. And let me say that the nodes A, B, C and D make their appearance at roughly those points of, of the, of the vector. Okay, global node A. What contributions does it get from element e 1? Right, from element e 1 it gets the, the F internal e 1 from which local node of element e 1? Right, local node 6. And from e 2. From e 2 it gets a contribution from local node 5. Right, this continues and let me try to do the same for global node C. So from element e 1, it gets F e 1 internal node 2. And from A two, it gets F internal, 1, okay? Right, and of course there could be more contributions here, right, because of the fact that there are more nodes, sorry, more elements off to the side, right? Okay, so all of those contributions would go into those entries for global nodes A and C. Okay. So that's how we go, set about assembling our global stiffness matrix K bar and our global force vector F internal. We'll end the segment here. When we come back, we will talk about how we treat the traction or the Neumann boundary condition terms.

\section*{ID: x-PTHIibfSs}
There was an error in board work that I made on, this slide here in front of me. The error is in, where I put that block matrix in the larger K bar matrix. Properly, if you look at those terms, K e1 sup 6 7 and K e2 sup 5 8, and look at where they come from, and in order to do this, let's just go back one slide. Okay? So now if you look at the way the elements and nodes have been written out for, or have been sketched out for this little assembly of two elements, you will note that the K e1 6 7 component Is map, is one that maps on to the global K bar AB block of the matrix. And the same holds for the K e2 sup 5 8 components, which are those two, okay? They also map onto the, to the global D bar AB block of the matrix. When you go to the block matrix K bar, what you will you note that I've written then in the K bar BA position for, in terms of blocks. This just needs to be moved up here which is the K bar AB position. Right? It's the AB position because the rows are got from there and the columns are got from there. It's a bit of a squeeze the way I've written it because the  K bar AA component turned out to be a little wider than I should have written out, but, the important thing is that this, block should appear right there. Okay? With that correction, which is an important one of course, we also note that the same sort of thing may have happened with the block that we've written out here. Let's just check that. So we have K e1 6 3 and K e2 5 4. Let's go back and see where they come from. So K e1 6 3 would be the A d component, right? And K e2 5 4, when you look at this assembly of two elements, would also be the global A d component. That term however, that block term however, is in the right position, okay? And once we look at things in that light, we see that now this K bar matrix is correct with this one correction. Right? That block should appear in that position. With that, everything's consistent and you can go ahead and, program this into your code.

\section*{ID: Nv22ARg6J90}
In this segment, we'll start looking at the homework coding template for homework four. All right, and that will be the 3-D linear elasticity problem, the steady state problem. Okay, so let's come over here to the code and we'll look first at the main.cc file. And as you can see, almost nothing has changed from the previous homework assignment. I, of course, changed the name of the include file but it's still 3D passing in the mesh size. This is, for example, 10 x 10 x 10 element mesh. Okay, create our object and then go through the same steps as before. All right, so we can go straight over to our header file. And we have our same header files here. But now, in this homework assignment, we are going to be using DL2's quadrature rules. We'll be using DL2's basis functions. And so that will make things a lot easier on the coding end of things, but then it does change the structure of our code a little bit. So we'll be looking at those changes here. First off, I'm going to declare order and quadRule as global variables. Actually not variables, they're constants. I've designated that here, that they are both constant integers. And I've defined them here, because I use both of these numbers in the constructor of some of our class objects, okay. So, let's scroll down and look at the declaration of objects in our class. Declaration of objects and functions. We have, of course, the same class constructor and destructor. Here I have a function called C, this is our elasticity tenser. Now, DL2 actually does have the capability of creating a fourth order tenser, which the elasticity tenser is. I've set that up here as a function where the inputs are just the four indexes, and then it outputs that component of the tenser. Okay? So you can use that when you're creating k local. The solution steps themselves are all the same as well as these first three class objects, but now we have, again, the DL2 quadrature rules. We have two quadrature rules, a quadrature formula for volume integrals and a quadrature formula for surface integrals, which is the face quadrature formula. We'll be showing you how to use those later on. Other than that, the k, d, and f matrices and vectors are the same. Slight change here, I've changed this object for the table. Instead of no location, it is dof location, or the location of the degrees of freedom. The reason I've done that change is because since this is 3D elasticity, we now have three degrees of freedom per node. And so, there are three times as many degrees of freedom in the system as there are nodes, okay. And, this table gives you the location of each degree of freedom. So, what that means is degree of freedom, zero, one and two, which all correspond to the same node, would have the same location. But they each have their own row within this table DOF location. Okay, course bound, you guys, map is the same. I'll scroll down to the construction destructor. Here in the constructor, again we're, we call the constructor for FEM dof-handler, but I'm also calling the constructor for quadrature-formula and face-quadrature-formula. All right, because they need to know what the quadrature rule is. And so that's what you've defined up at the top. The default value is quadRule of 2. Leave it at 2 when you turn in your homework. But if you want to you can easily change that to 3, 4, whatever, DL2 would automatically take care of that here. Also, you'll notice that I left order as a variable that can be defined. Again, I have defined it as one at the top, leave it as one when you turn in your homework. However, feel free to, on your own, to change it to a higher order of basis functions if you would like, d02 automatically takes care of that. Okay? But again, when you turn it in leave it at order equals one and quadRule equals two. Okay? So those are the small changes in our constructor again. Small change here with the names of the output vectors, because again it's a vector field instead of a scaler field. Let's scroll down and look at this function C, which is the elasticity tensor. This is the first part that you actually have to add that something. And it's simply to input the values for the Young's modulus and the Poisson's ratio. From those values, I can reconstruct the Lame parameters, lambda and mu. And with lambda and mu, I use the formula that you saw in the class, in the lectures, to create, define the particular component of the elasticity tensor. Now if you recall. C = lambda times your second order isotropic tensor, tensor product with, again, the second order isotropic tensor plus one-half mu times the the fourth order isotropic tensor. Okay? And what that means in indicial notation, is that we have Cijkl equal to lambda times delta ij times delta kl. Of course, delta here is the Kronecker delta, where if i equals j. Oh, let me write that down here. Delta ij is equal to 1 if i equals j and 0 if i does not equal j. Okay. Sorry, slight mistake up there, it should be 2 times mu instead of one-half. Okay, so we have 2 mu, and then here we have one-half times delta I K, delta J L plus delta I L, delta J K. Okay? Of course those can cancel out. And that's what you see in the code here. You notice I'm using these conditional statements here. If i equals j, if k equals l. If i equals j, then the condition is true and so it returns one. If i is not equal to j, then the condition is false or returns 0. So it's acting the same as a chronic or delta. All right, so it's a little bit cleaner or quicker than typing in an if statement, but it serves the same purpose in this case. All right? If we look at generate mesh this is exactly the same as our 3D heat conduction problem. We simply have to define our, the limits of our domain. And that creates a mesh for us. All right, for define boundary conditions, again this is something you'll fill in for yourself, and it will be very straightforward, similar to the previous assignments. Again, a slight difference here is that we're using DUF location instead of node location, but that's a small change. Another element that gets introduced here, though, is the fact that you may have different Dirichlet boundaries for each degree of freedom, for each nodal degree of freedom. And by that, I mean you may want to fix on a certain phase, the displacements and the extraction, but not fixed the displacements in the y or z direction. If that's the case, then your if statement, you would check not only the location of the degree of freedom, but you'd also have to find out what's the nodule, the corresponding nodule degree of freedom. In other words, is that degree of freedom a Dirichlet placement in the x, y or z direction? And so I've explained that a little bit here, in the notes, in the template. However, you don't have to worry about that in this assignment because our only Dirichlet boundary condition is to fix all degrees of freedom on the face where z equals zero. So all have to do is check is the z component of DOF location equal to zero? If it is, set that degree free equal to zero, whether it's in the x, y, or z direction, okay? But in the future, if you want to, you actually can distinguish between displacements in the x, y, or z direction when you're playing Dirichlet boundary conditions, okay? Let's quickly look at setup system. There's actually no difference here, other than the fact that we're creating dofLocation as a table rather than no location, but nothing that you have to change. Okay, so we'll stop this segment here, and in the next segment, we'll look at the single system, which again, will be the meat of this template.

\section*{ID: k3pswnNaT-U}
In this segment we'll move on to looking at the assemble system function for the homework four template. So let's look at that code now. You'll notice first off we have two new objects. We have this fe-values and fe-face-values. These objects are DO2 objects that hold information about the basis functions, the basis function gradients, information about the quadrature points, the Jacobians, and all this. But we no longer have to calculate the Jacobians. We don't have to write out the basis functions themselves. We don't have to write out the quantiger points It does all that for us, all right? So if you'll look, the first two are just fe and quadrature formula so that it knows what basis function order we're using and it knows what quadrature rule we're using. The third input value is actually a series of input values, of flags that tell fe values what information we're going to be using and that's the information that it'll be updating. So it makes a little bit faster or saves on memory that we don't need to update information that we won't actually be using. All right, so for fe values, which is what we use for volume integrals. We're going to be updating the values, which are the values of the basis function. Update gradients, that's again the basis function gradients. And then, this JxW, that's supposed to be J times W. J stands for the determinate of the Jacobian, and W stands for the quadrature weights, so it's all three quadrature weights multiply together in 3D. All right, so it takes the care of all of that 4s. The gradients we use in Klocal, the values we would use in Flocal if we had a body force. Okay, and course J times W we would use in any volume integral. If we move on to fe face values, you'll see I'm updating values. Quadrature points in J times W values. J times W values is as before. Only now you'll note that since it's a surface integral, it'll be the determinant of the Jacobian mapping from the 2D by unit domain to the 2D surface domain, right? update-values is, of course, still the basis function value on that surface. update-quadrature-points, this actually will give you the position vector of each quadrature point in the real domain, all right? So we need that in this problem because the traction for a Neumann boundary condition depends on the X1 component. Okay, so it varies as X1 varies on that Neumann surface Z equals 1. Okay, so that's why we're updating those values. So, let's scroll down into our element loop. Now, you can see here the first thing is that we do fe-values.reinit(elen), so fr-values is reinitializing for this particular element. So it's getting all the correct values for the quadrature, for the Jacobians and so on. The first step that we're going to look at is defining Klocal. Now before you look any any further at the code, let's look at the board at what that general form is. Okay, so in class we, or in the lectures we looked at Klocal and we had four indices that we were dealing with. Klocal AB ij. And the idea was that AB run over the nodes in your element. Okay, so I will go from 0 up to just less than the number of nodes, In the element. Okay, I and J are nodal degrees of freedom. And so that we run it from 0, 1, and 2 in this case for the 3D elasticity. All right, and the way we pictured that is that we had our Klocal matrix and then inside, we had these little sub-matrices. All right. And so, again there are eight nodes in a hexahedral element and so, these went from 0, 1, 2, 3, up to 7, all right, the same on the side, 0, 1 to 7. Now within the submatrices, we had indices 0, 1, 2, 0,1, 2. Now those correspond to the degrees of freedom at that particular node, okay? Now of course Klocal is not a matrix or matrices in d0l 2. It's just a matrix that's 24 by 24, all right? So we can look at this instead. This makes it, as 0, 1 2, 3, 4, 5, and so on up to 21, 22, 23. Okay, so let me label these. These would be our element degrees of freedom here. Going from 0 to 23, then we have our element nodes, Here. Going from zero to seven. And then we have our nodal degrees of freedom. Here. Going from zero, one, and two, all right? So you will notice when you look at the code again that our loops are actually looping over the element nodes and the nodal degrees of freedom. However, the indices in Klocal will be in terms of the element degrees of freedom. Okay, so how do we do that conversion? Because the indices start at zero it actually makes it simpler in this case. So we would do it like this. Klocal ABij corresponds to . I guess I should do square brackets on both of those, right? And you can see that that's true. Let's do it for 22 here. So that would be A is equal to 7, so 3 times 7 is 21. The nodal degree of freedom would be 1 at that point. So 21 plus 1 gives us an element degree freedom of 22, okay? And we'll work the same way for Flocal. For F local, it'll be F local A sub i, would be the same as F local of 3A, 3 times 8 plus 9, we'll look at that again in a second. Now let's look at how we actually calculate Klocal ABij. Let me write out the formula here. So we have Klocal ABij = the integral over the domain of the element, basis function A, the derivative to with respect to X of j, Our elasticity tensors Cijjl times our basis function B, ldV. All right and notice, we have repeated indices here. So there's an implied summation over j and l. For j and l going from 0. One and two, okay. All right, so let's look at what that will be in our node, in our code over here. All right, so first off, in our loops, we have a loop over quadrature points. Notice it's a single loop over quadrature points. d0l2 has combined all three loops since this is a 3D case into a single loop, all right? And it's keeping track of that for us. Now we have our loop over A and i, which is the loop over nodes A and then loop over nodal degrees of freedom. B and k again looping over element nodes and element or nodal degrees of freedom. Now, actually, I need to come back here and make a small change that I noticed. That's actually not the way I've written it here. That's not for Klocal ABij. It's actually for Klocal ABik, and the reason you should be able to see that is because j has already been removed through the summation. ink are three variables here, and so that's what should show up in the indexes of Klocal, all right? So that's why I have A and i grouped together, then B andk. j and l,, we have a loop there because of the implied summation over j and l. Okay, and now in here you'll define Klocal, using of course this integral that we're written out. Now one thing that's important for you to know is that when we use fe-values to get the gradient, which we do using fe-values.shape-grad() It's actually given us the gradient with respect to x and the real domain. You'll notice in the previous assignments when we wrote the basis gradient functions, the basis function gradients we wrote the gradient with respect to c and the bi unit domain and then we had to find the Jacobian. Do the inverse and so on, right? That's all taken care of here. Okay, so we don't have to deal with the Jacobian in order to make that a gradient with respect to the real domain, that's already done for us. Okay, so in order to access that gradient, you do fe-values.shape-grad, and then you'd input the element degree of freedom. Okay now, notice that's a little bit different than what we've written on the board here. In the lectures we always use the element node number to designate what basis function we're using. Here in d0l2, it's the element degree of freedom, okay? So it will be the same as whatever index you're using in Klocal. So this wouldn't be, for example, this wouldn't be A, this would be 3A plus i. And this wouldn't be B, it would be 3B plus j, for d0l2, okay? Sorry not j, plus k, all right. Again, you'll need to use the elasticity tensor function that I created before. And in order to use det to get your dertiminate of j times the quadrature weights we'll use fe-values.Jxw(q)*/ for this particular quadrature point. Notice also that we aren't inputting the value of c at the quadrature point, we're just inputting this index q which tells you what quadrature point we're at in the loop. Okay, also, to clarify, fe-values.shape-grad gives you a position vector. It's actually a d0l2.object, but you can think of it as a position vector. Or sorry, it's actually a first order tensor, which is very similar to a point and d0l team. Okay, so if you want to use a particular component, which of course you will, you can use just these square brackets. i or j or whatever it may be, okay? So, that should cover it for creating Klocal. Let's move down to Flocal. Now we don't have a body force in this problem or any forcing function like that. But we do have Neumann boundary condition. And so that will involve an integral, Over the Neumann boundary, okay? So let's write that out here. So for Flocal A sub i, which again is the same as in our code, it will be 3A or times A since is equal to 3, + i = this integral over a surface and so I'm going to designate that with this partial omega. And I'll use T to specify its attraction. It's the Neumann condition, and it's for a particular element, okay? And we'll be integrating hiI, so h is the traction, i is the component of the traction, the traction is the first order tense of our vector, times NA, and again it's a surface integral. All right? Okay, so let's go back to the code. You'll notice first, I'm doing a loop over the faces of the element, the current element. Of course there are six faces. So we loop over those and we're going to update fe-face-values for this, not only this current element, but also for the current face. That will put this quadrature points on the face itself that we're on. Now we're going to check to see if that face is at our Neumann boundary, okay. So this lm arrow face arrow center, that gives us a position vector at the center of the current face. And since I'm interested in the z component, I use square brackets too. Okay so here in this if statement, I am checking to see the point at the center of this face at the boundary z = 1, which is our Neumann boundary, okay? If it is, then I'll perform this surface integral, okay? If not then I will just move on. Either to another face, and if none of those faces were Neumann faces I'd move onto another element, and Flocal would be zero for this element, okay? But once we are, once we have found that Neumann face and we are performing that surface integral, we'll move inside and loop over our face quadrature points. Okay, notice that, the number of face quadrature points. Here I've extracted for you the value of X, the x-coordinate of the current quadrature point, okay, and I've done that using this fe-face-values.quadrature-point(1). If I wanted the z component then I would just change that zero to a two. If I wanted a y, I would change it to a one, okay. But then what you need to take that value of x and specify what the value of the traction vector h is at that quadrature point, okay. Now, once you've done that we'll move inside our loop over the nodes and the nodal degrees of freedom, okay. And, once inside there you will again use this integral that we've written out on the board to define Flocal. Now you'll notice I am looping over all eight nodes of the element. Even though we're only doing a surface integral. How are the quadrature points themselves, D02, is placed on the appropriate, correct, surface the correct face. So, we are only integrating over the surface itself. All right, notice again, we'll be using fe-face-values to find the basis function value. Okay, and you are still passing in the element degree of freedom. So that's 3 times A plus i. Not just A, okay. And again when you're finding JxW, the determinant of J times the quadrature weights. Again use fe-face-values, all right. Everything involved with the service integral will be using fe-face-values, not fe-values at all, all right? So once you've filled that in, you'll have Flocal created as well, and once you have Flocal and Klocal. A symbol system will be very straightforward. It will be exactly the same as previous assignments, all right. Applied boundary conditions will be applied in the same way. Again, you've already specified your knowing your boundary condition. Fixing all degrees of freedom, X equals 0. And I'll scroll down a little bit more just to finish up quickly. Solve is exactly the same and output results again is the same. Outputting our displacement results as a .vtk file which you can then open up using para view or visit to look at the displacements. All right so that concludes our this segment and it concludes our discussion of the template for homework four

\section*{ID: H1aHhCENGmM}
Welcome back. We are aiming here, to complete our assembly of the global matrix vector equations. And, talk about the final Dirichlet boundary conditions. So, to do that let's get on to the contribution that we have not yet tackled for the global equations. And this is the contribution from the Neumann or the traction boundary term, okay? So, the contribution. To global matrix vector equations. From the traction, right. Remember the traction is simply our Neumann boundary condition for this problem. Right. So the term we are talking about is this one. Sum over i, right, i running over the spatial dimensions. Right. Sum over i, sum e belongs to E Neumann, right. Sum over A, that's all the nodes in that particular element, right. We have here CAie Ft bar. We have the A index there. We have the i index there and we have the e index here. Okay, what I'm going to tell you in one fell swoop is that this is going to show up as c transpose Ft bar, right, globally. And I will write down on the next line the detail construction of the F matrix. And in order to do that, I am going to take I am going to write here first the C transpose vector a, as a row vector. Okay? And I am going to get myself room for it here. Okay, and the idea is that multiplying it is our FD bar vector. Okay. And here too, let me get myself enough room. Okay. So let's construct C. Sticking with the same assumptions I've made before we have a contribution from the very first numbered global node, okay. Because I'm assuming that our, in considering the case, where we don't have the Dirichlet boundary conditions on any degree of freedom on the first node. So, we have C1. Okay. And we're working with the outer sum first, right? The sum over spatial dimensions. So I'm setting i equals 1, and that is the 1 that shows up here. Okay, and that comes from some element, right? The fact that we're doing a sum over elements is already accounted for in the fact that we have this global node here, okay. Right so that contribution would be. F t bar. Let me see. It would be local node number one for that element. Okay? And since we are talking of global coordinate direction one. We would have a one here. Okay? It's going to be some element, let's call it, let's just leave it as E. Okay? And let's go on now, with this. And actually in going on, let me also use the same numbering that we used in order to construct the stiffness matrix and the F internal force vector, okay? So I have before me here those two elements, omega e1 and omega e2 and you probably noted them down in your in your book, or your notebook. So I would encourage you to go back and look at those. Okay? Because what I'm going to do now is look at the contributions to global node A. Okay? So let's suppose that global node A shows up here. Wrong C vector. Let's suppose that B shows, shows up here. Global node C shows up there, and global node D shows up there, okay? So that's, A is going to be here, B, C, D, okay? Now we start out as as I did for, for this very first degree of freedom, we start out with the spatial dimension one, okay? So the global node here, right, is cA1, okay. And let's suppose that on this force vector A shows up here. B shows up there, C and D. Okay? Now the only we would have a contribution from the A node is what? Okay. What we need to have is that the local well what we need to have is actually we, we can talk of it, of it in terms of the global nodes and so global node A, right, lies in partial omega t bar one. Right? That's the only way we would have a non zero contribution to the F vector from global node A. Okay? So let's assume that this is true. Let's consider the case where A does lie in omega, in partial omega t bar one. Okay? Alright, so you could go back and look at that at that figure that I drew back there two two or three slides ago. What I am seeing is that node A from that figure does indeed lie on, lying on, partially on , partially on omega T bar one. Okay. So, we would have a contribution, then, from element e 1. And the contribution from element e 1 would be F t bar local node 6, right, spatial dimension 1, element e 1. Okay? Right, now element e 2 also would have a contribution here, right? So we would have F, t bar, right? From element e 2, the contribution would come from local node 5, okay? The spatial dimension would still be 1, right? And of course, there's element e 2, okay? So this would be the situation if I'll reproduce that figure of two elements, somewhat more defined. All right, so again we have omega e 1, omega e 2, right? And I'd lo, I'd label those nodes as A, B, C and D, okay? What I'm seeing now is that let us suppose that D surfaces. Right, that, those are the sort of front surfaces of both the elements, right? Belong to belong to partial omega D bar. Sorry. Actually let me, let, let me write this properly. Okay, bo, both those surfaces I'm saying belong to partial omega t bar, okay, 1. All right. Okay, right. And, and then in, in terms of local numbering, what I'm seeing is that let me identify the surfaces I'm speaking of here. Okay, for element omega e 1, that's, nodes are 1, 2, 3, 4, 5, 6, 7, 8. And for. Omega e 2, the nodes are 1, 2, 3, 4, 5, 6, 7, 8. All right? Okay, so we would have those two contributions, right? Let's suppose then for omega, for, for the Neumann boundary in the 2 direction, right, which means the degrees of freedom are being controlled in the 2 direction. Sorry, not the degrees of freedom, but, but the traction components in the 2 direction are being controlled on the other surfaces, right? So let's suppose that it, it's on this surface and it's on this surface, okay? So let's supposed that this belongs to partial omega t bar 2. And this also belongs to partial omega t bar 2, okay? So the, can you think of which surface, do you see which surface I'm saying belongs to partial omega t bar 2 from element e 1? It is the 5, 6, 7, 8 surface, right, of element e 1. On element e 2 also it's a 5, 6, 7, 8 surface. Likewise, on element e 1 the 1, 2, 6, 5 surface belongs to partial omega t bar 1. And in element e 2, which one is it? It's the 1, 2, 6, 5 surface, okay? All right, so if you underst, if we are clear about that, let me put in the contributions then. All right, so for for the contributions from the 2 direction to the, to this traction force vector, would there be anything from node A, from global node A? There would be, right? So we, we would have a C A 2, okay? And that would show up now right here, right? It would be F t bar global spatial dimension 2, right. We would get a contribution from element e 1, right, from its node number 6, okay. Right, and we would get a contribution F t bar global 2 direction element e 2 local node number 5. Okay? All right. Which other nodes would contribute? So let's look at node B, okay? What contributions would it have? It would only have contributions from the 2 spatial dimension, right? Spatial dimension i equals 2. All right? So, for this node we would get a contribution from C B. Let's put the C B 1 contribution, then let's talk about the C B 2 contribution. Since the C B 1 contribution, right, on that node is 0, right? There is no contribution to the traction there, right? We are not controlling the traction there, okay? But that's a free surface, right, because that, that is a surface on which a traction may be specified, right? We have just not specified the 2 component there. So when it comes to the B node, we get a 0, okay? The contributions that do come there are from the 2 direction. And we have then F t bar, we are talking the 2 dimension, so we have 2 here. We are, okay, let's look now at the contribution from e 1. From element e 1, which local degree of freedom contributes there? It's the 7, okay? From F. Sorry from element T2. Sorry element E2. The contribution along the two direction, right, would be from local node eight. Okay? All right. Now, let's suppose that no other boundaries, of these two elements correspond to Neumann boundaries for the global problem. Okay. That means that if we go ahead and look at ca 3, aas well as cb in 3 dimension, right? We would get 0s, right? So here we would get a 0, and here too we would get a 0. Okay? All right? Let's see what node c does as I have drawn it, what traction surface does global node c lie on? Right? Also in the global spacial dimension one surface. Right? So when we come back here we have C and unfortunately the global  number of that degree of freedom is also c but hopefully we can cope with that, with that repetition. Okay. There is going to be something from the cc 1 direction right, from the one direction there. So we move along, right? And then we come to the c node. We get a contribution of the form F D bar, along the one spacial dimension, from element e1, right? So, which node from element e 1 contributes? It's the local node two. From element e 2. Which one is it? Local node one. Okay? Now, as I've drawn it, for node c, right? Global node c. There are, no other, spacial dimension, no other, traction boundaries that contain that node. Okay? All right? So what that says here is that for C c 2 and C c 3, I would get 0 and 0, okay? So let's just complete this particular line that I wrote. Global node a lies in omega t, omega t bar one. C also lies in partial omega t bar one. B Lies in partial omega t bar two. Okay. But to note, the way I've drawn it, A lies in this and in partial omega t bar 2. Okay? There's a partition there. Okay? So A the way I've drawn it node A is the only one that lies on two traction surfaces. C lies on one, and B lies on one. Okay. And which one the line has been written, has, has been denoted here. So, and, and for the more, for the way I've drawn it here, let's suppose that D belongs to no traction boundary. D lies in none of the boundary subsets. Partial omega t bar, i where i equals one, two, three. Given that, what contributions would we find in this traction force vector for no, global node D? You're right, all zeroes. So you get a zero, zero, zero, and we would go on. So, hopefully this process has given us some idea of how to construct this global traction force vector. Okay? This is what we are calling it. FT Bar. Okay?

\section*{ID: y1kCV1v8b-Y}
With all of that, when we put things together, we now have our global matrix vector equations. All right, and they are the following. C, T, K bar, d bar equals C, T, F internal plus C, T, F, T bar. Okay. What's the last step you need to take? Dirichlet boundary conditions, right? Okay? And in doing that, we need to rec, we need to come to the fact that our K bar, probably due without that arrow. What are the dimensions of our K bar matrix? Right, they are number of nodes times nsd minus Nd, right, where the Nd corresponds to Nd is the total number of Dirichlet degrees of freedom total number of degrees of freedom where Dirichlet boundary conditions are specified. Right, and this could be you know this could draw maybe, the, you, the, the one direction on some node, the two direction on some other node and the three direction on yet another node, if we sum this up we get three, in that case Nd would be 3. Okay, so, this times the number of nodes in the problem times nsd. Right? So K bar, these are the dimensions of our K bar matrix. Okay. Right. Okay. So clearly K bar is a rectangular matrix right? And what we need account for is the fact that when we look at the c transpose vector here, all right, multiplying K bar, right. And here we have our global d bar vector, right. Let's suppose that now when I label the degrees of freedom for the d bar vector, right, that is d bar 11 d bar 12, d bar 13. Okay. Now let me suppose that some global degree of freedom, all right? So let's suppose d a, d bar a 1, d bar b 2, and d bar c 3. Okay? Let's suppose that these global degrees of freedom are known Dirichlet boundary conditions. Okay? What this says is that d bar a 1 is known. I'm now going to write d bar a2 and d bar a3 because those are not specified. Right? Okay? Well, actually, actually, let me write them and, and specify and, and so to mark other ones that were known. Since I've run out of room here  let me just say the d bar, let me just get rid of d bar C, 3. Okay. So those are the ones that are known. Okay, so d bar a 1 and d bar b 2 are the, are two known Dirichlet boundary conditions, right? And, and there, there will be more, of course, in this sort of problem. Okay, so, that means that that degree of freedom is known, and that degree of freedom is known Okay. All right. And when we carry out this matrix vector product, we know that this entire column Right? And another one, right, are going to be known. Okay? So, for the way we had, numbered it previously, this column is what I would call the K bar, let me see. The column number here is the following. Nsd times a, right?, where a is the global node number, all right, plus 1. All right, because it corresponds to d bar a 1, right, 1, coordinate direction of global node number a. Okay. And this column likewise is K bar. Column number nsd times b plus 2, right. Right? This is the column.  Okay? So, since d bar a 1 and d bar b 2 are known. We do just what we've done before which is to account for the fact that those, that these columns multiplied by those known degrees of freedom can be moved to the right hand side. Okay? So we do that. Right? And then we are left with a reduced system, c transpose K d equals C transpose, F internal plus F t bar, right, minus d bar A1, which is a scalar degree of freedom, multiplying the column that we identified on the previous slide K bar nsd times a for the global node number plus 1 minus d bar. B2 scalar degree of freedom, multiplying the quantum number of K bar. nsd times global node number B plus 2. Okay. And as we've been doing in previous problems, that is our final F, vector. Okay? So what this implies for us then is c transpose, K d Minus F equals 0, right? And the degrees of freedom sorry, the dimensions of this K matrix now, it's square, right? because we got rid of all the Dirichlet degrees of freedom which were known and moved them to the right hand side. Okay, so K finally is number of nodes times nsd minus the total number of degrees of freedom that have their Dirichlet conditions specified on the N square. Okay? That is, those are the dimensions of K bar. Now of course we invoke our waiting function condition that our weak form, our finite dimensional weak form must hold for all waiting functions in the appropriate space. And the fact that it must hold for all waiting functions in the appropriate space is enforced here. By the requirement that this matrix vector equation that we have as the last line of the slide, must hold for all c vectors belonging to a Euclidean space of dimension number of nodes, times number of spacial dimensions minus ND. Okay? Which implies for us finally that we get back the same matrix vector from the equations. We know Kd equals F. Right? And we solve this for D, which will give us our global displacement vector. Okay? I should make one remark here that since we are talking of d being defined as K inverse F. Right? Under what conditions does the solution exist? Okay? Solution exists. Or there exists a solution d if our K matrix is positive is, invertible of course, Right? Under what conditions is it invertible. What, what, can you think of what it is that guarantees invertibility of that matrix? It turns out that for the 3D elasticity problem, K is positive definite. What conditions make K positive definite? One of the things that makes a positive definite is because our elasticity tensor, C is positive definite. Is there any other condition? Yeah. If you have some experience with these types of methods and with solving linear systems of equations, you will probably recognize that it has something to do with our boundary conditions as well. All right? We need to have enough Dirichlet boundary conditions to eliminate what are sometimes called rigid body modes in the context of elasticity. Okay? So K is positive definite and C is positive definite. And if the Dirichlet boundary conditions eliminate rigid body modes. Okay. And the question of how to do that is a little more involved which we really won't get into here at this point. All right. At this point we are actually done with our treatment of 3D linearized elasticity. This was our example of a vector problem and we'll end this segment and this unit here. When we come back, we will move on to a wholly new class of problems.

\section*{ID: Agvq4CxFTDU}
Welcome back. With this segment we are going to start a new unit, and this will take us away from elliptic problems. We are going to start looking at parabolic problems. Right? And we're going to, going to stick with linear parabolic PDEs in three dimensions, but for a scalar variable. The kinds of problems we are looking at, therefore, are very similar to ones we've looked at, we've already considered. They are the, the unsteady heat conduction problem in three dimensions, or the unsteady mass diffusion problem also in three dimensions. You will recall that previously we studied the steady state versions of these two physical problems, and, because we were looking at the steady state versions, those, particular PDEs, are what we call elliptic PDEs. When we bring back the the time dependence, and say they're unsteady problems, we have parabolic PDEs. Okay. So with that somewhat verbal introduction let's get on with it, right?. Linear. Parabolic PDE in a scalar variable, in three dimensions as well.  Okay? And, like I said, the physical problems we are considering here are unsteady. Heat conduction. Unsteady heat conduction and mass diffusion. In 3D. Okay? And just remember that unsteady here means that we're talking of time dependent. All right. So, what is the situation we have here? I don't have with me today my my basis vectors, but we don't really need them. We have our body, right? We have basis vectors here, three dimensional. Everything that we talked about, the, the steady state heat conduction problem holds. Okay, so we have surfaces on which we are going to specify Dirichlet and Neumann conditions for the temperature if you're doing heat conduction or the concentration, if we are doing mass diffusion. That's fixed, okay, that remains the same. We have a source dom. We have the notion of the conductivity tensor, or the diffusivity tensor. All the same. The additional component is that we are saying now that at every point in the domain. Either the temperature if it's the heat conduction problem. Or the concentration if it's the As diffusion problem, is changing with time. Because of flexes, or because of the source term. Okay? So at every point, we will have an addition, a time dependent term. Its going to be a first order time dependence, because that is the nature of parabolic problems, right? And that is indeed the nature of the heat con, of the time dependent on unsteady heat conduction and master fusion problem, right? Their first order in time. Okay, so with that setting, let's let's wr-, essentially write out the strong form. Okay, and as we've been doing, let's begin by drawing a picture. The figure, these are our basis vectors E1, E2, E3. Right, our domain, that, right? Three dimensional, of course. We have three basis vectors here. Omega. Right? Somewhat mercifully perhaps, we are back to a scalar problem. Right? So, we don't need to worry about the three different, decompositions of the boundary, right? So we have here, partial of omega u, right? Because u is now a scalar once again, and here we have partial, I believe omega j is how we denoted it, right? All right. Now point here has position vector X which we will use, right?. And at this point the picture here, the usual pill box argument that's given is the following, right? We look at a little elemental volume. Okay. What we see is that we have a flux is coming into it. Okay? And since we've already introduced the notion of a flux before, we can use it now, okay? So, The flux is coming into it, and, and for certain it could be exiting some part also, right? So next one. So this is our flux factor J, okay? Now, inside that little volume element, we have some source stone. And that source stone, you recall, if we're de, dealing with a heat conduction problem, would represent local heating, right, through some external source. Or for the mass diffusion problem, it would represent a local supply of mass, okay? So let me write an F if I can there. Okay? That's F. Now, what we are seeing in this unsteady description of the problem, the description of the unsteady problem is that the result of the fluxes, the net flux into that volume element and the effect of the source dom they're combined effect is to change either the temperature for unit time. Right? With respect to time either the concentration changes, or the temperature changes, with respect to time. Okay, so let's also add in here essentially a, just to portray this, let me say that there is a d u, with respect to d t dom also coming up, right? T of course is time, okay? So, the strong form is the following. As always, given the data, given, I think we were still calling it Ug back then, we had Jn, which is our influx condition. Our source f, right? And the constitutive relation that we are now very familiar with, All right, using coordinate notation kappa i j being the conductivity tensor, right? Given all of this, now, we have one extra piece of not quite detail, but it really is a coefficient that is relevant to the problem. And I want to put it down here to have relevance to make connection with the physical problems that we are trying to keep at the back of our mind. Heat conduction and as diffusion problem. That quantity is going to be denoted as rho. Okay. I'll tell you once we set up the problem what rho is. Okay. So given all of these, what we're trying to do, is the following. Find u, okay, such that the following holds, right. Rho, partial of u, with respect to time, equals minus ji comma i plus f in. Now, here is this, this part is important. When you are doing the steady problem you specify the, all our steady, all our previous problems were time independent. Right? They were all steady state problems. We specify the PDEs therefore only over a spacial dimension, over a spacial domain omega, right? Subset of R 3, in general, in the 3D case. But now we have time dependence as well. So we say that this PDE holds in a combination of the spatial dimension, or the spatial domain and the time interval of interest. And that is indicated by a cross 0 comma capital T, okay? So the closed 0 to capital T is our time interval of interest. 'Kay? And when we write omega across that time interval we are just saying that our PDE holds over a certain spacial domain omega. And over a time interval 0 to T. Okay? All right. As before we have boundary conditions. We have u equals ug on the Dirichlet boundary. We have our Neumann condition, minus ji, ni equals j sub n on partial omega j. Is our problem complete with specifying boundary conditions? No. We need initial conditions as well. Alright? Because it's a first order problem in time, we have a single initial condition. Okay? And the way we do that is to say that U, which can be a function of position. It is indeed in general a function of position and this is what we saw in our steady state problems. Right. So, we have U as a function of, as, as, as parametrized we have position and at time t equals 0, okay? Equal sum U. Not function of position only, okay? All right? And perhaps this is best clarified by also saying here that U is a function of position. And time. Okay? All right. That indeed does complete the specification of our problem. PDE boundary conditions and initial conditions. What I'm going to do here is just make one or two remarks. Okay? The first remark is that, we need to say something about this new coefficient we've introduced, rho. Okay? For a heat conduction problem And for heat conduction problems, can you tell me what rho is? Yeah. For heat conduction problem, rho would be the, rho is the specific heat.  Okay? And, and in the case of heat conduction, do you also know where our p d comes from? What, what physical principle leads to our PDE? It's actually the first law of thermodynamics. Okay? So in that setting rho is the specific heat, okay, and the way we've written it, rho would be the specific heat per unit volume. Okay? So it does as we went you know row with specific heat per unit volume. The specific heat also can be determined as a can also be defined per unit mass. Okay but in our in our setting for the way we've set up the problem. Loads the specific heat per unit volume. Right? It turns out that if however we were looking at the the mass diffusion problem. Rho is equal to 1. Okay? We don't need a notion of spe, of specific of anything like a specific heat in the context of mass diffusion. Mass diffusion just rises from a, physical principal, which is, the conservation principal. Okay, so that is the setting for, for, for, for this problem, sorry, that is the, sort of, setting of context for For, for the physical problems. Okay, so let me see. Is there anything else we need to really talk about here? Actually I believe not. So, we have laid down our strong form of the linear parabolic PDE in scalar varia, in a scalar variable in 3D. It connects up with our heat conduction, mass diffusion. Physical problems and  we'll end the segment here. When we return we will do the usual take the usual steps that we've taken before right? The weak form and then talk about the the finite element formulation.

\section*{ID: AFS1OqCquQ8}
There was a minor error in board work on this slide. It appears just about here where I written out incoordinate notation. The flux with the conductivity and the, what was meant to be the gradient in the field on the right hand side. And what's missing is, in this term, the evidence of the gradient. And that should appear as U comma J. Right, the comma was missing there, which did not suggest the gradient with respect to spacial coordinant. With that in place this equation and the rest of the slide are both consistent.

\section*{ID: 1ZSooWlRUYM}
Welcome back. We're ready now to work on our linear parabolic problems in three dimensions in scalar variables. What we did in the previous segment was setup the strong form of the problem. We'll just write it down very quickly now and, proceed on to the weak form and other things, all right? So we write, start out here with the strong form. All right. And just remember we are still talk, we are back now to talking about scalar variables. All right. Or scalar unknown, really, let me call it that. Okay, so the setting, as before, involves our domain. We have our basis vectors, e1, e2, e3. That is our domain of interest. It is omega, a point on it is x, the position vector x. And we have the setting of our Dirichlet boundary subset and the Neumann boundary subset, okay? This is the setting, what we are seeing is now given for data, right? We are given ug, jn, f. We have our, our, our old constitutive relation for these problems in 3D as well, right? Which is that minus ji equals, sorry. Plus ji equals minus kappa ij, u comma j, right? These are all the data that we use when we did the steady state problem, right? We have in addition, another coefficient, which I'm calling just rho, okay. And we made the point last time that this would be the specific heat for, unit volume, if we were doing the heat conduction problem. If we were doing mass diffusion problems, rho would typically be one, okay. Given all these data, what we want to do is find u such that. Right? The following holds. Rho time derivative with a partial time derivative of u equals minus j i comma i plus f in omega, the domain cross the time interval of interest, okay? For boundary conditions we have the same boundary, the same sort of boundary conditions that we encountered when we did the steady state problem, right? U equals u g on the Dirichlet boundary. And, right, minus ji ni equals j sub n, the influx heat, the heat influx or the mass influx, on the Neumann boundary. Additionally, we made the point that we need initial conditions, right, or an initial condition here. We have only one initial condition, because because of what? Do you, do you recall? It's because our problem has a single derivative in time, right? It's first order in time. Okay. So we need a single initial condition. And that is specified as u. Remember u is in general a function of position and time, but now we set the time equal to 0. And we say that this is some given function, u naught, suggesting the initial value, right, of u over the domain, okay? So this is what we have. And what we are faced with in this segment is setting up the, the weak form. All right, we are going to take the approach for the weak form that we'd taken before, which is, we, we have the strong form. We multiply it by a weighting function and integrate over the domain. All right, so to get to the weak form. All right. And remember this is going to be the infinite dimensional weak form. All right. In order to get to that, we see the following, right. Consider. W belonging to V, where V consists of now all functions such that w, equals 0 on partial of omega u, right, our same old weighting function, all right? We consider this, right? And we, essentially we multiply and integrate, okay? And we will do that in the next slide, okay. So what we are doing is we multiply. So we say the following, right? We have w rho, partial of u, with respect to time, equals minus w j i comma i plus wf, right? So we multiply all of this, and we integrate over the domain, right? So we integrate this over omega. So here we pick up a dv, all right? And the same thing happens here. We pick up a integral over omega. D v, and here, too, we pick up an integral, okay? That's what we have. Right, now, we proceed just as before, which is that we integrate by parts, okay. From here we integrate by parts, and just as we did in the case of the steady state problem, we integrate by parts only to have a different way to pose that, divergence theorem, right? We want to transfer that divergence theorem into something else, want to convert it into something else. And so we say all right, we see it there, and we are going to integrate by parts. Okay, now we are knowledge experts in this, so we don't need to go through all the steps. Let's just jump directly to the final form that we get on integration by parts, okay? And that is the following. It is, that, on the left-hand side now we have integral over omega w rho time derivative of u, partial time derivative of u dv equals. Now, the way that integral works is the, the way integration by parts works here is to give us two terms. One is integral over omega w comma i, j i, dV. I'll write the second volume term, which is at this point just a bystander, and in fact, indeed is a bystander through most of our, deriv, our, our, the development of our formulation. And we have, of course, the boundary term, right? We get integral over partial omega w ji ni, right? And that's in an, an integral over the surface so we have ds, all right? So nothing happening with the time dependent term on the left-hand side or the forcing function, okay? And then of course we take the usual steps which is to observe that this term, right? Is equal to integral over partial omega u, w ji ni dS minus integral over partial omega j w ji ni dS. Right, we have these two terms and then we invoke our boundary conditions, all right, on the strong form, as well as our, our homogeneous boundary condition on the weighting function, right? Given the way we've defined the weighting function, the way we've always defined the weighting function, we know that w goes to 0 on the Dirichlet boundary. So that term drops out. And here, we know that ji ni on the Neumann boundary is minus jn. All right, so, making these substitutions we arrive at integral over omega w rho partial time derivative of u, dV equals integral over omega, w comma i, ji dV plus integral over omega w f dv plus integral over the Neumann boundary, w jn dS, all right. Now, let me do just one more thing and we'll have the final weak form. I'm going to invoke the constitutive relation here, right? And we know that ji is minus kappa i j, u comma j. So, we invoke this, and then also observing that we have a minus sign in front of it, I'm going to move it to the left-hand side, okay? Right? So, what we have finally is the following. Integral over omega, w rho, partial time derivative of u, dV plus integral over omega, w comma i, kappa ij, u comma j, dV. Equals integral over omega, w f dV plus integral over the Neumann boundary, w jn dS, okay. This is everything we have, right. What, let, let me just write out now the finite dimensional weak form and we'll be ready to go, right? The finite dimensional weak form from here is obtained by just observing that any attempts to solve the, the infinite dimensional weak form are not likely to be any more easy than the strong form. So we decide to go to an approximate representation of it, all right. And what this says, is now find u h belonging to S h, all right. Which is a subset of S, okay? And what is Sh? Sh now is a collection of all functions of the type of denoted u h which, as before, we will expect to come from h1 on omega. Right, so the spatial dependence is going to be the same. Right, even in the time-dependent problem we are assuming that the kind of approximations we are going to construct will have the same dependents that we know from, from before on the spatial variable, okay? So we have this. U h equals u g on the Dirichlet boundary, okay? So find u h given this. Now we'll, let's assume all the data, right. So we know everything about the data, all right? Find u h belonging to S h such that for all w h belonging to V h subset of V, right? Where again, V h is also drawn from the space of H1 functions. Okay. Now, for all w h belonging to V h, the above weak form should hold, except that every function that is obtained from either w or u is replaced with the finite dimensional version of it, right? So we say that integral over omega w h rho partial time derivative of u h dV. Equals integral over omega w h comma i kappa i j u h, sorry, comma j. I realize I got the sign pro, I brought in the equality too early, plus sign here. DV equals integral over omega, w h f dV, plus integral over the Neumann boundary, w h j n dS, okay? This is our finite dimensional weak form for the, for the unsteady problem, right? Now, lets just stare at this for a few minutes, right? Or maybe, maybe not that long, but for a few seconds at least. Right, so now when we go through the whole process, what we expect is that just as before, right? We are going to do everything as before, right? Let's we, we, we are going to now, how do we go to these finite dimensional weak forms? Well we are, remember that, that, that these are, at this point, at this point, this problem is still finite dimensional only in space. We've done nothing about time, right? Because time is still very much of a true derivative, right? That is, that is a time derivative. We haven't done spe, anything special about approximating it yet, okay? So, as before, we will construct our finite dimensional basis by, by a partition of the domain, right? So as usual we will say partition. Omega equals union over e of each of these omegas. So e's, right, we have all that, right? Okay. And the picture is, is also the same as before. If this is omega you know, let's suppose again since we are in 3D, let us suppose that we are using our hexahedral element subdomains and. That is one of our elements, all right. This is omega sup B. Okay, all of that is the same.

\section*{ID: BnlrtvFcJRg}
Kay? And in particular let's suppose that we have let's, let's suppose that we have bi-linear elements, right? Here. It, it, it really doesn't matter whether our elements are bi-linear or not. It will come in only in a few more minutes, but but, but whatever it is, once we know that we have this partition into elements of domains, we can then go ahead and construct our basis functions, right? So, just as we did before we will construct basis functions. Okay. And, and here's where things start getting a little interesting. So, let's construct basis functions. We know that the, the way that we set it up is to, to construct basis functions which are, defined over every element, right, that's how we do it with this idea of compact support of the, of the underlining basis functions that we are going to, that we are going to use. And I will specify that here is, is independent upon position, and time, okay? So we want to construct a representation for this sort of function, using our basis functions, alright? So we will as before expand over the nodes in the element, NA, okay? Now, here is the, here is where we need to really start paying attention. So, any of the func, is, is parametrized by x because after all it is a, spatial basis function. But we already know all about that. We know how we're going to construct our basis functions from our, bi-unit domain right? Which is parametrized by c. All of that is exactly the same. So, this is where the spatial dependence of Is, taken care of. The time dependents of Right, is done in, in this, in the sort of formulation that I am describing here. It is done simply by having the coefficients here. Right? Which we know to be the nodal values of our trial solution field. Right? DAe all right? We just make these guys time dependent. Okay? In this sense, once we've done this what we are doing is a spatial discretization. Right? So this thing, this takes care of our spatial discretization. Okay, we haven't done anything to discretize time, yet. For this reason, the type of formulation I'm describing where the time dependence is held in the coefficients. Right? This type of a formulation, this type of a finite element formulation is often called a semidiscrete finite element formulation. All right? So this thing is often called a Right. Okay. Because there is as yet no discretization of time. All right. Interestingly, for w h, let's try to understand what we have. We know that w h is a function of position. Look back at our weak form, and think about whether or not we actually need any time dependence in our waiting functions. Is there anything in that weak form, you could look at the finite dimensional or the infinite dimensional weak form, it doesn't matter. Let's look at the finite dimensional, because that's the one that we're, we are actually working with. Is there any need for time dependence in our waiting functions there? All right, and indeed there isn't any. Okay. Because, it doesn't, there's no time derivative on it. There also is no integration over time, right? So there is really no nn, you know, w is something we specified. We don't really need to worry about any time dependence there. So w h is only a function of position, right? And this we, we already know, right? We've got this one very nicely covered. All right, we have our basis functions which depend on x through c and the coefficients here are c, A, e. So this is no different in any manner from what we did with the weighting functions for the steady-state problems. Okay, all right with this in mind, observe that if you are still looking at, at if you still have in front of you the finite dimensional weak form right, I'll pull it up again, here we, here you have it. If you look at it, you will, if you look at the second term of the left hand side and the two terms on the right hand side, it should be b fairly clear to you that through the entire process of our finite element formulation, right, setting up the element integral, well, first of all calculating gradients of fields as needed, setting up the element integrals going to matrix vector forms, first over elements then doing assembly over the whole domain, none of that is any different. Right? For the second term on the left-hand side and the two terms on the right-hand side. Okay. So, what this leads to, what this allows us to do is save about three seconds worth of me talking, and we can simply say from here, it should be clear that from here integral over omega w h, i kappa i j u h, j dv, right, at the end of the whole process, right, when everything is said and done. It's going to be essentially a c transpose K d. All right, we know how this all works already. And nothing is different. Except, except for one thing. What is different here? The c is just as before, the K is just as before, the d coefficients are time dependent, okay? All right, we have that, integral over omega, w h f dV plus integral over the Neumann boundary wh jn dS is c transposed f, okay? Just as before. Right? Because here there is no time dependence in in c nothing is different,. Note, however, that, right one could allow a time dependence in the forcing term. Okay, and in fact even in our boundary terms. Okay, so let me make a remark here. Okay, one can have ug equals ug function of time, right? So the Dirichlet function that we use, the function that we use on the Dirichlet boundary, could be a function of time. In fact, so could your influx. Right? There is nothing preventing this. Right? And indeed, the formulation can take full account of it. It could also have f, our forcing term could be, of course, a function of position, but also of time. Okay? And so in general we could have f equals some function of time. All right. Okay, if that's all, good, really the only term we need to worry about, when you go back and look at your week form for this problem is the very first term. Okay. It's only this term that we need to worry about now. Let me go to a different color. Right, this is the only term that we need to really focus on here. Okay? And so we will when we return in the next segment.

\section*{ID: MeRwljkxtIM}
So, we will continue, and what we observed at the end of the last segment is that the only contribution we need to really worry about and, and, and expect it to be any different is the new one, right? And, so we consider the time-dependent term, right? Right? And the time-dependent term is this one. Integral over omega, w h comma, sorry, there is no comma here, that's the whole point. W h rho partial time derivative of u, d v. Okay, this is the one we need to consider, sorry u h here. Okay. It should be completely straightforward now because, we, we already have our, expansions for w h n u h, right? And, in fact, in fact, let's just make one observation. In order to consider this, what we need to do is use derivative of u h with respect to time, the partial time derivative is after all nothing. So, and, and there, furthermore, restricted over a particular element, right? That partial time derivative in an element e is nothing other than sum over a N a, we know all about those, d A e dot, right? Where the dot denotes a time derivative. Okay? All right. Well then. The way this works out then is that, integral over omega w h rho. Multiplying the time derivative. Is. Sum over the elements. Right. Integral over each element of the following. Right? Now, let's, let's write out the, the usual sort of summation that we have. Sum over a N a c a e, right? And we know that this first parenthesis is what gives us the w h term, multiplied by rho, and for the next term, we have now sum over B, N B, d B, e dot d V. Okay? Simple as that. All right, so I'm just going to carry along this sum over e, as well all right, because we know, we've done this sort of thing so often in the past that we know how it all works out, okay, so let's just do that. So, this is a sum over e right, I'm, as before now I'm going to pull out the sums over A com, A and B. We have here c A e, integral over omega e, N A rho N B, d V. We have there the integral, and we get here d B e dot, okay? All right, so, if you stare at what we have here for the integral, that I've put in parentheses, it should be clear that for a given element, when you carry out that integral for a combination of, basis functions on node A and B, you get a scalar, right, in every case. Okay. This, resulting matrix, or, or, or these resulting terms, are going to be of a form that we've not, encountered yet, before, okay? We have encountered related, not really related terms, but if you think about the term that gives rise to our, conductivity tense matrix or our diffusivity matrix for this sort of problem, it involves derivatives on the N A and N B, right? It involves spatial derivatives, right, we don't have any of those in this term. Right? So, this is a different type of term. I am going to denote this integral M A B sub e. Okay? All right. And this term that I have just written out, M A B sub e can be further assembled into a, a matrix vector form, right? And we know how that happens, right. So then from here, if we just use, c e equals c 1 e up to c number of nodes in the element e. Right? And the same thing for d. E. D e we know is just d 1 e, up to d number of nodes in the element sub e, okay? However, we also know that in the particular form that we are dealing with here, we have a d e dot, right? Because it, we, we get time derivatives of each one of these terms. I'll make this dot a little bigger than the dots of the ellipses, okay? So, when we get all of that, what we observe is that, this integral that we started out with, this, mystery new integral, essentially reduces to, integral over omega w h rho. Right, so this integral that we're trying to evaluate is essentially now sum over e, c e transpose, right? By assembling all those you know, by replacing the explicit sum over nodes A and B, right, the explicit, replacing that explicit summation, which we had on the previous, slide here, right, this explicit summation. What we're able to do by defining these element level vectors we know is get rid of that explicit summation, right, and we are masters of this, as well. Okay, what that means is that our M A B scalar terms, slot themselves nicely into a matrix, right? They slot themselves into a matrix that I am going to call the M. Bar. A B matrix. Sorry, it's not A B. I, I just got rid of the A B. The M bar e matrix. Okay? And that is multiplying a d bar dot e, vector. Okay, you also know why I am using bars here, right? Rather than the final forms, because we know that we can have this issue of Dirichlet conditions there. Okay, so just because, after we apply the Dirichlet conditions the final matrix, matrices that we have are going to be reduced in dimensions.  Also, the final d vector, or, or the d dot vector that we have is going to be reduced in dimensions. Through exactly the process we've observed before, for handling Dirichlet conditions, okay? It's, it's, it's in, it's in sort of preparation for that, and in anticipation for that, that I'm calling these, this matrix M bar, and M bar e, and that, that vector d bar e, okay? That's all. Now, this matrix is what is called often the mass matrix, okay? In the context of finite element methods, any such matrix that's obtained by directly multiplying the basis functions, no derivatives, right, no spatial derivatives on the basis functions. Directly multiply them and integrate over the domain, maybe multiplying with rho, right, and you've that in some cases that rho could be 1, so that case is also covered, right? The important thing is that it is when we form a matrix of this type which involves a direct multiplication of the basis, functions over an element, right, and their integration, it's called a mass matrix, right, and in this case, of course, the element mass matrix, okay? All right. It's, it's a new matrix that we've not previously encountered, okay? Now, I should make a remark here. Which is, that if we have a general element that is far, that, that is not, that doesn't have one of its surfaces coinciding with the Dirichlet boundary, then of course we know that the, the, the c e vector for that element is going to be is going to be full. It's going to have as many entries as the number, as the nodes in the element, and likewise the d bar vector, right? For that case, all right, for that case what sort of a matrix is M bar? 'Kay. So. For a general element. Omega e, such that. The boundary of omega e intersection Dirichlet boundary is the empty set, right? So an element that does not have one of its surfaces coinciding with the Dirichlet boundary. For such an element we observe that the dimension of c e equals n n e, okay? Right? Right? Therefore, what that implies, that for such a, for such an element, what can you tell me about the M bar matrix? Okay, recall that the M bar matrix is now going to have this form. M 1 1 e up to M 1 n n e, e. M n n e, n n e, e, and here we will have M, n n e 1 e right, it should be obvious that this is a square matrix with, dimension n n e times n n e. What more can we say about it? If I have a general term here that is, M A B e and, we have a term here which is the. Through the transposed position in the matrix. What can we say about M A B, and M B A? Right? That's right. They're equal. All right, it just follows from the definition of one of those components, right? Each of them is just a mu, just the integral of n A times n B multiplied by rho integrated. All right, so they're symmetric, right? So basically we have M, M bar is a symmetric matrix. Okay. All right. One can also show using the properties of the of our basis functions that the M bar matrix is positive definite, okay? Right, and you recall what that means, right? So if we have, some general vector. Well actually, let well, it, it's true for M bar e, as well, but, but, hm. Yeah, that's okay. So M bar e is positive definite, so what we see is that, what that means as you will recall is that if we took any vector c, right, and we formed this product c transpose M bar e, sorry, if you too any c e transpose c e, right? And remember, c e can be arbitrary, right, could be any vector. This is, greater than or equal to 0 for all c e being in any dimensional vectors, right? In fact, it's equal to 0 only if c, the c e vector itself is equal to 0. Okay? This is what we mean by positive definite matrix, and indeed, M bar e is positive definite. Okay, all right, so this is one remark about properties of the matrix. There is another remark, which I am going to make, which is that M bar, as computed here, is what we call the consistent mass matrix. Okay. And. Okay. One can also do a lumping, okay, and, what a lumping involves is computing the consistent mass matrix and then summing up all the elements along a row, and putting that sum on the diagonal. Okay, so it's, it's, it's an approximation, it's not a, that's why it's not called a consistent matrix, right? So a lumped element mass matrix. Is the following, right, M e, say, tilde. Okay? Which is obtained as follows. On each diagonal, use sum over A. Sorry, sum over B M, in this case use sum over B, M 1 B e, okay? And put it there, and make the other 0. Okay. Likewise, you do that for all the diagonal elements. Okay. And, and every other element is 0. Okay. Right, which basically is telling you that M tilde e A B, right the A B component of the M tilde matrix, is simply sum over B. M A B e. Right. If, sorry. A equals B. If this is to be the case, sorry, I need to make a correction here. I need to have a new index for the sum. Okay, so if A equals B, which makes it a diagonal element, then you simply sum up over, you sum up the elements in that row. Okay? And put that, sum on the diagonal. Okay? And, it is equal to zero, otherwise. All right. It's just, it's just a sort of, quick and dirty way of diagonalizing the matrix, right. It's not a proper diagonalization through defining orthogonal basis functions or anything like that, right? It's just, just a way to simply lump it and definite a diagonalized matrix. And you also see why it's now called a lumped mat, element mass matrix, right? Because you're lumping all of the mass up at the, at the nodes. All right.

\section*{ID: rbpJXpAoizA}
There remained a minor error in board work on the slide that I have before me and that you have in front of you on the screen. It appeared in the way I wrote out mass lumping for that diagonal component. It's an error that you may have picked up on yourself, especially by comparison with this other term, which is correct. The error here is that I wrote that sum as a sum over e. It should properly be a sum over B. So what we want to do is draw an arrow right through that e and replace it with a clear B. With that, everything's consistent.

\section*{ID: cBN8hI2Cb-8}
Okay, so, so this is something to know, and we will also see what happens when we, you know, if we, we. From, from here we, I'm not going to consider the lumped mass, element mass matrix. I'm going to go back to the consistent one, when we finish the, the process, we will then redefine at the global level also, a lumped mass matrix, okay? But this, this, this was a very useful place to introduce it. Okay, so, with those two remarks in hand, let's just return to our formulation, which is the following, right? So we have we have now this, the following representation. Integral over omega w h rho Right? We've got as far as this. Consistent mass matrix. Okay? We're going to go from here to assembly. All right? Now we do just the same things the, the same things that we know from before, right? We say that okay, my c vector is you know, going back to our global numbering. Both nodes at c1, c2, up to c number of nodes in the problem. Okay? And this is. Global degree of freedom numbering. Okay, and the same holds for the d bar matrix, right, d bar again is d bar 1, d bar 2. D bar number of nodes. Okay? All right? Okay, oh, I, I should be a little careful here. This is not just number of nodes, right? I should be careful here. The size of the C matrix is less than number of nodes. It's number of nodes minus number of Dirichlet degrees of freedom, right? Number of degrees of freedom that have Dirichlet conditions on them, right? Whereas d bar is the full one. Okay, so we have this, what that let's us do is that we, we can go from here to saying all right if we have these vectors c trans, c and d bar defined. Then what we have here is c transpose n bar d bar, right, where n bar is the assembly overall elements of D n bar e matrix. Right, of the consistent element mass matrices. Now, this assembly process works just as before, right, just in the way we, just in the way we assembled our conductivity matrix or the diffusivity matrix, right, which is that we go, we look at every single degree of freedom, that global degree of freedom, that shows up on any element, right, and add the contribution from that element into the, into the global mass matrix. Okay, so that's it. So let's just recall with a very quick example how that works. Right, it's really no different from, it's identical to what happens with a K matrix. But it's probably worthwhile to just recall. Okay? All right, so as before element omega e1, omega e2. Let me label the global nodes as maybe A, B, C and D, and go to the local node numbering. Right, the local node numbering may be 1, 2, 3, 4, 5, 6, 7, 8 and on element e2 we have 1, 2, 3, 4, 5, 6, 7, 8. Okay, so with this setting in hand. Let's go ahead and look at how the m bar or, the global n bar matrix would be formed right? For just, for those nodes A, B, maybe AB, maybe CD as well. Let's see, okay, so So now we go to n bar, right, the global matrix, and let's suppose that the nodes we are interested in are A, B C and D. And those show up here as maybe A, B, C and D. Okay, so let's look at the contributions to the m, to the a a, position right, right, out here. So, from omega e1, we would get a cont, contribution which would be M e 1, 11, sorry, 22,  22 plus from m, from e2, we get indeed 11. Okay, let's do another one, let's do cc. Cc would be from e1, n e 1, we get 77, and ne 2, 88. Right? And of course, because node A, global node A and global node C will be shared by other elements besides e1 and e2, this, there could be more contributions to both of these, right? Okay. Let's do the AB term. Yeah, let's do the AB term. So, for the AAB term, what we see is that from e1, right? From e1, we get the 26 component, and from e2 we get the 15 component, okay, and maybe other ones too. Other ones from other, other elements that share nodes A and B. Just to demonstrate the, the symmetry let's do the B A term. So this was the, actually I think I already did the B A term. Let's do the A B term now properly. The AB term would be the a row and the b column. It would be the one here. Okay, so let's see. So, if we now we look at oh, no. I, I just realized that I, I actually did make a mistake here. Everything that I wrote here actually goes to the goes properly to the AB term. Right, so I was talking of the BA term but then I wrote all the AB contributions here. Right, so one needs to be careful about this, always, of course, okay. So the AB term means row A, column B. Okay, so let's do that. Row A, column B would be here M e1 26 plus M e2 15. Okay, and then let's do the row B, column A down which would be the one here. Okay? So row B, column A 10 would be from Me1 BA, right? So it would be 62 plus Me2, 51. Okay. All right. And so now we realize that because we've observed already that the element, the consistent element, of this matrix is symmetric. In fact, the lumped one is also symmetric. Anyway, because a consistent one is symmetric, we realize that the, that the global consistent mass matrix is all the symmetric, provided it is square, right? But, in general it is not yet square. We need to make it square, right? Which we will. Okay? All right. Now, let's, let's, let's go ahead with this with this right? And so, so, essentially this is how things will work. And, and what you're observing, is that the way you slot in the components of the same bar matrices are, is identical to the way we do it for the, for, for the, for the conductivity of the diffusive, diffusivity matrix. Okay? That assembly process is exactly the same. Okay this is a useful place to stop because all we do, really need to do when we return in the next segment is talk about how Dirichlet boundary conditions have an effect upon this term, this new term that we are considering. All right.

\section*{ID: -ChO6q-8Udk}
So, I just noticed that when I wrote out the globally assembled form of this new time dependent term, I missed a critical dot on it, right? So this is what we have before us. And when I wrote out this formula. Right, what I missed was the fact that there is a time derivative on the d. And that dot is what I missed out. So let me make a nice big green dot on it, okay. And that carries over here. Okay, with that correction, we are essentially ready to move on.

\section*{ID: Eza34lQzpK8}
Welcome back. What we did in the last segment, what we accomplish was a global matrix vector representation of the new time dependent term that arises in the unsteady form of the linear parabolic equation in three dimensions with scalar, with a scalar unknown. So let's so to speak, finish the job now. And we'll do it by carrying out the last remaining step in our finite element formulation. Which is accounting for our Dirichlet boundary conditions, okay? So the topic of this segment is, Dirichlet boundary conditions. Okay. So, for completeness, let's just and, and for connect, and for connections let's just write out our finite dimensional weak form, just, just the integral form. And then immediately after that, our matrix vector form for it, okay? So, what we have is the following. Integral over omega w h row, partial time derivative. Plus integral over omega. Wh,i, kappa ij. j sorry. U h comma j d V equals integral over omega Wh f dv, plus integral over the, the influx boundary, wh jn ds. Okay, these are the terms we have. Now, we've already written, so we've already written out the matrix vector form coming from the second term on the left hand side and the two terms on the right hand side. And we know very well that after accounting for the Dirichlet conditions on those terms we have a form that c transposed k d equals c transposed f. Okay, and this is all we would have if we were working with the steady state problem. What we've added onto this is the understanding that this term can be rewritten as c transposed M bar, the consistent mass matrix, d bar dot. All right, now, let's talk the Dirichlet boundary conditions on the Stein dependent term, only understanding that the Dirichlet boundary condition have already been accounted for on the remaining terms. Okay, so. So what we're seeing is that this form follows if the Dirichlet boundary conditions From the. From the integrals to be really precise about this, okay? For if this follows, if the Dirichlet boundary from the, integrals. Without time derivatives, Have already been accounted for. In the, in this matrix vector form.  Okay? In particular, in particular, note that that is what allows us to say that the K matrix is square. All right, otherwise the presence of Dirichlet boundary conditions makes K a, would, would make the corresponding matrix there a rectangular matrix. And that's what allows us to say that we have d here and not d bar. Right, the d here is only the final set of unknown Dirichlet conditions, and in fact, what I am saying here, what we are saying, is that F already has the Dirichlet conditions accounted for in there. Okay? So I'm sort of jumping ahead to that final step. All right. So this thing already has. The Dirichlet boundary conditions, from, the non time derivative term, right? The non time derivative integrals. Okay, it may seem like a piecemeal way of doing it, but I think we understand why we're doing that because we've already been over that part of the steady state problem. Yes in fact, you could view this as taking the steady state problem and adding on the time dependent term, right? How would you do it? Well, this is how. Okay, right, if that's what we were doing, then already, the Dirichlet boundary conditions would be accounted for in the f vector. Okay, so then we only have to worry about the Dirichlet conditions which are reflected in here. Okay, so let's do that. Okay, so, let's note, then, that C transpose. M bar, d bar dot, okay? Can be, or, or, or is indeed written as a C vector, right? C 1 up to some, C N N E minus N D. Right? We have that, multiplying Our m bar matrix and here on the right we have our big D bar vector. D bar dot vector in fact. Okay? So here we have, d bar 1 dot. Right? And let's suppose that here we have a d bar, d bar dot. Okay? Maybe I should make that dot in a different color. Well, never mind. That big dot is for, for the time derivative, right? The little dots are ellipses. Okay. So, it goes on and let's suppose that we have here a d bar, with a big dot, on a, on the B bar degree of freedom. And we, and with our, d bar n n e. Big dot. Okay? Now we know how this works. What we're talking about is that those degrees of freedom, have a Dirichlet boundary conditions. Okay? And we completed the Dirichlet boundary condition on degree of freedom a bar. Global degree of freedom Right on d bar, okay. And we know how that works out, because what, what we're seeing is that is that the, that this column, okay, that I'm going, that we denote as a, M bar, A bar. Right? That's a column. Right? And likewise I guess we'd get N, M bar, B bar column here. Right? And we know how that, what that implies, it just says that every element in that col, every component in that column is multiplied by the d a bar dot com-, component. And likewise, everything in the M bar B bar column is multiplied by the d B bar, sorry, t bar b bar dot, right. Degree of freedom okay, but those are known, right. So these are known, right. Not only are those, are those degrees of freedom known, but since they are time dependent, if they're known with every, at every instant in time, we can indeed compute their time derivatives as well, okay. So those components also are normal, all right. Those time derivative components, all those degrees of freedom, the Dirichlet degrees of freedom are known. So what do we do? Well, we take it, take them to the right hand side because the idea is that, that time dependence of those Dirichlet boundary conditions also drives the problems that we're looking at, okay? It's a time dependent driving of the problem with Dirichlet conditions. Okay? Right. So may, maybe I should just state that as a remark here. Right? So in the form that I've written things up in the previous slide. D bar, a bar, and d bar dot b bar drive the problem actually, let, let me qualify this further, drive the initial and boundary value problem Via time-dependent, Dirichlet boundary conditions. Okay? All right so well, what do we do? We know very well now, right? So, what this lets us do is to rewrite the whole problem now as C transpose M, d dot, plus C transpose K D equals C transpose F. Minus d A bar, dot M, M bar A bar, that column. Minus, d bar dot B bar, just a component, right? A time dependent or a degree of freedom, right, whose time derivative now, is dr, continuing to drive the problem, multiplying this column. Okay? Unfortunately, I started out by calling the F to the root earlier to pF instead of calling it F bar or F tilde or something. So, allow me now to simply redefine this is also F. Okay? So, redefine as F. Okay. So whatever F we caught from the earlier steady state problem has sort of been updated to account for the time-dependent driving with Dirichlet conditions, okay. All right. So what that lets us say then, is that we finally have C transposed M d dot plus k d, minus F with our redefined F, including these Dirichlet conditions, right. All of this equals 0 for all C now belonging to R of dimension n n e minus N d. Okay. And that of course comes from our, a specification in the weak form holding for all waiting functions. All right, and then we impose this, we see that the final matrix vector equations we impose to solve this linear parabolic problem in a scalar unknown in 3 D is this. Okay. So this is our semi discrete matrix vector problem. Semi discrete because we've really truly discretized only in space. All right. And what we really need to do now is account for how we, deal with the time dependent term here. And note that the all the time dependence has now been sort of put into this time dependent vector. Of degrees of freedom, right? That we need to solve for. So, so we'll end the segment here. When we come back, we will focus on the time integration.

\section*{ID: -CA1T2d97JI}
Welcome back. We'll continue with our development of the finite element method for these linear parabolic problems in three dimensions where we have a single scalar unknown. What we've managed to do in the last couple of segments is starting from the weak form we've written out the stro, sorry, starting from the strong form we've written out the weak form and arrived as far as the matrix vector equations, okay? And if you have your notes in front of you, you can go back and observe that the form of these final matrix vector equations was the following. Right, the matrix vector equations. Right, it takes on the following form. We have this new matrix, the mass matrix Md dot plus Kd equals F. Now in this form of the equations, we know that because of the, there is no methods being employed. The methods are on the weak form. The the boundary conditions are already embedded in this set of matrix equations, right? What we do need to count for, however, is the, the initial condition. Okay, so whereas we have this linear system of equations, right, essentially an a, a first order of ODE, right? So this is a first order ODE. ODE being ordinary differential equation, so let me write that out. Right, in our vector unknown, d, okay? Now, because we've dealt with all this business about, you know, number of nodes in the problem and the number of Dirichlet boundary conditions and so on, I'm going to, as we proceed from here on just say that d is a vector living in an ns ndf dimensional space, right? NDF is simply number of degrees of freedom, okay? So, those are the number of degrees of freedom we are served. NDF is the number of degrees of freedom we are actually solving for. Now, this is the first order ODE, and what that implies is that we do need an initial condition which is something we had stated in the strong form of the problem. We just need to incorporate it here, and that ODE that we're working with i, is the following, sorry, that dif, i, initial condition that we need to work with is the following. d at time to equals 0 equals the vector of conditions which are obtained by taking u not at the particular location of the corresponding node, okay? And this is all the way down to u naught at x, now written in ndf, okay? Right, instead of continuing to number by node, we've forgotten all about nodes. Now we're just looking at this linear system of equations where d as an R belongs to to an ndf lucrative space and this is what we have for the initial condition, right? It's convenient sometimes to also offer, to, to refer to this vector as just d naught, okay? Right, so this is the setting we have. One thing I should mention is that whereas we sort of carried through our development of the method by considering a consistent mass matrix, one can also do global lumping, okay? So a globally lumped matrix, lumped mass matrix. All right, let's denote it as M sub l for lumped, okay? It can be defined as this thing can be defined. Okay? And the way to do that is to simply say that MAB lumped is equal to the sum over c of MAC, okay?. If A equals B, right, and it is equal to 0, otherwise. All right? Okay, and in some cases the problem is often solved with the globally lumped mass matrix. Okay, so this is this is what we needed to complete from the previous from, from the work, we set, set out in the previous segment. We move on from here and begin looking at the integration algorithms for this class of problems, okay? And this is going to depend upon time discretization, okay? The reason for it is that the the form of the matrix vector equation we're working with is what we call a semi-discrete formulation. And I explained why it is semi-discrete because we've gone through the discretization in space, but don't get in time. Okay, so now we do time discretization. Now, in a move that may seem a little disappointing to some of you the time discretization for this sort of problem is done with finite difference methods, okay? However, the, the nature, the structure of the matrices M, K, and and the vector F depend upon the finite element method. So even though we're going to use finite difference methods in this series of lectures for integration of time dependent equations, time integration of time dependent equations. Those methods are, do continue to be affected by the, by the underlying finite element formulation for the spacial part of b d e, okay. So this part is going to be finite difference. Finite Difference methods, 'kay? With that, that's what we're going to use. Now, I'll make a remark here, which is that a fully, space-time, Galerkin finite element method does exist. Okay. So, space-time. Finite element methods do exist. And they have remarkably good properties as well. These types of methods are based on, carrying out integration. Over, omega and over the interval 0, T. Okay? So these are based actually on a formulation which is now going to look, well, I, I, I, I probably shouldn't since, begin writing this formulation because one, we'll get sucked into the rest of the development, but anyway, they're based upon integrating over omega and over, the time integral. And, they, they tend to have very good properties. For instance, they have properties of the type where, the accuracy. The accuracy with respect to time. Is of higher order. Then what we will be developing here. Right? Accuracy is of higher order than, with finite difference methods, F D for finite difference. They do come with a, they do come at a cost, though. In particular, one has to develop a finite element mesh over space and time. And this can pose a challenge when one is doing problems that are 3D in space, right. Because then you have a forth dimension in time, which can get to be a bit of a challenge to visualize, definitely. At any rate the methods exist and they're actually very sophisticated, okay. So we get back now to our finite difference methods, right. So the nature of the time discretization that we are, we are seeking carry out is the following. We are going to divide our time interval into sub-intervals. So divide our time integral 0,T into sub-integrals. Okay. And these sub-intervals are, 0, let me do it this way. These sub-integrals are t0 to t1, t1 to t2. So on until we come finally to. TN minus 1, to t N, okay. So we have all of these sub-intervals, and what we also imply here, of course. Is that t0, or t nought is equal to 0, and tN equals capital T. Okay. So, if you look at this what we've implied is that we've divided our total time interval into capital N sub intervals, right. So clearly here we have, N sub-intervals. Okay? The basis of our, of the methods that we will develop is the following, okay. So what we are going to do here is consider. A, consider an interval. T sub little n, to t sub n plus 1. Okay? All right. Where, where, of course, little n belongs to the integer integral 1, 2, N minus 1. Sorry, 0 to N minus 1. Okay. So we consider an integral of this type. The whole, basis of our methods is going to be what is sometimes called time stepping, okay. So, time stepping implies. Right. Time stepping implies that we know the solution at tn, and we want to get to tn plus 1, okay. Time stepping is essentially, is the following. Knowing, the solution at t sub n, find it at tn plus 1. Okay. All right, so we're really, sort of, hopping along in time. Okay, so if you think about it as, if you think about it as follows. That's the time axis, right. We've started here at 0, we want to get as far as t, right. We have t0, t1, right. We have tn, and we will have tn, t little n plus 1, and here, of course, we have t capital N, okay. So the whole idea is that well, I know what, what, I know everything at tn. How do I get to tn plus 1? Okay, in order to proceed, and also actually in order to Later on carry out some analysis, we need to lay down some more notation. Okay? And here is what we will do. So some more notation. Okay? When I write d, at some time t n. What I implied by this notation is the. Solution vector d, if one were able to do exact integration in time. Okay. So this is the time exact solution, or the time continuous solutions. All right. At time at t equals t n. I am going to stick with the term time exactly. Time continuous gets, gets a little bit more, ambiguous. The time exact solution. Okay? And this simply means if we were able to integrate our ODE exactly. Right? This is our matrix of vector ODE. Right? If we were this with, with bound, with initial conditions. If we add a method to exactly integrate this, then the solution that we would get at a given time t n would be this, okay. In contrast, I will use d sub n, okay, to denote the, the algorithmic, solution at t n. Okay? So this is the algorithmic. Solution, okay? By which mean the solution that's obtained by applying some sort of discretized time integration algorithm based upon the discretization that we've just spoken of. Right? The algorithmic solution obtained. By the method to discrete, to integrate the discretized, the, the time discretized ODE, okay. All right, so we have this notation. What we need to do is actually first of all say what we mean by the time discretized ODE. And here's what we mean, okay? Now if I were to simply re write out our ODE in the time-exact form, here's what I, I, I would write it as m just as I have it up there, but let me, explicitly put in the time. Okay. Supposing we were looking at it time t n. Okay? This would be m d dot at time t n plus k d at time t n equals f which potentially could also depend upon time. All right. Now when go to a time discretized version of this ODE, what we will write here is the following. We will write this out in the canonical form m. Now instead of d dot t n, we will adopt the notation that, you know, that well if d is our solution, we can think of d dot as sort of the velocity of the solution, right. Just, just as a term, it's really just the rate of the solution, but, but we use the, the sort of canonical or the generalized velocity for a rate here. And so we would write this as v, okay? Indicating a velocity. Now, this is not a time-exact quantity. So we are going to denote it as v sub n, okay? Using this sort of idea. Okay? Plus k d n equals f at n. Okay. This is what we mean by saying that well here, v at n is an, a discretize approximation, It's a time discretized approximation again. Time discretized, discretized approximation of d dot right. D n is of course just as we've defined above. Okay, so this is what we call what I will refer to as the discretized version of the ODE, okay. So this is the time discretized Are the time discrete, ODE. All right? Okay.

\section*{ID: 1lT0ssVpNzs}
Now, in order to proceed from here, let me actually write this time discrete ODE. In ord, for, for, for, for purposes of argument let me write it at time, tn plus one. Right, so this of course is the time discretize ODE at tn. Right, because everything is at n. Okay. Let me know write this at time tn plus one. Right? So, at T n plus one, that becomes in canonical form, it becomes N V and N plus one plus K, d at n plus one equals F at n plus 1. All right. All I've done is, is write out the ODE at two different. Times okay two different times and as well. All right so this is generally what we would call the the time discrete version of the ODE. And, and when we write it in terms of the philosophy it's often sort of canonical All right now, this is where we need to invoke special integration methods, right? Our integration methods are based upon what we call the same inte- it's a family of integration algorithms. Okay? That are sometimes called the Euler family for first-order ODE's. Okay. And here is how they are posed. Supposing we're considering the following ODE. Suppose we're looking at, y dot equals f of y. Okay? This is the, algorithm we're looking at. So, what we will see here is that the time discretized version of this or the ARD or the, the, the, the solution as post by the Euler family of algorithms is the following. All right, the algorithm then is Is the following. It is that y n plus 1 minus y n over delta t, okay? Is equal to f at y n plus alpha. All right? This is the algorithm as defined by the Euler family. I'll have some explanation to do here of course right? So what we have is the following. Just the algorithm where, delta t is simply the what is called the time stamp. It's d n plus 1 minus t n. Okay? And this is the time stamp Okay? Alpha is a real number. And alpha belongs to the closed interval 0 to 1. All right, okay. So. We have the setting, and, since we're talking about the Euler family for the very first time let me, so to speak, introduce you to the members of the Euler family. So, we have the following sort of setting. Four alpha equals 0. Right, this is called the forward Euler method. Okay alpha equals 1, is called backward Euler. And alpha equals one-half is called various things. I will tend to call it the Midpoint Rule. Okay? It's also sometimes called the Crank-Nicolson method. Okay, so those are special members of the Euler family. Of course as as indicated back here you can one can design algorithms. One can develop algorithms and use them for any value of between zero and one. Okay, so, one one final step I want to take in this segment is to note that the way this can be sometimes rewritten is as simply. All right, this, this think can, is sometimes rewritten as y n plus 1 equals y n. Plus delta t, times f at y n plus alpha. Okay? Now, in order to complete this description I need to tell you what y n plus alpha is. Okay? Y n + alpha is constructed simply as a linear interpolation between values at alpha equals zero and alpha equals one. Okay? And that tells you that, that you construct it as Y N plus 1, right? Let me see, alpha times y n+1 plus 1 minus alpha times y n. Okay? All right. So really so when you sit back and look at these Euler family of At the Euler family of algorithms, what it is doing is to approximate our time derivative, right so we approximate the time derivative as a linear, as a linearly varying quantity over every time interval. Right? Why don't, we'll just sayinh well,let me just take a linear, approximation of it over the sub, over the time stamp of the time derivative. This is our approximation for the time derivative, right? And by saying that we are requiring it to be equal f at n plus alpha, this is where the design of our algorithm also comes in over the various possibilities in the Euler family because if we are trying to go from tn to tn plus one. Right? We're seeing that the rate in the average rate across the entity of tn and tn plus one depends upon f and we could, we are free to evaluate f or we're leaving ourselves the flexibility to evaluate f at any value inside here. Right, so at, at any point inside this. So that point is a gen, is generically n plus alpha, t at n plus alpha. Okay. By choosing different values of alpha, including alpha equals 0, or alpha equals 1, or alpha equals one-half, or, or really anything else, we get particular properties for our method. Right? So, so this is where the approximation of the time derivative comes in. We have this side of it, which is the linear approximation, and here we are approximating the ODE by evaluating the right hand side at some T equals alpha. Okay? So this is the basis of the Euler method and we are going to apply it to our time discretized ODE when we return in the next segment.

\section*{ID: Eyh2ws5uZO0}
Welcome back. So at the end of the last segment, we wrote out our ODE in its discretized form. And we've also introduced the the Euler family. All right, we've met the Euler family, so to speak. All right. What you're going to do now is go ahead and apply it to our ODE. And the approach we are going to take is one of revisiting this discretized form of our ODE, okay? So go back to our time-discretized ODE. All right. We write it out as M v at n plus 1 plus K d at n plus 1 equals F at n plus one, right? Where what we can say now is that we can invoke our Euler family as applied to our definition of v, okay? So what this means is that we're saying here that d at n plus one equals d at n plus delta t v at n plus alpha, okay? Where v at n plus alpha equals v n plus 1, multiplied by alpha, plus 1 minus alpha times v at n, okay? And in writing out these two equations you can see that we have essentially applied this idea of the Euler family in there, right? Okay? All of this, of course, to complete our specification of an, of even our time-discretized form of the ODE. What we are saying is that of course with all this, we are given the initial condition, all right? And that initial condition is d 0. Okay? We know what we mean by d 0. Right, in the previous segment we explicitly said what d 0 is. All right? So now with you know, in general you, you really see this idea of time stepping. The idea is that given things at d n, right? Given quantities at t n, sorry. Given quantities d n and v n at t n this is a scheme for going from n to n plus 1. Okay. And it utilizes, it uses the, the initial condition here. All right. Now, there are two sort of canonical approaches to solving this time-discrete, discretized ODE. And we'll, we'll look at both of them in turn. The first is what is called the v-Method. 'Kay? Somewhat unimaginatively. All right. Here is the v-Method. In order to app, apply the v-Method and actually to apply the other method as well, we go back and look at this. Form of the problem and look at these two equations and do the following. Okay? We first note that. Then we can rewrite now d n plus 1 as d n plus delta t, times v n plus alpha but v n plus alpha is also written up for us here. So we'll write that as alpha v n plus 1, plus 1 minus alpha, v n. Okay, all right, so that is the first step, combining the two sort of the equations that define the Euler family. Now, in the next step what we're going to do is to rewrite this as rewrite the right-hand side as d n plus 1 minus alpha delta t. V n, plus alpha delta t v n plus one. Okay. Now what we've done here, if you observe, is rewrite d n plus 1, in terms of quantities that depend only on quantities at n. Okay? So let's denote this as d tilde at n plus 1. Okay? Now, we are getting into the realm of what are called predictor-corrector methods. 'Kay? And the idea is that if we want to calculate d n plus 1, right? We have this formula for it. We predict this, right? We say that well this is our guess for what it is, okay? And in general that will not be correct and we need to correct it with this term which is the corrector. Okay? So, written differently, d n plus 1, actually not so differently, d n plus 1 is equal to the predictor d n first 1 tilde plus the corrector alpha, delta t v n plus 1. Okay, so this is a predictor-corrector method. Okay. This v-Method that I'm talking about is one where what we do is return to our time-discretized ODE. Right, which is this one, Mvn plus 1 plus Kdn plus 1 equals F at n plus 1, right? We return to this equation, and we try to rewrite it only in terms of v, okay, and the way we can do that is to essentially use for dn plus 1 here, this predictor-corrector form. Okay, so substitute. Predictor-corrector form for dn plus 1, all right? What that does then is leave us with M v at n plus 1 plus Kdn plus 1 tilde plus alpha delta t Kv at n plus 1, okay? All of this equals F at n plus 1, all right? Now, I said that the reason we call this the v method is because we're going to try and rewrite our discretized ODE entirely in terms of v. Right, entirely in terms of vn plus 1. So that is indeed what we do, okay? So we get here this implies M plus alpha delta tK vn plus 1 equals Fn plus 1 minus Kd tilde at n plus 1, okay? We solve this equation for v, for vn plus 1 and we now have the corrector, right? Because then we can add vn plus 1 from here, up there, and we get the corrected state, all right? Right, so, so basically we solve. It's this inverse acting on Fn plus 1, minus Kd tilde n plus 1, okay? Right, now I'm going to make a couple of remarks. Okay, and those remarks are the following, right? Now, the first remark is that if, instead of M, we used the, the lumped-mass, okay, and, furthermore, we use alpha equals 0, okay, this will be forward Euler. Okay? 'Kay, have this particular combination, and what we see is that for vn plus 1, which is lumped-mass inverse Fn plus 1 minus Kd tilde n plus 1, okay? Because the lumped-mass is a diagonal matrix, its inverse is trivial, right? So as a result we don't really solve equations here, right? We don't solve, we don't have to solve with a, a, a linear system of equations by, by somehow effectively generating that inverse, okay? So this is what we call an explicit method. Okay? In contrast we have if alpha is not equal to 0, okay, all right, then we have in that case even if our mass is lumped, we still have equations to solve, okay? So this, this now an implicit method. All right? Okay, so these are two, two remarks to make and what we will do is move on and proceed directly to the other canonical solution method, which is the d-method. Okay? And the d-method once again, we could look at our predictor-corrector form. But now, we sort of turn this around to eliminate v from our discretized ODE, all right? What that implies then is we write here we get vn plus 1 equals dn plus 1 minus dn plus 1, tilde, okay, and divided by alpha delta t, okay? This of course, holds only if alpha is not equal to 0, okay? All right and in this case we get v from here and then we substitute it back into our discretized form of the ODE, okay? And. Okay? So, when we substitute in our discretized form of the ODE, M will be n plus 1, we had dn plus 1 minus dn plus 1, tilde, divided by alpha delta t, right? That is our approximation for vn plus 1. Well that's our expression for vn plus 1 using the Euler family. And here we have Kdn plus 1 equals F at n plus 1. And this again we reorganize by simply multiplying through by alpha delta t, okay? So we get M plus alpha delta tK. D at n plus 1, equals alpha delta t. F at n plus 1 minus Md tilde at n plus 1, okay? Now in this form of course you realize if we can get to the final form it doesn't matter if alpha is equal to 0. Therefore this works ev, even though I, I, I wrote that in, in, in defining vn plus 1, right? Alpha even though I wrote that alpha is set equal that, alpha cannot be equal to 0, right? We still get back our we will still able to actually solve the equation and move on, okay? So this would be the v-form and now what we get here is dn plus 1 equals M plus alpha delta tK, dn plus 1, sorry. Inverse here times alpha delta t Fn plus 1 minus Md tilde, n plus 1, okay? For this sort of method if we use a lumped-mass, right, we use a lumped-mass here. What is seen is that we require less effort to set up the right-hand side, okay? All right, if we use a lumped-mass here, there are fewer operations, not less, less that limit that fewer operations. Fewer operations to form the right-hand side. Okay? But the actual equation solving effort in the two methods, the v-method and the d-method is the same, all right? So here, here we have it our you know, this, the, these are the two methods by which we can set up the the, the solution of this problem you observe that you're, you're still solving exactly the same, sol, the same serof, the same set of equations so the methods are, are, are, are really equivalent. It's just a matter of which, which variable you attempt to eliminate first, okay?  right, so what we can begin to do now is to proceed to the to actual solution and well, sorry, we, we've already looked at the, at the solution techniques. What we can do now is to, is to set ourselves up to carry out an analysis of these methods, okay? And that may perhaps be done best in a separate segment. So when we return, we'll take up the analysis.

\section*{ID: nL6u-s460qM}
In this segment, we'll start looking at the coding template for homework five. So let's turn and look at the code now. Again, homework five is the transient heat conduction problem. And you'll notice some differences here in the main.cc file. One is that I've created this constant alpha, which is an input for the constructor for our FEM problem object. Alpha is to specify what Euler method we're using. Again, alpha can range anywhere from zero to one. For the homework, you'll be doing it for alpha equals zero, one half, and one. But, other than that, the mesh generation is very similar, and setup-system and assemble-system. However, now we also have to solve functions. We have solve-steady, which solves our steady state problem, and then solve-trans which, obviously, is the transient solution. In your header file you'll have a function that calculates the L2 norm of the difference between the steady state solution and the transient solution at a given time. And so I'm also outputting to the screen here, some of those results of the L2 norms, there will be four L2 norms, It will calculate the L2 norm at time equal 0, 1,000, 2,000, and 3,000. Those are some of the results that we'll be using in the grading. Let's shift over to the header file now. Just as in homework four, I've defined order and quadRule as global variables at the top here. Again, when you turn in your assignment, use order equals one and quadRule equals two. But for your own edification, you can  change that order and quadRule if you'd like. The L2, again, will take care of any any order our quadrature rule. We'll scroll down and look at the declaration of functions and objects. The first four solution steps are the same, but then we have this solve-steady and we have apply-initial-conditions, since it is a transient problem or includes a transient problem, we have to include initial conditions. So you'll be defining those within this function. That function is called within the solve-trans function itself. I now have two output results functions, one for the steady state result and one for the transient results. Notice for the output-trans-results, I have as an input an index. So that's just to distinguish between the output results for, say time equal zero and time 1,000, it just puts an integer onto the file name, so that it doesn't overwrite the solution file. And again now we have another L2 norm function again, so if you've missed that from homework two, it's back. You'll notice here we only have the quadrature formula for volume integral; since we don't have any Neumann conditions, we aren't doing any surface integrals on this problem. You'll notice I have more matrices and vectors here than before. We have capitol M, which is our global mass matrix. There is again K and then we have a generic system matrix. That's because when you're doing the transient problem, you have to do a couple of matrix inversions to solve, for example, the VM plus one. And so, system matrix is a generic matrix that you'll be defining later on, in the solve-trans function. We also have D-steady, which is the steady state temperature profile. D-trans, which would be the transient temperature profile at Tn or Tn plus one. V-trans would also again be similar. It's the transient time derivative of the temperature at each node. Either for time Tn or Tn + 1, depending on what part of solve-trans you're in. And then again our vector F and RHS. RHS is a generic right hand side vector, similar to system matrix that you'll be populating within solve-trans to solve for your Vn + 1 vector. Okay? We also have this standard vector of doubles, called L2norm-results. This is going to store the L2norm at various time steps. And then alpha. Alpha's just the input, from our constructor, ok? Specifying again what Euler method we're using. So if we scroll down to the constructor, again you can see we're inputting alpha and storing that within our class and very well alpha And if we scroll down to generate-mesh, this again is exactly the same as previous assignments where again, you just need to define the limits of your domain. Define boundary conditions, would be very similar to previous assignments. Very similar to homework three, which was your previous heat conduction, your study state heat conduction problem, except now you'll notice that I have two maps here. Boundary-values of D and boundary-values of V. I actually skipped over those higher up. They are class variables, class objects. And since we have these two fields that we're dealing with, we're dealing with the temperature distribution and we're dealing with the field of the time derivative of the temperature. And so we have boundary condition on both of those. The boundary conditions on D are 300 at one end and 310 at the other. They are fixed temperatures, so what does that tell you about the boundary conditions on V? Which is the time derivative of the temperature. Tells you, of course, that the derivative of D would be zero at those boundaries, where temperature is fixed, okay? So you'll be doing a similar loop to check where your node is to see if it is at an Dirichlet boundary. If it is at a boundary, you'll put that information into these two maps, using the globalNodeIndex, which is the same as degree of freedom index for this scalar problem. And you'll passing the prescribed temperature value and the temperature time derivative, which again we said would be zero. When you have, at fixed temperature, alright? So that will take care of your boundary conditions. Again set up system is very similar to previous problems, there's nothing for you to change here. Let's look at a symbol system very quickly, before we end this segment. This is going to be sort of a mixture of the previous two homework assignments, your heat conduction problem and your, and using FEvalues in the linear elasticity problem. Again, we have FEvalues here, we're going to update the values, update the gradients, and the Jacobean determinative of the Jacobean times the quad digital weight values. I set up this variable row and that's the specific heat per unit volume. It's a constant that we'll be giving you in your homework assignment, that you'll use to define M local, and then to get your global mass matrix. All right so let's move inside the element loop. And first we're going to be populating M local. Let me write out that integral for you here on the board. MlocalAB is equal to the integral over the element's domain, of row, which again is your specific heat pre-unit volume. Now, potentially that could vary across the domain. For this homework problem we're keeping it as a constant, so I can pull that out of the integral. But then, it's simply the basis function BA times the basis function B. Integrated. Okay, so this is fairly straight forward, and from your experience doing Klocal and Flocal this should be pretty straight forward for you. Pretty simple. Okay, so you'll edit that there to find that here. We scroll down and we have to create Klocal. This will be the same Klocal as you've created in homework three. The only difference is you don't have to worry about the inverse Jacobian since, using deal two's basis function gradients. We already have the gradient with respect to the real domain. So that simplifies our quadrature loops a little bit. All right so we have looping over AB. The quadrature points, which again all three quadrature loops are grouped into a single quadrature loop with. Then we do i and j to perform the multiplication with 0ur conductivity tensor, kappa. Alright? We don't have a foreseen function in this problem, so Flocal will be zero, and so we don't even bother in this assembly to assemble F, our global vector. But you will be assembling K and M very similar to previous assignments. Right. The solve-steady function acts very similar as the solve functions in previous assignments. The small difference that I've done, is I've Instead of applying boundary values, instead of applying our boundary conditions within the assembled system function, I've shifted it down to solve-steady. Okay. And within this function, we'll solve for the steady state solution, and we store that result within the vector D-steady. Okay, so that's nothing new. Up to this point we haven't solved for anything more than what you already solved for in Homework 3. Which is just the steady state heat conduction problem. In the next segment we'll move on to looking at solving the transient solution as the time progresses

\section*{ID: iM-Ht1x7e2M}
Now in this segment, we'll move on to looking at solving the transient heat conduction problem in the hallmark five template. If we look over here, now that we're solving the transient problem, we have to plan initial conditions. And I've actually set up most of this for you. What we need to do is we loop through all of our nodes, and we check the position of each node. And, depending on the position of the node we'll define the initial temperature at that node. Pretty straightforward. You'll just need to insert those values, in some cases it actually depends on the x value of the note itself, as a function okay? All that information is given in the homework, you just need to calculate the value. Now that takes care of d naught. Storing the values of the initial values of the temperatures in D-trans. But now we will also need the initial values for v. We need v naught. And so to do that, we will solve a matrix vector system, okay. Now remember, this problem is based on this equation. We have M times V plus K times D is equal to F. F in this problem is of course zero, but in general, it wouldn't and it could be changing with respect to time. Actually, all of these objects could be changing with respect to time, potentially. So, we can also put the initial values in this equation, and if we need to solve for V naught, well first we move KD over to the right hand side This becomes our right hand side vector. And then simply do a matrix inversion to solve for v not. Now in this problem the mass matrix and the stiffness matrix don't change with respect to time, which is why I can get away with calling the symbol system only once. Okay, so let's look in the code at how I'm doing this matrix inversion, and how I'm doing these matrix vector operations. Okay, I've done all of these for you, because you'll be doing several of these on your own within the solve trans function. Okay, so first, I set RHS equal to K times D trans. And to do that I used this dot vmult function. Now with sparse matrices it will do a matrix vector multiplication. I pass in RHS and D trans. And it's acting on my sparse matrix K. But what it does is it stores within RHS the values of K times D-trans, okay? In the next step, I just take RHS times equals minus 1. Essentially I'm just changing the sign on all the components in my RHS vector. Okay, so this point RHS is equal to minus K times D trans or minus K times D naught. And then finally, even though F is equal to 0 and in this case I've included it for generality, I do RHS add 1 times F When is the scalar coefficient to F. And so that takes RHS and adds on to it one times F, which is, now, gives us a right-hand side vector of F minus K times D trans, or F minus K times D naught. Okay, so now my right hand side vector is set up. In this case, I'm only doing a matrix inversion on M, so I don't have to do any matrix operations there. But I did, up here, copy M into the system matrix, okay? So that's the same as saying system matrix equals M. Now that all these data objects are set up, I can apply my boundary values. Now note that, since I'm solving for V trans in this case, I will apply boundary values of V. The matrix that I'm inverting is system matrix, the solution is v trans through an outside vector is RHS. I create this sparse direct umf pack A, which copies over from system matrix. And then I do my matrix inversion. Now it may be a little confusing. I do A dot vmult here, but you notice it's doing a matrix inversion and then multiplying it by the right hand side vector. It's just the way the functions were named and so you just need to remember that with the sparse direct GMF pack, sparse matrix dot vmult does the matrix inversion. However, when you're dealing with a sparse matrix and a vector dot v mult, just as a simple matrix vector multiplication without any matrix inversion. Okay? All right, so with that background, let's scroll down to the solve trans function. Okay so the first thing we do in solve trans, is apply the initial conditions. So that's the function that we are just looking at. And the next is to define delta-t. I have delta-t set to equal one and that will give you a stable algorithm for all alphas, alphas = 0,1/2, and 1. You're free to change that on your own however, when you turn in your assignment, leave it at delta t = 1, okay. I declared this vector D-tilde, which you remember we use as we're updating. From DN to DN+1. And then we go into our time loop. Now I'm going to point out here that we are using the V method here. Okay. And so we'll look over those equations as we go through this code. All right, so the first step is to find detailed. Let me write that out for us here. So detailed, n plus 1, again this is a global vector, is equal to d at time n plus delta t times 1 minus alpha v n, right. These are equations taken from the lectures so you can look it up there. So this is step one. Step two is to use d n plus one tilde to find v n plus one. And we have this system maybe, equations here. We have M plus alpha delta tK, and I'm not going to re-derive these during the lectures, but here's the result. Sorry, we haven't quite inverted that yet. going to delete that. Okay, so M + alpha delta t K times vn + 1 is equal to Fn + 1, which in our case is just 0. Minus k times d tilde n plus 1. Which of course you would then to solve for vn plus 1 you would do a matrix inversion. So we take m plus alpha delta tk, inverse times fn plus 1 minus k. Detailed m + 1. This will be referring to as our system matrix in the code. And this we'll set up as the right hand side, the RHS factor. Okay, once we have the n plus one, step three is to find d n plus one, which is d tilde n plus one, plus alpha delta t, v n plus one. Okay? Note in the code dn at this point is d trans, vn is also v trans. Here we update v trans to be Vn+1, and at this step we're updating d trans Sorry that's a capital V in our code. Updating D-trans to be dn plus 1. Okay. So we'll be following these three steps in the code itself. Okay, so let's go back to the code. You'll notice I've given a list here. Of how to perform these matrix vector operations. A lot of these we've already seen above in the apply initial conditions function. You can use this same .add function with the system matrix, like we did with the vector. Again dot v mult will do a matrix factor multiplication. If you want to set m equal to or system matrix equal to a vector or to a matrix excuse me, you would use copy from. However with a vector you can just say RHS equals f for example. We already saw this. I'm operator times equals with a vector to change the sign. Okay, so that should be enough for you to be able to do all of these matrix vector operations. Okay, so the first step again was to find D tilde. And again, at this point D trans it does hold the values of D so then nv trans holds the value of V sub N. Okay so once you found D trans you will then create system matrix, as we wrote on the board and RHS. Again as we wrote on the board. Once you've done that, we apply boundary values of V, again because V trans or VM + 1 is the solution vector we're solving for. So we apply boundary values of V using the system matrix that you've defined and the RHS factor. Okay, and that matrix inversion happens here with these three lines. Again using the sparse strict plume of pact matrix, A. Okay, once you've done that matrix inversion and V trans is now equal to V n+1, you can use D tilde and V m+1, V trans, to solve for D m+1. Okay and there you have the solution for this time step. The last thing to do is to output results. Now I have it set to output results every 1,000 time steps. You'll notice I'm using this modular operator. So every time the current time step index, divided by a 1,000 gives a remainder of 0. Then we'll output the transient results as a VTK file. And also store at the L2 norm of the results in this L2 norm results. Or, sorry, I'll calculate the L2 norm of the difference between the transient solution at this current time step and the steady state solution. I'll store that within this vector here, okay? Alright. So the last step, after outputting results, or the last function I should say, after the output results functions, is to calculate the L2 norm. This is going to be very similar to the L2 norm that you calculated in Homework Two, except of course you can now use dl2 basis functions. You will still need to interpolate to find U steady and U transient in exactly the same way you interpolated U H, you will find that element solution in the torched point. And you will construct the intergrand of you know, the square of u-trans minus U study. Okay, and then we we return the square route of that value to actually calculate the l2 norm itself. Alright, so that should be pretty straight forward with what you've seen before, and that concludes our discussion of the template for homework five.

\section*{ID: NLQd-oA5qqY}
Welcome back. We are going to take the first steps towards analyzing our ODE as written out using the Euler family. Okay, and in order to carry out this analysis there are couple of things that we have to do. The first is the following, let us go back to our discretized ODE and write it out in a slightly different form. Okay, the forms are completely equivalent to what we've seen before, the v and the d form. But, but, for, for the purpose of analysis, let's do the following. Okay. So, the, the title of this segment is analysis. Okay? And in particular, we are going to look at what I will call model decomposition. Right. In order to get there, let's do the formula. Let's write out the equation to these formulas, right? We're going to write it out as M. Okay? We are going to directly write a time discretized approximation, a finite difference approximation if you will, of our time exact time derivative, right? So, we're going to write d dot here as approximated as d n plus 1 minus d at n divided by delta t, okay? So that's our approximation of M d dot. Now the, where the Euler family comes in is when we say that everything else is evaluated at n plus alpha. All right? This is how we want to view it. Now, one more thing. We are interested in understanding what the basic properties of our integration algorithms are, what the basic properties are, and as often happens in the study of ODEs, we will take advantage of what is sometimes called a homogeneous form of the ODE, which is obtained by, by, by setting the forcing equal to 0. Okay so, so what we need to analyze here is we want to analyze the stability, stability and what we will define as the consistency of the time integration algorithms. Right? We will define what we mean by their stability and consistency. But in order to do this and especially with things like the notion of stability it makes sense to actually set the forcing equal to 0. Okay? Think about why this may be. Why may it make sense to set the forcing equal to 0 if we want to start out by looking at stability? Okay? Now, it's, it's simply because well, you know, how a solution evolves in time can depend upon the forcing, right? You, you give it a forcing which makes it keep growing in time, well it will keep growing. Right? So we want to get that out of the out of the picture. Right? And instead we want to understand you know, the rigorous way to pose it is to just ask the question of, ask the question that if there were no forcing, how does my time discretization and my, my introduction of this, of this Euler, Euler family of algorithms affect the evolution of the problem, okay? So we, what we will look at is the, we will consider the homogeneous ODE. Okay? Now, the time exact version of the homogeneous ODE that we are working with is this. M d dot plus K d equals 0, okay? With the initial condition d at 0 equals d naught, right? And the time discretized version of it is what we've written up here M d n plus 1 minus d n divided by delta t plus K d n plus alpha, okay, equals 0, with the 0 being given. Okay. So the first of those equations that I just wrote is the time exact homogeneous ODE and the next one is the time discretized version using the Euler family. Okay one thing you will recognize here is that in all of this what we're seeing is that dn plus alpha equals alpha and dn plus 1 minus alpha. Sorry, alpha dn plus 1 plus 1 minus alpha dn. Okay? That's what we have. All right. Now, in order to proceed, we are going to do the following. We will, we're first going to study the question of, we're, we're first going to look at stability. But even before that what we want to do is carry out what we will call a modal decomposition of the problem. All right. In order to carry out a modal decomposition, we are going to take a step which is to invoke a related eigenvalue problem. Okay. We will invoke the related, generalized eigenvalue problem. Okay, and the generalized eigenvalue problem we want to invoke is the following. I'm going to write it as M for a vector here. Let me not use d but let me use phi, okay? The, right, so the problem we want to look at is M phi equals lambda K phi. Okay. Sorry, let me turn that around. Let me write it as, sorry, let me write it as this. Let me write it as lambda M phi equals K phi. Okay, this is the generalized eigenvalue problem. And, and perhaps you know, in a very obvious step, let me just change the left hand side and the right hand side. So we have K phi equals lambda M phi. Okay? All right. Where, so when you look at it in this form, it if you haven't seen a generalized eigenvalue problem in the context of linear algebra before. You will, you have seen a standard eigenvalue problem probably where a standard eigenvalue problem is obtained by just setting N equal to the to what you would call the identity matrix in the corresponding dimension. Okay, so let me just state this. So a re, remark is the standard eigenvalue problem. Is, a standard eigenvalue problem would be of the form K phi equals lambda phi. Okay? And all of this of course, phi belongs to the Euclidean space or real space with number of, of dimension ndf. Okay so, what I've written down here will be a standard eigenvalue problem which underlies eigenvalue problem is where instead of having the the corresponding identity matrix here, right? We have sorry, that's the isotropic denser. Instead of having the, the identity matrix here, right, we would have the we have some other matrix here. Right, and in our case it's a matrix n which we know to be symmetric and positive definitely. All right? Okay, so this is the generalized eigenvalue problem we want to consider, right? So it is this one that we will be working with. Okay? Now what one can do from here is the following, okay? What one can see is that for that problem let say, phi sub m, right? Where m equals 1 to n d f. Okay? Let these be the eigenvectors. Okay, and let lambda m be the corresponding eigenvalue. Okay. All right so, so lambda m is the corresponding eigenvalue. Now what one can do is one can show that it's possible to con, to construct a so-called orthonormalization of the of the eigenvectors, okay? So, the eigen-vectors and eigenvalues satisfy the following equation. The eigenvectors and eigenvalues of course satisfy K phi m equals lambda m times M phi m. Right? Little m going from 1 to number of degrees of freedom. All right? Okay. So given this, it is possible to construct a, a, it's, it's possible to choose the phi n such that they satisfy a certain orthonormalize a, a, a certain property of orthonormality, okay? In particular the property for orthonormality that they satisfy is the following. The phi m, right, the set of eigenvectors, m equals 1 to ndf, right? This set of eigenvectors can be orthonormalized. Okay? They can be orthonormalized to a different set, psi sub m, okay? M again equal to 1 to ndf. Okay, they can be orthonormalized to a set psi m which are such that they are, this orthonormal, this orthonormality property is an orthonormality with respect to m. Okay, what that means is the following. If we take psi m and dot it with M mass matrix, psi k. All right. This product is delta m k. Okay? Where delta is our Kronecker delta, all right? Okay? So this is the Kronecker delta. Okay, and furthermore the set psi m. Right? That set of eigenvectors. I mean, I see they're orthonormalized. What I can further see is that they are M orthonormalized. Right, it's simply, it can be interpreted as being orthonormal with respect to M. Okay?

\section*{ID: DhhryQHWoLI}
The type of autonormalization we need here it can be constructed by what is called the GramSchmidt process, okay? It can be constructed by the GramSchmidt method I guess. Okay? And what this does is it lets us start out with a linearly independent set of eigenvectors which would, which maybe the ones that you originally solved for by solving that generalized eigenvalue problem. Okay? So this is a linearly independent set. Okay? Why do we know that the eigenvectors that we get by solving that generalized eigenvalue problem are going to be linearly independent? That's right. It's because k and m are both positive definite. Okay, that's what guarantees the existence of linearly independent eigenvectors. All right. In some situations, we may have repeated eigenvalues, in which case the eigenvectors are not uniquely defined, but nonetheless they can be picked to be linearly independent. Right? Okay, so so we, we are assured of linearly independent eigenvectors phi and the GramSchmidt process, GS is short for GramSchmidt essentially takes us to a si sub m. Right, which are now M orthonormal as we have defined. This is a completely standard process procedure that is available in many books on linear algebra or, or sometimes even books on partial differential equations because the method does exist to, does extend even to continues eigen functions. All right, so, so we have that. Well, what does that then say for our, for, for, for our problem, okay? Note that, what this tells us is the following. Right? So now, because we have the set psi M we can construct the following sort of the product right? Supposing we take M times, M acting on psi K and we dot it from the left with psi M. Okay. All right. Now. Okay. And then we also have a lambda k here. Okay. The reason we have a lambda k, is because I started out by writing M psi K. Okay, so what we get on the right hand side is psI M dot K psI K. All right. Okay, when we get, when we look at this sort of product now what we observe is that because we have this M orthonormal property for our autonormalized set of eigenvectors. We have here lambda K, delta mk. Right, on the left hand side. Okay? And here on the right hand side we have now a result for forming the sort of quadratic product between psi M, K and Psi K. Okay, so alternately what this implies for us is that if we form just as we form for psi M a product, if we do the same thing for, if we do the same thing with by, by using K in it, right. Right, this is equal to lambda m, okay? We get lambda M because of the fact that we have this Kronecker delta product acting here. All right. So we have this additional property. Okay? All right. Okay. So, with this background in hand to let us now move on to trying to understand how we analyze this how we apply it to analyze our problem. Okay? In order to proceed there, let us first use the, another property that is given to us by orthonormalization. Now, because we started out with a set of linearly independent vectors phi m, okay, and we proceeded to orthonormalize them. We also know that this set is linearly independent, okay, therefore it spans the space that we are working in Okay so, let's state that, all right now the set psi m, m equals 1 to Ndf forms a basis in R Ndf. Okay? What that means then that any vector, okay, say d, right? Any vector d can be written as sum over N running from one to Ndf of d sub m. Psi sub m, okay? We have this expansion of d in the basis. Okay, all right. Okay, and now if we ask, well how do we compute this, how do we compute such a, a representation? We can compute such a representation if we have the dm's. Okay? And in order to know what those dm's are, we observe that we can simply multiply this, equation from the left by our matrix m. Okay, so. To get the coefficients d sub m, okay, what we do is that we first full construct Md equals sum m equals 1 to Ndf. Dm, M, psi m. Okay? Just using linearity. All right, okay, now, once we have this we can now dot this vector equation on the left by psi k, right, our eigenvalue, okay, and we can do the same out here. Okay. Now, using the linearity of the dot product, what we get is the following. Okay? What we get on the right hand side is sum m equals 1, to Ndf. dm psi k dot M psi m. All right, but then what is this? This is just delta km. Where did that come from? From the fact that we've constructed the psi m's, that entire set of psi's to be m orthonormal, okay, but then this explicitly when we compute the sum knowing the delta km is the chronicle delta, we get d sub k. All right, so here we have a definition for what our coefficients are. These coefficients, dk, are uniquely defined, okay? And therefore this expansion, right, of d in terms of it's, of that basis is also unique. Okay. So, let me just write a few comments about this, about this decomposition, okay. So, when we say that d can be written as sum over m, going from 1 to Ndf. D sub m psi sub m. This is what we mean by the modal decomposition of d. Okay? Each psi m is what we will call a mode, right, so psi m is the mth mode. Okay, and then d sub m is the corresponding modal coefficient of d. Okay? And, because we have a, representation for d for, for the dm's or the dk's in general as we derived on the previous slide. This representation, okay, what is this representation? It is that the dm, let me get rid of this because it might look like a minus sign. The dm the dm's are defined as psi m dotted with M d. Okay? So we have a unique representation for the dm's. Okay right. We are going to end this segment here, thanks.

\section*{ID: C6o1k5ZLDF4}
Welcome back. We have just embarked upon the analysis of our time integration algorithms for parabolic equations, right, as of, of, first starter in time. What we've accomplished along that direction in the last segment is a recognition that there exists this generalized eigenvalue problem that one can identify, which provides for us a basis on which to carry out a modal analysis of the problem. Okay, so let's just recall that aspect and charge ahead. Okay so we're engaged here in the analysis of our, of the time integration algorithms. All right for linear parabolic systems. Right and you will recall that this is based upon the generalized eigenvalue problem. Right, and that generalized eigenvalue problem takes on the form K, which is our stiffness matrix. Sorry, our conductivity or diffusivity matrix. Acting on a vector, that is psi M right? K psi M equals lambda M, M which is our mass matrix psi M, okay? This is the basis for it and we went ahead with this, right? What we found and I'm not going to go through the entire analysis I'm just going to recall the critical parts of it, is that one can orthonormalize the, the eigen vectors psi with respect to M. Correct, so we found that psi L dot M, psi M equals delta LM, chronicle delta, and, because of that, this result also lets us say that psi L dot K psi M equals lambda L okay? All right actually properly equals lambda M multiplied with delta LM. Okay, that's what it actually is. Okay, so it is zero, unless M equals L. Okay, the second equation, the right hand side of the second equation. Okay, and if L equals M, it's equal to psi L, which is the same as psi M. All right, and you recall that in this lambda M is our eigenvalue, right, corresponding to the eigen vector psi M. Okay, so this is orthonormalization. Okay. All right, so with this in hand, what we observed I think at the very end of the segment is that since these vectors psi form and actually span the space, they serve as a basis. Okay. So, what we have is an expansion In the psi m basis. Right, where m equals 1 up to ndf, okay? All right, and this letter say that a vector d, right? Such as the vector of global degrees of freedom that we're now working with can be expanded as sum over M d sub m psi sub m, okay? Right, and we saw that each of the d n's, the modal degrees of the, the, the modal coefficients each, right? Each d n is given by right it's equal to psi M dotted with M d. Right? This is everything that we need to know. Right and these are the modal coefficients. All right. What we are going to do now is with a summary of our analysis, of sorry, with the summary of our modal decomposition of the problem, right. Right, so this, this, this result is the modal decomposition. The modal decomposition of d. All right, so, with this, with these results, with this summary of results, we can go ahead, all right. And, what we are going to do now is, extend this idea of the modal decomposition to, the equations we are working with. All right. So, we're going to start here with a modal decomposition. Of. The time exact O D E. Right. Right, and remember everything that we were working with homogeneous equations, okay. So which equation is that? Think about it. It's this one. M d dot plus K d equals not the forcing term on the right, or well, the forcing term is equal to 0. All right we have this with this initial condition that d at times 0 equals the d 0 vector. Okay, so now we're going to do a modal decomposition of this equation itself. The way that works is the following. We already know the modal decomposition of d. Okay. That modal decomposition I'm going to write in slightly different notation, the notations not terribly different. I just want to move the m from being a subscript of the coefficient to a superscript. Okay, so that's not a power, that's a superscript. All right, and you'll see just in a bit what I want to do with the subscript. I need them in there, okay. This times psi. Let me move this also to the after the superscript. Right, the m there is also a superscript. Now remember that this d vector of ours is time dependent. Right, that's why we're able to take a time derivative. Well, we could always take a time derivative, but that's what makes sure that the time derivative isn't doesn't vanish, right. Okay, so d is a function of time. Okay, my question is where on the right hand side does that time dependence go? Does it go into the coefficient? Right, the d n's or the sines, the eigen vectors? Right, they go the time dependence shows up in the d n's. Okay, and more than just a convenience that it has to be that way. Why is it that the sines don't change with time? It's because the sines are, are the eigenvectors of the generalized eigenvalue problem. But the matrices that define the generalized eigenvalue problem, the K and the M matrices are fixed in time, okay, because we are doing a linear problem. Okay, so let's just state that, that's useful to remember, okay. So, this what, what, what's behind this particular decomposition, right, where the time go, where the time dependence is held in the, in the coefficients, right, is the fact that the set of psi m. Right? Are fixed in time. Because, our conductivity matrix K and mass matrix M are also fixed in time. Okay, that's what allows us to do this. Okay, what you see, what that also allows us to do, what that immediately lets us see is that. D dot, right? D dot is now sum over n, d dot n, function of time, psi. All right? Some of cos running from one to number of degrees of freedom. Okay? And we are going to make these, and you are going to substitute both these decompositions into our time exact O D E, right? So what this implies then is let me just say this as substituting. Okay? It implies now that M multiplying the sum over little m d dot m, psi m plus K sum over m d m, psi m equals 0. Okay what I'm going to do next is is apply linearity, right? So what this then apply, what this implies is that because M and L are matrices and because we're dealing with linear algebra there, what we can write here is that sum over M, d dot m mass matrix acting on psi m plus sum over m, d m, conductivity matrix acting on psi m equals 0. All right. Now I am going to dot this entire equation on the left, well it doesn't matter with left or right, I'm going to dot it because it's a vector equation finally. I'm going to dot it with a specific eigenvector lets say psi L, okay, so now I'm going to take psi L, okay? Dotted with the sum d m dot, psi m. Plus psi L dotted with the sum over m, d m, K psi m equals 0. Okay? Note that something has changed between those two equations, right? Something very obvious has changed. 0 here is no longer a 0 vector, it's a scalar 0. Right? Because I'm after all taking a dot product off a vector equation here, this was a vector equation, and I've taken the dot product of it with another vector of the same dimension right, psi L. Okay, but now we know how this all works out, right? Because we know exactly what happened with psi L oh I'm sorry I am missing here, sorr,y I'm missing an m here, so let me just clean this up. I'm missing here. That matrix okay, all right. Now its all right, okay. Well let's carry out these dot products right and and we know what we get from these.

\section*{ID: kMwd1mVNq1I}
Kay, so we know how this works out. We get in the, in the, for the first term, sum over m, dm, dot psi L dot M psi m, plus sum over m dm psi L dot K psi m, equals 0. All right, now this we know from the orthonormality condition is delta lm, okay? And this we know also from the fact that we have the generalized eigenvalue problem at hand. This turns out to be lambda m delta lm, okay? And then when we account for the fact that there is a sum over m in each case, the Kronecker delta does its job and leaves us with the following, okay? What it leaves us with is on the, for the first term, we get dl dot plus, from the second term we will get dl, well let me put the lambda in front, we get lambda l dl equals 0, okay? All right, and this came around, came about just by applying the properties of the Kronecker delta and using that sum, right, over m, okay? Now, we did this for an arbitrary mode, or an ar, an arbitrary eigenvector psi l, 'kay? So this holds for all, well, this holds for all l actually, where all l equals 1 to ndf, okay? All right, this is what we will call our single degree of freedom modal equation. Okay, produces to a really simple form. All right, okay so we have this and what I'm going to do is very systematically do essentially the same thing for our time discrete homogeneous ODE, okay? Okay let's first write out that ODE, right? We agreed that as far as analysis was concerned, we were going to work with it in the following form. Okay, this is the form that we're going to work. We're going to rewrite this by just expanding out the n plus alpha, and also multiplying through by delta t, 'kay? So in one step, I'm going to say that I have M dn plus 1 minus dn plus delta tK times alpha d at n plus 1, plus 1 minus alpha, d at n equals 0. Okay, just multiplying through by delta t and expanding our dn plus alpha, okay, so that in itself. Right? Okay, and let's go ahead from here and essentially introduce modal decompositions of dn plus 1 and dn, okay? All right, so the modal decompositions we are going to apply here are. D at n plus 1 equals sum over m, the modal coefficient, m, evaluated at n plus 1. Okay, this is the modal coefficient the nth modal coefficient of the d vector at n plus 1, right, the time tn plus 1, right? Psi m, okay, I don't need to really to write the same thing for n, because it's just replacing n plus 1 with n. Okay, right, so likewise with dn, 'kay? We do carry our those modal decompositions and then we are to substitute them back. All right, so now when we make we we go ahead and, and plug them into the last equation from the previous slide, we have M, okay let me do two things in one step here again if you don't mind. I am going to write, I'm going to combine the matrices multiplying dn plus 1 and the matrices multiplying dn, okay, in a single step, okay? So for dn plus 1, I have M plus alpha delta tK, right, multiplying dn plus 1, but dn plus 1, I'm going to write using this modal decomposition sum over m, d, right, that coefficient, the dm coefficient at n plus 1, right psi m, 'kay? And the thing's multiplying dn, I'm going to write as follows. Okay, I'm going to write this as minus M minus 1 minus alpha, delta tK, and hopefully I caught everything I need to, yes I did dn, but dn again I'm going to write using the modal decomposition. Right, all of this equals 0. Okay, so I've taken two steps here relative to the last equation that I wrote on the previous slide. I've combined the matrices that multiplied with dn plus 1 and dn, okay? The dn plus 1 and dn vectors as well as using the modal decomposition for dn plus 1 and dn, right? So this is And this is that modal decomposition, okay, that's what we have, right. And, you know what I'm going to do next, right? What I need to do is go from here, this is a vector equation of course, go from here to a scalar equation for a single degree of freedom, and we're going to do that by simply dotting this on the left, by dotting this with psi l, right, so I'm going to do psi l, dot it with all of this, 'kay? Likewise here, I have psi l, dotting everything, okay? And then using linearity and so on, what we get here is the following. We get sum over m, dm, n plus 1, multiply psi l dot m psi m, okay, all right, plus alpha delta t psi l dot K psi m, 'kay? All right, all of this minus sum over m, dnm, we open parenthesis here, psi l dot M psy n, sorry, M psy, M psy m minus 1 minus alpha delta t psi l dot K psi m, okay? All of this equals now, what is it, vector or scalar 0, scalar 0, okay? And then, everything works out just as expected. This thing gives us a delta lm. From here, we get a lambda m delta lm, okay? Here again, we get delta lm. And from here, we get lambda m delta lm, okay, all for orthonormality and from the eigenvalue problem, right? And then we account for the fact that we do indeed have a sum over m and what that lets us say then is that because of the action of the Kronecker delta we are left here with essentially the action the, the what the Kronecker delta does because of the sum over m, is that it effectively converts that index m into l, 'kay? So we have delta l n plus 1, okay, from the first term here, 'kay, we get 1 plus alpha delta t. Here again, because it's Kronecker delta acting he, acting on, on, multiplying the lambda m, we are left with alpha delta t lambda l, okay? Minus d at n for the l'th mode times 1 minus 1 minus alpha delta t lambda l. Again that lambda l comes from here in the action of the Kronecker delta, right, all of this equals 0, okay? This is our equivalent for the time discretized ODE of the single degree of freedom modal equation for the time exact ODE, okay? So this is the single degree of freedom modal equation, right, for the time discrete problem. All right? 'Kay, it is to be compared with the single degree of freedom modal equation that we obtained for the time exact problem, okay? All right, this is actually a great place to stop. When we return, we are going to analyze the two single degree of freedom modal equations. Oh, just one, one more thing. I should mention that this holds for all l equals 1 to ndf, 'kay? All right.

\section*{ID: PGw-wgZgTn0}
All right we are ready to move on, and were ready to move on with the analysis of our single degree of freedom model equations. For both time exact as well as the time discrete problem. Ok? So, we are going to look at at these two model equations, right, single degree of freedom modal equations, okay. So, we have here our single degree of freedom. Model equations. Right, I've used an abbreviation for equations here. All right, we have for the time exact case. We have d dot l plus lambda l d l equals 0. Okay? Now an o d e is not complete without its without what? Initial conditions, right? So the initial condition that we have here is the following. Right? This is after all and o d e for our L coefficient to the model expansion. Right? So the initial condition that we have is simply the following. It is that dL at 0, okay, is obtained as. Psi L dotted with Md at 0. Okay? Which is of course psi L, dotted with Md not, okay? And just for convention let's call this, let's call this quantity, right? Let's call that quantity d0L. Okay? That's the initial condition. Okay? All right, time discrete. And time discrete case is the following. It is d L n plus 1, times 1 plus alpha delta t, lambda L, minus d l n, 1 minus 1 minus alpha delta t lambda L equals 0. Okay? And again, this holds how do we bring in the initial condition in this case? We just say that we're given that that quantity we had identified for the time exact problem. All right? Okay. Now, one thing I want you to note is that the way we write at thi, at this time discreet single degree of freedom model equation is by starting from the, from the right. From the time discrete matrix vector equation, and applying model analys, modal decomposition to it. Right? We could equivalently have gone the other way around, right? So, we could have equivalently taken the, the model single degree of freedom model equation for the time exact case, right, which is this one, and then time discretize this. Okay, if we do this, and it's a very s, very useful exercise, it's actually a very simple exercise, you would end up with this. Okay, so this is your mark, I'm just going to give it to, give you in the form of a, sort of flowchart. Okay? So, what we have done here is the following, right, we have at one end, the time exact. Matrix. Vector equation. Right? Where this is Md dot plus Kd equals zero. Okay and I'm going to put a box on it here. Here that is Md dot. Plus Kd equals 0. Okay? Now one way of looking at what we did was, we went from here down to our time discretize case, right. Right, and this is the following. M d at N plus 1, minus d at N. Okay. Divided by delta t. Alright. Plus K dn plus alpha. Right? Equals 0. Okay? So this is the time discrete. Matrix vector equation. Okay? What we did then, is two things. From Md dot plus Kd equals 0, we went to our single degree of freedom SDOF, for single degree of freedom, model equation. Alright, this is the time exact version. Alright. Alright, which is essentially Dl dot plus lambda L, dl equals 0 plus initial conditions, right? Everywhere we have initial conditions. Okay? And then what we also demonstrated is how we can go from here to our single degree of freedom model equation. Right, but this is the time discrete case. All right? Which takes on the form that we've written just above here d L n plus 1, 1 plus alpha delta d. Lambda L minus dn at L, 1 minus 1 minus alpha delta d. Lambda l equals 0. Okay? That's where we are. The one step we haven't taken and which is very easy to take actually is this one. You could start out with our single degree of freedom model equation, the exact form and essentially do a time time discretization of it. And indeed you do get this. Right? Okay. We've done everything but that last dashed the step cor, corresponding to that l, last dashed arrow. Okay its very obvious why this works do you know why this do you recognize why this works? That's right because the process of time discretization, as well as the process of model decomposition are essentially linear operations, okay. And, and therefore in this sort of setting they commute, right, those two operations commute. Okay. All right. Okay. It, it, it of course not all linear operations commute. But, but because of the nature of the sort of operations we're carrying out here, they do indeed commute. All right.

\section*{ID: 1CMy5-PKQG4}
Okay. So let's get on. So what, what we're aiming to do in this segment is get some sense of the stability of the equations that we need to look at. Now if we want to look at stability we have to first understand we must first understand the stability of the time exact case, all right? Because that is the, the sort of behavior, the sort of response we are aspiring towards for our system, right? For our algorithmic system. Okay? So, in terms of stability, let's first understand the time exact case. Right, now, we've derived the single degree of freedom, modal equations, for a partic, for an arbitrary mode L. Okay? Now, everything we do holds for every mode, right? The, the, the, our, our analysis holds for any mode, because we're really working for an arbitrary mode. With that in mind, I can afford, I believe, to drop the explicit, use of the modal index, L. Okay? All right, so I'm going to drop that, right? So, from now on, the time for, for the time exact case, since we're working a single degree of freedom, for the time exact case and for the time discrete case, when we are working with single degree of freedom modal equations, I'm just going to write them as d dot scalar plus lambda d equals 0. Okay? But one more thing, I've got rid of one index L, but I want to bring another one. The one I want to bring back is the, I'm going to use h, right? And it's not an index, it's a superscript. Why am I bringing h back here? Remember h is our old friend the element, size. Right? It denotes the element size. The fact of spacial discretization. Why am I bringing it back? Right. It's because our n and k matrices depend upon our discretization, our spatial discretization. Right? So the ei, eigenvalues we're working with here are truly the spatially discretized eigen, the, the eigenvalues corresponding to the spatially discretized system. Okay? So lemme, lemme just state that here. Lambda h is the eigenvalue of a mode, or corresponding to a mode, that was, that is obtained after spatial discretization. Okay, so the factor spacial discretization, which is which shows itself up, which, which shows up in the finite element size, is indeed reflected in lambda h. Okay? Alternately, because these are partial differential equations, one could do a fully continuous analysis of them, and then one would have a eigenvalue corresponding to modes, but those would not be discretized modes, okay? Those modes or those eigen those eigenmodes would be actually eigenfunctions, not eigenvectors. Okay? So there would be a, so there's a difference. We're really working with the eigenvalues of the, of the spatially discretized system here, and that will show up, it's, it's for that reason that I'm bringing back our memories of h here. Okay? All right, so the time exact case is this, plus of course boundary condition. Sorry, the initial condition, right? So this in fact, d(0) equals d not, right? On the previous slide we've used the modal index L, but we've just decided to drop it here. Okay? Without, without risk of confusion, because everything we do holds for every mode. All right, so what is the stability of the system? How do we, how do we know what the stability of the system is? This equation is one of the simpler ODEs you're likely to encounter. Okay? We can directly write down the exact solution. All right? So the exact solution is d as a function of t equals d sub 0 exponent of minus lambda h times t. Okay? It's easy enough to check that that is indeed the exact solution. If you plug it into your, equations, you will find, and into our ODE, you will find that it satisfies the ODE, and it also, respects the initial condition. Okay? Just set T equal to 0 on the right hand side, since exponent or minus 0 is 1. You get back d at 0 equals d not. Okay? So this is the exact solution. All right, now, what about lambda h? What do we know about lambda H? It's an eigenvalue of the system, right? What is lambda h? Lambda h turns out to be greater than or equal to 0. Okay? All right? Why is lambda h greater than or equal to 0? We're not going to prove it, but do you know what properties give us this? It is the fact that M is positive definite. Right? And K is usually positive definite, but in the most general case, if we, if we want to also allow for insulation along certain directions, if you're talking of heat conduction or the possibility that there is no transport along certain directions if you're talking of mass diffusion, then k is a positive semi definite Okay, earlier on we'd used the fact that K can pretty much be taken to be positive definite, unless you really want to have insulation. We'd use that fact to make the observation that arriving that, that we are going to get eigenvectors that, that are linearly independent. Okay? All right. So, we have this, we have this sort of situation. All right, if that is the case since lambda h is greater than or equal to 0, what can we say about d? Say tn plus 1, right? And we know what this means. It just means that we're evaluating the solution at time n plus 1. What can we say about d at tn plus 1 relative to d at tn? Right? And, and here I'm using the fact that because of the nature of our time discretization and our choice of the progression of time instance, tn plus 1 is greater than tn. Right? So, let me just recall that also. We are, of course, using here the fact that we are progressing in time, so tn plus 1 is greater than or equal to tn, right? It's usually, it's, it's greater than tn. We nev, we never use it equal to tn, because that would mean we have a 0 times. Okay? All right, so given this, what should I use in this blank here? I've left a big blank spot between d and tn plus one and d and tn. What relational operator do I use? Right. Because the exponent of a negative argument is less than one, right? What we see is that this is a decaying function. Right, it's monotonically decreasing. Okay? Or another way at looking at it is, that d at t n plus one divided by d at tn is lesser than or equal to one, provided of course d at tn is not 0. Okay, but, nevertheless it's monotonically decreasing. All right, so we have monotonically decreasing, sorry, time dependent coefficient for our mode. Okay? All right. And this essential says that the nature of our heat conduction equation or our, or nature of our, mass diffusion equation, the kind we are looking at here, is for the solution to tend to be K. Okay? There is no tendency for the solution to tend to increase, provided we have set the forcing equal to 0. All right? And it is on in order to expose this characteristic of the equations we are working with that we are considering the homogeneous case. Because clearly, you could be supplying heat to increase the to, to, to raise the temperature, or you could be, you could have a local supply of mass, or, or, or, again, influx of heat or mass in order to push up the temperature or the mass concentration at, at any point. And therefore, these modes could be increasing in a problem that is in homogeneous. Okay? But we wanted to get to this fundamental characteristic of the equation, so we just assume the homo, we, we are considering the homogeneous case. All right, so why are we doing this? The reason we are doing this is because we want to understand what is the exact behavior that our algorithmic equations should aim to represent? Okay? All right. So, with this in hand, let's ask, ask ourselves what, what the same sort of analysis tells us for our time discrete equation. And this is really quite easy, because the way we've written out the time discrete equation, it is, it is in algebraic form. It just gives us dn plus 1 multiplied by co, by some factor minus dn multiplied by some other factor, okay? So the time discrete equation, if you go back and look at it in your notes, and I have it right up here in my slides, so let me just flash it up again, it is in the middle of this slide, right? Marked out  as time discrete equation, time discrete case. Okay? So I'm going to rewrite it using the same notation that we are now following, which is to drop the modal index L, and instead for lambda, bring back the superscript h. Okay? So the time discrete case we'll use in this notation is the following. It is d times 1 plus alpha delta t lambda h equals, I'm just moving it to, to the right hand side, equals dn 1 minus 1 minus alpha delta t, lambda h. Okay? Sorry, and here I have dn plus 1. Okay? All right, now, let's try to find that same ratio that we'd found in the case of the time exact problem. All right? So, the equivalent ratio to form here is dn plus 1 and d divided by dn. Okay? And this as we see is equal to 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Now, in the context of our time discrete algorithmic problem, we tend to call this a ratio, we tend to denote it as A. Okay? And A, very obviously is picked because we really want, because this is really an amplification factor. Right? Inasmuch as it is obtained as a ratio of dn plus 1 at, over dn, it essentially tells us how is our time discrete solution getting magnified from one time step to the other. All right? Or getting amplified from one time step to the other. Okay? And you note that A depends upon, if you want to think of it in that way, it depends upon alpha, it depends upon lambda t, and it depends upon lambda h. Right? Okay? So, what that says is that, yes, our integration algorithm matters, right, whether we're choosing one type of the oiler fam, one member of the oiler family or the other. Our time step matters for stability, that's not, that should not be a big surprise. But interestingly as well, the spatial discretization we've used does matter. Okay? And this is why I took pains to point out that the eigenvalue we're working with for any mode, is really a discretized eigenvalue in the sense that it, it reflects the spatial discretization. All right? And that too does affect our amplification factor. All right? When we, well let's just do one more thing. What do we mean now by a stable problem? Right? We wanted to reflect what we saw for the time exact case. Okay? So, for stability the requirement we want to have, right, is the following; we want to say that the magnitude of A should be lesser than or equal to 1. Okay? All right. Now, you may wonder why we are going with the magnitude. All right, what would happen, and why not just say, a has to be less than one? Well, what you're going to see, is that, this is a, after all, we're doing approximations here, right? We're constructing the, the reason we're the getting districtizations is we want to approximate the time dependent behavior. Well, one, result of that, approximation and of that districtization is that it possible for our solution to sometimes go negative here. Okay? So that is something that we will have to deal with. Right? And that's why we recognize that A could be, could, could be negative, dn plus 1 over dn could be a negative ratio. Solution can change signs, we're just recognizing that the algorithm may do that and therefore we are restricting ourselves further by saying that the magnitude of A has to be lesser than or equal to 1. Okay, we are not guaranteed, positive solutions always for our time discrete problem. Okay, so when we come back, we're going to end the segment here, when we return, we are going to, apply this, this condition to our, time discrete, problem and see what it tells us.

\section*{ID: h2S5IFUpVuc}
All right. What we're going to do in this segment is complete our stability analysis of our Time-Discrete, Single Degree of Freedom, ODE. All right? Or a single degree of freedom equation. Okay. So let's recall where, where we got. We got as far as the amplification factor. Okay. And that amplification factor gives us dn plus 1 divided by dn equals A, this is the amplification factor. And it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? All right. Now what we want to consider is the fact that what we're going to use in our analysis is the fact that alpha lies in the, in the closed interval, zero to one, right? Delta t, which is our time step is greater than zero. Okay? And lambda h, we've agreed is greater than or equal to zero. Okay? All right. Now the stability condition. Or the stability criteria is that the magnitude of A is lesser than or equal to 1. Okay? All right. So this is what guarantees decaying response, right? From one time step to another. Okay? By the way, the fact that we arrive at this condition is actually the, is what we call a linear stability condition. Okay? And the linear aspect of it comes from the fact that we have indeed a linear problem here. And for linear problems, one can state the stability requirement by just saying that well, if your solution does not grow from one time step to the other, you have a, you have a you have a stable problem. Okay? When we go to non-linear problems, we can't quite use that. Because the physics of non-linear problems can actually lead to solutions growing, right? Over certain regimes, right? And that, that is the right physics for those sorts of problems. So let's get back to this. So, if this is what we have let's see how we can, we can extend this. What this implies, of course, for us directly is that minus 1 should be lesser than or equal to A should be lesser than or equal to 1. Okay? So, A has to lie in that interval. All right? Okay. So let's look at this. So what that means then is that minus 1 is lesser than or equal to 1 minus 1 minus alpha delta t, lambda h divided by 1 plus alpha delta t lambda h is lesser than or equal to 1. Okay? Now because of the conditions I put up here, right? We are guaranteed that our denominator is positive, right? So that says, if we are now what, what this lets us do is safely multiply through by that by the denominator and we get here. Minus 1 plus alpha delta t lambda h is lesser than or equal to 1 minus 1 minus alpha delta t lambda h is lesser than or equal to 1 plus alpha delta t lambda h. Okay? All right. Okay. So to move on, let's just go to the next slide and let's look at the right-hand side of that inequality. Okay? So consider in form, right? 1 minus 1 minus alpha delta t lambda h is lesser than equal to 1 plus alpha delta t lambda h. Okay? We can consider this, we see that these two cancel out. Okay? And we manipulate things and what we see here is that we get alpha plus 1 minus alpha delta t lambda h has to be greater than or equal to 0. Okay? And we do this by simply moving what remains on the left-hand side of the inequality on the first line over to the right-hand side. Okay? Now those cancel out, all right? And we are left with sorry. There's a delta t here. What we are left with here is the delta t lambda h is greater than or equal to zero. Okay? That's what the right-hand equalities says. All right. But this is always satisfied. Okay? All right? Let's look now at what happens. And, and in fact, this is always satisfied regardless of alpha. We satisfied for, for all alpha, all right? Because alpha doesn't even show up in this final form. All right. Now let's consider the left-hand side inequality. Okay? And that left-hand side inequality is that minus 1, sorry, minus 1 plus alpha delta t lambda h Is lesser than or equal to 1 minus 1 minus alpha delta t lambda h. Okay? All right. So now let's work with this a little. When we work with this a little, let's, let's, let me rewrite this by moving This term over to the right-hand side and, and, and rewrite them. So when we do that, we get 1 minus 1 minus alpha delta t lambda h plus 1 minus alpha delta t lambda h is greater than or equal to 0. Okay. That is the condition which must hold and this condition you note involves alpha, delta t and lambda h. Okay? Continuing to work with it, this implies that two plus 2 minus 1 all right. Minus two alpha, hm. What's going wrong here? Alex, you may need to just give me a pause here. I've got a two here. That over back, okay. Alex, I'm going to backup, I made a mistake here. Go back to the beginning of this last equation. Okay. I note that I need a plus sign there. Okay. Put that plus sign and go ahead and now, I get 2 plus 2 alpha, right? One alpha coming from here and another alpha coming from here. 2 alpha minus 1 times delta t lambda h is greater than or equal to 0. Okay? That is the condition we need. Let's see what else we can do with this now. I could write this out as 2 has to be greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? All right. All right. Now let us look at whether this can, under what conditions this may be satisfied. Okay?. Now let's save this and move on to the next slide, okay? Let's consider the following cases, okay? So case one. All right? And for case one, let us suppose that alpha is greater than or equal to one-half. Okay? What, when, when that happens, what that says, what that means is that 1 minus 2 alpha, delta t lambda h is always what? Right? It's always lesser than or equal to zero, okay? Which implies that yes, if alpha is greater than or equal to half, 2 is always greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? Right? This thing always holds, holds unconditionally. Right? And by unconditionally, what we mean is that if alpha is greater than or equal to half, our stability condition holds for all delta t, right? For all delta t greater than zero, right. Okay. So we have a condition, we have a, a situation of unconditional stability of our method. Okay? So this is unconditional stability if alpha is greater than or equal to half. Okay. So, in particular, observe that alpha equals half, which is the Crank Nicholson method or the midpoint jewel as I sometimes call it, as a lot of other people call it. Right? Or even alpha equals 1, which is backwards Euler. Right? These are both unconditionally stable. You can take a time step as big as you like and you're okay. Okay? A solution may be terrible, it may be very inaccurate, but you're still stable. The solution will not blow up, right? The amplification factor will remain, the magnitude of the amplification factor will remain bound by, by one. Okay? Case two is the interesting one. Okay? Case two is that alpha belongs to zero comma half not including the half, right? Okay. All right. Which is basically zero is lesser than or equal to alpha is less than a half, okay? This is the other case. All right? So now what happens is that we have a situation of that's okay. So, so in this case, we have a, we must still have all right. So we, we are trying to see whether 2 is greater than or equal to 1 minus 2 alpha delta t lambda h. Right. This condition needs to be satisfied for alpha, such that zero is less than or equal to alpha is less than half. Okay? All right. This is what we're trying to look at, okay? Now observe that since alpha lies in this range that I've written on the right this parenthesis is always what? Is it all, what can you say about the quantity in that grid emphasis, in those parenthesis? Right? It's always positive. Okay? So now we get a condition, where we see that on, on, as far as delta t is concerned, delta t must be lesser or equal to 2 divided by 1 minus 2 alpha. We're able to divide through by 1 minus 2 alpha, because it is indeed greater than 0. Okay, so it is a positive quantity. 1 minus 2 alpha is positive. But then we see the delta t has to be lesser than or equal to 2 over 2, 1 minus 2 alpha, times lambda h. Okay? So this is a situation of conditional stability. Okay? So the algorithm is stable, right? It can be stable. But it depends upon the size of delta t, okay, conditional stability, which means conditional upon delta T. Okay? So delta T has to be bounded by something. Delta T cannot be too big. Sort of makes sense, right? You have an algorithm that is not entirely stable, that is not always stable. In order for it to stable well, take very small delta t's, right, take very small times 6. Ok, so, an example of this is of course alpha equals 0. Right, which is forward Euler. Okay. All right, now, to end this segment let us consider what else can happen here with delta t. Remember that lambda h, because of the nature of our Eigenvalue problem, lambda H if you ask yourself, what is the order of lambda h? Okay? And where does lambda h appear? Order of lambda h depends upon the Eigenvalue problem, on the Eigenvalue problem. K psi, where psi is the mode shape corresponding to this particular mode. K psi equals lambda h, n psi. Okay? Now, what this suggests, is that lambda h is of the order of, the order of lambda h is of the sort of terms that are like M inverse K. All right? Right. So it's with the order of the norm of terms in M, inverse K. All right. Now, I want us to think about how do M and K depend on the element size? Right, because these after all are obtained by the spatial discretization. Okay? So, recall M is of the order integral over omega e, and then we have the sum over e, sorry, we have the assembly over e and all that. We have the assembly over e, integral over omega e of terms of this type, right? We have, we have shape functions, right? So this is really of this is really like M A B in a particular element. Okay, so I can actually get rid of this, right. I'm just doing this element twice, sorry, I'm changing things around a lot. Okay, element twice, this is what we have, right, integral over omega E, N, A, N, B, D V. Okay, K AB from an element is of order, integral over omega e NA,X NB,X right? These are derivatives. Right? So there are derivatives involved in KAB and yes. KAB may also have a so this is xi, xj, K, kappa here may have a kappa ij. Okay? Dv, all right, and yes the mass matrix may also have a specific heat there. Right, or row could be one if you are doing diffusion. Okay? What is, what is the order of these sorts of terms? These sorts of terms, because their shape function derivatives, are of order one over h. Okay? As a result, what we're seeing is that MAB is of order may be 1, right. But KAB is of order h to the minus 2, okay. As a result, the order of lambda h, okay, is h to the minus 2, okay? In terms of the element size. Now what does that say for delta t? Delta t has to be lesser than or equal to 2 over 1 minus 2 alpha lambda h. Well, this lambda h is like h to the minus 2. Okay? As h gets smaller, our restriction on delta t gets more and more stringent. Okay? So the remark that I will make here is that restriction On delta t, gets more stringent. As h tends to 0. Essentially as we refine our finite element mesh, our spatial discretization, we see that it has an effect even on our temporal discretization if we're working with a conditionally stable method. Right? For the time integration, okay. So, this is how the finite element discretization also makes its presence felt in the time discretization when one is working with a semi discrete method and happens upon a conditionally stable problem, conditionally stable method. All right. We'll end the segment here.

\section*{ID: 12IQlVHX17o}
Welcome back. We are moving steadily ahead with our analysis of our Euler family of algorithms, or time integration algorithms for our parabolic PDE. What we've managed to accomplish in the last segment was an understanding of stability properties. And I'm going to start this next segment with very quickly summarizing that those results and moving on from there. Okay. So, the result that we had was for stability.  Okay? What we found was that for the Euler family, right? If I look at it, look at the stability results in terms of this parameter alpha. Right? What we find is that for alpha greater than or equal to half, we have algorithms that are unconditionally stable And what this means, is that it does not matter how big our time step is, okay? So unconditionally stable, also meaning any delta t greater than 0. Okay? Any of them will give us stability. Right? For the other cases when we have, alpha lying between, alpha less than one half, and remember alpha has to lie between 0 and 1, right? So, it can't get smaller than 0. So, if alpha is less than a half, what we have are methods that are conditionally stable. All right, and the condition that we obtain is one on delta t. We find in particular that delta t must be a lesser than or equal to 2 over 1 minus 2 alpha, times lambda h. All right? Now, I want to say something more about this result. Observe that for lambda h, which is the the discrete eigenvalue corresponding to the particular mode that we are looking at, right? Lambda h can take on different values for different modes. So when we are looking at the full matrix vector problem, what we need to ensure is that this conditional stability is satisfied for all modes. Okay? So this result must hold for all modes. Right? Consequently, what we want to look at, what we want to impose is this condition for the maximum eigenvalue. All right? This implies that really the kind of condition we must work with for our matrix vector problem, is that delta t is lesser than or equal to 2 over 1 minus 2 alpha lambda h max. Okay? The maximum I can value over all modes. Right? And that is the, something we obtain from an eigenvalue decomposition of the problem, which also we have looked at. The final result we noted, is that lambda h and therefore lambda h max also, varies as the elements size to the minus 2. Okay? All right? And what this implies then, what this means for our methods, is that the spatial discretization does in deed fact affect our, time integration, right? In particular, it, it affects our choice of time stamp. Okay? What this translates to then is that delta t is or, or delta d max really, if one is following this conditional branch of the algorithms, of, of the stability of the algorithms. It means the delta D max,uh, goes as, h square. Okay? Consequently as we refine the mesh, the spatial mesh, we are constrained to using smaller and smaller maximum time steps. All right? Okay, so this really a summary of the type of our stability results. What I would like to do to move on is revisit our amplification factor, okay? So let us recall, let's recall the amplification factor. All right. And that is something we denoted as A. And if I recall correctly, A is equal to 1 minus 1 minus alpha delta t lambda h divided by one plus alpha delta t lambda h. All right? This is the amplification factor. And we recall also how it applies to our algorithmic problem. We see what we have is that dn plus 1 is equal to A dn, okay? Now, what I want to do is look at the effect that this amplification factor has on the behavior of high order modes. Okay? So, we are going to use this fact to look at the behavior of high order modes. Okay? And for our implementation, what this means is, first of all what do high order modes mean? What is a mode of a high order? What characterizes the order of mode? It's not simply the number, right? It's not simply what number we ascribed to it, the m or the l that we've been using, right? We know that m equals 1 to ndf and the, where mappears is in labeling the mode, right? Psi m. Right? Just m having a large number does not make it a high order mode, that is simply an arbitrary choice we have made of labeling modes. Right? So what does make a mode high order? It is the value of lambda h. Okay? Essentially large values of lambda h are the higher eigenvalues, and in any linear system those are the higher order modes, right? For the system, it turns out that those are the modes that have higher order spatial frequencies. Okay? So so it's lambda h, but really we can we, once we have lambda h and we have a what we know about lambda h, a once we know we have a time step, this really implies lambda h delta t. Right? And it's doesn't need convenient to work in terms of lambda h delta t, because this is how it shows up in our problems, right? Okay? All right, all right, so let's look at what effect we had on our amplification factor. And to do that, I am going to plot up on the vertical axis, the amplification factor. And on the horizontal axis, I'm going to write, I'm going to plot this quantity, m, lambda h delta t. Okay? So the idea is that we are looking at the effect of higher order modes and, and, but, but the way we are plotting it up and the way we're, we're studying it we could get to the same behavior either by looking at lambda h large or just the effect of having a very large delta t. Okay? All right, so here I'm going to plot like I said I'm going to plot the amplification factor. Right? The formula for which we have on the previous slide. All right, it's, it's useful by the way, also to look at what the amplification factor is for our exact equation. Okay? And I'm going to write that up here for the exact equation, for the, for the exacting, the time exact equation. All right, the time exact single degree of freedom model equation we know that d of t equals d at 0 exponent of minus lambda ht. Okay? So effectively we have here a sort of time-continuous amplification factor. All right, we can just look at this quantity as being the amplification factor, right? And then if we apply again to the idea that we want to look at how it varies between one times step in the other, all right, from tn to tn plus 1, what you will see is that the amplification factor for the exact problem is essentially exponent of minus lambda h delta t. All right, the idea is that you could take this exact equation, simply write it out as a mapping from dn to dn plus 1, okay? And then you will see that the amplification factor you get is exactly the exponent of minus lambda h delta t, right? So, and, and, so that's just an exponentially decaying function, right? So let's write that one out first. We see that the amplification factor can have a maximum value of 1, all right as written here, right? The exact amplification factor. And I'm going to try to draw a an exponentially decaying function, and hopefully I can draw it reasonably smooth. Okay? So, this is what I'm going to call A exact. Okay? And in parentheses here I'm going write out the limit that A exact tends to as lambda h delta t tends to infinity. Okay? Clearly, as lambda h delta t tends to infinity, exact goes to 0. Right? Okay, now we're going to return to our actual amplification factor, right, our algorithmic amplification factor. And look at what value it takes in the limit for the different members of our, family, okay? So, since I have all this room here, I'm going to make use of it and I'm going to write the actual amplification factor here. And recall that it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Because we want to look at it in the limit as lambda h delta t tends to infinity, I'm going to simply divide through by that quantity. Okay? So I get 1 over lambda h delta t minus 1 minus alpha divided by 1 over lambda h delta t plus alpha. All right? Okay. Now, so I'm going to look then at for various values of alpha, I'm going to look at what happens as limit lambda h delta t tends to infinity. Okay? Okay? Let's start at the top of the range of alpha. Say alpha equals 1. All right? For alpha equals 1, when lambda h delta t tends to infinity, if you look at that limit, you basically get 0. Okay, all right? So this function right so let me plot this out now. Let me use here's black. All right, so if it is as something like this. I should admit here that I'm not paying very careful attention to how it behaves relative to the, relative to the exact exponential at the left end of the limit. Right, I'm, I'm not paying much attention to that, so, but you can check that, okay? So, don't worry about the behavior on the left. What I'm interested in is the behavior for high order modes, right, which is 4 lambda h delta t tending to infinity, right? So what we have here, is that A, and instead of saying exact, I'm going to say, I'm going to write as a subscript here the, the value of alpha. Okay? So a1, and of course I'm using the colors, so a1 as lambda h delta t tends to infinity, which is the, the amplification factor now for the backward Euler algorithm, right, was alpha equals 1. This thing also goes to 0. All right? Let's change colors here again, go back to red. What I'm going to do is write them for all the others, okay? Let's write, and I'm going to write them only for, for the ones that we really care about here. Let me write alpha equals half, right? Which is the Crank Nicolson method. What we see is that for alpha equals half, substituting that value of half in there, we see that the amplification factor goes to minus 1. Okay? And for alpha equals 0, which is the fold Euler method, we have what do we have? We see that that limit is actually, the limit doesn't exist. Right? Because as we we simply set alpha equal to 0 that traction becomes unbounded, right? But let me allow me to write a unreal number, an unreal number, basically it's unbound, so it goes to infinity. Okay, that's not strictly to say that infinity's not a limit, right? But anyhow, you know what I mean. All right, so this is the situation we have. Let's let's look at what these things appeared like. So if that is 1, then this is minus 1. Okay? And okay, I'm going to show you what Crank Nicolson looks like on this, in green. Crank Nicolson essentially tends in the limit to minus 1. Okay? So a half, or, or the midpoint rule, as I used to call it, a half tends to minus 1. Okay? And finally for our backward Euler algorithm, what we see is that we actually get sorry for our forward Euler, we've already plotted our backward Euler. For forward Euler, what we see is that it is a it's a straight line. Okay, it's a straight line, and with, with a negative slope it falls to right, it leaves the plot. Okay, so this is a for 0, the forward euler algorithm, which is infinity. Right? It's actually it goes to actually minus infinity. Right? Okay? So that's how they behave, and you can plot up some of the others also in between here. So, what does this mean? If we stare at this plot, what you see is that when it comes to the high order modes, it is backward Euler, which has the behavior of the exact equation. All right? Alternately, we may state this as saying that backward Euler tends to dissipate with high order modes. This is numerical dissipation, right, in addition to the, to the physical dissipation that exists with the heat conduction or the master fusion.

\section*{ID: dm-A1LpzZPA}
Right, so, remarks. Backward Euler. Right, which is alpha equals 1 damps out, well, I don't like the term damps, I'm going to say dissipates. Dissipates high order modes. Okay, and this is what we call a numerical dissipation. Okay, and because as I observed this has the same high-order behavior as the exact equation, it is often preferred. Okay? Numerical dissipation, right? And, let me also say here, it is similar to time exact equation. Right. The second note here is that there really isn't much to say about forward Euler by the way. It should come as no surprise to us as th, that that the amplification factor for forward Euler tends to minus infinity as lambda h delta t gets large. We've already seen some evidence of it. And in what way have we seen evidence of it? We've seen it in the fact that if delta t gets too large we know that the, that the forward Euler method is no longer stable, right? And that is also reflected in, in the, in the amplification factor going to minus, going to, getting unbounded in this case, minus infinity, as delta t gets too large, right?. Remember lambda h delta t can be can get large if either lambda h or delta t get large. If you have large times that, we know that there is a tendency for forward Euler to blow up, so to speak. Right. And we see that in the amplification factor, tending to minus infinity. Okay, so let me state that though. Forward Euler. Forward Euler, which is alpha equals zero. Has unbounded. Lambda has unbounded A. Okay, all of this, by the way, is in the context of high order modes, right. So I should say up here, this is all for high order modes. Okay, so everything I'm writing here applies to high order modes, okay? All right, so, this should come as no surprise to us, because we know that forward Euler has this tendency to, to sort of lose it, okay? Remark three, is that the surprising thing is that the midpoint rule. Or it may be a surprise. That the midpoint rule, which is alpha equals one half. Right has, has as we've seen A tending to mi, to -1, okay. What does this mean? What this means is that dn plus 1 for the high order modes, right if this is a high order mode I'm writing out, right? Kay, if dn plus 1 is the model coefficient of of a high order mode. What we're seeing is that dn plus 1 tends, is equal to minus dn. This leads to oscillatory behavior in the high order modes. And it's not uncommon to see solutions for from, from the mid-point rule which with respect to time, all right, if we're plotting certain modes with respect to time, what we will see is that if this is t1, t2, t3 and so on. Right? What we may very well see is that you get you get essentially oscillatory behavior. Okay? Something that's done is to simply form a time average, okay. And to take a time average you basically go damp out, well you don't truly damp out, but you, eliminate the effect of these oscillations in your post processed solution. Right, time average the, time average the solution, okay, to get around this. All right, this is important to know about the behavior of these methods in the high frequency limit, right, for high order modes. Okay. So we've looked at stability, we've understood high order, and the behavior of high order modes. What we are going to do next is essentially prepare ourselves for for talking about convergence. Alright? And the way we do that is by first looking at the notion of consistency. Okay? So, we look now at the idea of consistency. Alright. I order to get to consistency, let's go back to our discretized equation. Okay? And that equation, as you may recall, is the following. It is dn plus 1 times 1 plus alpha, well, no, let me back up a little here. Let me write it in fact in the following manner. Let me write it all the way back as dn plus 1 minus dn over delta t plus lambda h d plus alpha equals. Now, up to recently, up to a few seconds ago we were looking at the homogeneous problem, right? Which we got to by turning off the high order modes, sorry by turning off the forcing. We're now bringing back the forcing. Okay? And I'm going to write this as F at N plus alpha, on the right hand side. Okay? If you wonder what FN plus alpha is, FN plus alpha, right, for this particular mode, is got by looking at F sub n plus alpha, the whole vector, right? And essentially, right, dotting it with whichever mode we are looking at. Okay? If we we're looking at mode L we would dot it with the Lth mode, okay. But since we've agreed that we're going to drop the explicit mention of the model number, right, we will just write it as FN plus alpha here. Okay, alright, now, let's multiply through by delta T and see what we get. We get dn plus 1 minus dn plus lambda h Delta t. I'm going to expand out dn plus alpha, as alpha dn plus 1, plus 1 minus alpha dn equals delta tFn plus alpha, alright. Working with this a little more, what we get is dn plus 1 times 1 plus alpha delta t lambda H minus one minus, one minus alpha delta t lambda h, dn minus delta tFn plus alpha, equals zero. As a final step I'm going to divide through by one plus alpha delta t lambda h to give me dn plus 1 minus. Now when I divide this quantity by this I get back my amplification factor. So I'm going to jump a step and simply write that as A dn minus delta t divided by 1 plus alpha delta t lambda h, Fn plus alpha equals 0. Okay? Right, and I'm going to take the final step of calling this quantity here l at n plus one. Okay? Or I think I'll call it m, doesn't really matter, let me just call it l at n. Okay. All right? So this is how I want to write out my time discrete model equation, right, including the forcing. Now, the notion of consistency is the following. What would happen if you were to take the time exact modal equation, and plug it in here. Okay, so now let me now write that as d at tn plus 1 is the time exact mode corresponding. To lambda h. Okay? The particular value of lambda h gives us a certain mode, and that's it. Okay. Now the question we ask is what would happen if we were to take our time exact model solution and plug it back in here. All right? So what we are saying is that if we do that we get d at t and plus 1 minus A d at tn, right? This is also th, the time exact model value at time tn minus minus Ln. Okay, because the last term Ln does not depend upon on the, upon the modal values. Okay? Now, this equation is equal to zero. Correct, right? The left hand side on this equation is equal to zero. My question is, if we were to not plug in our exact solution here, right, as we, as I've done. Will the right hand, will what I've written here on the left hand side be equal to zero or not? In general, it's not equal to zero, okay? In general, however, one can write it out as delta t times some quantity tau. Okay? Tau which in general depends upon the time. Okay, right, the reason we are writing it out as delta T times tau is that, because we recognize that having started from this form of the equation, we've actually multiplied it, multiplied the ODE through by delta T. So we expect that whatever the right hand side is, it already has factor of delta T sitting in there. Right, so we choose to write it in this fashion. Okay, now. This is the general form that we would get for the exact equation. All right. It turns out that we can now make the following identification. Okay. What we say is that. Sorry. See, let me back up to the previous slide. Let me just say one more thing here. If we can show that our exact solution satisfies an equation of this type, okay, we had what we call a consistent method. Hour is no what no let me back up a little more its not quite a consistent method, this is a consistency condition. Okay? This is a consistency condition where we have a consistent method if we can show further. That tau, which I've written as depending upon the, the actual time. If you can show that tau is lesser than or equal to sum constant c times delta t, to some power K, okay? Where K is greater than zero. Okay? If we can show this, then we have a consistent method. Here's why. If we can show that this holds, one can say that as in, in the limit, as delta T tends to zero. Right? What we will find is that d at tn plus 1, minus Ad at tn, minus Ln, which is equal to delta t c delta t to the power of k, right? This thing also tends to zero. Right? This entire thing also tends to zero. Okay? All right, if k is greater than zero. Alright, and therefore what this suggests is that, yes the equation we're working with, the finite difference equation we're working with in time, is one that when we plug in the exact solution, though it does not immediately satisfy the exact solution, at least it does satisfy the limit of, of, vanishing time scales. Okay, so in the limit delta t tends to 0, the time discrete equation. Admits. The exact solution. Right, and which I'm sure you will agree is a useful property for a method. Right, for a numerical method. Okay? I should mention, of course, that in that, in that result t of tao N eq, is lesser than or equal to c delta t to the power k. C is a constant, and K is what we call, k is the order of accuracy. Okay? And it turns out that k equals 2 if alpha equals half, right. The midpoint rule. And it's equal to 1 otherwise. Okay. All right? So, the midpoint rule, as you may expect, because it does look at the, it does apply the algorithm between n and n plus 1. Gives you higher accuracy then any of the other methods. Right, so backward Euler and forward Euler are, are, are down here. And any other method also has Euler factor of c 1. Okay? Only the midpoint rule is second order accurate. Okay, this would be a great place to end this segment.

\section*{ID: 4Dj3U2ZSj0I}
So, we'll proceed. What did in the last segment was look at high, the behavior of high order modes and also introduce this notion of consistency of the finite difference approach to time discretization. I ought to mention that proof of that, those results for consistency, and order of accuracy are obtained by essentially looking at the time exact solution, carrying out Taylor series expansions, and finite Taylor series expansions, and then, and then manipulating them. It's a, it's a fairly straightforward exercise but somewhat tedious, okay? So, in this segment we are going to essentially move to the end, end game for this particular topic. We are going to look at convergence. Okay, so we are going to look at convergence. Of. Time discrete. Solution. All right. In order to talk of convergence, as you may recall from our early treatment of error analysis for the finite element method itself, we need first to identify what our error is? Okay. So we define the error, and as we've done before in the context of the finite element solution itself we will define the error as the time, as the discrete solution, right? But in this case, we look at the discrete solution at n plus 1, minus the time exact solution at n plus 1. Okay? All right. Now this is what we define as e at n plus 1. Okay? Now, e n plus 1 is of course, is just another vector in the same space. Right? Right, just as d and t are sorry, your d n plus 1 n d at tn plus 1 are vectors and this rndf space. But what that means is that because we have a basis our orthonormal basis, we can expand e as well in that basis, right? So we can essentially carry out the modal decomposition. Of e at n plus 1. Right. And that model of decomposition essentially is that e at n plus 1 equals sum over m, e m for the mode at n plus 1, right, those are our scalar modal coefficients times psi m. All right, just as for any other vector. Okay, now how do we pose the question of convergence, right? We say that something has converged, right? Or we say our solution has converged, have converged. Convergence Right, is the requirement that as limit as the limit of n plus 1, tending to infinity, right, as we take more and more steps, right. What we expect to see is that e n plus 1, dotted with M e n plus 1 equals 0. Right, that limit is 0. Okay. Can you tell me why it is okay to, or can you think, why it is okay to put M inside there? Normally one would say that well, the error has to vanish, right? But why is it okay to put M in there? It's because M is positive definite. Right? All right, so let me just state this here. Since. M is positive definite, all right. What we see is that e n plus 1 dotted with M e n plus 1, right, equals 0, if and only if e n plus 1 itself equals 0. Okay? Only if, it's only when that vector itself is equal to 0 that, that quadratic product is equal to 0, since M is positive definite. And so, and this is why it means that well, yes, if, if this particular limit is tending to 0, it has to be that e itself, e n plus 1 itself is tending to 0. Okay, so this is why it is okay to consider this, as our convergence criterion. All right. But a more importantly, we are not going to work in this form. Right, we are not going to work, with e dot with M e. We're going to work in terms of modal coefficients, because our entire analysis has been in terms of modal coefficients, right. But it's okay, because I'm going show that for this quantity tending to 0. Right. This quantity that I put a brace on, okay, that quantity tends to zero, only modal coefficients themselves go to zero. Okay. So let me, I've told you what I'm going to show you. But in order to write it out, let me say the following, but not the following. Okay, e n plus 1, dotted with M, e n plus 1 equals sum over m, e m, n plus 1 psi m, dotted with M sum over l, e l n plus 1. Psi. Right, and we're going to put parentheses here. Okay. Each of those sums is for one of those e's right, in the quadratic product. All right? Okay. Well, we know how this works now. This is equal to, let me see how I want to write it, right. It is the sum over m and l, right, of, e n, n plus one, psi m dot M psi l. All right, and for no particular reason I'm going to put parentheses around that, and here I have e l n plus 1. All right? However, we know that this is delta m l. Because of the order normality of the psis with respect to m. Right? Well, if that is the case, we know that, this product that we started out with, e, n plus one, dotted with m e, n plus one, equals sum over m, e m n plus one. E m n plus one. All right, essentially what has happened there is the chronicle delta, has been used to, to turn that l index into an m index, right? So this is basically just the Euclidean norm if, with respect to the psi basis, right, so this is sum over m, e m n plus one, the whole square. Okay, so now it's clear that if this quantity is standing to zero it means that the modal coefficients, that sum itself has to tend to zero. Right, which means the modal coefficients themselves have to go to zero. Okay? So what this implies, finally, is that, limit, n plus one, tends to infinity. E, n plus one, dotted with M e, n plus one, equals, essentially limit as n plus one tends to zero, sorry, tends to infinity. Of the sum. Okay? All right? So clearly, if this, has to be equal to zero, if this limit is equal to zero, it just means that, each, every single modal coefficient, tends to zero. Okay? All right, so limit n plus 1 tends to infinity. E n plus 1. Dotted with M e n plus 1, equals zero. If and only if, each E m n plus 1, equals zero in the limit. All right? So, it means that it's okay to just look at what happens with the modal coefficients. Okay? All right, so. Study convergence. Of modal coefficients, right, E n plus one. Right and now dropping the explicit, mention of the modal, index. Right I've just dropped n. Okay? All right, well that's what we're going to do. Okay, so, how do we set it up? We set it up by going back and writing out our time-discrete, discrete equation, our modal time-discrete equation in the form that we set up in order to pose the question of consistency. All right, so here is what I say. Consider. The time discrete equation in the following form. D n plus 1 minus A d n minus L n equals zero, okay? And consider the, the exact equation also forced into this form. We know, however, that the exact equation does not cooperate, and. Give us a zero right hand side, instead we get delta t, tau at t n. Okay, right, what I'm going to do now is subtract the second equation from the first, okay, so I'm going to change signs. Okay, and now when I add those two together, I get on the left hand side, right, d n plus one, minus d t n plus one, is essentially our e, at n plus one. Right? Okay? Right? It is the, error in the corresponding modal coefficient, right? And here I get minus A e at n. All right, the Lns cancel out, and I'm left with minus delta t tau at t n. I'm going to rewrite this as an expression that will allow me to write out a recursive formula for the error. Error at n plus 1 equals A times error at e minus delta t. Tao at tn, okay? I'm going to use recursion now, and write from here e at n equals A e at n minus 1 minus delta t tau t n minus 1, okay. What that implies is that on substituting e as written here in that expression, right? I get e at n plus 1 equals A square e at n minus 1, minus A to the power 0 delta t, tau at t n minus 1, okay. Right. Minus A to the power 1 delta t tau tn minus 1, and I just realized that here it is just tn. All right, I can take yet another step, okay. What, if I were to now rewrite an equation for e n minus 1. All right, I would get A e n minus 2, minus delta t tau at t n minus 2, okay? And making the substitution, right? You see how this is going, right? What I would get is that e, sorry. E at n plus 1 equals A cube e n minus 2, minus A to the 0 delta t tau tn, minus A to the 1 delta t tau tn minus 1, minus A squared, right. Minus A squared delta t tau at tn minus 2, okay. Now one can go on with this, and I'm sure you've all done it at one stage or the other with similar expressions. When you go all the way back to, the zeroth step, right, so the initial time, we get e at n plus 1 equals A, to the power n plus 1. Okay, right. A to the power n plus 1, e at 0, okay. Minus, sum i going from 0 to n, A to the power i, delta t, tau at tn minus i, right? You can check that this is what the recursive formula reduces to, okay? All right, let's stare at this. What is e at 0? What is the error at time t equal to 0? It's equal to 0, right? Because we've made sure that our initial condition is actually obtained from the time exact solution, right? I mean, at the initial time there is no error, right, because we fixed the initial condition.

\section*{ID: HFNrmuZVVNo}
Okay, so what this gives us then is that. This tells us then that e at n plus 1 equals minus sum i going from 0 to n A to the power delta t tao t at n minus i. Okay. All right. Let's work with this. And we're going to work now, by invoking some inequality. Okay. In particular what we can say, first of all, is that the first con, the first step that we will take is not an inequality. It is to say that the magnitude of e n plus 1, right, is equal to essentially magnitude of sum i going from 0 to n, A to the power i delta t tao t n minus i, okay? Right, we're taking the magnitude of that sum, the absolute value of that sum. Now, is where things get really interesting. We, now's where the inequalities come up, okay? And, and remember when we write the inequalities, I'm going to start out by writing this. And you recall when we looked at the, our error analysis for the finite element method I made the point that when I write such an inequality, what I mean is that the previous right-hand side, this one is bounded from above by what I'm about to write now as the new right-hand side, okay? So, what I get is that, that is lesser than or equal to i going from 0 to n the magnitude of the, the absolute value of each one of those terms, and that's A to the power i, A to the power of i delta t tau add t n minus i. Okay? All right the absolute value of a sum is bounded from above by the sum of the absolute values. And this result is a very standard step in analysis. It is called the triangle inequality. Okay? But there is more. We can say further that that, that our most recent right-hand side itself is bounded from above by this expression. Okay? Essentially what we're seeing here that is that the that any absolute value, which is a product, right? So the product, the absolute value of a product is boundary from above by the product of the absolute values. Right, and this result is called the CauchySchwarz inequality. Okay? But now we have more. We have stability, right? If our method is stable, what we are able to say is that sum i going from 0, to n of 1 times magnitude of delta t times tau tn minus i, bounds from above our previous right-hand side. Okay? Essentially, what I've done is replace, A i with 1, right. And why, why am I able to do that? It's a property of the methods we are looking at. What property are we applying here? It's stability, right? Because we know that the magnitude of A has to be lesser than or equal to 1. The absolute value of A has to be less than or equal to one for stability, therefore, A to the power i has an absolute value also lesser than or equal to 1. Okay, so if you re-substitute that with one, we get a bounding from above of what we had as our previous right-hand side, okay. But now the story goes on, now we know that delta t is a positive quantity, right. So what we can do here is, we know that delta t's a positive quantity and, furthermore, we know that for tau, the condition of consistency allowed us to say something about it. Okay, so in one step I'm going to do this. I'm going to pull the delta t out. All right. And I have inside my sum. C delta t to the power of k. Okay. Why am I able to say this? It's because we said that we have consistency, okay? All right, we have consistency. Now one could introduce another step inside here, which would be to first say well, if all the tau's over all the n minus i's are take maximum of them. Okay and then I get to this point, right. So let me say that one can see inside of here, when they first, if one wanted to be really careful about this then I guess one should be really careful. So one would say, that first of all you would have a previous step inside here, which is to say that the, that this right-hand side, right? Using that, one could say that sorry, the entire right-hand side, not just that. One could say that this entire right-hand side, first of all would be bounded from above by sum i equals 0 to n delta t times the maximum over all i, t n minus i. Right. One could take that step, right? Which is to say, that well since each of the tau, you know you have a different value of tau for each time step. Let's consider the maximum over all i, okay. But then we know that that maximum has to be bounded from above by C delta t to the power of k, because we have consistency. Right, so then we get to this step that I have here. Right. Okay, all right. Okay now well, what do we get here? We have delta t times sorry, the sum here goes from 0 to n. Sorry, right. Okay, we have delta t times a sum of n plus 1 steps, because i is going from 0 to n, of that quantity c delta t times c delta t to the power of k. 'Kay, so we can very well say that it is just like multiplying delta t essentially n plus 1 times, right. So this thing is lesser than or equal to t to the power n plus 1. Sorry, t at n plus 1, not t to the power n plus 1, but t at n plus 1 times c plus delta t, to the power of k. Okay. All right, just summing over those n plus 1 steps. Okay, right right. Now and now from here, we see however, we see that as delta t tends to 0, right, because k is greater than 0, right? Then as delta d turns to 0, we see that the righ- hand side also turns to zero, okay? All right, but. Limit delta t tends to 0 of c delta t to the power k is equal to 0 for k, greater than 0. Right, and where do we get this? Why are we allowed to see a case greater than zero? Once again it is the result of consistency. Okay, so what we see finally, is that the absolute value of our modal coefficient of the error is bounded from about by, essentially by 0. Right. Which means it, it, it still tends at 0, right, and the limit as delta t tends to 0, right. What we are seeing is that, that quantity is lesser than or equal to 0. Okay, so indeed we have conversions. Right, and that's the end of our proof. Okay? Make a quick remark here, which is that what we've seen here is a demonstration of the use of consistency. And stability. Implies convergence. Okay, which is a very standard it's actually a tier in numerical analysis. Okay it's called the Lax theorem. Okay? All right we're actually done with this entire topic of methods for parabolic problems. And what and, and the approach we've taken if you just to summarize is to carry out a standard spacial discretization using the finite element method. But to the time discretization using finite difference methods, and since here we're looking at first order of problems. We looked at the Euler family. We analyzed its stability understood the behavior of its higher order modes looked at consistency, and looked at how stability and consistency give us convergence. Let me make one more remark now about about the be, about the use of the different algorithms, okay. In particular, let me do this, okay. Let me say, let me say that here I have alpha, okay, and let's look at alpha equals 0, alpha equals one-half, and alpha equals 1. Okay, and just for connection to other things. Let's give this, this algorithms their names, right? So this is forward Euler. Right, this is the mid-point rule, or the Crank Nicolson method. And this is backward Euler. Okay let's look here at the stability. Let's look at order of accuracy. And let's draw a straight line. Okay better, and let's look finally at high order modes, right. Okay? Stability of forward Euler is conditional. Right, and we've seen the cons, the stability condition. Midpoint rule and backward Euler are unconditionally stable. Okay? Order of accuracy, forward Euler has order of accuracy 1, midpoint Euler has order of accuracy 2, backward Euler has order of accuracy 1. For high order modes, limit as lambda h delta t tends to infinity. Limit of A in the tends to infinity not 0. Okay forward Euler is let me just see it tends to minus infinity right, because nothing can be equal to minus infinity, right. Midpoint rule. Limit lender each delta t, tends to infinity is equal to minus 1. Okay? Oscillatory behavior. For backward Euler we see that limit lender h delta t tends to infinity. Oh, sorry, limit of A here. Okay, here too, limit of A equals 0. Okay, so it damps out high order modes, dissipates them away. So this is the, broadly speaking, the behavior of the three main are the three most commonly used members of this family. Depending upon the problem you choose your method, okay. All right, we're done with this segment and with this unit here when we return we will take up the problem of Elastodynamics.

\section*{ID: KtNJYj66s1E}
We're going to start a new unit, and a new equation now, we're going to go on to hyperbolic equations. We've looked at elliptic and parabolic equations, we now move on to hyperbolic equations. The canonical example here is elastodynamics, in whatever dimensions you wish. We're going to look at it in three dimensions, because we've left 1D far behind now. Okay, so, title of this topic of this unit is methods for hyperbolic. Linear of course. Pdes in vector unknowns. All right? And the example is linear, r i, linear elastodynamics. In 3D. Okay? So the setting is the following. I don't have my lego vectors today for my bases but I'll use my fingers okay. So that's my basis and this is the body of interest. When we considered linearized steady state elasticity. We were interested, of course, in how this ball would deform or how this continuum potato would deform. We didn't, however, consider the so-called dynamic effects, or the, or the, the effects that lead to wave propagation, all right? We couldn't, for instance, for that reason, also study the problem of this ball being actually tossed through space, right, and tumbling through space, and deforming perhaps at the same time. Right. We couldn't look at the time evolution of that problem. Right. Because we were looking at the steady state problem. So now we take away restriction, and look at the full blown elastodynamics problem. Okay. So Here we have the, the setting is essentially the same, as far as our pictures are concerned here, right. So we have our figures are concerned here, e1, e2, e3. We have a body of interest, right. This is omega. As we've done before, we have a decomposition of the domain into Dirichlet, into a Dirichlet subset for that particular component of the displacement field, right? And the corresponding Neumann subset. Okay? And this holds for i equals 1, 2, 3, right. X would be a point here, right. Which will be described by its position vector. Right, everything that we've seen from before holds, right? The decomposition of the, of the boundary into. The Dirichlet. Sorry, I got the union in the wrong position. It's the union of this Dirichlet boundary and the Neumann boundary. And, of course, those are disjoined, we know that. This is the empty set, and so on, right, we have all of this. This of course holds for I equals 1, 2, 3, right, three dimensions. Okay. I'm going to straightaway put down the strong form of the problem, right. The strong form of the problem is the following. Now, given data, ugi, t bar i. F i, right? In addition, we need some more data now. We need also other functions which I'm going to denote as u i 0, okay, and v i 0. Okay, we are going to use them for initial conditions. Alright so given all of this, and of course, the constitutive relation sigma ij equals C ij kl epsilon kl. We also know that we have the kinematics, right? Epsilon kl equals one half partial of uk with respect to xl, plus partial of ul, with respect to xk. Right? We have all of that stuff. Right, the only new things that you're seeing here are these two. Functions which I'm telling you right now we've been used in initial conditions. There, there, there is one more. We do need another coefficient, which I'm going to denote again here as rho, rho here is just the mass. Sorry, the mass density. Okay? Let me get rid of these arrows from here so that it's not confusing to think that they're pointing up from density. Those are the initial conditions. All right, we have all of this. The problem that we are trying to find is the fall, is to find u i okay, now it's a function of position and time. Okay. Right, and remember that i runs over one two three. Right, so everything that I've written on the first line, the first five functions is down here. Right. These functions are all just components of vectors. Right. Okay, so we want to find u i such that. Such that the following holds. Rho, second derivative of u i, with respect to time equals sigma ig comma j, plus fi, okay. In omega cross 0, comma T. Just as we did for the time dependent parabolic problem, right. We say that our pde must hold over the spatial domain and the time interval, 0 to T. Okay? Let me leave this here and then go on to the next slide to write out boundary conditions. Boundary conditions are no different. Right? For boundary conditions we have u i, at some position x and time t equals this given function ug for component i at positions x and t, and time t. Right. Now note that we're allowing here that Dirichlet data to vary with time. Okay this just allows us to have time dependent Dirichlet boundary conditions. Just as as we had for our time dependent heat conduction or time dependent mass diffusion. Right? We allowed the possibility that the Dirichlet conditions varied with time. Okay right at any point x belonging to a point in the Dirichlet boundary, right, for that particular displacement component. Our Neumann boundary condition or our traction boundary condition also is as before. Okay. Functional position and time for a point x belonging to the corresponding Neumann boundary. And note here that I'm continuing to use t bar for the traction function where as the t here is for time. Okay so it was, it was an anticipation of this final clash of notations that I have been using t bar for the traction. Okay, initial conditions. Our elastodynamics equation our pde for elastodynamics is a second order pde in time. And therefore, how many initial conditions do we need? Two, right? So we have u i at some position x but at time t equals 0, equals u, how do I write it, u i 0. Okay, which could be a function of position. We're allowing, we, we of course need to allow the possibility that well not just the possibility. We have to allow for initial conditions so, to be defined at every point. Right. So for every point x we have an initial condition of the displacement. Right. What it basically says what is the initial configuration of the body. Okay? So, this holds of, of for all x in omega. It is second order, so we need two initial conditions. Next initial condition is for u i dot x comma 0 equals the specified distribution of velocities. Okay, what this means is that at the initial condition we are saying that not only do we start out knowing where every point on this body is, right, that is the first of those initial conditions. What we are saying we must also know what the initial velocities are. Okay. That's the second initial condition. All right, this is it. This is our strong form. Okay. I'm going to straight away go ahead and write out the weak form. Right? The weak form. Of course, this is going to be the infinite dimensional weak form, but we know that going from there to the finite dimensional weak form is not such a big thing. The weak form. Okay? Given all the data that I have just put out there, right? I'm not going to repeat the data, okay? The weak form is find. U i belonging to S, okay. Where for our purposes here S consists of all u i such that u i equals u g i on. Okay. Find this such that for all w i, belonging to V where V consists of all weighting functions. Remember, w i is our weighting, our, our weighting functions. v belongs to w i such that w i equals 0 on. That Dirichlet boundary. Okay. Such that for all w i belonging to V, the following cond, integral condition holds. Now, integral over omega, w i rho, second derivative, second time derivative of u i. Plus integral over omega, w i comma j, sigma ij, dV, plus sorry, is equal to integral over omega w i fi dV. Plus as before, the sum over spacial dimensions 1, 2, 3 now. Integral over the corresponding Neumann boundary, w i t bar i dS. Okay, that's it. Now, if you stare hard at this weak form, you should observe that it is obtained by just adding one term to our weak form for the steady state elasticity problem. Right? And that extra term is just this one. Observe furthermore that this term requires no integration by parts. Right, it's literally obtained by looking at the left hand side of our strong form, which is right here. Okay, look at the left hand side of the equation at the bottom. Multiply that by Wi, the weighting function. Integrate of the domain, right? We know that the rest of the stuff on the right hand side is what attracts integration by parts, right? Especially the divergence of sigma, the first term on the right-hand side in the strong form. Well, I know directly how the weak form arises, nothing new here, right? Just add in that, this, extra term on the left hand side. All right, we'll end the segment here. When we return we will simply write out the weak form, sorry, the finite dimensional weak form and go directly into the finite element of matrix vector recreations. Good.

\section*{ID: cTKU18RMjEM}
Welcome back. We are now on to, developing methods for, hyperbolic PDE's, in, three dimensions in vector unknowns. And specifically the the example problem we're working with problem we're working with is that of linear elastodynamics in 3D. What we've accomplished for this problem is a statement of the strong form and of the weak form. We'll pick up from there and write out the finite dimensional weak form and plunge directly into matrix factor equations now. So, we are doing, like I said, the problem of hyperbolic PD is, in vector unknowns. And in three dimensions. Okay? Of course, this is, these are all linear, right? So we're doing linear hyperbolic PDEs and so on. Okay? And also the canonical problem we're talking of here is linear, the last two dynamics in 3D. Okay? I'll direct you write out the weak form, but the finite dimensional weak form, which you know, really requires very little, extra specification over the infinite dimensional weak form. Also, I will, spare us the details writing out of the data here, right? We're very familiar with all of that now. All right. So, the task we have here is to find U i sup h. And remember the h now indicates that we're talking of a finite dimensional problem. Find u i sup h belongs to the space S h which is a subset of the larger space S. And in, and particularly we are thinking of s, h as consisting of all functions u, I, sup h. And we expect these to belong to h 1 over the domain. Such that u, I h, equals u i given on the corresponding Dirichlet boundary. All right? The, the part of the Dirichlet boundary that corresponds to that particular spatial dimension, denoted by i, all right. We have this. Such that. For all whi belonging to vh. Which is a subset of we where we H consists of waiting functions WI sup H, also H1. Search that. WI Sup h vanishes on that corresponding Dirichlet boundary. Okay, so find u i sup h such that for all w i sup h belonging to v h, the following condition holds, right? The usual integral equation, except now that we have the one extra term, right? The term that's second order in time. Okay, so we have integral over omega, w i sup h rho, second time derivative of u, integrated over the volume, plus integral over omega, w h i comma j, sigma h i j DV equals integral over omega. WH sup I, sorry, sub I. FI, DB, plus sum over spatial dimensions. One to three in this case, or n, s, d in general. Integral over the corresponding boundary. W, h, I, t bar, I D, s. Okay? That is our finite dimensional weak form. All right? And I didn't state that anywhere. So, let me just do this. All right? So, this is r finite dimensional Weak form. Okay? All right. Now we know how things play out from here. Everything works out just as before, right? We're going to use the same basis functions we're doing 3D so you can think of trilinear hexahedral as being the simplest of those elements into which we decompose the domain. Everything works out just the same, okay? Also note that each of these terms is going to give us our, our standard sort of contribution that we know so well. Right, when we account for the fact that sigma H satisfies the constitutive relation, right, which makes it. Truly linearized elasticity. Right? What we get from this term is a very standard one that we've worked with at least once in great detail. Right? So we recall that this term gives rise to c transpose Kd. All right, and these two terms here give us. C transpose F. Okay, also, now. When you recognize that. The second time derivative and the second time derivatives are in need of DT squared there. All right. When you recognize that this term on the left hand term, essentially enrolls the waiting function W multiplying the second time derivative of the trial solution, right? And you work through things just as we work things out in the case of, the parabolic problem in 3D. What you will see is that this gives rise to, what sort of matrix? We have coming from 'w' and we're going to have a 'd' coming from to u right? In particular, we get a d dot d double dot because we have two time derivatives on u. What is a matrix that goes between them? It's one we've encountered. It's the mass matrix. Okay. So essentially, this is the form that the matrix vector equations take in the case of linear elastodynamics. There's just one detail I want to point out here, which is something to do with the construction of M, because of the fact that our vector d has at each node three scalar degrees of freedom. There is one little detail about the construction of M, which I'm going to show you right now, okay? So let me just say here that we're going to look at one little thing to do with the form of M. For this problem, okay. And in order to understand that, what we need to do is consider the, element integral corresponding to that term. Okay, so we consider integral over omega. W, h, I, row second time derivative of u, I, u, I, h sorry there should be an h there and an h here. D, v okay, we recall this is simply a sum over the elements. Integral over omega E, WHI row second time derivative of. Right. Remember this ok. So. Okay, we we're going to work with just that element in a group to clarify things.

\section*{ID: u5appqgEPzM}
Okay, we'll do it here. So, the element integral. Okay, integral over omega e. W h i, rho, second time derivative. Right, this one. I'm going to go straight away into, writing it out, with our basis functions and summing over basis functions and all that sort of thing. Okay? So, we know that this is integral over omega e. Actually let me skip some more steps. Okay, because we're now such experts at this that we don't really need to write every single thing. Okay, let's do it this way. Fine, sum over a comma b. C a e i. Okay? And you recall that c a e i is each of those is the i'th degree of freedom at the a'th node of element e. Okay? This. Integral over omega e. N a, let's have the rho there. N a n b d v. And outside of this integral. Come the degrees of freedom that, are used to, construct the representation for the trial solution, right? So that will turn out to be d b element e degree of freedom i, okay? And what is implied here is a sum over our, spatial dimensions i. Okay? Now, the form that I have in parentheses on the right hand side. Okay. The everything including the integral, is what we identified previously to be the a b component of the consistent mass matrix. Okay, same thing. The same thing happens here, except for the fact that there's one thing we need account for. Which is that there is a sum over i here, okay. So let me just mention this here. Recall sum on i, okay. And what that does is lets us write this thing out as sum over a comma b, c a e i, integral over omega e. Rho n a n b d v, but now what I'm going to do is, I wrote that d v too close, I'm going to slip in here a Kronecker delta, okay? So allow me to do this and I will show you why I'm doing this. Okay? Since I've slipped in the Kronecker delta and given it indices i j. I'm going to turn the d b e i from the previous line, this one. Into d b e. J. Okay. All right. As a result, now when we put this all together what you see is that what you observe is that though the contributions to the, from the integrand are the same as we had for the, for the linear parabolic. Problem with the scalar unknown, right. What we've done here to counter the fact that we have vector unknowns, is to include this Kronecker delta right there, okay? All right, now when we put everything together, we get the following. Okay. We're going, we're now, in what I'm about to write, I'm going to abandon the explicit, writing of the sum over, over a and b. Okay? And in fact, let me give myself more room here. Just before I go ahead and write that I know that I've forgotten the second time derivatives on the d. To account for the fact that we have a second time derivative right there. Okay? Now everything's fine. Okay. So the way I'm going to write this by dropping the explicit sum over a and b is the following. I get, c one e right? Transpose okay? C two e transpose. So on up to c number of nodes in the element, e transposed. Okay, this one for instance, c two e transpose, is simply c two e one. C two e two. C two e three. Okay? All right. All of that now multiplying some big matrix which makes up our mask matrix. Okay? And out here we get our vectors, or vector of d one e double dot d to e. Double dot all the way down to d n n e. Element e. Double dot. Okay? And like I did here for c two e let me just point out that this is the little vector d two, e one, double dot, d two, e two, double dot, d two, e three. Double dot. Okay? All right, now. Note that for each combination of nodes from the c vector and the d vector, okay? We get a little matrix, okay? And that little matrix is obtained by writing out here we have here let me show you what we get here. Okay. We have here a little matrix which is row n a. N b d v over the element and let me actually make it n one n one to show you the very first combination okay? But this matrix, that integral itself, multiplies what you may call the three by three identity matrix. Okay. That's because it's this matrix that I've just written, that little sub matrix that I've just written. Which multiplies, which, sort of intercedes in the multiplication between c one e transpose and d one e double dot. Okay, this continues until we get another such matrix. Right, each of these, each of these blocks is a little matrix, right. Each of these is an integral over omega e. Of, you know, n something, n something else, integral over d v. Okay? All right? All the way down. So we have how many such blocks? All right? We have n n e such blocks in the column wise direction and n n e such blocks in the row wise direction. Okay. All right. All right. And what's important to note is that each of these little matrices is itself an identity matrix, right? And this is what our Kronecker delta does for us. Okay? And so the general term in here would be something like integral over omega e rho n a, n b, d v one zero zero zero one zero zero zero one. All right, you'd get other such block matrices everywhere. Okay. All right. And this matrix that I've written out here is our element mass matrix. Okay, in consistent form, okay, the consistent mass matrix. All right so this is an important thing to note. This is slightly different from what we saw for the linear parabolic p d, where each degree of freedom was, was, is essentially a single scalar. Unknown. Right? And, and in that case, this, the, this identity matrix essentially collapsed to the scalar one. Okay? The fact that we are doing, dealing with vector unknowns here, just, has, has essentially expanded that scalar one into a, into an identity matrix. Right? Three by three identity, that's all. Other than that, everything is the same. Okay it's probably useful for us to ask, now what are the dimensions of m, e? Okay, what will they be? Right the dimensions of m, e are number of nodes in the element, times number of spacial dimensions, squared, right? Those are the dimensions of m e. Right. Okay. That's it. This is our element mass matrix. The only thing that's different with, with regard to what we've seen before. Assembly works just as before. Assemble over degrees of freedom, common degrees of freedom across elements. Belonging to the same global degree of freedom. Implies that those corresponding matrix contributions from every. From, from, from the neighboring elements add up. Okay? All right. So let me just state that here. Assembly proceeds as before. Over global degrees of freedom. Okay? What we are left with, finally at end of the process, right, after we, we, we say that. Well this has to, our, our weak form has to hold for all c belonging to the appropriate, Euclidean space, right. What we get at the end of it, is the following set of equations, right. We get m d double dot plus k d. Equals f. Okay? When we were not doing elastodynamics, but essentially elastostatics, this first term was missing. Okay? It's just shown up now, right. Everything else is the same. We know that boundary conditions are buried inside here, right, including time dependent Dirichlet boundary conditions. That also works just as we saw earlier, right, in the case of the time dependent parabolic problem, okay. If we have Dirichlet conditions, they are time dependent. Well we know exactly what they are at every time, at, at any time, right they will go to the right hand side. What we would have here in addition are initial conditions, right. D at zero. Equals the, the vector of, well it essentially equals the vector form by taking every Dirichlet condition. At degrees of freedom lying, corresponding Dirichlet boundary. And, putting them all in a vector d nought, okay? We did this for the parabolic problem. We need another boundary condition here, sorry, another initial condition, because we are second order in time. That's d dot at time e t equals zero, which we will denote as v nought. Okay? All right? And v nought, you remember, is simply, it's simply constructed of, u dot. Sorry, it's actually I think just the vectors, the value of the function v nought, right? At points x a, right? For every degree of freedom, a, that lies, sorry, for every single degree of freedom a. Right, for the initial condition has to be specified over the entire vector D. Okay? This is essentially it. Right, just as we specified initial conditions for our parabolic problem, we have two initial conditions now for this problem. All right? Okay, so that's really all we need to know about how the matrix, the matrix equations are obtained. The process is exactly the same. We get another mass matrix. There are some detailing mass matrix, it's at, at, for corresponding to every combination of degrees of freedom, it's slightly blown up. We have that little three by three identity matrix sitting there. And we have two initial conditions, right? And we know how to specify those initial conditions. So, so really, that's what we need to know. We'll stop this segment here. When we return, we will talk about time discretization.

\section*{ID: m9wMmcEG71A}
Welcome back. What we've accomplished in the previous segment is a explication hopefully of the matrix vector form of the problem for lin, of linear elastodynamics in 3D. Now we proceed with the time discretization and understanding a little bit about the about the methods that are used to solve this particular problem, which is also a ODE now. Okay, so let's start with the matrix vector problem. Okay. What we derived at the end of the last segment was the following. Md double dot plus Kd equals F, right. This is the second orderal d in time and we have initial conditions. D at time T equals 0 is d naught, and d dot at time T equals 0 is that vector, V naught. Now, we can proceed from here, but it's useful to include one extra element in here. And that extra element is a sort of throwback to the times when to the times really even before finite element methods became very popular in structural mechanics when it was common to write out matrix equations of this sort for for structures, right? People would, would, the notion of using nodes and degrees of freedom had, had already been established, especially in the context of structures like crosses and frames and so on. And in that setting of structural mechanics, it was common to include in addition to the mass and stiffness matrix that we see here, matrices that we see here, a damping matrix, okay? So and, and here's how this was done, okay? So including the effect of structural damping. Okay. The way this will be done will be to include a damping matrix of C. Again, I'm just following the standard notation that tended to be followed in this business. So, we have yet another matrix. All right, so we have C which would often be modeled using what is called Rayleigh damping. And this was done in a very sort of simple, very empirical manner by taking some constant, a, multiplied with the matrix M plus some other constant b, multiplying the matrix K. Just like that. Okay? All right? Empirical, but it was found to work. And, there are other reasons why this works. We don't need to go into those reasons. This is the model for what is called Rayleigh damping. Okay. Where a and b are constants. All right? And that was it. There was no attempt to try and derive these this new damping matrix from any more fundamental partial differential equation. That can be done but, but, but it was not necessarily done in, in, in writing this out, which is just explicitly written out in this form. Okay. And then the form, the damping would be, would be included in, was the following. So now we would get the equations of, elastodynamics. With structural damping. Right. And that equation would turn out, would be Md double dot plus Cd dot plus Kd equals F, all right. Where the idea of damping was, that this is some sort viscous damping essentially that was being modeled, and you see the effect of viscosity. If you're familiar with that sort of physical phenomenon and the fact that you have a single time derivative on this extra term that's been introduced to the problem. Okay, that's just like a first, that is indeed a first time derivative, a single time derivative on the d vector. Right, plus boundary, sorry, plus initial conditions as usual. Okay? This is essentially the model. Now, from here, one can go on and write the time discretized form, just as before, right. From here, what we do is for time discretization. All right, we do exactly what we did before, which is to say that our interval, zero to T we write as the union of all these time intervals, of t0 to t1, so on all the way up to t sub N minus 1 to tN. Right? It's the union of all these intervals, right, of each of these subintervals where t naught, in the way I've set up the time interval, t naught would be 0. Tn would be capital T. Okay? We have everything just as we knew from before. All right? And then we say again that d At n is the time, discrete. Approximation. Write approx for short. The time discreet approximation to d at t n. Okay, just as we did for the parabolic problem. Okay, and then we get our time-discretized, our time-discrete matrix vector equation. Okay. Let me go back, just for a second to have you look at it. Right, we have it here. So, you see M d double dot plus C d dot plus K d. Now d double dot, because d of course displacement, d double dot is essentially the acceleration. Right, likewise d dot is indeed the velocity vector, right, at the global degrees of freedom. And d is the displacement factor. With this is mind, the time-discrete matrix-vector equation is often written as M at n multiplying a n plus one, where a n plus one now is the acceleration, right. The approximation to the acceleration at time t, n plus one. Plus C v at n plus one, v being the velocity, plus K d n plus one equals F at n plus one. Okay? With initial conditions now being that d not and v not are known. Okay? All right. Now, the family of algorithms that is commonly used to solve this equation, this time-discrete form of the equation is what is called the Newmark family. Newmark family of, algorithms. For second order ODEs, right? They're second order because they're second order in time. Okay? The way this family works is the following. For, now, we need to have some parameters for this family. Just as for the Euler family of algorithms for first order equations, we have our parameter alpha. Here, because these are second order ODEs, we need two parameters, it turns out, and one of those parameters I'm going to denote as gamma. And gamma belongs to the closed interval zero comma one, just like alpha did. And the other parameter I'm going to write as twice of beta, where twice of beta now belongs to the interval zero to one. Alternately beta belongs to the interval zero to half. Right? The closed interval zero to half. All right? Now, with this in place, here's how the Newmark family works. It says that d at n plus 1 equals d at n plus delta t times v at n plus delta t squared over two, times, 1 minus 2 beta, multiplying a n. That's the approximation at the acceleration at time tn, plus 2 beta, multiplying a n. That's the approximation to, to the acceleration at time t n plus one. All right? And then because it's a second order algorithm, we need something for v n plus one as well. V n plus one is equal to v n plus delta t times one minus gamma at a n, plus gamma at a n plus one. Okay? Those equations together with our time discrete matrix factor equations written above here, and of course initial conditions, right? Initial conditions here are just that d not and v not are known, okay? This summarizes our, family of algorithms for linear elastodynamics, okay? Now, let's talk about solution techniques, 'kay? The solution technique, that I'm going to talk about, I'm going to talk about a single approach, not two approaches as we did for the parabolic problem. The method I am going to talk about is what's called the a method, the a being for acceleration. Okay, and here is how it works, we again define predictors and correctors. Right? We say that d n plus 1, tilde, equals d n plus delta t v n plus delta t squared over two. One minus two beta a n. Okay? That's the predictor for d. The predictor for v is v n plus delta t. I don't need anything in the denominator, it's just delta t. Times 1 minus gamma a n. All right? And just as we did before for the parabolic problem, we've looked at the update formulas for d and v and simply extracted out those parts of the formulas that come from everything known at time t n. Okay, so these are our predictors. The correctors are. Right, those are predictors, and the correctors are the following, right. The predi, the correctors are obtained by simply writing d n plus 1 equals predictor Right, plus corrector. Now the corrector for d n plus 1 is delta t square beta a n plus 1. And the corrector for v is. This one, right? These are the correctors. Right, and what I've done now is write the corrector step for both of them. The a-method essentially, just like the, the methods in the case of the parabolic problems, the a-method is obtained by substituting these corrector steps in the original equation. Okay? So what we get is on substituting these corrector steps. Right, on substituting we get the following. M times delta t square beta plus C delta t gamma plus sorry. I'm getting ahead of myself a little, so let, let me just rewrite this line. I'm trying to skip a couple of steps, and I realized I was already making errors, so I'll just back up a little. Okay, so when, when we substitute these correctors, what we get is the following. We get M a n plus 1 plus C multiplying, C multiplies v n plus 1. So we get the predictor plus corrector. And for K, K multiplies d n plus 1. So again, we get predictor plus Corrector. Okay, this is the entire left hand side. All of this equals F at n plus 1. Now this is the so-called a-method, and by using that, those predictors and correctors, what we've done is to rewrite the equation entirely in terms of a at n plus 1 and predictors for d and v. All right? So this essentially then lets us rewrite this as M plus C delta t gamma plus K delta t square beta, all of this multiplying a at n plus 1. Equals F at n plus 1. And then the terms multiplying the predictors are just moved over to the right-hand side. And why can we do this? Right, it's because the predictors are known, right? They depend only upon the solution at n, which we always assume we know when we construct these time-stepping algorithms. So we get here, right, we get minus C d n plus 1 tilde minus K d, sorry. Sorry, it's C v n plus 1 tilde. C v n plus 1 tilde plus K d n plus 1 tilde. All right? That's our method. Okay, we can now go ahead and invert this, solve for a, once we have a at n plus 1, using our corrector steps we get back d n plus 1 and v n plus 1. Okay? The only thing we need in order to sort of start up this algorithm is a at 0. Okay? In order to get A at 0. Just use the equation at 0. All right, and by equation here I mean the time-discrete equation. And that is M a 0 equals F at 0 minus C times v at 0. This works because v 0 is known, right? It is just the initial condition. Minus K d at 0. d 0 is also known. All right? v 0 is known, d 0 is known. Okay? Okay. So here we have it. That is our standard solution approach for this problem. We can end this segment here. When we return, we will start our analysis, and that analysis also is going to be based upon our approach of modal decompositions. All right?

\section*{ID: gbzCrkea-2o}
welcome back in the last segment we set up the time discretized equation for linear elastic dynamics and also looked at a canonical solution of technique for it right what I call the a method we get into analysis now as for the parabolic problem the analysis is based upon a suitably chosen eigenvalue problem and and the way we carry out this analysis is to use I can value problem to construct a decomposition of our OTE in two modes ok so our analysis is least on the eigenvalue problem space in the following eigenvalue problem on the good omega square m side equals case I alright where Omega square u we recognized to be the natural frequencies for each army goes the natural frequency at natural frequency of oscillation right in the context of our problem because each Omega would be a natural frequency of oscillation okay with this problem in as a basis and then as before proceeding to construct em or target can proceeding to consider I can functions are right conductors side that are M orthogonal right so the size r m or talking I convectors ok when we do this right we and we proceed just as we did for the for the parabolic problem right so we're right by doing this we can now do things like saying that any vector like the d vector can be constructed through a sum over l.d soup l side and right where each side L is an eigenvector again these vectors are mr Turner ok should probably site here right and say that l equals just as we saw before 12 total number of degrees of freedom in the problem ok we do this foodie and of course we can do this for every other up for other vectors that show up in the problem right we can do this will be an A and so on okay are we take this approach and what we see is that we get when we take this approach we get a reduction to ndf single degree of freedom model problems right or moral equations all right now people to go back and write this full-time exacto de rduction 2nds single degree of freedom more equations and we just continue here see of good time exact OD alright and the form of those equations is the falling right to the time exactly remember because it's pretty much the way we did for the parabolic problems right remember we wrote out the reduction for the for the time exactly d and then extended that to go time to strategize rudy is over the time exactly we would get dl double dot plus 2cl army girl each l arm d l dot plus Omega each square d l equals 0 if we consider the homogeneous case right remember the homogeneous cases when we have zero right hand side now r omega each l is just reminding us that that we have natural frequencies but those natural frequencies because they depend upon our matrices which are obtained by spatial discretization write those natural frequencies also reflect the effect of spatial discretization ok so these are what we will call the finite-dimensional or species specially discretized natural frequencies ok just as for the parabolic problem we considered lambda each sub L right which is simply the which simply was the effect of spatial discretization upon the eigenvalues it's something him right so that's America each l \& 4 c sub L which actually properly should better be written as a see each subnet ok there's an H and agile because that also does reflect the effect of spatial discretization okay see each sub L is simply our remember the constants we use to define really damping it's those constants divided by the corresponding natural frequencies ok and this is what is called modal damping ratio ok all right now the way we proceed with our analysis is the following because we have a second-order de we are we rewrite our second order o de using a technique that's very well established in ordinary differential equations we write it as 2 first-order Cody's right so we rewrite the second-order OTE as to first order Cody's alright and in order to do this we say they're right we are now looking for the solution of a vector it was just the two vector where y is d and d dot ok alright it's not difficult to rewrite that single low second-order only in terms of this right arm and in fact what we will also do is that as we recall from the case of the parabolic problem ok for what we get for the time discretize problem is the falling maybe get a mobile be composition of the time discretized problem also write in model form ok and that problem is the following weekend II and +1 Modell plus 2c each sub L Omega H sub L the n plus 1 mode and plus Omega each arm and square d and +1 right where each of these is a moral coefficient the elmora coefficient of the corresponding vector ok this equals 0 is the homogeneous problem ok alright right and now the relations between the model coefficients between the DN plus 1 VN plus 1 and n plus 1 will be satisfied with those those model coefficient satisfy the same conditions that we obtained from numark family right for the full vectors okay right so we get new mark family equations relate dn1 at al v sorry it's not the vector right it's the mode v + + 1 @ l and he and plus 1 at ok alright those are the equations that involve the coefficient though are the parameters gamma and Vito

\section*{ID: PQsLr4Gr-9U}
Okay? And then, when we do this reduction of a second order ODE to two first order ODEs, for the time discretized problem, okay, we get the following form. We get the following form. We get y at n plus 1, okay, equals A, which is now our amplification matrix, y at n plus L at n. Okay? All right, and let me tell you just once more, that Y at n plus 1, is d at n plus 1, v at n plus 1. All right? This is of course dn, vn. And A here is a 2 by 2 amplification matrix. Okay? It plays the same role as our scalar amplification factor for our parabolic problem, okay? And the definition of A, as well as the definition of this two vector Ln, reflects the reflects the Newmark algorithms right, with the gammas and betas and everything. Okay? We can work out all these details, but it's just tedious detail, which we are not going to truly use. Okay? All right, so then this is our time discretized equation, now written in, in the form of two first order ODEs instead of one second order ODE. Okay? I'm going to give you a summary of the stability results right now. Okay, stability. Okay? So, if 2 beta is greater than or equal to gamma is greater than or equal to half, we have unconditional stability. Okay? If on the other hand, gamma being greater than or equal to half, beta lies between 0 and gamma over 2. These conditions together give us conditional stability. Okay? Now remember, we are talking about a single degree of freedom modal equation, okay? Where, because of the fact that we have a second order ODE, what we are solving for are dn plus 1 and vn plus 1, right, both of those modal coefficients. The conditional stability holds when omega h, right, which is the frequency, corresponding to that particular mode, right? Remember, as before we're suppressing the modes, okay? Actually, even back here, we are already suppressing. Mode, number, index, all right, L. Okay? So that's going on here as well. So when, even though I've just written omega h here, it's really omega hL for each L, right? We need to consider this for each L, okay. So the stability condition also requires that omega h delta t, should be lesser than or equal to a quantity that I'm going to denote as omega critical, okay? Where omega critical. Equals the following. Ch, and again, this is Ch sub l really, but for every mode, okay? C h times gamma minus half, plus gamma over 2 minus beta, plus c h gamma minus half, sorry, c h squared gamma minus half, the whole square. All of that to the power one half. The whole thing divided by gamma over 2 minus beta, okay? That's the critical frequency, okay? What we see here is that there is the effect of damping. Okay? And, what we also observe is that the effect of damping, right? And, and we have damping when c h is, greater than 0. The effect of damping is to increase the critical frequency, okay? So we have the undamped critical frequency. Let's say omega critical u for undamped, okay? This is got by setting c h equal to 0, okay? And it is just gamma over 2 minus beta to the power minus half, okay? Right? I just want to point out that this undamped critical frequency is a lower bound to omega critical, all right? So what we're seeing is that the undamped critical frequency is lesser than or equal to the actual critical frequency, when you have some damping, okay? All right. Okay? So what we see as well is that omega h delta t, which needs to be less than the, than the critical frequency, is, it satisfies this sort of a condition. Okay? What I mean by saying this is that actually, let, sorry, let me not write this line. This is, is sort of attempt to state a condition. Let me not write that, that equation, it, it can be misinterpreted. Instead let me say this. The undamped critical frequency is a more stringent condition. On omega h delta t, right? It's really a condition on delta t. So what we are saying is that instead of saying that it has to be less than the actual critical frequency, right? Instead of using the condition that I have at the bottom of this slide, right? If instead of that, we were to say that, well, omega h delta t has to be less than this quantity, okay? Then we are actually imposing a more stringent condition upon our algorithm, our time integration algorithm, okay? Okay, with this in hand, I'm just going to list out properties of some sort of canonical, almost classical, members of this Newmark family, okay? And, I'm going to do this part in a table, where I'm going to list the method here. I will say what type it is. And by type I mean is it implicit or explicit? I will list here beta, gamma, let me see, what else do I need to list here, right? I will list here the critical frequency for stability for the undamped case. And finally, I will also write here the order of accuracy. Okay? So, the methods we are going to consider are the following, the first one we will consider is the, what is sometimes called the Trapezoidal Rule. We consider four methods, okay. I'll write them out first, trapezoidal rule, think linear acceleration. We have the average acceleration. These are all names of methods. And finally we have the central difference method. And, when I say central difference and trapezoidal method, trapezoidal rule, note that they, they will not be the same as what you may be familiar with from first-order ODEs. Simply because we are using terminology here that has been established for second-order ODEs, okay? All right, all of these methods are implicit except for the, except for the central difference method. Okay? Now, stability. They're all for, they all use gamma equals half, okay? Now, the trapezoidal rule uses beta equals one quarter. And because this combination of beta and gamma makes it unconditionally stable, there is no question of what the critical frequency is for stability, right? It's unconditionally stable, all right? So there's nothing to say there. Linear acceleration uses one-sixth. And what happens here is that the the critical frequency is 2 root 3, okay? Average acceleration uses beta equals 112, gamma equals half. And if I remember the undamped critical frequency is square root of 6. The central difference method finally uses beta equals 0. And the undamped, actu, critical frequency here is 2, okay? For order of accuracy, all of these are second order. Okay? One caveat though is that the explicit you get a truly explicit algorithm only for M and C being diagonal. All right? It's just a summary of some of members of the family. As you can imagine, because we are talking of a, of integration algorithms of second order ODEs, the numbers of this family are, are, are a few more, right? It's a fairly large family. Okay, we can afford to stop this segment here.

\section*{ID: jR41sQ12ixE}
All right. What we saw on the previous segment was a fairly quick statement of the stability conditions for the Newmark family, right? And we saw the explicit results for some of them. We didn't derive any of them. What I'd like to do in this segment is outline the, the sort of analysis that leads to those, to those conclusions. We won't get into a completely detailed step-by-step derivation of the results. Unlike what we did for the parabolic problem, okay? But we'll sketch out the, the approach. Okay, so the stability analysis here, as in the case of the parabolic problem. Is based upon examining a particular object. Can you recall what object we examined in the case of the parabolic problem? We examined the amplification factor. Here, too, we have an amplification factor, but it is a matrix. Okay. So, the stability analysis is based on, an eigenvalue analysis of the amplification matrix, all right? Okay. And, the way we proceed with this is to, if I remember that the amplification matrix is what we denoted as A. Okay and you also recall that the amplification matrix is what showed up in this formulation of the problem as here, y n plus 1 equals Ayn plus Ln when we consider the full n homogeneous problem, okay? So what we're talking about is analyzing this, and all our detail of the Newmark family is sitting inside there, right? The particular values of gamma and beta we've chosen, and so on, okay. Here is the condition, okay? We define what is called the spectral radius. The spectral radius of A, right? And we denote that as we've used rho quite a bit, so let me use something else here. Let me just say r, okay, spectral radius A, r sub r function, okay, okay? This is defined as the maximum over i, okay? The maximum over i of lambda i of A, where those lambdas are essentially the eigenvalues of this two by two matrix. All right, and because it's a two by two matrix, of course, i just runs over one and two, okay? That's probably not even worth using an index there. Okay, so let's say that max i equals 1,2 right? Essentially it's the maximum eigenvalue, okay? Not just the maximum eigenvalue, but actually it is the magnitude of it, okay? Where we have accounting for the fact that the matrix A may not always be symmetric, and therefore, it could have complex eigenvalues, right? Accounting for that, we write our spectral radius as being defined as the max i equals 1, 2. Now, that magnitude that I wrote up there is properly the square root of the product of lambda i and its complex conjugate, which is going to be denoted as lambda i of A bar, okay? Where that bar implies the complex conjugate. Of lambda i A, okay? That is our spectral radius. Now the condition for stability requires that. All right, it requires that r, the spectral radius, should be lesser than or equal to 1. Okay, the spectral radius is defined there, should be lesser than or equal to 1. I'm going to say a little more about this condition. We have r can be lesser than or equal to 1, if lambda 1 and lambda 2 are distinct. Okay, it turns out, that if lambda 1 and lambda 2 are distinct the condition that we get that r can be lesser than or equal to 1 involves the fact that the eigenvectors of A. Are linearly independent. Okay, on the other hand r has to be strictly less than 1, right? Not lesser than or equal to 1, it has to be strictly less than 1, if lambda 1 equals lambda 2, right? We have repeated roots, okay? And in this case it turns out that the eigenvectors. Of A, are linearly dependent. Okay? I'm going to do very quick demonstration of why this is the case, okay? So, let's look at the two cases, okay. Let's first look at what happens if they are linearly independent eigenvectors. Okay. If they are linearly independent eigenvectors, then let's look at what happens for the homogeneous problem as we go from one time step to the other, okay? Essentially, what we see is that yn plus 1 equals A yn, right? But then yn is equal to A yn minus 1, and so on, right, yn minus 1 equals, tatatata, right? Goes on. So what we are seeing here is that with every step is getting multiplied by itself, right? If you just make these substitutions in here, we see that, right? Okay, so what we are seeing is that, after a certain number of steps, we're seeing that yn plus 1 equals A to the power n plus 1 times y0. All right? So, what we need to worry about is what is happening with A. All right, as it gets multiplied by itself, what are the powers of A? If we have linearly independent eigenvectors, one can show that A, okay, can be written as some matrix P, times a two by two matrix, which is lambda 1, 0, 0, lambda 2. P inverse, okay? As a result, we get from this, we get A to the power n equals P, lambda 1 to the power n, 0, 0, lambda 2 to the power n, P inverse, okay. So now you note that even if lambda 1 were equal to 1, all right this sort of a form stays well bounded, okay? All right, okay. This sort of a form stays bounded, right? So the amplification that is applied to the initial condition to get say, the nth time step solution does not get unbounded, okay? Things are different if you do have, if we have linearly dependent eigenvectors. In this case, the best we can do is write A as, let's say, some other matrix Q, two by two matrix, lambda 1, 1, lambda 2, Q inverse. Okay? This is the case of the linearly dependent eigenvectors, okay? Now, if you go through the process now, and calculate a to the power n. >> Okay, what you get is a form where you get Q, lambda 1 to the power n. You get lambda 2 to the power n, and all is looking good except for the fact that here you get n lambda, well lambda 1 is equal to lambda 2 here, okay. So it doesn't really matter. Let me just do this. In the case of linearly dependent eigenvectors, you have lambda 1 equal to lambda 2. So, you get n times lambda 1 to the power n minus 1, Q inverse. And now, do you see a problem as n gets large? Okay, what you note is that if you look at what happens here? And you have lambda 1 equal to 1. Okay? You see that the off diagonal term becomes n, okay? Right. In that case this term becomes n. Right? And then as you go to higher, and hard, higher, as you go, as you advance in time steps you have this sort of gradual sort of tendency towards unboundedness. Okay, so what happens in this case is that the off diagonal term. Diverges, as n. Okay? All right. Further analysis of this problem is based upon essentially solving for lambdas, right? So. Solving for the lambdas, right? The equation that we need to solve for the lambdas is the following. Lambda square, remember this is just the characteristic equation that you use to solve for the eigenvalues of a matrix, all right? So we will write it as lambda squared minus 2, A1, lambda plus A2 equals 0, okay. And here A1 equals one-half trace of A, and A2 equals the determinant of A. Okay? All right? With this form, it's just a simple quadratic equation, right? So we get lambda 1, lambda 2, are equal to what is it? A1 plus or minus square root of A1 squared minus A2. I believe, let me just look at that. Yeah, okay, so these are lambda 1 and lambda 2. All right. Now, here is the, sort of stability condition again written in terms of A1 and A2. Okay, because of course stability depends upon the values of lambda, but then since we have lambda 1, lambda 2 given by these conditions for A1 and A2, we can write it out in terms of A1 and A2. Okay? The conditions that we get are the following. We get minus A2 plus 1, divided by 2 is lesser than or equal to A1 is lesser than or equal to A2 plus 1 divided by 2 if the magnitude of A2 is less than 1. Okay? All right. And. Okay. And otherwise, we get minus 1 is less than A1 is less than 1 if the magnitude of A2, sorry, it's not just the magnitude of A2. It's A2 itself. Here too, it's just A2. If A2 is equal to 1. Okay, one can plot this thing up in this result up in an A1, A2 space, and here's what we see. If this is A1, and here we're plotting up A2. Okay? Let me see. I think, I'm going to mark some critical points here. These are the points 1, 1, minus 1, 1, and down here, I have the point 0 minus 1. Okay, in this, we have between 1, 1 and 0, 1, that line, and here we have that segment, okay? The first condition, this one okay, holds everywhere except for that line. Right, because that dashed line is A2 equals 1. Okay, so this sort of inverted triangle that I've drawn is the acceptable region. Okay for stability, if A2 is less than 1. Okay, if A2 equals 1, right, then what it says, is that on that line, it's got to eliminate, it's got to be, you've got to leave out those two points. Okay? That is the region of stability that we have here. Okay. Okay, we can end this segment here.

\section*{ID: Nnmwcy0fQzs}
All right. In the previous segment what we looked at was, again, a sketch of the way we would approach the stability analysis for our modal equations for linear elastodynamics, all right. What I'd like to do here is actually take a step that we also took in the case of the parabolic problem which was to go from stability. Our understanding of stability to also very quickly cover high, high order models, okay? So, we've derived stability conditions on A. All right? Our amplification matrix. And in particular, what we said was that if A1 equals one-half of the trace of A. And A2 equals determinant of A. What we found was that the conditions are based upon lambda 1, lambda 2 equals A1. Plus or minus square root of A 1 squared minus A2. Okay? This is how we determine lambda 1 and lambda 2 from our characteristic equation for this matrix. And then we impose conditions at lambda 1 and lambda 2 can be, need to be lesser than or equal to 1. We've also understood when they have to be strictly less than 1. All right. So these are the stability requirements. Right, so we also said from here r, which is  the maximum of the square root of lambda i and lambda i bar. Right? Where the lambda i bar refers to the complex conjugate. Right? Okay, and then we say, finally, r has to be lesser than or equal to 1, right? And we've understood when it can, when it needs to be strictly less than 1. All right, so this is how we go about our stability analysis. Now you recall that when we looked at the parabolic problem, we looked at the, the amplification factor, and also used it to tell us something more about the high order modes. Okay. And we saw that the different members in that case of the Oiler family did different things to high order modes. Okay. It emerges that in the case of this problem, also the amplification factor plays a role. And in order to damp out high order modes, okay. In order to damp our high order modes, high order modes. Are decaying right? If if our eigenvalues become purely real, right? Sorry  it's the other way, sorry. High-order modes are non-decaying, sorry, are non-decaying, if lambda 1 and lambda 2 are purely real. Okay? And therefore in order to damp out the higher-order modes, what we need is that the discriminant of this relation, right, should be negative. Right? So what we need is that A1 square minus A2, is less than 0 for damping or digging, of high order modes. Okay, so this is really a sort of parabolic condition and if we go back and now plot out this parabolic condition on our, A1 A2 space that we introduced at the end of the last segment, here is what we'd see. That is 1, 1. This point here is minus 1, 1. And this point here is 0,1. Okay. What we did do last time was observe that our stability condition gives us this sort of triangular region. Okay. In addition to this, it emerges that our condition for damping out of higher order modes requires that there is a parabola which takes on this shape. Okay. So we need to remain within that parabola, not outside of that parabola. So, this is the region in which we get damping of high water modes and okay according to that condition. Right, so it is in that region that we get, damping of high order modes. Okay. Let me see if I can get back to my original color here. Okay, the effect this has is the following. You know, the stability condition for the condition for, for, for stability independent of time step series, right? So, so really the unconditional stability, okay, condition is the following. It is that beta is greater than or equal to. Gamma over two. Okay? So this is unconditional. Okay? It doesn't quite make sense to say stability condition and then fit an unconditional stability condition, so let me say stability. Requirement here. Stability requirement. All right, all right, so this is the unconditional stability requirement. It turns out, however that we need, beta to be, not only greater than gamma over two. But, this requirement of damping out of higher order modes, requires that beta should be greater than or equal to gamma plus half the whole squared to damp out higher order modes. Okay. And one way in which this manifests itself is in the effect on, the spectral radius. Here's what happens, right. So the effect of, of this requirement on the spectral radius is the following. Okay, turns out that if you plot out the spectral radius, here it's r, and here we look at, delta t times, 2 pi, omega h, okay, and this is just like we plotted in our, previous, study of the parabolic problem. We plotted up the amplification factor on the hori, on the vertical axis, and on the horizontal axis we plotted up lambda t times lamb, sorry, delta t times lambda h, which was the eigenvalue of that problem, all right? In this case, you're plotting up delta t times two pi omega h, which is effectively like delta t divided by the time needed of oscillation or something, okay? So for r, if this is the value 1.  Right? What tends to happen is that as delta t over two pi omega h increases, the entity times two pi omega h increases, if you have values of gamma or, or beta that are, you know, we satisfy the unconditional stability requirement but they have a, they're not as big enough as to satisfy the second requirement, right, of being greater than gamma plus half the whole squared. Okay, here's the sort of thing that happens with the spectral radius. And along this point is where you get, real, fully real eigenvalues. Okay? So you get real eigenvalues around here. All right, and this is for, gamma by two lesser than or equal to beta, lesser than or equal to gamma plus half of the whole squared. Okay? If on the other hand you have, beta exceeding gamma plus half the whole squared, then you tend to get, a behavior which looks like this. Okay? This is for, beta greater than gamma plus half the whole squared. Okay, and this is what leads to damping of high-order modes. Okay. So, that is what we need to know about, how high-order modes depend how, how our analysis of high-order modes follows from our stability analysis. Perhaps the last thing I want to say about this particular problem is how we go about getting to the, to a notion of conversions. Okay. And before we end, and, you recall that stability and consistency are the two requirements that lead to convergence. Okay? Now, in order to understand consistency of this problem, we go back to looking at this form of the, evolution equation for the time this could rise, modal equations, okay? A is our amplification matrix. Remember y is our two vector which has the displacement of the velocity in it. Right? And Ln is what we get from the forcing. Okay, so now we're going back to the inhomogeneous problem here. Okay, again, this is exactly the way we set things up for the parabolic problem. All right. Where Ln was the effect of our putting back the forcing. Right? We, we turned off the forcing when we studied stability. We turn it back on when we study consistency. Okay. So, this is the time-discrete problem. Okay, the time-discrete modal equation. Now, for consistency, we say that well, if instead of the time-discrete, solution, we were to look at the time exact solution, and just plug it into the equation we've written above here, right? We would get y of t n plus 1 equals A, our amplification matrix, times y and tn, right? Y of tn plus one and y of tn are the corresponding exact solutions. Okay? We get a plus Ln, right? But in general, the exact, the time exact solution does not satisfy our finite difference equation. Right? So we get, in addition, a term delta t times now a vector tau depending upon tn. Its a vector tau because of course y is a two vector, right? So tau also has to be two vector. Okay? So this is the time-exact, model equation. Okay? Now, consistency requires. That tau be written as again, a, a, a 2 vector, c. You can think of it as c being two constants, c1, c2, okay. c times delta t to the power k. 'Kay, where, or we could write this as tau1, tau2. Equals both functions of t n. All right? They are equal to c1, c2, times delta t to the power of k, where k is greater than 0, and c1, c2 are constants. Okay? Right? And, when I listed the various some of the members of this family, I said that, that the, their order of accuracy was 2. Okay? That order of accuracy is given to us by k. Right? So, when a couple of segments ago, I listed a, several members of this family and said that they all had order of accuracy 2, what it means is that in this consistency condition, the order of accuracy, k, is equal to 2 for all of them. Okay? Now, when we use consistency and stability to get a convergence, right, the so called Lax Theorem, right, which is that consistency and stability give us convergence of our time integration algorithms for this problem, right? That convergence condition is dependent upon writing out our error. E again is a 2 vector, right, because we're looking at the error in the displacement and the velocity, right, for every mode, okay? So, what we get is that e at n plus 1 equals A to the power of n plus 1, times the error at, at time t equals 0, all right. And, the error at time equals 0 is also a 2 vector because it has the error in the displacement and the error in the velocity. Okay? Plus sorry, it's minus sum i going from, from 0 to n. Right, you get i going from 0 to n, delta t, A to the power i, okay? The tau vector, which is the 2 vector I wrote just above here, at tn. Okay, this is the condition that we need to evaluate, we need to work with for convergence, and, everything works out just as we saw for the parabolic problem. Right? It's just that we have to deal with a matrix vector equation and then matrix vector inequalities here, but things essentially work out. Using what we know about stability and what we have written out for consistency, we can prove that limit of en plus 1 equals 0 as delta t tends to 0. Right, its limit as delta t tends to 0 of en plus 1, right, is equal to 0, all right? You get a 0 back to there, right? So you get the error in the displacement and the velocity tending to 0. Well, that's what we need to know as far as the analysis of this problem is concerned. And, that also concludes our rather quick study of methods for linear elastodynamics in 3D.

\section*{ID: IzvVVTTSu6Q}
There were a couple of errors in both work in this segment. The first error appears on this slide where I talked about how for damping of high-order modes, we need to go a step beyond merely, looking at unconditional stability. The inequalities that we are talking about in this case, appeared here, and the error is in this very last, inequality. I wrote it as just gamma plus one-half the whole square. It should be gamma plus one-half the whole square, divided by 4, okay? That gives us the correct condition to be satisfied for damping of high-order modes. The next error appeared two slides later. There we go. And it appeared in the way I labeled that equation. I called it as it appears there, the time-exact model equation. That is not quite correct. The equation I've written out there is obtained by substituting the time-exact, or the time-continuous solution into the time-discreet model equation, okay? When we do that, we find out, we discover that the time-exact solution does not actually satisfy the time-discreet model equation. Instead, we're left with this extra term here, right? And it's on the base of this term that we talk about consistency and accuracy of the method. So, properly this equation should be labeled like it should be labeled just what I called it, which is the time-exact solution, substituted into the model equation, okay? So, let me call it that time-exact solution. S-O-L-N, short for solution. Okay? Substituted. In time-discreet. Model equation. Okay? When we do that, we get this extra term. Like I, reiterated, a minute ago, and that is the basis for our analysis of consistency, and accuracy of the method. With that things workout consistently.

\section*{ID: UiZmpi9nTi0}
So if you've got as far as this particular video, I expect that you have also taken the time to watch many of those that preceded it and in the process have learned something about the finite element method. You may also have tried the quizzes and hopefully some of the programming assignments as well. If you've done a good proportion of all of those, you are actually pretty well prepared to go on and do other things with the finite element method, and also branch out to maybe learning about other topics which could then use the finite element method. Recognizing, of course, that what we've done so far is essentially meant to be an introduction at about the graduate level in any university. What I'd like to do with this last Lecture is point you to some resources out there, some of which you may already have known and maybe you know about all of these resources. But I'll just point you to some of them as things that you could go on to do having done all this work to learn the finite element method. So I'll start with this slide and this information in front of me. You see, I've talked about telling you a little bit about open source finite element codes and also about some related course material that's out there. As far as the open source finite element methods are concerned, you see these two links in front of you, The first is for Deal.II which of course many of you have probably tried out already, maybe for the assignments or maybe even on your own. And the other is for a different collection of softwares which I will also tell you about. So here we are on the Deal.II website and like I said many of you may already have seen it and tested things out on it before. Deal.II is essentially a collection of, as it says, open source code and libraries, which, with the background you have gathered, will allow you to go on and gradually build up more and more of your own repertoire in using finite element methods. You can go to this website yourself and browse around it. But let me tell you how you could very quickly start to be even more effective than you have been so far. If you click here on dev, you can see the drop down list and you go to tutorials. You come to this page, which actually sets you up to look at a number of examples with theory, the mathematics, and even code that will allow you to gradually go to more and more problems. There are a number of ways in which you can look at it and the way I like best is to click on the list here and you see there's an extensive list of examples of problems solved with the finite element method running all the way up to 50, 50 plus problems. So you could start out on with any of these, maybe you could even dive in if you think you've already accumulated some expertise and can afford to go on. So this is something that I would really encourage you to consider. And of course, you have been using Deal.II in a somewhat reduced framework as we were teaching you this introduction to finite element methods. Let's go back then and look at the other example of a software that I also have up for you, the FEniCS Project. So we click there, and it takes you to that website. The FEniCS Project is also a collection of free softwares, as you see right here and they explain what they mean by free software, and their philosophy and so forth. In this case you could go down and go along and look at applications. You'll see there's an extensive list here of different types of applications that you could use from this collection. If you scroll down, you see some things that you would begin to recognize, you see there is a solid mechanics library. There's something here for time integration, which you also know a little bit about. Is another kind of equation. There is this micromagnetics and so on, right. There's also software there about going massively parallel, and really many things. And the nice thing about Deal.II and FEniCS is the fact that they are open source, they are free, and they are also actually very advanced in terms of combining computational science with modern computer science. So I'd really encourage you to consider these two sites more closely. So let's go back then, to my slide here. So those are two of the open source softwares that I would really encourage you to consider. There are others out there of course, and by no means in mind am I saying that the others should not be considered. These are things that we use in our research group. Moving on then to courses. This course that you have been taking, the Finite Element Method for Problems in Physics, is going to continue in an on-demand format after it ends with this first iteration. And what that means is that will be available for you to start working with at any time that you like. You can start as you like and finish within a certain amount of time after that. You don't need to wait for a particular time when the course is going to be offered to start. These lectures are also available on YouTube, and in order to get to that let me do something slightly different. What I've brought you to is a page of a page on Open Michigan as it says, but let me show you how to get there. Well you can see how to get there, you just go to open.umich.edu, and that will bring you to a landing page where the University of Michigan has provided a whole host of open educational resources. Here, if you then go to the search bar and you type in Introduction to Finite Element Methods, I'd looked for it earlier. Do a search, shows you where the material is available. You click on Materials there and you see some of the lectures that you're already familiar with. If you click on the YouTube icon there, it will bring you to the whole series of lectures that you've been using on YouTube, right. So these lectures are already there, they actually were there even before our MOOC began, and I know that some of you had discovered these lectures, okay. So this resource is available. Also, with Open Michigan, we have provided a different series of lectures and this is a series called Lectures on Continuum Physics, okay? Search for that and there we go. Lectures on Continuum Physics. There we go. All right, so this is another series that we had recorded and provided, also about the same time that finite element lectures were provided. You click on Materials, and again you'll see all kinds of resources there. You see the video lectures. There are also assignments, which are available as PDFs. If you click on the YouTube link, likewise you go to this series on YouTube. Okay? Now, this series of lectures is also going to be shortly provided on a MOOC platform, okay, as we see here at the bottom of the slide. And this launch of lectures on continuum physics as a MOOC is expected to happen sometime during 2016. We're not certain exactly when, and this really depends on how quickly we can get all the pieces together. But the video lectures are already available, it's just packaging it as a MOOC that is going to take a little more time. So that's it really. I wanted to provide a brief indication of where you could go from these lectures that you've been following. And of course, there are many many other resources already available on the web, and we would encourage you to try as many of them as possible. That's it for now. And stay tuned. We will be back either with more in infinite elements or definitely, pretty soon, more on continuum physics.

\end{document}
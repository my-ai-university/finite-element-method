\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}


\section*{ID: X-8Af756-H4}
Okay, so over the last two segments, we've done the hard work of, first of all, defining what functionals are. Observing that we can define a free energy functional in the context of, of elasticity problems. And then we've also gone ahead and constructed the notion of the derivative of a functional with respect to a field. And this we said we wanted to do in order to be able to talk about extremization of three energy functional and thereby talk about equilibrium. Okay? So now we'll in the segment, let's just apply it to our function all right. So, extremization of pi, right? Let's actually compute it. We know how to do it, right? So here's what we want to do. What we want to do is find u belonging to s, such that, now, remember that that variation that we constructed was based upon our field w. Right? But w was really meant to just give us a certain type of variation. Right? Therefore what we want to say is that when we try to find the extremization of pi we want to find extreme point. Right? We want to find this field u that extrimizes pi but for all possible variations. All right? In a certain class. So what we say is that we want to find u belong to s such that for all w belonging to b, right? And recall that s satisfies the remember, s consists of u, such that u at zero equals u not. And then v consists of w/w of zero equals zero. Okay? All right. Find u belonging to s such that for all w belonging to v the following holds, right? d d epsilon of, pi u sub epsilon. Evaluate it that epsilon equals zero, right? As to the extremization, this is the derivative that we've written out. What is the extreme condition? The derivative vanishes. Okay? Let's do this now. What this implies then is that d d epsilon of pi all right, and I'm going to write out pi. And let me give myself big enough brackets here. Okay, d d epsilon of integral over omega, okay? Here we have one-half EA. Now it's not just u, but u plus epsilon w, the whole thing, x squared d x minus integral over omega f times u plus epsilon w A, dx minus tA times u plus epsilon w, okay? All of this, we take those derivatives, set epsilon equal zero, right? And then say that all of this should be equal to zero. Okay? That's what we need to do. Now, observe that in computing this derivative with respect to epsilon, the integral really does not pose us any problem. And this is because epsilon is not a field, right? It's just a number. If you would actually go ahead and compute those integrals you would get some value depending upon epsilon being a real number. And you just differentiate with respect to epsilon all right? So we're actually free to carry to take a derivative with respect to epsilon into those integral signs all right? What this implies then is the following all right? Okay. We have here right? So now let's take this derivative inside, right? So we have here integral over omega one-half E a, now because of that square. We get 2 u plus epsilon w, x right? Times w, x, right? Dx minus integral over omega, F Times w A dx minus tA times w, okay? All we've done here is recognize that epsilon belonging to real numbers is just a it simply multiplies out each of these integrals. So we're free to take the derivative with respect to epsilon and into the integral sign and this is what we get, all right? So now after computing all of this, we have to set epsilon equal zero. All right, and then the extremization condition is that this result is equal to zero. And just to make it clearer, let me draw a partition here between the previous line and this one. Okay? Now of course, the sum simplification that one can get. All right now, using epsilon equals zero in what we had on the last slide which is right here. Okay? We observed that epsilon shows up in only one place here, right? So setting epsilon equals zero. Reduce this integral to the following form. Integral over omega. Now, we have EA u comma x times w comma x, dx minus integral over omega, fw A dx minus t A w. Okay, this is what we get by setting epsilon equal 0. The extremization condition is that this is all equal to 0, okay? The final step, I'm going to reorganize this, just one of these integrals, into the following form. W comma x, EA, u comma x, dx minus integral over omega w f A dx minus. Oh, I just realized that here we had, since we had u evaluated at L, what we get here is w also evaluated at L, okay? And so it is on the previous slide as well. Okay, so here we get w at L dot t A, right? All of this equals 0. All right, now you may have already noticed it, but then, what we have here is thanks to our constitutive relation, it's just sigma, okay? So finally we get integral over omega, w comma x, sigma, A dx minus integral over omega, w f dx. But there is an A here, minus w at L, t A equals 0, okay? And you recall that this is the condition we said must hold for extremization for all w, okay? And you recognize that what we have here is simply the weak form. Okay? So it is that when we have working for us, a so-called variational principle, right? And in this case, the variational principle is that the free energy functional is extremized, right? When we have that variational principle working for us, we can indeed derive the weak form from a using these so called variational arguments by starting out with the energy functional, the free energy functional. So remarks. When an extremization principle. Is available. Right, and in this case, what is the extremization principle? It is the ex, extremum, or extrema, of the free energy. Okay? So, when an extremization principle is available, the weak form. Can be obtained Using variational calculus. Right? And often this is combined into saying that a variational principle exists, right? One says, therefore, that a variational principle exists. Okay? Let me see. All right. So this is a useful thing to know. Now, what we see in this case is that, of course, we get back the weak form when we invoke elasticity. However, we also know that the same weak form works for other problems as well, right? It works for heat conduction. It works for mass diffusion, right? So one can ask the question, what about those problems? The fact of the matter is that there does not exist a physical variational principle for problems of heat conduction and mass diffusion, right, at least not one that talks directly of extremization of certain free energies. All right? Those, some of these arguments are applicable to mass balance or, or to mass transport, but in a more restricted sense, okay? A separate series of lectures addresses that question. But anyway, so, what about those problems? Well, one can use the same weak form, but one cannot demonstrate that it, that it is avail, that can, that it can be obtained from the sort of variational principle. Therefore, we tend not to pose those weak forms as being derived from the variational principle. The issue with those sorts of problems is that the quasi-static or the steady state problem is only a very restricted case of heat conduction or mass diffusion. The full time-dependent problem does not subscribe to an equilibrium state and therefore, these kinds of principles are not applicable. All right? So pertaining to that, I will state that this. Derivation is not appropriate. For the physics of our heat conduction. Or mass diffusion. However the mathematics works, right? You may, you may simply ignore the fact that a physical principle does not exist and just say, well I don't really care. I'm going to define something that I call an energy or an energy like quantity for heat conduction or mass diffusion. And go ahead with this process, right? So the mathematics works, but the physical principle does not exist, and therefore we tend not to present it as such, okay? So let me just state that the mathematics, however, works. The fact that this principal of this variation principal exists for problems with elasticity is a very powerful one. And it is used to construct much more general and, and indeed more powerful finite element methods, variationally based methods, for problems involving elasticity, okay? And that is the topic of a more advanced class in finite element methods. I should probably also state that problems for which variational principles exist are. The following, or at least some problems for which variational principles exist. For elasticity, right? In all its forms. Okay. Property actually that exists for elasticity at steady state. Okay, variational principles exist for elasticity at steady state. But once you talk mode elasticity where there is nonlinear elasticity, or linearized elasticity, or elasticity. With other constitutive conditions added on to it like velocity or damage or physical elasticity or other things. It works. Okay? Another problem in which it also works is the Schrodinger equation at steady state. Okay? Schr√∂dinger equation, right? Also at steady state. Okay? And therefore, for these kinds of problems and, and some others, as well. One can construct variatonal principles sorry, one can construct basic as well as more advanced finite element methods using this approach. Okay? It does not exist, for instance, for problems of heat conduction mass diffusion, like I said. Also, the problem of fluid proof, right? For the  equations there does not exist a variation of principle. All right. That's about what I wanted to say in this segment, and indeed for this unit. At this point we're done with everything we wanted to work on for 1D Linear Elliptic Problems. In the following unit, we will embark on multidimensional particles

\section*{ID: URy5pgetGHI}
With this segment we begin our treatment of problems in multiple dimensions, and we're going to go straight away to three dimensions here. The problem we'll start out with is in the context of mathematical description of PD's. Also a linear, elliptic pde in three dimensions with the further specification that the unknown we are solving for is scalar. Okay, so that's what I'm going to write out, and we'll start with it. So we are now looking at linear, elliptic, pdes in three dimensions, Okay? With scalar unknowns. Or scalar unknown. Right? Because it's a single unknown we're solving for. So linear, elliptic, pdes, three dimensions, scalar unknown. Right? The canonical problems that are, canonical physical problems that are des, that are described by this sort of pde include heat conduction. Right? Steady state heat conduction. And also at steady state. The mass diffusion problem. All right? Okay we'll plunge right into it. We'll start out with the strong form. Okay? So, the strong form of the problem we are looking at is the following. So, let me deal with this. Before I start writing out the strong form, let me just right, sketch out the domain that were trying to solve things on. Just as for the one deal in your elliptic problem, we sketched out this this idea of a bar sort of embedded in a wall, right? Fixed in a wall, So let me do that. Now, because we are doing things in 3D we are going to make use of vector notation. All right? So we have here. What I would refer to as our basis vectors. Right? And these will be denoted by e1, e2, e3. Okay? Note that the notation I use for vectors is to put an under bar. Right? On them. That's, that's just the convention I follow. When we get to needing tensors, I will do the same thing, and we will simply distinguish between vectors and tensors by context. Okay. So the domain we're interested in is a some arbitrary domain, and I will draw what, in the context of continuum physics, is often referred to as a continuum potato. All right? So we have that here. The domain of interest here is labeled omega, as before, except that omega now lives in 3D. Okay? So let me write out things here. Ei where i equals 1, 2, and 3, right, the set ei why, where i equals 1, 2, 3 constitutes. An orthonormal. Cartesian basis. Okay? Do you recall what orthonormality means? The ortho refers to perpendicular, and normal refers to unit magnitude, okay? So what that means is that if we look at ei, dotted with eg, this is equal to delta ij, where delta ij is the chronicle delta. Okay? And you remember what the particular properties of the Kronecker delta are, right? Where delta ij equals 1 for i equals j, is equal to 0, for i not equal to j. Okay? And we note that this directly covers the orthonormality property, right? Because if i is equal to j, it says that each of these e's, e 1, e 2, e 3, is of unit magnitude, right? And if i not equal to j, tell us that they are perpendicular to each other. Okay, so that's what is implied. And here I will make it more obvious by doing this. Okay, so these are perpendicular to each other. All right Cartesian, for our purposes, simply means that they are fixed. Right? The basis vectors do not change. Right? They are fixed in space. All right let me also, just for, the purpose of making this completely obvious, state that we are doing all of this in three dimensional ambient space. Okay? So this is the setting for the problem we want to consider. I have some props to help us through the rest of these lectures. That represents our basis. Right? You can think of this as E1, E2, E3. Each of these are, is of unit of magnitude, and they're, of course, perpendicular to each other. The domain of interest to us is this one. Okay? This is our representation of the continuum potato. It happens to a, to be a University of Michigan Nerf or not Nerf, but a, but a University of Michigan foam football. All right? But this is the domain over which we will be describing everything happening. And this is omega, for our purposes, all right? The, the other thing that we will need about omega is we will, we will repeatedly use the, refer to its boundary. Right? So, omega is as before with our domains, omega is an open set, right? So when I talk about omega, I don't include its boundary. Right? We will use certain notation for the boundary of omega. So let's put down that bit of notation. The boundary is going to be denoted, partial of omega. That does not imply that we're taking anything like a derivative, it's for our purposes, it's just notation. All right? Okay. So that's pretty much what we need to begin with. Okay, so yeah, maybe I should just say one more thing here. Omega is open in R3. And if this is the first time you're encountering it, I will also say that partial of omega is the boundary of omega. Okay, so this is the setting we have, all right? In this setting, what we are trying to do, is the following. Let's state the strong form. We are interested in finding some function, u. Okay? Find u, given some other quantities. Okay? So we are going to be given f again. F, as before, is a function of position, except that now position is a position vector. Okay? It's going to be denoted as X. All right? And 'kay, at the risk of going back and forth, let me just add one more thing to that figure I had on the previous slide. I am going to say that some point, x here, has a position vector. Right? So, that's the position vector of point x. In the context of our props, okay? So we're talking of the position vector from the origin of this Cartesian basis, to some arbitrary point in this domain. Right? In our continuum potato and omega. Okay? So, so, okay, so we remember that this is where the basis is, we're talking of the vector from here to there. Okay? That, that is what we, we, we have in mind. I could use this as a prop, right? So if, if, if, if the tail of the spin, the top of the spin is where the Cartesian basis was, and we want to talk about this point, we have that. Okay? So that's the cartesian vector, sorry, that's a position vector for x. Okay. Right. So we need that because we want to talk about the dependence of this forcing function. Right? So the forcing function is a function of x, it's, it's parameterized by x. It can be defined at, at at any point over the terrain. So we're given f f of x. As before, we are given u sub g, all right? And we are given, j sub, n. Okay? We're also given the constitutive relation. Okay? We're given the constitutive relation that I will initially write using what is called issue, or coordinate notation. Okay? We are given a constitutive relation j sub i equals minus kappa ij u,j. Okay. Alright. Where again, since this is sort of the first time I'm writing it out, I'm going to tell you that i, j equals 1, 2, 3. All right? We're given all of this. All right? Right. So we want to find u given all this, this information, such that, such that, again, sticking with coordinate notation minus ji,i equals f in omega. All right? And we also have boundary conditions. Right? B.C. for boundary conditions. U equals u sub of g on partial of omega sub u, and minus j dot n equals j sub n on partial of omega sub g. Well that's our strong form. Now obviously, I have a lot of explanation to do here, right? I've introduced all kinds of terms, and I haven't really, and I need to make, make them clearer. Let's begin let's begin with something that we already know. Let me say, let me tell you more about what, what I mean by decorating the boundary with these subscripts u and j. All right? So we have our basis again, this is something that I'm going to end up drawing repeatedly, at least over the next couple of segments. Okay? We have e1, 2 and 3, we have our domain omega, right? And now the boundary is partial of omega, and rather than mark it as partial of omega, I'm going to tell you what partial of omega u and partial of omega g could be. Let us look at that actually let me make that an open interval, sorry. Let me look at that interval. Right? So that would mean all the points that lie between those two braces, if you would, just walk around the, walk around the boundary. Rght? That is what I may choose to call partial of omega u, and the compliment, right, of that set, is what I'm calling partial of omega j. All right? What this simply means is that we've taken our boundary and partitioned it into disjoint subsets such that they're disjoint, just like I said. So partial of omega u intersection, partial of omega g, is the empty set. Okay? Phi here denotes the empty set. This symbol denotes intersection, okay? So the intersection of these two, subsets is empty. And, partial of omega is always written as partial of omega u, union partial omega j. All right? So really, if you think about the way we've marked out these boundary subsets, I've chosen to make partial of omega u open. But that means the partial of omega j is closed, right? In order to make sure that we don't lose the boundary points between partial omega u and partial omega j, right? Right? And, and of course their union gives us the total boundary, partial omega. Okay? Just a way of, you know, splitting the, the boundary. And then what we're saying is, when we go back to these boundary conditions ,what we're saying is that we have we've specified u equals u g on one part of the boundary, okay? And actually let me take off this brace bracket. If you're specifying u equals ug on some part of the boundary, what kind of a boundary condition is that? Recall, from our treatment of the problem in one dimension? That's right, that is the Dirichlet boundary condition. Right? And the other term, the other boundary condition, is our Neumann boundary condition. All right? Okay. So this is what we mean by the boundaries. Right? By the boundary subsets and the boundary conditions. Let's go back now and talk about the other quantity we introduced here without much without much fanfare, j. Right?

\section*{ID: IHCOKP-j7ZE}
J i is what we will call a flux vector. Right, in. Coordinate notation. 'Kay, and for coordinate notation i equals 1, 2, 3. For our purposes, this is just a way to specify explicitly, the components of a vector in three dimensions. Okay if, if we were doing a class in continuum physics. If we, we were doing lectures in continuum physics, this would be set up much more carefully than that, right? But for our purposes, it just denotes the components of a vector. Right? And when we explicitly refer to the components of vector that sort of notation is called coordinate notation. All right, so then what they're saying is that j if we wanted to write the vector and so called direct notation, okay? J is just the collection J1, J2, J3, okay? And this is what we call Direct notation. If properly the left-hand side of this last equation J equals J1, J2, J3 is direct notation for the same vector. Okay, so we will often switch back and forth between the, between these two type, between these two notations, though for what we need to develop we are going to use coordinate notation a little more than direct notation. Okay? But they are essentially just different ways of describing the same thing since J is a vector there, there is also a way of describing the J as a vector. To simply saying that J, being a three dimensional vector, j belongs to R3. Okay. Just this notation, all that means is that j is a 3D vector, okay? You know that exactly the same thing can be said for x as well, right? So, likewise, we have x equals x1, x2, x3, arranged together as a vector. Right, and x belongs to R3, and this thing has more meaning because not only is is x a vector, a three dimensional vector, but x also is a point in three dimensional space right, so this has even extra, even more meaning here really, more physical meaning. Okay. That's what we have. Now let me see. What else do we need to state here? Okay. In order to say more about this it's actually useful to go to a particular physical problem that this PDE could represent, okay? So in order to say more about this let's consider heat conduction. Heat conduction at steady state. Right? In 3D right. Let's suppose this is the problem we are talking about. One thing you recall of course that steady state simply means there is no time dependant right? The time dependent has been dropped from this description of heat conduction. Okay, so this is the keys what is u? What does u represent in the, in the problem of heat conduction? Can you recall from you recall from your study of heat conduction previously? Right, it's the temperature. Okay, J then is right the heat flux vector. Right, which is essentially the amount of heat. Crossing perpendicular to a unit area. Per unit time. Okay. That is J, right? Now, when we have the constitutive relation. Ji equals minus kappa ij u comma j, okay? This represents for us now, it essentially tells us that the the heat flux vector is driven by the temperature gradient. Right, so u comma j u comma j you recall cause this just partial of u with respect to xj. Right, so that's the temperature gradient. This constitutive law goes by the name of the Fourier law of heat conduction. Okay? Now, here well, let me just state this here, this is the temperature gradient. Temp as short for temperature, Okay. Do you remember what kappa ij represents here? What is it called? Right, in general, it's called the conductivity tensor right, or the heat conductivity conductivity tensor. Okay? Right. This a denser can be thought about again in our setting for the purposes we need here it can be taught off as say generalization of a vector and with the provisional basis that we have, what we find is kappa ij can be well that's not really right. With use of a base is that we have here, we can also write the heat conductivity tensor using direct notation, it would be kappa, right? Also with an under-bar, and like I said by context, we will understand whether something a vector or a tensor. Here it's a tensor, why do we know it's a tensor, because of the way it acts in this equation. Okay. So capital is direct notation for the heat conductivity tensor, and just as we could represent, the direct notation for a vector in terms of its components or related to coordinate notation, kappa can be written with the use of a basis, which we have, as a matrix. All right, and that matrix consists of the components kappa 11, kappa 12, kappa 13, kappa 21, kappa 22, kappa 23, kappa 31, kappa 32, kappa 33, okay? Now, it turns out that this tensor kappa is symmetric and their reasons for it to be symmetric so we would consider Kappa to be symmetric, which me, which we write as kappa equals kappa transpose, right? So this means that in general that the 1 2 component is the 2 1, 1 3 is equal to 3 1, and 2 3 equals 3 2. Right? Which is also written in coordinate notation as kappa ij equals kappa ji. All right, kappa has another property which is important for the physics of heat conduction, 'kay. So not only is it symmetric, right? It is also what we call, positive semi definite. Okay, and what this means is that if C belonging to R3 is vector. Right? Then. If we construct the following quadratic product, right, we allow kappa to act on C. Right? And the product of kappa acting on c gives us back the vector, right? Just like if you know, you've all, you've probably experienced linear algebra, a matrix acting on a vector gives you back a vector. All right, so that's what's happening. So capital T is a vector, if we dot that vector with c 'kay, c dot capital C, is greater than or equal to 0 right, for all c, right. For any c. Right? Well, actually, I, I, I realize I don't really need to say for all c. All right. So if c is a vector, c.kappa c is greater than or equal to 0. Okay? What this means? That there are actually some directions right, for which we are allowing the possibility that there is no conduction of heat in certain directions, right. So we are allowing the possibility that for some c, right, some vector, vector c this thing could be 0, all right, okay. So and this, this the fact that we're allowing the possibility for this quadratic product to equal 0 is what makes it semi definite, all right. In terms of Neumann condition, all right? Do you recall what we are allowing the possibility for by including that product being equal to 0? What physical possibility are we allowing? It allows the possibility that for some direction c this material actually acts as an insulator, all right, that there is no conduction in some direction. All right, so this basically corresponds to. If c.kappa c equals 0. Importantly, this has to be equal to 0 for c itself not equal to the 0 vector. Right of course if c's a 0 vector then that's a trivial result. Right? But if there exist some direction c not the 0 vector for which this quadratic product is itself equal to 0 then, there is no conduction, no heat conduction. Along c, right, in that direction. Okay. The very last, well, actually there's mo, not the very last thing, but there's one more thing I wanted to say, here. When we go back and look at this law, here. This, of course, the Fourier Law of heat conduction. What it tells us is that the heat flux vector is directed along the negative temperature reading, right. So tre, heat tends to flow from high temperature to low temperature, provided we also have this property of c being positive semi definite, okay. So let me state that as well. Heat flows down a temperature gradient. All right, that's what the formula of heat conduction tells us with the additional condition that c is positive semi dense. All right, the last thing we need to state here to wrap up our our introduction to this strong form of the problem, is the boundary conditions, right. So when we, when we return to the boundary conditions, the first one is fairly straightforward. When we have u equals ug on the Dirichlet boundary this is simply a temporary boundary condition. Right, so in the context of our continuum potato. Right, we have our basis here, the continuum potato here, the, the region of interest. Let's suppose that the maze part of the boundary is the Dirichlet boundary or the temperature boundary. What you're seeing is that on this part of the boundary we have the, we're, we are controlling the temperature, right. We're setting the temperature to be ug, ug of course could be a field, right. That's important here. So what we're seeing here is that this could be a nonuniform field, right, so that's possible. Okay, that's allowed. And then for the second part of the boundary this condition can be written as we're writing it as let me see how I wrote it actually, probably sorry, and let's go back here. Okay, I did it right. Okay, this part of the boundary, when we wrote it in direct notation, I think we wrote it as minus j.n equals j sub n. Okay. What that dot? What the n there is doing? Is that it is the unit outward normal to the boundary. Okay. It's a field. It's a vector field. Right, that's what n is. So, when we look at minus j.n, what we're seeing is that given the fact that n is a unit outward normal, what we're seeing is that we are controlling the heat influx. Boundary condition, okay? All right be, because n is the u, unit outward normal j.n would be the outward heat flux, and the minus sign essentially makes sure that it converts that into a heat influx. We're saying we're controlling the amount of heat flowing over the compliment to the Dirichlet boundary over the Neumann boundary, right, so over the blue part of the continued potato. We are controlling the heat, the heat flux, the heat influx, right? So we're controlling me, that of the heat influx. We're controlling the amount we're getting there, right. Okay. So, so we'll note that we're not controlling the vector itself, right? We cannot control the vector, and that the theory of PDE tells us that, in fact, physics also tells us that we cannot control the entire vector right? What we can control is just the normal component of the, of the heat flux vector here, okay? I should also write this in coordinate notation, in coordinate notation is just the dot product, right. So it is minus ji and i equals jn, j sub n. All right and this is coordinate notation. Okay, I believe that completes our basic introduction to the weak, to the, sorry, to the Strong form. We will end this segment here.

\section*{ID: e1xzkNGA8Jk}
So, there was a question on why, when we go from 1D problems to multiple dimensional, or 3D, 3D problems we don't directly go up to, elasticity, whereas we really set up the 1D problem mainly in the context of elasticity, right? E, E, Even though we said it applies to other problems, heat conduction and diffusion as well in 1D, we did repeatedly make use of the, of, of, of the physical analogy of, of the physical, underlying phenomenon being, elasticity, right? The reason we don't directly go to three dimensional elasticity is because for 3D elasticity to be posed our unknown is the displacement, which is a vector. Right? And handling vector unknowns is more complicated from the standpoint of the PDE itself. RPDs turn out to actually be 3PDs for each of the vector components. The constigitive relations get a little more complicated, and actually the boundary conditions need some careful bookkeeping. And, as you may imagine, the same thing translates over, carries over into our finite element implementation. So, we will get to 3D elasticity, of course. We will look at vector problems in 3D but it's probably going to be a little easier having first gone through the scaler problem 3D. Okay? So, to be followed.

\section*{ID: 4lIS70sa6Xs}
Okay, so let's continue. In the previous segment, we began looking at the strong form of the problem for linear elliptic PDEs in three dimensions with scalar, with a scalar unknown, right? We wrote out the, the strong form, we wrote it out in coordinate notation, learned a little bit about the quantities that show up. And the very last thing we did was make a connection with the, the physical problem of heat conduction at steady state. Let's carry on with this and what I want to do first in this segment is for completeness, also make the connection with mass diffusion in three dimensions, okay? So just as we looked at heat conduction, let us now consider the problem of mass diffusion. Okay? Of course, the, the, the strong form is the same so we're not going to rewrite it. Let's just say what the variables would translate to here, u in this case would be a concentration, right? Concentration of some field. All right, now it could be a concentration either in terms of mass or unit volume or maybe number of particles of a certain kind per unit volume. Al right, so this could be a concentration in terms of mass over unit volume or number. Volume unit for u or it may even be, normalized by some reference concentration to be rewritten into something to call a composition, right? So I will just write this here. Composition is a essentially a re-parametization of the composition that is often used in physics, okay? So that's what you would be right? What about the j? Now that we know direct notation it's safe to use still, to just do that, right? J in this case would be the mass flux, right? The mass flux or the number flux. Right? Which would be, mass flow perpendicular, perpendicular to a unit area or unit time. Okay, or alternatively, if you were doing number flux it would be the same sort of thing, right? We would be talking here of the number of some particle flowing perpendicular to a unit area by unit time, right? So number flow Number of particles Flowing Perpendicular to unit area and so on. Right? You can complete that statement, all right? So that's what j would be. Our constitutive relation j equals minus kappa. Now, now if actually, I realize that this is probably the very time I'm writing the entire constitutive relation in direct notation, okay? So, the way we would write a temperature gradient would be that, right? Being the spatial gradient operator, okay? Could be this or perhaps you are familiar also with writing that gradient as partial of u with respect to the vector x, okay? So if we had this constitutive relation kappa then would be the diffusivity tensor. Okay? And as we did from the case of heat conduction, we would make the observation kappa equals kappa transposed. It's symmetric and it's positive semi-definite, right? So, if c not equal to zero and c being a vector, okay? If, if c is a non zero vector, then c dot kappa over c is greater than or equal to zero. okay? To the same, the same mathematical properties as the heat conductivity tensor. All right, and then the condition that u equals u, g, would simply be a condition of stating that we are controlling the concentration or the composition, right? On Dirichlet boundary, all right? On the specified sub set of the boundary. And finally saying that j, sorry, minus j dot n equals j sub n on omega is sub j would be the mass in flux boundary condition. Okay? So, you know, everything else is well, everything else is really the same between the heat conduction and the mass diffusion problem in 3D, okay? At steady state it turns out, actually, when we, when we take away the steady state assumption also there is this analogy between the two. And of course the equations would be the same, we will study those equations as well and of the finite element methods for them. So, okay. So, at this point we've looked at the strong form and, and coordinate notation. We've also looked at some direct notation just for completeness let me write out the PDE of the strong form in fully indirect notation, okay? Okay, so this is, find u given u sub g, j sub n, f, and it's constitutive relation J equals minus kappa, u, okay? Find u, given all this stuff such that Now, j, I, comma, I, is essentially the diversions of j, 'kay? So we get minus del dot j, right? And del dot is the diversions operator in direct notation. Equals f in omega, 'kay? The boundary conditions, the  boundary condition is straightforward because it's only on the scalar unknown. U equals u sub j on partial omega u and the boundary condition is minus j dot n equals j, j sub n on partial omega sub j, okay? All right, and this actually is it, right? For as far as our strong form is concerned in direct notation, okay? And one, one useful thing to look at here is what happens to this equation when we make the substitution of the constitutive relation, right? So when we make the substitution, we so substituting. J equals minus kappa grad u in the PDE, right? We get minus del dot minus kappa grad u equals f, right? In omega, okay? And then something that's commonly done, is that if if kappa is specially uniform, right? So that means kappa is not a function of position, right? What does gives us then is the following equation, right? It gives us minus del, sorry. It gives us, if kappa is uniform, it tells us that we get kappa contracted with the hessian of u, okay? This equals f, all right? Now it's useful to write this coordinate notation to see exactly what is meant here. That cont, the the double dot there which I've referred to as the contraction essentially is an extension to two tenses of the idea for that product, all right? And what this thing is doing for us is, it is doing kappa, i, j, multiplying u, i, comma j, okay? In this context, what we have here, that is the hessian operator. Okay? We could also take the special case, which is often done. Which is that which is to assume or to consider cases where kappa i j is can be written as a scalar kappa multiplying the chronicle delta, okay? Sometimes called the Kronecker delta tensor. Okay, when we do this, what it implies for us, is that now if we go back to the problem, where that we wrote just about, the form that we wrote just about. And here we're noting that not only is the tensor kappa representable as a scalar multiplying the chronic of delta. And that furthermore here too we're seeing that kappa is the, the scalar kappa is uniform, right? If this thing is uniform then what we arrive at is kappa delta ij, u comma ij. All right? Equals f, right? And here if you observe what the action of the Kronecker delta is it reduces that relation, that PDE to kappa u comma ii equals f, okay? In omega. Okay? All right, so this from of the equation is often called the Poisson equation. Okay, so this is good. Okay, in direct notation the same thing will be. It would be kappa. Now, there's square u but note that I do not have an under bottom on the square, implying that it's not a tensile this is just a Laplace operator, right? It has very a few  Laplace. Okay, all we've done is write the, the Laplacian using that operated , okay? So and of course, in this sort of case if we have indeed that kappa is that the tensor kappa can be represented as a scalar kappa, right?. If we say that we have that representation then for the Neumann boundary condition. Right, so the Neumann boundary condition which is minus j dot n equals j sub n. This is actually reduces to the requirement that minus kappa grad u dotted with n equals j sub n, okay? It reduces in this case to a requirement on the normal radiant of temperature, okay? So this thing is this is the normal, normal gradient Of the temperature u, okay? And these are simplifications that are commonly used. In fact, it is very often assumed that kappa does indeed have this representation by the scale of kappa and the chronicle delta, right? And what this translates to, is that the physical the physical sort of situation that this represents. So we have kappa equals that, sorry. If kappa ij equals just the scalar kappa, multiplying the chronicle delta, what we have is, what is called isotropic heat conduction Okay? We're specifying here that this body is such that when heat is flowing in here, heat flows only in the direction of the temperature gradient, right? So if we in, int, introduce a temperature gradient in one direction, it does not induce heat flow in any other direction, okay? That's, that's what it reduces to and furthermore, it also says that the amount of heat flow we get by introducing a temperature gradient in one direction is exactly the same heat flow that we get. By introducing a temperature preheating in another direction. In both cases, the heat flow back would align with the direction of the temperate preheat itself. And the amount of heat flow would work to be the same, if we specify that the same temperature created in different directions. Okay? So, those are the sort of simplifications that are often used in representing heat conduction, all of those equivalently mass diffusion as well. Okay, we're actually going to end the segment here when we return and pick up the next segment. We are going to find ourselves ready to look at the weak form of the problem.

\section*{ID: KVzu-zulHdY}
There was an error in board work on, this particular slide. It's a very simple error. It's one of the most common ones, however. It occurs right about here. And, the error is that our definition of j, itself, is that j is equal to minus Kappa grad u. As a result, the minus signs cancel out, and what we have in the next line is plus Kappa grad u.n equals jn. Okay, so with that little fix, that slide is all correct, and so hopefully is the rest of the segment.

\section*{ID: Hig3Gn8Ls6U}
Welcome back. So, we now find ourselves ready to look at the Weak Form of the linear elliptic p d e for a scalar, for scalar unknown in three dimensions, 'kay? So, let's just state the weak form and, and, and move on with the thread because we've written the strong form, and at least a couple of times, and in different ways, so All right. Here is the weak form of the problem, 'kay? I'm going to first state it, and then, obtain it, right? So, find. U belonging to S, right? Where S is equal to space of functions, right? So, equal to some functions u, such that u at u equals u g on The Dirichlet boundary, 'kay? Find u in this space from the space given everything else, right? Obviously we're given g, u g. We're given j sub n, right? We'll be given the data f of, and the constitutive relation. Our constitutive relation now is j sub i equals minus kappa i j u sub j, okay? Given all this, right? Find u such that Such that, for all w belonging to V, where V consists of space V consists of holds functions w. Where w, again, just as we saw before for the waiting function, vanishes on the Dirichlet boundary. Okay, find u belonging to S, given the data such that, for all w belonging to V, the following condition is satisfied. Integral over omega w, i j i d V, okay? Equals Integral over omega w f d V, okay? Minus integral over omega, sorry, integral over omega sub j, w times j n d S, okay? Some things to note here, first of, this is this is our weak form, 'kay? Some things to note here are that we have here the elemental volume d V, and d S here is the elemental surface area, 'kay? That should be clear because on the golf course is a subset of three dimensional space, it's volume in 3D. And therefore, it's boundary partial omega sub j, that's subset of its boundary, is indeed a surface, right? Okay, so, that's the situation we have, right? So,  d V is the element of volume that's, that allows us to integrate over the, over this entire volume. And d S is every little elemental surface area, okay? So, this is the weak form, what I'm going to do now is get us to it, right? I'm going to show, show you how the weak form is obtained, right? As before, we are going to take, we're going to take one of the approaches that we took before which is to start out with the strong form, which you've already put down, and get to the weak form, okay? So, here's how we do it, right? So, we start out by considering Consider the strong form. Okay? We have used this stuff, right? So, we have find u Given all the data And I'm just going to put down the constitutive relation here directly without calling it a constitutive relation Right? So, given all this stuff, find u such that, st is short form for such that. We have minus j i, i equals f in omega. Because I'm a little short of space here, but I want to have it all on this slide, I'm going to put down the boundary conditions here. Dirichlet boundary condition, and our Neumann boundary condition Okay? This is what we have. Now, the approach we are going to take is the one that we took formerly, the, the approach that we took in obtaining a weak form from the strong form for the 1 d problem, okay? All right, so, what we have, is we have the strong form. What we are going to say, is consider W belonging to V, right? Where V, of course, is the, is the has the properties that we stated before. It consists of all functions that satisfied the homogeneous Dirichlet boundary condition. Right? That's what that is. Okay? Now just as we did for the 1 d problem, what do we do here? Do you recall? That's right. We multiply the pde of the strong form by w, and we integrate by parts, right? So multiply. Pde. Right? To the strong form strong form S f being short form, for strong form of pde. Okay? Multiply the pde, the strong form of the pde by w. Right? And then, we integrate by parts. Okay, that's what we do. So, let's do it in steps, right? First let's multiply the pde, and integrate it over the volume, and then we'll invoke integration by parts, okay? So, integrating it, by multiplying it by w, and then, integrating it over the volume, we have this, integral over omega minus w j i comma i d V. Right? We took the minus divergence, from the left-hand side, and multiplied it by w. Okay. And this is equal to integral on the right-hand side, integral over omega w f d V. Okay? Okay, down here is multiply the pde by w, and integrate over the volume. Okay. We're now, going to do integration by parts, and if you recall, the application of integration of parts, and functions in three dimensions, right? Where the integrations happen in three dimensions. We of course do that to the left-hand side too, right? So, this is the one that we are going to integrate. By parts. Now, when we did this in the 1 d problem, I did mention I think, that the integration by parts is really nothing other than an application of the product rule of differentiation. Combined with the, the divergence theory. Okay. That indeed holds in, in multiple dimensions as well. And to bring uh,it's useful to bring that out, because it makes very clear what how, how the integration by parts proceeds. So, we'll do that. Okay? So, recall here that when we say integration by parts, what we are talking about doing, is the product rule And Divergence theory. Okay? Firstly, the product rule tells us the following, right? It tells us that we really need to look at w multiplied by j i comma i, as being just one term, in applying the product rule of differentiation, right? So, it tells us that, that really is integral omega. W j i, the whole thing, comma i, and we should make sure not to forget our signs, right? This plus w comma i j i. Okay? So, if you expand out the first term in the integrant, you will see that it can, that one of the terms it produces because of the product rule cancels out the second term. Right? And we're left with, what we had in the first line. Okay? So, this is equal to integral over omega w f d V, okay? But then, when we stare at our left-hand side integral, we realize that we can actually, view w j i as essentially a vector, which it is. Right? Because j is a vector. Right? Soum, this is a vector. Right? Component i. And that term, is essentially the divergence of w j. And there, we invoke the divergence theorem. Okay? All right? So, this step is really just a product rule. When we do the divergence theorem, we get the following. Right? The divergence theorem tells us that this integral over omega is really an integral over, over the boundary of omega. Okay? Minus integral over the boundary of omega of w j i n i. Right? Where n i, is of course, our unit outward normal, something that we'd observed in one of the previous segments. Right? It's this, integrated over the surface of the body, plus next term remains a volume integral. And the right-hand side stays the same. Okay? And where, how we get this is through the Divergence theorem. Okay? What we are going to do next, is take the surface integral. Sorry. Take this surface integral over to the right-hand side. When we do that we get, integral over omega w comma i j i d V equals integral over omega w f d V plus. Now, that integral over the entire surface partial omega, I'm going to break up two integrals over each of the two subsets, we've identified for the boundary, right? And those subsets, you recall, are the following. One of them is the Dirichlet boundary partial omega u, right? And so, here we have w j i n i d S plus integral over the Neumann boundary w j i n i d S. Okay. Now, let's stare at the last two boundary integrals. What can we say about the first one? What can we say about the first integral? Think about it. It's over the Dirichlet boundary. It's w j i n i d S. Right, we can say that over the Dirichlet boundary because w, we are picking to live in the space V, which satisfies homogeneous Dirichlet boundary conditions. We have that, right? So, that first term, that first integral drops out. What about the second integral? We're starting with a strong form, which includes the boundary conditions, explicitly, right? And in particular, the way we wrote out the boundary conditions we specified that the heat influx vector, which is minus, it's not the heat influx vector. The heat influx minus j i n i on the boundary, right, on the Neumann boundary. Is, heat influx, if you're talking the heat conduction problem, right, and mass influx, if you're talking mass diffusion. Anyway, j i n i, we identified as being minus j sub n, okay? So, when we put these things together, we arrive at the form that I had put down, originally, right, I think at the top of the previous slide. Integral over omega, w, i j i d V equals integral over omega w f d V, okay? Minus integral over the Neumann boundary w j sub n d S, okay? This was the, what I posed originally as the weak form, okay? And so we've got there, right? We've obtained it from the strong form, right? Now, in the case of the one d problem, we also went the other way, right? We demonstrated that the weak form, and the strong form are completely equivalent, and indeed, that holds in, in this case, too, right? It holds for actually every problem, right? The strong form and the weak form of any PDE are completely equivalent, all right? And we can adopt the same approach in proving it, in this case, as well. So, we could start out with this weak form, we would have assumed the space V. We would be given the data. We would essentially do integration by parts in reverse. And then we would go through the tricky business of invoking those variational arguments, right, on how did the, on, on, on the manner in which w can be chosen, right? Or, or, or the very fact that w has to hold for, for, for all functions living in V, allows it to state, state that it also holds for certain spes, special functions, okay? And that is what would bring us to the strong form. We're not going to go through that argument here. It holds, you can try it on your own if you like. But it, it's not crucial for what we want to do, which is, of course, to work with a weak form, okay? So, we're just going to stop here, as far as deriving the weak form is concerned. Let me also, make a remark. Let me just state here, actually, it's more than just a remark, so I am going to state it in sort of the main text, so to speak. Which is to say that the weak form. What we demonstrated is that the weak form is implied by the strong form, right? This is, this is, this is what we just demonstrated in the, in the little derivation that we did. 'Kay, one can prove also that the other direction holds. Okay, it's not difficult. You can just follow the notes there and, and follow the steps we took in the one d case. Okay, we're not going to show this, this, we're not going to show the right-hand side implication. We demonstrated the left-hand side implication. Okay, well, this is it, right? So, so this is, this is our weak form, and this is what we're going to work off to, to develop the finite element method. That will however, be best done in a different segment. Before we end this segment, there's just one remark I want to make, which is actually, applicable to the, to the strong form, okay? Just a remark I want to make here, which is that, if you look at, recall the PDE of the strong form. PDE of strong form, recall the PDE of strong, the strong form, right? Which is minus j i, i equals f, right? Or in in direct notation it is minus del dot j equals f, right, in omega. I just want to recall for us the interpretation here, now,  you know, that if we take this body, and we look at the PDE that we had the PDE really holds point wise, right? So, it holds over every little infinitesimal volume in this over this domain, right? So, if you will go inside, cut it open, and take a little volume, that PDE the, the strong form of the PDE holds over that little infinitesimal volume, 'kay? And what I just want you to do is recall for us here the interpretation of the minus divergence, all right? In this, in this, in this setting in 3D, the minus, the, the divergence itself, vector is the, is the total net outflux over that little volume, okay? So, the negative divergence is the total, is, is the net influx into that little volume. And all this PDE is saying in strong form is that their net influx into every little infinitesimal volume is driven by what we recognize to be a source term, okay? So, let me just do that little make that little argument, too. All right, so, if we have our, this is, this is our body, omega. What we're talking of doing here is considering a little volume element. Okay? And we are, the, the net influx is. Is obtained from, you know, by considering, with, with this little pill box type argument, which is sometimes presented often in the context of classical fluid mechanics. And sometimes, maybe also in the context of heat conduction, right? So, this is the net influx, right? So, so this net influx here is denoted by minus divergence of j, okay? So, what we're seeing is the net influx, right, over a little volume equals the source term, right? Or minus del dot j equals the source term, which is f, right, at every point in omega, right? So, that little pill box there represents a point in omega, right? So, we may choose to say, that okay, that has the position vector x, right? So, the point out there has the position vector x that, that little pill box has been constructed about that point x, okay? All right. I just wanted to recall this argument of this, this interpretation because I omitted to mention it. Okay, so we're done with the segment. What we we've done very quickly actually is get to the weak form. When we return, you know how this is going to proceed. We are going to set up the finite dimensional weak form. We're going to first observe that this is the infinite dimensional weak form. We'll set up the finite dimensional weak form, and develop our finite element methods. All right, we'll stop.

\section*{ID: e6jm1iNtblk}
All right, welcome back. We continue with developing our finite element method for linear elliptic PDEs in three dimensions, with scalar variables. Okay? So, let's let's just start by recalling the setting, that we have. So, remember this is where we are. We have our basis vectors, we have our domain of interest, over which we are solving this problem. And, if we are done thinking of heat, heat conduction or mass diffusion again, we call that we have two different parts of the boundary. I think maybe I call the maze the Dirichlet boundary. This could be the Neumann boundary. Over the Dirichlet boundary, we're controlling the field itself. Either the temperature or the concentration field for the diffusion problem. Over the Neumann boundary, we are control, controlling the influx, right? The influx of heat or mass. All right, and we have distributed sources. All right. For, for, for either physical interpretation of the problem. Okay. So in the last few segments, we started out by looking at, we started out with the strong form of the problem. And then derive to weak form. I, I stated that they are completely equivalent. I showed the equivalent in one direction. From strong form to weak form. Okay? And we spent some time understanding all the different terms in there. So this is where we are. Where we were, at least. And what we're going to do today is pick up from that weak form, and take the steps that will eventually get us to our finite element equations for this problem, right. And as you will recall from the 1d problem what that means is that we need to work out the finite dimensional weak form. All right? And, for setting the context, let me go to our usual sketch. We have our bases. And that is our domain of interest, omega. We have some point in there which has position vector x. I'm not going to draw the position vector just not to get this diagram too busy, and we have here our boundary conditions. Bound different boundaries, right. We may have, we have the Dirichlet boundary, and the Neumann boundary. Okay? So this is the setting that we have and the, the infinite dimensional weak form that we have already derived is the following, right? I'm, I'm going to first write the infinite dimensional weak form, and then we get to the finite dimensional weak form, right? So what we've got as far as, is the following. We've said all right, let's what we need to do here is find u belonging to S, which includes the Dirichlet boundary condition. Right given all the usual data in the problem. Given, sorry I called this u not, it's ug. Given ug. Given our mass influx. Given our forcing function, and given our constitutive relation which is j equals sorry, I'm going to, I'm going to stick with writing this in coordinate notation. So I'm going to write this as ji equals minus kappa ij, u comma j, right. Given all this, find u such that, for all w belonging to V, which consists of w that vanishes on the Dirichlet boundary. Okay? For all w belonging to the, to the space V, we have to follow in weak form. All right? We have integral over omega. W comma i. Ji, dv equals integral over omega w, f, dV minus integral over the Neumann boundary. W, jn, dS. Okay. Let me just make sure that this all works out, yeah. All works out, right. So so, so this is our weak form that we derived. And of course, you know, at this point, we haven't said anything special about our spaces S and V, except for the fact that S includes the Dirichlet boundary condition, v includes the homogeneous Dirichlet boundary condition. So at this point, when posed as such, we are really talking of an infinite dimensional weak form. And as we made the observation in the case of the 1d problem that does not make it any easier to solve than the strong form. It bears complete equivalence to the strong form. And so we really haven't made any steps towards making it easier for us to solve or to deve, towards developing approximation. Right? And just as before, we develop approximations by going to a finite dimensional form. Right? So the finite dimensional form is the following. Right? Just the statement of it is going to look very much the same as before for the 1d problem, right? So what we want to do now is find belonging to Sh, right? Which is, is a subset of S. Okay? And as before Sh is a finite dimensional function space. All right, okay let's say a little more about Sh, all right. The way we construct Sh is again going to look like we did like, like everything we did in the 1d problem. Sh of now consists of functions like. Right. And now we specify as we did in the 1d problem that we are interested in functions that live in each one. Right, over the domain of interest over omega. All right, and because Sh is a subset of S, it must inherit the Dirichlet boundary condition as well. All right. So equals ug on the Dirichlet boundary. Okay. All right. And then the rest of it just follows right, given everything else that we have. Of course, we've given ug. We are given the influx condition. We are given the forcing function. And we know that the same constitutive relation applies. Right? Okay. I guess properly when we are stating the constitutive relation, the context of the finite dimensional weak form, we are no longer speaking of u being drawn from the full space S. So, we can already put an h there and there. Okay. And you recall that just as we did in the 1d problem, the h is the soup. H is just to remind us that these are finite dimensional functions and the way we construct them of course, critically uses the, the notion of an element size. Okay? So that's what the h show, indicates. All right. So, we're going to find in this sort of space. such that For all wh in vh subset of v, and where vh consists of functions wh. Also, in each one. On an eh, over omega. But satisfying the homogenous Dirichlet boundary condition. Okay? So, for all such wh again, the finite dimensional version of our weak form is satisfied. Right, and that takes on the form, integral over omega. Wh comma x, sorry. I'm lapsing to my notation for for the 1d problem. Wh comma i, jhi, dV equals integral over omega wh, f, dV minus integral over the boundary, wh, jn, dS. Okay, and once again, we observe that the data are not finite dimensional function. Right? We have exact representation of that data. Right? Okay. And, and you'll note that really in writing out this weak form, the only thing we did was replace all our any function that is drawn or, or obtained from u or w with the corresponding finite dimensional version. And of course, we defined what the finite dimensional spaces are going to be, right? As for the 1d problem, we are drawing then from H1. Okay, so this is our finite dimensional weak form, for the problem, okay? And, and you recall now that as in the 1d problem, what we need to do in order to proceed with the, with the formulation is to define what we mean by these finite dimensional spaces. Okay?

\section*{ID: fPuMMUM5JAs}
So, as, as in the 1D problem, the finite. Dimensional Weak form is the basis of our finite element formulation of the problem. Okay. Now, and, and of course, what we have to do is essentially define or construct the spaces that we need. We need to define S-H and V-H. In order to define these spaces, what we will do is construct a partition of our domain, omega. So we define S-H and V-H by partitioning. Omega into. Subdomains. Omega sub e. Where I think we were working with an omega sup e actually, so let's just stick with that. Omega sup e, okay? Where, e, once again, runs between 1 and, nel. Right, nel as before, stands for number of elements in the partition. Okay? And, furthermore, since each omega e is a subset of omega, which itself is a subset of r 3, we are indeed talking of each element sub-domain omega e as being a three dimensional sub-domain. Okay, so let me just draw the picture that we have in mind and then we'll say more things about it. Okay, so this is omega, and what we are trying to say here is that we have we're thinking of partitioning this cell sub domain. Then, sorry, there's domain omega into sub domain such as this one. And maybe another one, right next to it. All right, and so on. All right, and maybe we'd call this one omega e1 We'd have omega e2, right? And so on. Okay? So mathematically, just as we had before, we see that omega is for each omega E Really is open in, in R 3. Right, so, we consider each of these formula at least to be an open subset, and therefore our total domain omega which is the union of each omega e. e running from 1 to number of elements, right? And so, and, and you recall that as in the 1D problem to make sure that we pick up, all the interface points, right? The points of the interface between two of these elements of domains. We apply closure on the left and right, okay? And that is an, an equality. All right, and of course we also know that omega E 1, intersection, omega, E 2 is the null set. Okay, since each, since each of them is open, their intersection is is definitely a null set, right? Because we we, we don't get any boundary. So, then, but but, again, that's really, a technical thing. We do indeed, imply that they are non-overlapping elements. But, what happens on their boundary is really a technical thing. Okay, so, this is the setting. What we have to do now is, talk about how, how we construct these these sub-domains. And once we do that, we will also be on our way to defining the the underlying finite element basis. Okay, there are, of course, many ways to partition these partition the domain omega into subdomains and three dimension. What we will start off with is actually not even the simplest search partition but it is it is indeed the one that is most easy to get to given what we know about the 1D problem, okay? And what that implies is that we are going to construct a partition. Where each omega E. Is a, is, is what we call a hexahedral element, okay? So, consider hexahedral. Element sub-domains. Omega e, all right? e going from 1 to, and yeah. Okay, so, what we are talking of here is the following: We have this picture We have one of these element subdomains. Right? And I'm going to denote this one omega E. That is our domain omega. Right, so that's omega e. Right? Now in order to proceed with our final element formulation let's pull this element out of there, right? Let's examine it more closely. Okay? So that is our subdomain. Right? This is omega e. Okay, the simplest case that we are going to start out looking at is where on this hexahedral element, we are going to construct basis functions that are trilinear. Okay? So we, we will consider. Trilinear. Basis functions. Okay? Now if we are talking of cons of considering trilinear basis functions, what this implies is that. We need to do this by introducing nodes over these hexahedra elements where we have one, one node at each Vertex point of the hexahedral element and no more, right, no other nodes. So, our nodes are at the hexahedral points, sorry, at the vertex points of the hexahedron. Okay, so we have. There we go, all right? This means we have eight nodes, okay? So this is when we're going to work with trilinear basis functions. We have there, elements of the means are sometimes also called eight noded elements. Right? They're sometimes called bricks, a very colloquial way of describing their, their construction. Okay. Okay, so that's the setting. Now what we will do is as before we will denote coordinates for physical coordinates for each node okay if this so this is element omega E using the idea of local Numbering of degrees of freedom and of nodes. Let us suppose that this is node a for this element. Okay, so, we are going to denote the coordinates of that point as x sub e sub A, okay? And of course, your A equals 1 through 8, okay? All right, so when I have X, A, sub E, i'm going to see that this is local, this uses a local numbering of notes, all right? Okay, and, the numbering that's followed is. Actually we get to the numbering that's followed in just a little, let's, let's, let's just stick with this now. All right, if we have if we know that a runs over one through eight, let us then use that to write out our basis function straight away, okay? So now what we have is Sub E, right? So that's In element E, is the sum, A going from 1 to number of nodes in the element, which we know by the way, is 8, right? Because of the fact that we've chosen to construct trilinear basis functions, right? With this we have our basis functions and we're going to use our, the same notation that we had before right. These basic function are general, written as, being parameterized by physical position, right? X, by position, physical space x. And, just as we had before, we are going to multiply them By decrease of freedom N D A sub E, and here too I'm using a local numbering degrees of freedom Right, and this local numbering of degrees of freedom essentially reflects the local number of nodes. All right, we have this. And then we also have w h in element e is the sum. A going from 1 to number of nodes of the element. N a function of physical position C B E. Okay. And at this point thinks look very much like thinks before except for the fact that we have a much, greater number of nodes, but we know, how these nodes are positioned right there, they're vertices of a, of, of a hexahedron. Okay, so now if you think back to the way we proceeded in 1D, we took our physical element and regarded it as being obtained as through a mapping from a so called parent domain. Right? We're going to do exactly the same here. Okay? So we will think of omega b as being obtained. By a mapping from a parent domain. Again, we're going to denote that as omega sub c. All right, having stated that this is probably even stop this segment. Because once we get into talking about pairing domain omega c it is going just difficult to extract ourselves very quickly to end the segment, so we just pick up that definition in the following segment. Okay.

\section*{ID: PpyXWtOm-wI}
Okay. So we're well into our development of the finite dimensional weak form for our linear elliptic problems in three dimensions with scalar variables. We are in fact at the point of having already decided to work with hexahedral elements. And I've stated that these will be constructed from a, by mapping from a parent domain, okay. So let's just pick up right there. So mapping from the parent domain which I've already said we are going to continue to call omega z to omega e, okay. So, here's what we have. We have omega e, and I'm somewhat purposely going to draw this as, an irregular hexahedron. So, maybe something like that. Okay. Let's see. Okay, that's reasonably irregular as a hexahedron. Okay, noted still. Okay? But now, the point is that we say that this is obtained from a mapping from a domain in which things are indeed regular. So we have a nice, cubic domain, a nice cubic, structure in the parent domain here. Okay. So this is where we are constructing this, element omega e from, right? And this is omega sub c. Now, in this domain, we, since this is three dimensional, we have coordinates c, eta, and zeta, okay. And in terms of c, eta, zeta, again this is a bi-unit domain. Okay, so omega c is also a bi-unit domain just as, we had in one d, okay. What that means is that in terms of coordinates of, the nodal points in this domain we have for these nodal points. We have the following coordinates, okay. So this point here is minus 1, minus 1, minus 1. This point is 1, minus 1, minus 1. This point is 1, 1, minus 1. And this point here is minus 1, 1, minus 1, okay? On the top surface we have 1, minus 1, minus 1. Sorry. This is minus 1, minus 1, and 1. This point here is 1, minus 1, 1. This is 1, 1, 1. And this point here is minus 1, 1, 1. All right so, so these are the points and, you note that in general each of these points can be referred to as c A, eta A, and zeta A, right? Where A, once again, runs from 1 through 8, okay? But here it matters, that we get, that we establish a numbering, okay? So, let's what I'm going to do is just so that we can, distinguish things I'm going to use a different color here to, to, mark our, nodes, okay? So I'm going to write the local node number. There are only local node numbers, of course, in the parent domain, but we're going to call them 1, 2, 3, 4, 5, 6, 7, 8. Okay? Those are the values taken on by A in the parent domain, okay. Once we have this what we need to do next is go ahead and construct our basis functions, okay? The basis functions that we have are going to be written again in terms of c, eta, and zeta. And I'm just going to list them right now, okay. So, with things set up in this manne,r in general we have this formula N A as a function of c, eta, zeta, right, is equal to, sorry, I need more room here. Okay, N A is equal to. Something expressed as a function of c, eta, zeta. Okay? And here we're already taking this approach that, though we originally introduced our basis functions in A's as being paramaterized by, physical coordinates. We are thinking of them, as, as, as physical positions in turn being parameterized by positions of this parent domain, okay. So, here is the general formula for them. N A is 1 over 8, 'kay? 1 plus xi times xi A times 1 plus eta times eta A times 1 plus zeta times zeta A. Okay. If you look at this form, what you will notice is if you also recall the way we wrote out our Lagrange basis functions, Lagrange polynomial basis functions in 1d, right in 1d, we had only one of these contributions, 'kay? Here in 3D, we have three of them. They're just multiplied together. And this particular way of constructing our basis functions is called a tens of product approach. Okay, so the NAs are, so, the NA written as a function of xi, eta, and zeta, are what are called tensor product functions, right? The idea is that you take your basic form, established in one dimension and just multiply, extended by multiplication to the other two dimensions, all right? And the factor of one-eighth will, of course account for the fact that we have one-half to the power of 3, okay, for the three dimensions. Okay perhaps for clarity it's, it's actually useful to write out the basis functions explicitly, that's what I'll do now, 'kay? So here we go. N1 function of xi eta zeta is 1 over 8 times 1 minus xi times 1 plus eta times 1 plus zeta. N2 function of xi eta zeta is 1 over 8 times 1 plus xi times 1. Sorry. These are both minus 1 minus eta, 1 minus eta . Okay, so it's 1 plus xi, 1 minus eta, 1 minus zeta. N3 equals 1 over 8, 1 plus xi times 1 plus eta times 1 minus zeta. N4 equals one-eighth 1 minus xi, 1 plus eta, 1 minus zeta. N5, one-eighth, 1 minus xi, 1 minus eta, 1 plus zeta. N6 equals one-eighth, 1 plus xi, 1 minus eta, 1 plus zeta. N7 is one-eighth, 1 plus xi, 1, min, 1 plus eta, 1 plus zeta. And finally, N8 is 1 over 8, 1 minus xi, 1 plus eta, 1 plus zeta. Okay, that's the whole lot of them, all right, all written as functions of xi, eta, and zeta. Now if you look at these, you, you will immediately see two properties that follow, right? From these we directly get the chronicle delta property, okay? So NA evaluated at xi B eta B zeta B, okay, is equal to delta AB, the chronicle delta. All right, and we also have this other property which is that the sum over all shape functions, right, at a point. Right at any point, indeed, is 1, all right? Properties that were identical to what we saw in 1D, 'kay the second property allows us to represent constants. The first property is the one that gives us this interpolation property also of these Lagrange polynomial basis functions. 'Kay, so I should probably state here these are the Lagrange polynomial. Basis functions. In R3, in three dimensions, okay? All right and you, and also you, you know why they're trilinear, okay? Note the trilinearity. Right? It's each of these functions is linear in each quadrant, right? For the three coordinate directions here. Okay. So these are the functions that we use to construct our finite dimensional trial solution as well as finite dimensional waiting function. Okay? And furthermore looking ahead. We also are going to use the same basis functions to interpolate the geometry. Okay? So the actual map, okay? The map. From omega C to omega E, okay? Is obtained by really interpolating. X as the function of C and here, I will use the general notation C as a vector to represent the noted coordinates C zeta, zeta. Okay? So the map is obtained by interpreting C x is a function of C, right? Where I will see, see, I'll state it the very first time and we'll use it in the future. All right? We get this interpolation by simply observing that x be e. Parameterized by C is now simply, sorry. Xe paramaterized by C, which is any point in the physical element sub domain. Right? Is obtained as an interpolation, A equals 1 to number of nodes in the element. NA dependent, depending on the coordinates in the paring subdomain. Okay? Multiplied with the actual nodal coordinates, right? In the physical subdomain, but where I've used the local numbering of nodes. Okay? All right. So these are, we just remember that these are just nodal coordinates in physical domain. All right? And that means we're dropping off. For that element, omega e. We are talking of a typical nodal coordinate as being that one, right? We interpolate over all of those and we get any point inside here xe. Okay? That's how things work and, and, and that is constructed from a mapping, from our nice regular, at least this is intended to be regular, parent subdomain. Okay? So that is the mapping that we have underlying our geometry, as well. And you remember, the term for this kind of a formulation. This is, remember, right? It's called an isoparametric. Formulation. Okay? Isoparametric, referring to the fact that to interpolate our final dimensional waiting function as well as the geometry, we're using the same basis functions. Okay? So we have isoparametric formulation. With this, we know how to go ahead now, we will be able to compute our fields including the radians. Right? And we'll end this segment here. But when we come back, we will look at how to use the isoparametric formulation to go ahead and compute gradients and so on and also do the integration. The approach is going to exactly what we did exactly like the one we followed in one dimension, except that we, we basically need to generalize all of those ideas into three dimensions. All right. Let's stop here.

\section*{ID: Bci19ifM8WI}
Welcome back. We'll continue with developing our finite element formulation for our linear elliptic scalar problem in 3D. Okay, as we proceed one thing that may be of use, or one thing that may be advantages to see is how to interpret the basis functions that we are using. Since we are working in three dimensions, that actually gets a little difficult. And, and by interpret, I really mean something as simple as how do you visualize them. All right, visualizing them in 3D is a little difficult because of the proliferation of dimensions. And so what I propose to do in the first few minutes now is give you a the 2D case where it's actually easier to plot up these basis functions and get more insight into what they're doing. Okay, so let's look at that, right? So, so we'll continue with the same material, but we're just doing hm, taking an aside, so to speak, here. To hopefully enhance our insight. Okay? So, the aside here is we're going to look at Lagrange polynomials. Okay in particular we're, we're looking at the linears right? All right. In 2D All right, and so properly, rather than linear, these are really bilinears therefore, okay, because of 2D. All right. So in 2D, the situation would be the following. Instead of the use of my continuum potato represented by the foam football here, I will use a true 2D domain. Okay? So this is omega. We've we have the partition. Right? Into sub-domains, and that is a typical sub-domain. Okay, we're considering now the case where our sub-domains are polyhedral. Right, in particular, we're looking at sorry, we're not looking at polyhedral, we're looking at quadrilaterals now. Okay? Right and that, that's a typical, that's a typical sub-domain. Okay, so what we've done is we've partitioned omega into quadrilateral, lateral element sub-domains. Right and these are omega es, right? As usual e equals 1 to nel. Right, and, and, and everything else holds right, so we have indeed each of these as open. So we have omega, the closure of omega is the union over e of the omega es and the closure of that union. Okay, so we have this. So we partitioned it into quadrilateral element sub-domains. And on these quadrilateral sub-domains, we are going to consider the use of bilinear basis functions. So, consider bilinear basis functions. Okay, the picture by, the picture that we use in order to help us with the construction of these basis functions is is one that continues along this theme of using an of, of using a parent sub-domain from which every element, every actual physical element is drawn. Okay, so this is omega e. And now we, we suppose that we can construct this from a parent sub-domain which is now a square. Okay, it's a square, and it lives in a space where the, where the coordinates are labeled, z and eta. Okay. Since we are working with bilinear basis functions. Or we want to work with bilinear basis functions. And we have four nodes, of course, on the physical element. And so on the pair and sub-domain as well, we have four nodes, right? And this pair and sub-domain, you recall, is omega c, all right? Same notation. The points in the z, eta domain are picked essentially as before. Accounting however for the case that, accounting however for the fact that it is now a square. Okay, so the, this domain is again a bi-unit domain. So that says that these coordinates in terms of z and eta are minus 1, minus 1, 1 minus 1. 1, 1. And 1 minus 1. Okay. All right. Again we have we number these nodes as A equals 1, 2, 3, 4, all right. Okay, and again using notation that we've employed before. We may say that that the coordinates of node A, where we, of course, we're implying this local numbering of nodes, and therefore of degrees of freedom as well, right. Okay, so the coordinates, let's, let's also make it clear here, the coordinates of local node A. Okay? Those coordinates, in general, can be denoted as zA, eta A. Okay, and they take on the values that we've put down here. Right? Minus 1, minus 1, 1 minus 1, and so on. Okay. In this setting then here are our basis functions, right? I'll go to the next slide. So, the basis functions are the following. Now as before though we know we're going to use these basis functions in the physical domain, we take advantage of this barren sub-domain, this bi-unit domain to parametrize our basis functions in terms of z and eta. So, we have here n1, z comma eta equals one-fourth, 1 minus z, 1 minus eta. N2. One-fourth, 1 plus z, 1 minus eta. N3. One-fourth, 1 plus z, 1 plus eta. And, N4. One-fourth of 1 minus z, 1 plus eta. Okay? These functions also can be constructed as you would expect using the general formula for Lagrange basis functions. Okay? And I put down that, that general formula a little later, all right? I'll, I'll give you the general formula for 3D and then you'll, you'll, it should be clear how to simplify it to, for, to the two-dimensional case. Okay. Also you note that a hint to how that general formula is going to be constructed is to notice that here we have again this idea of tensor product basis functions. If you look at any one of these basis functions, for instance this one. All right, N1, it should be clear that it is a product of a basis function in the z direction and in the eta direction. All right? And the factor of half times half gives us one-fourth here. All right? So that, that tensor product idea continues. All right. These basis functions again we observe given our definition of the specific coordinates of the nodal points in the parent domain, right? These basis functions also do satisfy the Kronecker delta property. So we have NA, zB, eta B, equals delta AB. Right, the Kronecker delta property with which we are by now, very familiar. Also, when we sum up over all the nodes in the element. We sum up the basis functions. The result is one at every single point in the, in the element really. Okay? This has been expressed of course in terms of the coordinates in the parent sub-domain. But then, because we know that we're going to construct a mapping by mapping the geometry also we using the same basis functions. Right? Everything will work out. Okay. So, the reason to go into all of this was to be able to sketch out these basis functions and that's what I'm going to do now. Okay? So in order to look at them, I'm going to give you. I'm going to attempt to give you a perspective view here, okay? So, I'm drawing the pattern sub-domain here, but because of the, the attempt to present this as a perspective it doesn't seem quite squished. Okay. So that's our sub-domain. Those are our coordinate directions. Z and eta. Right? Our local node numbers are A equals 1, 2, 3, and 4. Okay? What I'm trying to signify here is that each of these node numbers has been written on a plane that is below the plane of the, of the actual element itself, right? Of the sub-domain. All right. That is our origin. Okay? Z, NA, eta equals 0. I'm going to draw just the sh, the, the basis function corresponding to N4. All right, so. I do this by changing colors here. And let's go with green. Okay, so, if you value at N4, what you see is that at z, N eta are equal to minus 1 and 1, respectively, right? It takes on the value of 1, right, so this value is 1. Okay, so this is N4 equals 1, okay. From there, if you stick to one of the coordinate planes, it comes straight down, right? Okay, linearly, right? So it's, it's linear along eta as we just showed. And along z, okay? Once it reaches this far edge, the function stays zero. All right. And so, as a Kronecker delta property suggests, N4 equals 0 at nodes, other than node 4. Okay? Now, at this particular point, at the origin, it takes on the value of one quarter, right. And this can be checked by just substituting z, N eta equals 0. And then 4 in fact in any one of the basis functions, right, all the basis functions value it to a quarter at the origin. So we'd get, it's just about that big, right? Okay? And what I'm going to try to do is suggest the shape that it has here so it's, it's essentially going to be a. It actually falls pretty rapidly until it gets to that point one. Sorry, one quarter. And from here, it will probably form a little less rapidly. Okay? Okay, I recognize it doesn't look quite quite smooth, but anyway that, that, that's just my poor ability to draw it. Right? The important thing is that at this point right at that point in 4, equals a quarter. Okay? So, so this is what happens with N4. The same sort of thing can be checked with any other basis function, with N1, for instance. And it will look similar, it will be, it will have this sort of tent-like structure. Right? Which obtains a value of 1 at equals 1. Right? And that node ends, and goes down bilinearly to 0 at each of the other norms, okay? So, so this is, this is an attempt to provide a little more insight into what is happening with these basis functions. This tent-like shape is a way to think about them. And as you can imagine, this is, this is more difficult to represent in 3D. So we, so we just stick with the 2D representation to provide a more visual sense of what these functions are doing. All right. I think we just end the segment here, and when we come back, we will launch into we will continue with the derivation of our finite dimensional weak form.

\section*{ID: 8VgQ-1AIMKM}
All right, we continue with our finite dimensional weak form and recall that we are actually doing things in three dimensions, right? So that side, that you've, probably experienced in the previous segment was only to give us a little more insight into the basis functions. So, return to the, to the final dimensional weak form. All right, and let's recall what that is. It is the following, integral over omega w h comma i, j h i, d v equals Integral over omega, w h f d v plus, or sorry, minus integral over the Neumann boundary. w h j n d s. Okay, so this is our finite dimensional weak form. Of course, we already have over this our partition into elements of domains. All right, so what that allows us to do is recalling that partition which is the omega equals the union of omega is the union over e, omega e, right? What this, what the above integral gives us, right, the, what the above integral equation gives us, is the ability to write it out as a sum over the elements of the the sum over e. Integral over omega e. w h, comma i, j h i, d v, equals sum over e, integral over omega e, w h, f. Dv plus sum over now, let me use a different set here for this sum. And, I'd like you think about why I need to use a different range for this last sum. Kay? Remember that now it has to be over elements that have a surface coinciding with the Neumann boundary. Okay? W, h, j, n and I've gotta be careful with signs because this is not a plus, it's a minus. Okay? D, s. Now, I'm going to say here that it's a sum not over all of the elements but Over e belonging to e sub N. Okay? Sorry, belonging to. Let me make that really a set, so make, let me make that a script e. Okay. What I saying here is that e belongs to the set E sub n if the boundary of element e intersection the Neumann boundary. Okay, is not equal to the empty okay? All right, so all this means is that we're considering elements that, coincide with surfaces coincide with. Okay, so we return to our continuing potato, or maybe we should start calling it our football now, right? And I'm showing you the Neumann boundary, the blue bottom boundary, this is our basis. So, any element that has a surface coinciding with this surface is included in the set E sub n. Okay? All right. That's, that's the sort of se, that's the sort of notation we're using here. So, let me just draw that here. All right? So we have an element, we have our domain, all right? And I think we have been denoting that as the Dirichlet boundary. This is the Neumann boundary. Right? So, we're talking of an element that could have a boundary coinciding with this Neumann boundary. Fedodite does have a boundary coinciding with anointment boundary. Okay, so this is an element say omega e. Right? Where e belongs to the set. Okay? And it is only over that set of elements that this very last sum is, computed. All right? Okay. So, we have the setting. Let's stand still at this finite dimensional weak form. And note that we do need to compute some gradients, as we've had to for the one dimensional problem, right? We need to compute gradients there, as well as for this term. Because we remember that gh of i is just minus kappa ij. U h comma j. All right? So we need gradients as well. Okay, so in computing gradients, just state this, we need function gradients. Right, which is not a surprise. We needed the same things in the 1D problem as well. Now, I'm going to take a step here that will make our notation a little more concise, okay. And what that involves is the following. I am going to take this, the, the vector z, eta, zeta. Right? And write it as, simply, z1, z2 C, 3. Okay, this is exactly the way in which we are writing our physical coordinates as well. All right? This is just as we're seeing X, physical position of a point, is X 1, X, 2 X, 3. In terms of its physical coordinates. Okay, this is going to save us some space and just give us a briefer type of notation as we get further into the development of the waveform. All right. It's going to be immediately clear why we need this sort of thing. Okay, so now we need to consider u h over element e, comma I in general. This is sum over A. A going from one to number of nodes in the element. N A comma i, D a e. Right. And the same thing for w h e comma i is the sum over the nodes in the element. NA comma I, D, sorry. C C A, E, okay? So we need to compute these gradients of basis functions. All right? What I am going to do here is leading up from the notation that we've already established for the coordinates in the parent subdomain, I am going to say that we will use uppercase indices. For components of xi. Okay? For, C in, sorry. Okay, all right, what, what this means is that the components of c, we're going to write as c I, where we're using an upper case I, running between 1, 2, and 3. All right? Okay. So using that and recalling that the way we are developing our basis functions is to parametrize them in terms of the Coordinates in the parent subdomain. We have, quite clearly, N A comma I, which, after all, is just partial of N A with respect to X I. All right? We are going to write this as, partial of Na with respect to CI, capital I, and using the change rule, we have this. Okay, what we have applied here is a certain convention. Do you know what that convention is, we're, we, we we've applied a certain convention to one of the indices here. All right, of course we are implying here that there is a sum on I equals 1, 2, 3. All right. This is the so-called Einstein summation convention. All right. We will use that repeatedly. Okay. All right this is straight forward for us to calculate. Right, and I'm actually not going to go and write it out again. Right because we know each of the NA's in terms of the corresponding C's. Right, C1, C2, C3, which were formally our C89 . Okay, so just remember that we have. All right so we talk about taking this derivative, with respect to a particular See I, right. That I may be 1, 2, or 3, right? We've, we've written out these shape functions. We know how to take those derivatives, right? Straightforward. Now the thing that remains to do is to figure out this quantity, okay? Right. How do we get that?

\section*{ID: Bn4u-HCV69I}
Okay, now you may recall how we did it in the 1D problem and take take that as a cue. All right, can you think of how to do it? What we need to do is recall. The mapping. Okay? So, that mapping was written as follows. Right? We said x. Right? The position vector of any point in the physical sub-domain. Right? It could be in reparameterized in terms of its, position on the parent sub-domain. And the way we caught that parametrization was by using the same basis functions. To expand, to, to, to interpolate, if you want to use that term, to interpolate the physical coordinates of the nodes. Where a now follows the local numbering. Okay? This is our map. Okay? Are now using so called coordinate notation, right? What as well. Right, what we know is x, sorry. Each x little i component, right? Can be parametrized by the full c vector. C1, c2, c3, and this is then just sum a going from 1 to number of nodes of the element, N A c x e, for element e but now component i of the x vector. All right? I've done, I've really written the same thing in both equations except that in the first case it's direct notation, the second case it is using coordinate notation. All right, and this is, this. Okay. From here just as we did in the 1D problem, we can go ahead and compute, partial of x i, with respect to c capital I. All right. Which is just, sum, over the nodes in the element. Now, N A comma I, capital I. X A, e, i, right? You note that there is a proliferation of indices here, right? There are superscripts and subscripts all over the place. The subscript for the element e is just coming along for the ride here, it's really not doing much for us right now. Okay? All right, so this is how we compute this derivative. Now of course this doesn't help us immediately because when, if you recall the form that the chain rule takes as shown here, what we need is actually the inverse of that created, right? Because we need the derivative of CI with respect to xI. Right? The thing marked with a question mark. All right, so how do we go about that? Okay? In order to do that, what we need to observe is that for the mapping that we have here, I'm going to, and I'm going to draw it here. We have our physical element. Our element in the physical domain. Okay, that one, right and this is element omega e. Right. Which we've obtained from this nice bi-unit domain. Right. Now we are calling these c1, c2, c3, all right? And what we've done, essentially, is to observe that for any point here in that domain, we can, given an arbitrary point c in the parent sub-domain, we actually have a map. All right? Okay? And that map is x of c. All right? Now, that is a vector map. Okay? In the context of looking at configurations especially if you have a background in continuum mechanics or some other field where you're looking at configurations and their mappings, this is what we call a point to point map. Okay, so this is a point to point vector map. It's almost superfluous to say vector there because there are, are representation points shown indeed as vectors. Right? We're using position vectors. Okay. So what that tells us is that we can com, compute the, the gradient of that map. Okay? The, the gradient of the map. Right, which is actually properly in the context of mapping configurations, the gradient of the map is also often called the tangent map. Okay? Okay. The tangent map is what I'm going to denote as a tensor, right? Because x is a vector, c is a vector, we're going to compute the gradient of x with respect to c. That gives us a tensor. Okay? So, J is. This derivative. Okay? This is what sometimes gets called The Jacobian of the map. All right? Now, this is direct notation, right, for this tangent map. We can also talk coordinate notation. So in coordinate notation, that tangent map. Is j, little i, capital I, is the derivative of the xi coordinate, x little i, with respect to c, capital I. Okay? Right? Now if you've studied continuum mechanics, you will recognize that to be something. Right, that it is essentially the deformation gradient from continuum mechanics. Okay? From the kinematics of continuum mechanics. Anyhow, we are not going to use that nomenclature. We just call it the Jacobian of the map. All right? Which is what it is mathematically. Okay. Now again, it's, it's, it's sort of the detail of a, a formal, or, or, or rigorous detail, to observe that, well, you can truly represent a tensor only if you have a basis. And if you have a basis, you can then represent tensors or squared matrices. Okay? That is actually a carefully constructed argument, but we don't to go into that argument, right? So we can represent it. As a matrix. Okay. All right? And truly, the fact that we represent it as a matrix comes from the fact that we have a, we have basis vectors in the physical domain as well as in the pairing domain, but we won't get into that detail. Okay. So, so J is simply, that matrix J is simply this. Right? All right. And, of course, there are terms here. All right. And you see this is just writing out what I had on the previous slide using coordinate notation. I'm writing it out explicitly here. Okay? So, we have this. Now, why should I bother with this? Because note that the map that we have is continuous, and it is smooth. It is actually what we call a c infinity map. Right? We are able to take an infinite number of derivatives of this map. Okay? So, the map, x of c from omega c to omega e is c infinity. Okay. All right, we can take, so actually a very smooth map. If it's a very smooth map and J is, a, partial of x with respect to c. All right, it turns out that, rigorously, its inverse exists. Okay? Okay, so there exists J inverse which is partial of c with respect to x. That is what J inverse is indeed by definition. All right? But that's easy to do now. We have G in front of us, we can compute that explicitly, because we do indeed have an explicit representation for each of those x1, xi's, x little i's with respect to each of the c capital I's. All right? And therefore, it's easy to compute J inverse. All right, so the code, so J inverse represents a partial of c with respect to x and, indeed we have J inverse is the matrix, the matrix representation of it, is partial of c1 with respect to x1. Partial of c2 with res, sorry. Z1 with respect to x2. Right. All the way to, to, to this last 3, 3 component. Okay, it's a 3 by 3, so that's not difficult to invert. It can actually be inverted exactly if we care to do that, okay? And we do indeed care to do that, right? But then you note what we've done, right? Essentially what we have is J inverse, right? We look at its components, capital I, little i now, okay. It, these components are indeed the terms we need or the factors we need in the application of the change rule. Okay? All right. So the key here is because of the fact that we are explicitly constructing this map, right, the vector to vector point map, X is a function of C, we can compute the tension map. It's just a 3 by 3, easy enough to handle, can be explicitly inverted. Right? In closed form, and the components of that inverted matrix are indeed the ones that we need for our chain rule. Okay, this is actually an excellent place to stop this segment.

\section*{ID: Jaa7VVKo1-g}
All right, we'll continue. So what we did the previous segment was use this mapping, between the parent and physical element sub domains to observe that we have actually a convenient way of writing out our gradients. Okay. We'll continue now from we, from there and consider explicitly the three integrals that go into our weak form. Okay. So, with that background now, we are getting into, the assembling the integrals in the finite dimensional weak form. Okay. The integrals. In. The finite dimensional. Weak form. Okay, so, let's consider the first of our integrals, right, so. We are going to consider this one integral over omega e, w, h comma i, j, h, i, d, v. Okay? We first invoke our constitutive relation to write this as integral over omega e. W h comma i. We pick up a minus sign here, minus kappa i j, u h comma j. D, v. Okay, that minus sign just came from the fact that the flux is written as minus the conductivity tensile. Multiplying the, gradient of the field of interest. Right? The final solution field. Okay, we'll take care of that minus sign later on. Okay. So we have this and now, essentially, what we are going to do is use what we've just discovered about writing out the gradients of w h and u h. Right? Okay. So, over element e this lets us write it out as, integral over element, over omega e. Right. Of w h comma i, we will write as sum over a. I'm not going to write the limits on a. Right. We know the limits, they go from one to number of nodes in the element. Well okay, let me write them as always for the very first time and then I'll abandon writing that. Okay. Sum over the nodes in the element, n a comma i c a e. I put parentheses on that. Kappa i j, okay. Open another set of parentheses for a sum over b, going from one to number of nodes in the element, okay? N v. Comma j, d, a, e, d, b. And, we have a minus sign. Okay? And as we did in the one d case, perhaps it is useful just to remind ourselves that, that is w h comma i, and that is u h comma j. Okay, and now we, use our discovery of how to write those gradients of basis functions. Noting that those gradients as expressed here are readings with respect to physical coordinates. Right? Okay. So this is now integral over omega e minus integral over omega e, open the first set of parentheses. Sum over A. And I'm going to abandon writing the limits from here on. Now, n a comma capital i. Okay and recall that when I write n a comma capital i, we are talking of taking derivatives with respect to, what? Right. With respect to the, coordinates in the parent sub domain. Right. This times c I comma, little i. Okay? Multiplying c, a, e, okay? Capital I, j. Open parentheses again, sum over b, abandoning limits, n b comma capital J. C J comma little j d, b, e, d, v. And then note now that I was not quite as careful when I wrote that. That's not a D, A, E. That's a D, B, E. Okay. Right, note that what I'm doing here is just, partial of c i, with respect to x, little i. Okay. Same thing here. Okay. Partial of c capital J with respect to x little j, okay? And this is. Partial of n a, the a basis function. The basis function for node a, or the basis function which is one at node a. With respect to c capital I, and this is Okay. Right? Fine. So we know how to compute these quantities now. Right? Okay. So. Let's move on, and actually let's make our expression here a little simpler by observing that the degrees of freedom of course are independent of position, and therefore they can be pulled out of the integral. Okay. So we take the step that we have taken before in the one d problem. This is equal to minus, now, sum over a and b. Okay. We have here c, a, e. Integral over omega e, n a comma i, c, capital I comma little i kappa i j n b comma capital J c capital J little j. Okay. All of this integral d v. And just for complete clarity I'm going to put parentheses on this. Right, on the integral. And note that the whole thing is then multiplied by d b e. Okay, some things to note. Though, perhaps one way to think about this is that we have an integral, right, and we have a complicated looking integrand here. Okay. Each integrand, right? Has indices a and b, right? And these indices a and b come from the particular degrees of freedom over the element that we are talking about. All right, so this would be, so the picture here would be, let me draw it here. The picture here would be In the physical domain, we have our element and we have a node a. All right? Or b. Okay? The index a or b runs over those nodes, right? It runs over the degrees of freedom that correspond to the nodes, right? And over the others of course. Right? It runs over all of those. Okay. So having chosen a particular degree, a particular combination of degrees of freedom for a and b, right? Okay. For that particular combination of a and b we have the functions n a and n b, okay? Now, inside of the, inside of our integrand. Observe that there is another comp well there is another set of multiplications happening and I'm trying to look for a different color here. There we go. Okay. Using green. Okay. Inside the integrand there is Einstein summation convention applied all over the place okay so there's a Einstein summation convention here. Okay? Here here, As well as here. Okay? Again, if you've studied continuum physics and done this in three dimensions, you've probably seen this sort of thing. Often. Okay? So, whereas, we have Einstein summation convention for summing over indices that correspond to either physical or parent domain coordinates. That's what the little i j and the capital i j refer to, right? We have used an explicit sum. There, to run over the degrees of freedom of the element. Okay? I just want to make this little bit clear. Okay, so if that part is clear, we'll proceed and in order to proceed, we observe that. The way I've written it out so far, even though we've actually moved over to writing our you know computing derivatives with respect to the barren sub domain coordinates. Our integral is still over the physical element, right? And that elemental volume here really defers to the physical element. Okay? The next step we are going to take is to convert this also into an integral over the parent sub domain. Okay, so now we are properly changing variables into the parent sub domain. Okay? So we are really completing change of variables. To c. All right? And what that means is that we need to essentially convert the elemental volume. Okay. Now, here is our map. Okay. Okay. That's omega e. And this is constructed of course from omega c. Okay. And we have our map. X of c, right? So for an arbitrary point, physical point little x here, and an arbitrary point c in the parent sub domain. Okay, what you want to think now about is how do we represent the elemental volumes, right? And in order to do that consider the fact that we have here. An elemental volume. Okay, that's my D, capital V. And here, we have an elemental volume, that I'm going to represent as d, v, c. Okay? D v sup c, just telling us that that's an elemental volume in the parent sub domain. Now, a another result from the fact that we have this mapping and that we also have the tangent map, J of c. The result that follows from this is the following. One can show that d v equals determinant of J. D v c. Okay? Again this is a standard result. If you've ever played around with these sorts of configurations, can be proven and it's conventionally done if you're studying kinematics of deformable bodies. Okay. But we are just going to use this result. Okay. We are essentially going to use this result in our, integral, right, that we're working with. This one, right? We are going to try to replace that, elemental volume with this expression. Right. That is however, perhaps best done in a separate segment. So we'll stop the segment here.

\section*{ID: wfje8-ALRlE}
Okay, so we have everything that we now need to write our integrals in terms of coordinates in the parent subdomain. So let's just go ahead and write that. And in particular the integral that we are working with is the following, right? So, so actually let me title this. Integrals in the finite dimensional Weak form. And in particular we're working with this integral All right. And where we are is a realization that this integral can now be written as minus. And that minus comes from the constitutive relation. We've agreed to pull the sum out, and write it as a sum over A and B, and for brevity we are not writing the limits that A and B run over. Right? In the general case, they run over one to number of nodes in the element, each of them. All right. We have an integral now. Okay? And inside this integral we have terms of the form, out here we have C A B, right? We don't want to forget that. Okay. The integrand is made up of terms of the following form. N A comma i C capital I comma little i kappa i, j, N B comma capital J C Capital j comma little j. And now, previously we had d, v, being the, the elemental volume in the, physical subdomain. And we just agreed at the end of the last segment, to write it as in terms of an elemental volume in the parent sub domain. But that does involve our including the determinant of j here. And let's recall that it depends upon c. Okay? D V, c. The integral here now can properly be written as an integral over omicron c. We recall that the, this is the integral which we enclose in parenthesis. And we have here, d, d, d. Okay? Alright now, if we stare at this integrand, it looks a little, a little daunting, doubtless, but we observe that there is a The Einstein summation convention is at work. And so, really what we have left is going to be an expression which takes on values depending upon the specific element degrees of freedom, capital E and capital B. Okay, the, the capital I, capital G, little I and little G are going to be so to speak contracted out, to use the terminology of densers and vectors. Okay right. So, so when we carry out this, this integral and let's assume that we do know how to carry it out. First of all observe that this integral can, can be is, is, is easy to carry, is relatively easy to carry on because our domain omicron c is a nice regular sub domain. It's a nice regular domain, it's a bi unit domain. So let me just write that little bit here. So the integral of omega C is the one thing that I want to make a difference with the way I wrote, so, so this is now a triple integral, okay? Where, C, 1 equals minus 1 to 1, C2 equals minus 1 to 1. And C3 also equals minus 1 to 1. Okay. Everything that we have in here, I'm not going to explicitly write them as depending upon c, but we do recognize that indeed, n a comma i will depend on c, because n a is trilinear. All right, and the xis. Okay, so derivative with respect to any one of those coordinates is going to leave us with something that's bilinear in general, it'll depend on two of the xis. All right? If you look at xi i comma i, and you recall how we are going to compute it It too will depend upon C. And why's that? Right. That's because we are going to compute it or we are going to identify it as being the components of the inverse of the tangent variable. Right? And the tangent map, consists of first derivatives, right? So, this thing, this, too, will be a function of c, right? Kappa ij. Well, it depends. If we have something with, uniform conductivity or uniform diffusivity, it would be a constant, right? But that does not res-, we are not restricted to that. We could have something that's a function of, this thing could. Potentially be a function of, position. Right? Could be a function of position and because we know how to write the physical position as a function of z that too would be a function of z. All right, the kappa Okay. And then we have NB, which of course, we've all ready discussed, right? We know how to write its derivatives. It's going to depend upon c as well the derivative of c itself with respect to little x little j, right? And then again, we have the determinate here. Okay? And this again is an integral over c 1, c 2, c 3. We close our parenthesis. Right, in the places multiplying d B element e. Okay? So everything in that integrand is properly a function of the xi 1, xi 2, and xi 3. That integral can be carried out. Alright, for simple cases this integral is not that difficult to carry out, analytically, and by simple cases I mean things where situations where Kappa is indeed independent of position. Okay, however we're not going to get involved right now with evaluating it analytically, because we already have a way to evaluate this, right? Do you remember how we're going to evaluate in, in the general case? Right. Using numerical integration. Okay. So we'll come back to that later We'll come back es essentially to how to do that in three dimensions, okay. But the point I want to make now is that if one looks at the integral here, okay? That can be evaluated. It's going to give us a scalar value, right? Because all the little i's, capital Is, the little js and capital Js are going to be contracted away. It's going to give us a scalar value, however. That is going to be indexed by the, by the capital A and capital B corresponding to the local degrees of freedom, okay? So I am going to denote this K, AB, sub e, okay? The AB is obvious, why it needs to be indexed by AB. It's also obvious why it needs to be indexed by little e. Okay, because this is for a specific element, okay? And so we have it. Our integral on the left is now just minus sum over A comma B, c Ae KAB, for element e, dBe, all right? And now, we take a step that we took in the one d case as well. We are going to well actually, what are we going to do? Do you recall what we do next in, in setting up our finite element equations? We get rid of our some, our explicit sum over a and d by going to matrix vector notation. Okay, let's do that on the next slide. So what we're doing is using matrix vector notation. We're using matrix vector notation for local degrees of freedom. In order to keep track of local degrees of freedom, we're going to use matrix vector notation, right, and really for their numbering. Okay? And what this means is that for element e, if we have c 1 e, c 2 e, up to c number of nodes in the element e, all right? These are the degrees of freedom. As you recall that are used to interpolate the weighting function using our basis functions, all right? Okay, so using this, right, and of course the same thing for d's. D 1 e, d 2 e, all the way up to d number of nodes in the element e, all right? Using this, what we observe is that we essentially are able to write our our integral, right? This implies that we can write integral over omega e. As now, minus we're going to write the c vector in a, as a row vector, right? And you recall how we did this in the one d case as well. Okay? We get a big matrix here. Which consists of entries K 1 1 e, all the way up to K 1 number of nodes in the element, e, okay? And, the last entry here is k number of nodes in the element time number of nodes in the element, e, okay? That is the matrix that we get. And this is multiplied by a column vector using the notation that is sometimes employed in linear algebra. Okay. All right. And, for, even for the gravity we could write this as minus c e, right? Where ce, or ce transpose is that row vector. All right, this, Ke, right, Ke is the matrix, de, okay? And by, by reference to what was done in the one d case, we would call this the element conductivity. Or diffusivity matrix, right, depending upon what problem we were solving. Depending upon what physical problem we were solving, okay? All right, clear enough hopefully. We can now follow exactly that same approach to look at the first integral on the right-hand side of our finite-dimensional weak form. Okay, so now consider integral over omega e w h f d v. With everything we've gathered together this should be relatively quick, all right? We're going to write this as integral over omega e. W h, you recall, we will right as a sum over A N A. There are no derivatives here, right? So, we have N A, C A e. I put parentheses on this to remind us that this represents w h, multiplying f, d v, okay? We can take several steps all at once now. Let's do that. Let's, pull the summation out, c, A e. We have an integral over omega e, but right, but let's plan to write that with a change of variable as an integral over omega c. All right. Inside here we continue to have N a because it does depend on position. Multiplying f and let's recall that f could be a function of position but through our mapping we have that. Okay, and any, of course, is the function of position. Okay. We're writing everything as a function of coordinates in the parent sub domain. That leaves us with just d v, right, but we know how to handle that. We know that it's a determinant of the jovian of our mapping. D, V, C. Okay. Sorry that's a little to squeezed in there. So let me write it out here more clearly. Okay. That's what we have. Right. And, of course, this is now, again, we can take several steps at once. So we can write this now as we can abandon our explicit Writing of the, of the sum over A. Right. And we know how to do that. That base, that can be done by going to this rho vector notation. With a c degrees of freedom. The integral now explicitly is an, a triple integral. C 1 equals minus 1 to 1. C 2 equals minus 1 to 1. And z 3 equals minus 1 to 1. Right? Our integrand here consists of a column vector of the shape fun, oh sorry, the basis functions. It's all, they're often called shape functions, but a term that I've been avoiding. Okay, we have the basis functions. All right, we have this. We have f, right, which could be a function, which could depend upon c through x. We know how that dependents comes about. There's an abuse of notation in, in using the same symbol f, but I will, just dock that abuse of notation. All right. And then, we have the determinant of j, function of back position. Okay. We have this and and yes, and we observe that this integrant is essentially, integral, sorry, it's integral is over D C 1. D C 2. D C 3. All right. Okay, we can compute this integral in general. Again, we will look at how we handle these types of integrals using numerical quadrature, numerical integration. And we observe that we essentially get what I will write out as being c 1 e, c 2 e, c number of nodes in the element e. Times, the column vector which I will denote as F internal. Okay? Degree of freedom one in element e. F internal, degree of freedom 2, element e. Coming down as far as f, internal Degree of freedom, in an E, element E. Okay. And finally, we could write that as C, E, transpose F Internal for element e. Okay? All very nice and concise. All right, and the, the advantages that having done it with, with the, in excruciating detail for the, 1D case we know you don't immediately have the sense to work on it. All right, we'll stop this segment here.

\section*{ID: kIiF9Mn7OXA}
Welcome back. We'll continue with our development of the matrix vector weak form, for the linear elliptical PDE with scalar variables in three dimensions. So what we are going to do today is essentially not only completed assembling the matrix vector weak, the matrix vector weak form. But also talk a little about associated issues which arise mainly from the fact that we are looking at problems here in 3D. So so the topic of this segment and the next couple at least is the matrix vector weak form. Recall that we at the end of the last segment, we were working with the local element level integrals. Okay, And we had developed again the local element level matrix vector representations of these integrals. In particular we worked with the left hand side integral we, the, the bilinear term. We worked with the, the forcing function from the right hand side. And now that brings us to the, to the integral that imposes the Reimann boundary condition. Okay. So, let's start with this one, that one. So what we're doing now is to consider the integral integral over partial of omega e sub j of minus w h j n d s. Alright? And it's useful right away to recall for ourselves the sort of term we're working with here, right. So the, the situation we have is the following. We have our bases. We have our domain and we're looking here at an element that has an edge. Or in this case, a face really, that coincides with that, that coincides with an, with the face of the, of the body, of the body of interest itself. Right, so we have an element of that type, okay, and the whole point is that in this case, so this is omega e. Okay? And we may think of that face as being partial of omega e sub g. Okay? And we recall that partial of omega e sub g. Is the intersection of the boundary of that element omega e with the alignment boundary of the problem. Okay, so that's really the face of the element upon which we're imposing the Reimann boundary condition. Alright, using our finite dimensional basis function it takes on the following form, it is now minus integral over that of okay, a sum of NA CAe Jn dS. Now, one thing we've got to be careful about here is that we can indeed consider the sum to be a sum running over the entire set of element nodes. Right? And, and let's start out with writing it in that fashion. Okay, but now we recognize that in this picture that we've drawn here, not all the nodes have corresponding to themselves shape fun, basis functions. That are non-zero on the surface of interest. Right? On the interface of interest. If we say for instance, that this is the interface of interest that we've, that we've indicated, then let me highlight in a different color the nodes that lie on that Boundary. Let's suppose that those are the four nodes lying on that boundary. Okay? It should be pretty clear to us that it is only the basis functions that are one at those nodes that will contribute at all to this integral. Okay? So, having made that observation, there are. Th, th, there are at least a couple of ways in which we can process and this, the, the, the, the different ways of proceeding simply correspond to different ways of doing bookkeeping here. Okay? For one thing, one may say that one may define., Okay. When we define a subset, right? When we define a subset of nodes, a, right? Which A sub n shall we say okay? Which consists of all nodes A such that x a e, right? Which is the corresponding nodal point, okay? Right? Using the local node numbering such that x a e belongs to partial of omega e sub j, okay? Right. And then what one can do is simply restrict that sum to a lying in this set. Okay? In, in this case, I've used the subscript n, to suggest that this is the, set of nodes or degrees of freedom corresponding to the Neumann boundary data, okay. So let's use this approach, right? So what we see then is that minus integral over this surface, w h j n d S equals. Now again, I'll take several steps all together, right. I'll do this, I'll pull our summation out and I'll just say a belongs to the set we introduced, right, script A, sub n. Okay? We have your cAe, and I apologize, that c looks too much like an e. C a e integral over over partial of omega e sub j Na jn dS. Okay? Now we make another note, which is that we can of course convert from here to our parent sub domain. Right? The parent domain from which we construct every single element, okay? So in that setting, let's suppose that we are still talking on the same element here and so let me go to the next slide to get this done. Now we're going to elucidate a manner in which you carry out this integral, right? So for that purpose, let's suppose that we are now working with an element, right, some general element. Right. And on this element, let's suppose that. Surface of interest to us, is this one, right. Let's suppose that this one is the surface partial of omega e sub g. Okay, now this element of course is always constructed from the same parent domain. The nice, regular element in this volume domain. Okay. Right, we have that sort of a mapping. All right. So, let, let's suppose, just for the purpose of argument, that, the, that the face partial of omegas, omega e sub j is, is the mapping of that face. Okay? And the way I've, drawn things out, the face of interest here which I will mark just for the purpose of argument again I'm going to mark this as partial of omega c sub j. Okay? It is the face in the bi-unit domain in the parent subdomain that gets mapped onto the face of interest to us, which is the face on which we are imposing the Neumann boundary condition, okay? Essentially all we need to do here is now recognize that if we construct a sort of, a lower dimensional mapping, right? Which is the, the mapping that converts the area of this particular face, right, the one marked as partially forming e sub j, here, which, which obtains the, the, this particular face, from omega c sub j. Okay, what we observe here is that the integral that we need to carry out, okay, which is coming from the previous slide, it is minus, sum, AE belongs to A sub j, cAe integral over omega e sub j, NA jn dS. All right? Let's observe that this thing can essentially be constructed as. Integral over partial of omega z sub j NA jn, right. And, what we will do here is write determinant of let me just write this as J sub s, dS c. Okay? Well, what I'm talking about here is the idea that, for the mapping of the the faces, we can, we actually need to worry just about, we, we need to worry only about how to map from this phase, partial omega c sub g, to omega, to partial omega e sub g. Kay? And Js is that mapping. Okay, so Js, insert Js is what we may define as the area coordinates, right, that correspond to a that correspond to a, to a new set of variables. Maybe x tilde one comma. C2. X tilde one comma c3. X tilde two comma c2. X tilde two comma c3. Okay? And what we mean by this is is the following. If what we're referring to here is the following. If we look at the surface that we are working on in our physical domain, it is this one. Okay, so this is the surface partial of omega e sub j. Okay, what I'm suggesting is that we can now define local coordinates on the surface. Right? And these local coordinates are what I'm referring to as x tilde one, x tilde two. Okay? All right? And they, these ones are obtained from a mapping of. That face in the parent subdomain which is c2. C3. Okay? And this is easy enough to do because in terms of c2, sorry, in terms of c2 and c3, we can indeed express the local coordinate x tilda one, x tilda two. Okay? Okay, what we need to do here is define the map x tilda, as a function of c2 and c3. All right? Okay? And then the determinate that we're talking of is simply the determinate of this particular mapping, right? The detail of how to construct this can sometimes seem challenging, especially if the surface partial omega e, as I tried to represent here or here, is not a plain surface. Okay? That takes a little more work, but it can essentially be done. Often they will indeed be plain surfaces. And then the, the, the, this mapping is, is straightforward. In particular if partial of omega e sub j is a coordinate surface, the mapping is actually very, is, is almost trivial. Okay? So this is how you would we would go about doing it, right. So

\section*{ID: yNGlwbXLPA4}
So as an, as an example, a particular sim, a particularly simple case, right? As an example, what we may have, is a situation where our domain may be such that. This may actually be one of our, we may actually have, in these, in these two directions. Our coordinate axis may be our, our physical coordinates maybe x 1, and x 2, right? And then, if our phase omega e sits nicely, in here. Right? Sub g sits nicely, here. It's easy, to construct it, as a mapping, x 1, x 2 functions of C 2, C 3. Right? As I'm suggesting here. Sorry, that's stupid. Okay? In this case, the mapping is very easy to construct. Okay? All right. So, so at any rate, what we get then, is is the following. We get minus, sum A belonging to The set C A e integral partial omega e sub j, N A, j n, d S equals minus sum A belonging to the set. Sorry I, I realize that at some point I, changed this notation to say, A sub j, it should be A sub N here. Okay. A sub N, C A e. Now, this integral over the parents of domain, simply becomes an integral over say, C i equals minus 1 to 1. And some other C j equals minus 1 to 1. Where in the example that I was constructing, I was saying i equals 2 and j equals 3, for instance. All right? So once we have this, we have an A here. J N, we have the determinant of this lower dimensional map, All right? The determinant of the Jakobian effect map properly, and the And the, the elemental quantities here over which we're  I just d C i, and d C j. All right? Now, when we carry this through, what we get is the following, right? We can now, abandon the explicit sum over the local degrees of freedom A, and instead, we go to a vector. Right? Which now consists of I guess C A 1 e, C A 2 e, up to C a 4 e, right? Because any phase will have four nodes, right? In this case of four degrees of freedom, right? In this case. And we have here when we carry out this integral, we end up with essentially A set of Of components of a vector, right? And we will call this thing f I forget what notation we used earlier. But let's just call these f sub j, to indicate that they are coming from the, the boundary condition. Okay? And f sub A, sorry, f sub j A 1, f sub j A 2, f sub j A 4. Okay? And this holds for A 1 to A 4, right? Belonging to that set A N. Okay? Now, this is just an attempt to be systematic with giving us a way to think of the set of degrees of freedom, that correspond to the Neumann boundary condition. Okay? And also, an attempt to point out, that yes indeed, this integral that needs to be carried out over one phase of this element, right? Can actually be constructed in a fairly straight forward manner, from a phase of the parent subdomain. All right. The setup may look abstract, but when one has actual degrees of freedom, actual elements, it actually turns out to be relatively, straightforward, in most cases. Okay, there are some special cases where for instance, the surface is indeed inclined, or doesn't coincide, it is, is not a plane. Or does not coincide properly with a with a with a coordinate plane. Where the evaluation can get a little, more complicated, okay? But the approach is the same, fundamentally. All right. As, as a very last step, one could actually, you know, one could go away from this representation of a reduced vector here. Consisting only of the entries corresponding to the degrees of freedom belonging to this set. 'Kay? One could simply, expand it out, right? Okay. Right? So, expanding out All  expanding out to include all the degrees of freedom, in element omega e. Here is what we get. Okay? We get minus, this is what we'd started out with. So, we're going all the way back to the integral, that we're trying to impose here, right? Minus integral over partial omega e, sub j w h j N d S. Right? This can now be written as minus Let's suppose, the first minus C A 1 e. Let's suppose the very first degree of freedom from this element did indeed, belong to the set A sub N. Okay? Let's suppose that the next two are, maybe C 2, C 3 e. Okay? So, what we have is A 1 is 1 2, and 3 does not belong to this set if, okay, let's see 2, and 3 does not belong to the set. Maybe 4 does belong, A 2 e 5 does belong. Right? Maybe 6, and 7 don't belong. And 8 belongs. All right, I'm taking a particular case where, where here. These degrees of freedom. Okay? So considering the case where local degrees of freedom 1 4, 5 and 8 are A 1, A 2, A 3, A 4, it's this set of degrees of freedom, which basically make up the. Okay, this is the special case, I'm considering. Okay? Then, what this thing would multiply here, would be a vector, okay? Which would have a contribution, f j 1, okay? It would have 0 contributions, for the two, and three positions. It would have a non-zero contribution ,for f j 4, also f j 5. 0 for 6, and 7. And a non-zero contribution for f j 8. Okay? So this is, this a particular example I'm considering, and actually pre, constructing this The final matrix vector form. Right? In this case, we use only vectors. All right. You will also note, that as I was going along with this example I kept changing actually the particular degrees of freedom, which la, lay on this phase. But hopefully, that actually helps to, because it, it, it, it will hopefully, force you to think about exactly, which degrees of freedom, do lie in it. And, and how to write it in each case, okay? By constantly changing the, case that, that I said was laying on this so, sorry, the degrees of freedom that laying on the phase. All right. So at this point, we are done with assembling all of the element level integrals. When we come back in the next segment, we're going to talk about how to put them all together. Right? How to assemble the final matrix vector equations. All right.

\section*{ID: J0wvkiF9U7k}
Okay. Welcome back. We'll continue with now assembling our element level matrix vector representations into our global matrix vector weak form. All right at the the end of the last segment we had obtained a representation 'kay, so for the following integral, right? You probably have this on you notes, and just taking it down. I'm just copying it down from my saved slides here. Right. We said that. We, we considered a, a specific example where we said that that local degrees of freedom one, four, five, and eight were the ones that lay on the face that coincided, the face of the element that coincided with the Neumann boundary, okay. So those, so we wrote that out in the following form. And for the resulting vector that we have here, the, the column vector. The first element was non zero, the next two would be zero, the fourth would be non zero, the fifth would also be non zero. Right, the sixth and seventh would be zero and the eighth would be non zero. Now I am going to do is so, so this is, this is an example, right? This is an example where local degrees of freedom 1, 4, 5, and 8 are the ones that belong to that set, okay. What I'm going to do is just to have common notation. Observe that this can, this then can of course always be written as minus c e transpose, right? The vec, the, the role vector of all those degrees of freedom F j e. Okay, all we need to observe is that in this column vector F G, sub e, some of the contributions are 0, right? These are the ones corresponding to the degrees of freedom that do not lie on that particular element face coinciding with the Neumann boundary. The rest of them are in general non zero, okay. So the reason I'm doing this is, because this is what we want to use, when we pull together all the other integrals that we've developed over previous segments to represent sorry all the different representations we've de, developed for the, for the other integrals in the weak form. Okay, so what you want to do is use in the following, right. We now have a way to write the left-hand side, okay, as a sum over e of terms of the form c e transpose K e d e. Okay? We derived this a few segments ago, equal to sum over e, c e transpose F internal e. And now, this new term that we've developed here, right. This is again an, a, a sum over elements. But, I'm not going to write, I'm, I'm not going to I'm not going to say that this sum for the traction term extends over all the elements, right. Instead adopting the notation that we had on the, in the previous segment to designate the degrees of freedom lying on the Neumann boundary, I'm going to say that in here belongs to a set E sub n, which I think we actually already have defined, right. The elements corresponding to elements which have some part of their surface, of their faces, lying in the, lying on the Neumann boundary. Okay, so we have e belongs to this script E n c e transpose F j e, okay. The other sums run over all the elements. E going from one to nel, e going from one to nel. All right. Okay at this point I want to point I, I want to bring to your attention the fact that we can just multiply this through by minus, right, by minus 1. Changing all the signs so that we get something that we may feel a little more comfortable about. Okay. So we basically transfer the minus sign onto the forcing function. And this brings us to a, another little detail, which it is useful to bring up at this point before we go ahead to assembly, okay? And I'm referring to the fact that the way we've set things up here that term has a negative sign, okay. So I'm going to refer to that and, and I'm going to do that, in an aside. I need to make a, an, an, I need to make an Aside here. Okay. Essentially the form of the equations that we have, okay, the form of the equations that we have. Is obtained. From the strong form, right, going all the way back to the strong form. Where the strong form had the following appearance. Right. Minus divergence to flux equals, a force in function. Okay, now this particular form had been adopted by me, keep in order to be consistent. Right, to be consistent, with the way heat conduction or diffusion equations are typically written in the field of numerical methods that treats these types of problems, okay. However, it's probably useful to also recognize that this is a steady state equation. Okay arrived at from an equation of another type, right? Now, let us suppose that whatever quantity that is undergoing transport like for instance, the temperature, right? If, if one were to include the rate of the temperature, right. Let us go away from steady state but really look at how the steady state problem arises, one would get an equation of the following form. One would have c derivative time derivative of u equals minus divergence of j minus f, okay? Where this would be if you were thinking of the temperature problem for instance, right? The heat conduction problem. This left hand side would be the rate of change of temperature. All right, and the statement here would be that rate of change of temperature was driven by this term which we all ready have previously observed to be the total influx of heat into a little elemental volume. Right. Instead of total let me call it net heat influx. Okay. Right, so that is one way in which temperature rises, simply by heat being transported into a little volume. And then this term would correspond to the local heating. Local, maybe distributed Okay? What we did was, start out with this you know, what, what we did could have been arrived at and probably is arrived at, by starting out with this time dependent form, right. So this is, time dependent form, right. And seeing that well, we have steady state, and therefore we set the time rate of temperature equal to zero. Okay? And then of course that we would have been left with the right hand side equals zero, which we would have rearranged to form presented here. Okay? All right. And, and this is the form that we worked with, okay. But you observe that when viewed in the larger setting of a time dependent or transient problem. The local distributed heating or alternately, if you're looking at a diffusion problem, the local supply of mass, okay. For the way we're writing things out is properly minus of f. Okay. Right its just a matter of the way we we took the signs when we started out with the PDE in this form, all right. So what this suggests, is that if we look back to the equation that ended the previous slide, 'kay. It makes sense for us to simply flip the sign on the forcing tone, okay. And you know, we give it a different symbol, right. And we'll, that will be properly the forcing that is obtained from considerations of how the Steady State PDE can be arrived at from the transient problem? Okay, so so what we are going to do is redefine f bar equals minus f, right, and if you go back, and redo all our derivation with just this little change, what we will see then is sum over e integral w h f d v integral over omega e of this. Right? With the minus sign, 'kay. Is of course integral sum over e, integral over omega e, w h f bar, d v. Okay, and accordingly the minus sign that showed up when I wrote out the the weak form as a sum over elements, right, at the minus sign of forcing term would be absorbed, okay. So, what I'm proposing is also to write now, sum over e, C e transpose, K e, d e equals, wha, what we have so far is this sorry, I noticed I flipped the positions of this transposing element here, okay. c e transpose, K e sorry, c e transpose. F internal e plus sum over e belonging to e n, c e transpose F j e, right. Right, this is what we had at the end of two slides ago. What we're, what I'm proposing now, is to replace this with sum over e, c e transpose, F bar internal e. Okay. Right. Where the association that we're making here is that now. That that this is essentially equal to integral over omega e w h, f bar d v, okay. All right. When we do things this way, f bar turns out we be properly the forcing function that would make sense even in the case of a time dependent problem which is something that we will go to. Okay. So for our purposes there are no big changes. It's just a matter of flipping a sign on that term. Okay, and it actually makes everything consistent now, with the time dependent case. Okay. So, if we agree with if we agree about this little change we can now go on, and essentially we will now go to our step of finite element assembly. Okay. So, the assembly of global finite element equations right, in matrix vector Okay. All right and, and, you know, very well how this precedes essentially it's a matter of looking at each of our element level contributions such as this one. Right or, or, or this one and this one. And understanding how they map into the global vectors? Okay. This is relatively straightforward to do in the case of the 1D problem. It turns out to be a little more complicated in the case of the general three dimensional problem. And what we have to grapple with here is the idea of mesh connectivity? Okay so that is, what we are going to look at now? Okay.

\section*{ID: 2daVbA9fB8Q}
When I went to the time dependent form of the of the strong form of the equation, in order to from there derive the steady state form of the equation, I wrote out the equation in this form. What I omitted to mention there is that this coefficient c is the specific heat of the heat conduction problem. Right? So let me write that here. Right, so it really is a specific heat of the particular material that we are working with. So let me call that the specific heat of the material or the specific heat of the medium. Okay, with that clarification, that equation should make complete sense now.

\section*{ID: HHtenbozn28}
And in order to do this I am going to draw a section of the mesh. I'm going to sketch a section of the mesh. And let's suppose that we're talking of a node there. Now that node lies in general, that is just an interior node. It lies at the intersection of a number of elements. It's important to draw this one carefully, so I'll try to be a little more careful. Okay. That is where things start and to complete this picture, I'm actually going to go to other colors. So, I hope that'll make things clearer. Okay. This, I'm going to extend out to there. Okay. We have that and I will, okay. I'm going to also do the following. I have that. I realized that I drew a line which actually should have been dotted. Okay, I'm going to redraw that line, and make it dotted. Oh dashed. Okay. Then this line also is dashed. It is joined by this one. Okay. This one comes out, comes down on the way here, joins up there. What's this, then? This thing and that should also be dotted. Okay. This one is complete. This one is complete. And right I need to have this, this, that, that. Okay, and back here, I have another one, here. Okay, and this goes all the way down here. Remarkably enough, I think I've completed it. Okay. What I'm trying to show you here after a few minutes of hard work, is the fact that I have here, if you look at it carefully a node. All right? I have here a node, which I'm going to denote as A bar because it's a global node. OK? So what I have here is a global node, global degree of freedom A bar. OK and that's the node corresponding to the global degree of freedom. All right? I want to emphasize here that it belongs to how many elements? I know that may look like a messy sort of figure but I think the basic idea is conveyed. It belongs to 8 different elements. Right? And Let's label the elements as, to which it belongs as, the different color again. Let's label the elements to which it belongs as the element, that is sort of in, front, bottom, and to the left. Is e1, the one next to it is e2, the one behind it is e3, and the one all the way in the back is e4. And I'm going to try to, to, to label it, okay? We continue. I'll call this element, which is to the front, up, and to the left, as e5. The one that's front, up and to the right, as e6. The one that is behind, to the right and up, is e7. The last one is e8. Okay? So what I'm trying to show here is that a, a, general internal node belongs to 8 different elements. Okay? Since we are working with hexahedron. So, global degree of freedom, A bar, in general. Belongs to 8 elements, okay? They are marked e1, up to e8. Okay? All right? So, Now what one can observe is that if we follow a, numbering system in which, for a single element we have a numbering system of the following type. Right, the local degrees of freedom of this element, right? We've been talking of numbering as 1, 2, 3, 4 down there. 5, 6, 7 and 8, okay? This is A equals 1, okay? So, this is local degrees of freedom numbering. Okay, and the big question here is, and this is an important one, how does one represent the way our global degree of freedom, A bar, shows up as a local degree of freedom in each of the eight elements to which it belongs, okay? This the onset of this question is, is what we call or what, what, what generally describes this sort of information is what we call mesh connectivity, okay? The way this is typically provided in, in the context of a finite element. Problem, right? Is to provide enduring input, right? This is provided, provided typically in an input file.  Right? It maybe an input file or in some cases, if the meshes a fairly regular the, this sort of thing can be just generated on the fly. K, typically provided otherwise in an input file. Okay? And the way that, that file is specified is the following, all right What you will have is a listing of elements, right? So, you may have your element, the list, listing of nodes for element e1 or the degrees of freedom for element e1. You may go on to other things and then you may come to element e2. You may have e3, e4, e5. Maybe, they're not all sequential. You have e6, e7. Sorry. E7. I think I'm running out of room here, so let me just give myself more room. E1, e2. That is how I indicate that they need not be sequential in numbering, right? E3, e4. So the elements don't have to be sequential. E5, e6, e7, e8. Okay? So The elements surrounding node A, the, right? The elements to which node A bar belongs do not have to be sequentially numbered in the global, in, in, in our global mesh, okay? All right. Now, but then let's look at things now. What we do for element e1 is simply list the global degrees of freedom that correspond element e1's local degrees of freedom, one through eight. Okay? So let's see how thing work out for this. In this particular case, and I realize I did indeed miss a couple of inches. So let me just provide that in the figure. I did miss this edge. No, I think things are complete. All right. Now. With this setting. Lets look at element e1 as listed here, as drawn here. And ask ourselves a question. What is the position of global degree freedom A bar, relative to element e1? Following the numbering, the local degree of freedom numbering that I have here, it, appears to me that A bar is local degree of freedom seven. Okay? As I speak, I see even more, one more place where I've missed out an inch. Let me get that in here, it's this edge. Okay. All right? So for element e1, A bar is local degree of freedom number seven. So, what one would have here is that you would have other global degrees of freedom in the positions one, two, three, four, five, six. In position seven, you would have A bar. In position eight, you would have something else. Okay? So we go on with that. For element e2, the way I've drawn things out here, A bar would be degree of freedom eight. Okay? So, for element e2, all these positions would have other global degrees of freedom. It's position eight would have A bar. Right? The number a bar, right? Okay. Where a bar would be the global degree of freedom number for that particular node. Okay, things would go on. I just completed for this case for e3, it is five. Okay? For e3, it's five. So one, two, three, four. A bar. Which is five, six, seven, eight. For element e four which is the one on the bottom layer behind and to the left, right? The one I didn't mark. A bar is degree of freedom six, okay? So for e4 is one, two, three, four, five. A bar, seven, eight. Carrying on, now for e5 it is a degree of freedom three. So one, two, A bar, four, five, six, seven, eight. For element e6, it is right. For element e6, it is number four. So it's 1, 1, 2, 3, A bar, Five, six, seven, eight. For element e7, it is actually degree of freedom one. So we have a bar, two, three, four, five, six, seven, eight, and finally for element e eight, that is local degree of freedom two. So, we would have something else in position one, A bar in position 2, three, four, five, six, seven, eight. Okay. So, this information is the critical bit that carries all we need to know about mesh connectivity. All right, and now as you have more and more elements in the mesh, you would have a similar array, if you like, for each element. Some some authors, and some books I guess refer to this as a local destination array. Okay? But that's, that's just a very specific name for it. Anyhow, the important thing is that we need to provide this sort of information. And then, where would we use this information? Well, we would use this information now in recognizing that for As we set about assembling our, matrices and vectors that come from our element, level integrations, we assemble them into global matrices and vectors as global matrices and vectors are going to have components that are numbered by a bar. Right? So any place where a, an element has a local degree of freedom corresponding to the A bar degree of, global degree of freedom, the corresponding component in the stiffness matrix for instance will be added or the corresponding component in the force vector will be added on. Okay? So we'll stop this, segment here. When we come back we will take this particular case and, try to construct, the contributions, to the stiffness matrix and force vectors. Okay.

\section*{ID: fgi9UyFJvnk}
All right. So, welcome back. We'll continue with assembling our global finite element equations in global matrix vector form. All right, and we will do this by using this idea of mesh connectivity. And in particular I'm going to try and to use that example that we constructed to illustrate how, how this proceeds. Okay, so assembly of global, Matrix, Vector, Equations, which really is just the vector matrix weak form. Okay. Essentially, what happens is that the expression that we had on the, on at some point in the previous segment, which is sum over e ce, transpose Ke de, equals sum over e ce transpose f r with our redefined forcing function. F bar internal e plus sum e belonging to the set of elements that had one of their faces lying on the Neumann boundary. C e transfers f j e, right? We have this. Now, this will be, will be replaced, as we know, with a set of global vectors right, vectors and matrices which I'm going to write now as as follows. I'm going to write this as c transpose. K K bar.  D bar equals c transpose F bar. Internal, okay? Plus c e transpose F j. Yeah, just Fj. Okay. The reason I introduced bars here is something you may recognize from our treatment of the one dimensional problem. We know that we still need to account for the proper handling of Dirichlet boundary conditions, and when that is done you will recall that the matrix in place o k bar as well as the vector in place of d bar are going to be a little different, right? They'll have different sizes. Okay? It's just to, leave us that freedom that I'm calling this k bar now. Okay, let's first look at what k bar is itself. K bar is defined then as, through the sort of abstract representation of the assembly over all the elements of the individual element stiffness matrices, right? And likewise, F bar internal is the assembly over elements of F bar internal over each element e, okay? Let's now talk a little bit about K bar, okay? So, so essentially, this process of assembly is something that we go through by looking at mesh connectivity, and simply putting together elements the components of K bar, by accounting for the local and global degrees of freedom. Okay. So let's suppose we were looking at K-bar. Okay. And this will be a big matrix. Okay, let's look in in general, let's suppose we are looking at the at the contributions to the particular row where the row and the row, and column, right? Corresponding to the A bar degree of freedom. Okay? This is where the A bar degree of freedom arises. And likewise perhaps here. Okay? All right? So, what I'm going to give you is a single expression here for all the contributions to the, to the A bar, A bar component of the stiffness matrix, okay? All right. Now what we see is that in terms of the elements stiffness matrices, okay. We have contributions from Ke1 plus Ke2, Ke3, Ke4, Ke5, Ke6, I need more room, as often, plus Ke7 plus Ke8, okay? All right, now, if you look back at the way we described our global degree of freedom, A bar as being related to the local degrees of freedom of elements in 1 through e8, okay? What you will note is that in element e one A bar occupied the seventh local degree of freedom, okay? So the contribution that comes from that element to the A bar A bar entry here would be Ke 1, 7, 7, okay? For e2, it would be Ke bar 8, 8. For e3, it will be Ke bar 5, 5. For e four, it would be seven, sorry six, six six. For e5, it would be 3, 3. e4 it would, sorry, e6, it would be 4, 4. e7, it would be 1, 1, and e8, it would be 2, 2. Okay, note also that because I'm looking at only the A bar A bar component, right, of the global stiffness matrix, or the global in this case conductivity matrix, sorry, we have also the diagonal contributions from each element. Right? If instead of looking at the A bar A bar contribution, I were looking at maybe the A bar B bar contribution, right? Where B bar was some other global node. Okay? All right. Then, we would have contributions to this A bar, B bar component, of the global conductivity or global diffusivity matrix. Okay? Where here, we would have in general, I'm going to maybe write one or two of those contributions. Right? Let's suppose this came from some element E i, okay? All right? Now, let's suppose that in element E i, the A bar global degree of freedom corresponded to the local A degree of freedom. And the B bar global degree of freedom corresponded to the local B degree of freedom. Okay? In element E i. In that contribution, K A B from element E i, right? From the local conductivity matrix of element E i, would show up in the global A bar, B bar position. If in a different element A bar and B bar also showed up as local degrees of freedom. Let's suppose, an element is subject. Okay? And let's suppose, here, in element e j, the global a bar degree of freedom corresponded to the local c degree of freedom. All right? And the global B bar degree of freedom corresponded to the local d degree of freedom. That's where they would show up. Okay, so that would be an off diagonal contribution, okay, from different elements. And, and of course, there could be different elements in here, right? In general, contributing to that, to, to the global A bar, B bar, component of the Stephanos matrix. Okay? So this would, of course, continue. Right, and you would, you would essentially populate the entire Stephanos matrix by doing this. But note in particular that once you have the mesh connectivity information and you have the, the, the, the conductivity, the, the local element conductivity matrix from each element. It's actually a fairly simple matter to slot them into their global positions, their global conductivity matrix, okay? All right, and essentially the same sort of approach holds for the F bar internal, and also the F j, okay? All right, so let's just complete that that bit of assembly as well, okay? It's essentially likewise, right? Let's suppose I'm looking at F bar internal. The global matrix. Sorry, the global vector. Okay. It'll be this, create long vector, right? Because it has all the global degrees of freedom in it. And for essentially, for the same sort of situation that we had, let's suppose that we were looking again at the A bar contribution to this, right? And the A bar would show up maybe somewhere along here. Okay? So now this would be fairly straightforward. What we would do is we would look at F bar internal from elements e1 to. All right? Okay, and now this is fairly straightforward, right? For element e1, we know that the global A bar degree of freedom corresponded to the seventh local degree of freedom. For e2, it was the eighth. For e3, it was the fifth. For e4, it was the sixth, e5, third, e6, fourth, e7, first, and e8, second, okay? That's how things would add up, right? And of course for any other global degree of freedom for B bar actually, if I just continue with the sort of approach that we'd used previously. What we, for, for the global conductivity matrix this is what we would get. We get F bar e i, B plus F bar E J D, okay? All right, and this would be because the, we concluded that, or rather we set it up, so I said the B bar global degree of freedom corresponds in element e i to the B local degree of freedom and in element e g to the d local degree of freedom. Okay? And you will do the same thing for, for, for all for all the other entries, all right? And essentially the same thing with F bar. Sorry, not F bar. Here. Essentially the same thing with our F j global vector, which represents the contributions from the tractions. Okay? All right? If, again, let's just do it for B. For global degree of freedom, B bar, right? So b bar and the global degree of freedom, were, lucky enough to find itself, on, on one of the Neumann degrees of freedom, right? The degrees of freedom in which we have no known boundary conditions specified, then we would have here F j element e i. Contribution from local node B plus F e j contribution from local node, D. Just note that the j that I'm using here is just a superscript to indicate towards that it that this contribution to the forcing comes from the Neumann boundary condition that we're denoting as j, with that we're writing as jn, okay? So the same thing would carry on, okay? All right, so that's how you would assemble these global matrices and vectors that go into our global finite element weak form, right? In, in matrix vector form.

\section*{ID: frxjuCGpVck}
There was a minor error in board work on this slide. It's something that you may have picked up yourself, and it occurred right here. I used ce transpose on the right-hand side of this equation. As you would probably have realized by just examining the equation, that can't be ce transpose, especially because everywhere else in that equation, we have c transpose itself. So. What I'd written here as ce transpose, should be just c transpose. And I can do it by just crossing out that e, okay? Now, when you look at the equation, we have c transpose on the left-hand side. As well as on the right-hand side, and that equation is consistent.

\section*{ID: yUAs8V1t1IQ}
So, let's return, then, to what we had for our global equations, right? So we return. To c transpose k bar d bar equals c transpose F bar internal plus c transpose F j, all right? On the previous slide we looked at how we use our mesh connectivity information to construct these global matrices k bar and global vectors F F bar internal and F j. The next thing we do here is account for Dirichlet boundary conditions, okay? Now, suppose. That, global degrees of freedom. Right? A bar, sorry, B bar. And so on, right, belong to a set that I'm going to call A sub D, right? Or maybe I should be calling this A bar sub D, okay. Which is the set of global degrees of freedom on which we have Dirichlet boundary conditions specified, okay. This is the set of global degrees of freedom on which Dirichlet boundary conditions are specified. Okay, well we know that if that if A is the local degree of freedom in some element, if A is the local degree of freedom in element. Say e i, 'kay, corresponding. To global degree of freedom, say A bar belonging to the set, right, the set of Dirichlet degrees of freedom globally, right? If this is the case, then we know that in that element, w h sub e i, right. Is constructed as a sum B equals 1 to number of nodes in the element. B not equal to A, NB CB e, all right? This we know very well, right? Okay, which essentially corresponds to the fact that when we look at the right, we, we see that this implies that now when we look at the c vector, c transpose, right. Right, it will have you know, c whatever, c D bar c E bar, right? Corresponding to various global degrees of freedom, except that what will happen is that there will, there will not be, you know, sorry. It will, it will just go on right length, right, like this, right, F bar and so on. What we'll, what we'll be missing here is A bar degree of freedom will be missing, okay. All right. And this will hold for all degrees of freedom that have Dirichlet boundary conditions specified on them, all right. So if this is so, we know that now the dimension of c transpose is going to be somewhat reduced, right? So, now if the measure of set A bar D, right? Which is, which in this case is just the number of degrees of freedom belonging to this set, right? If measure of A bar D, which is number of degrees of freedom belonging to A bar D, right? If this is equal to ND, okay, what we know is that we can say something about the size of c transpose, right? What we are seeing, then, is c transpose is a row vector of dimensions the following, right? Number of spatial dimensions times total number of nodes in the problem, right, minus N sub D, right? This is the dimensionality of the row vector c transpose, okay? But our vector d, our global vector d, so your global vector d bar, okay? As we've written it in our global vector matrix equations has all number of spatial dimensions, n sub s d, times number of nodes. Okay, of course number of spatial dimensions because you are developing a problem in three dimensions, is 3, okay? All right, but then, we also observe that d bar consists of entries which are the form d1, d2 and so on, okay. But now let me see. We said a bar and d bar belong to the set of global degrees of freedom in which Dirichlet boundary conditions have been specified, right? So when we come to d A bar, d B bar, right, going all the way down to d n s d times number of nodes, right? What do we know about d A bar and d B bar? We know them, right? These are known, okay? So, what this also tells us is that our global matrix K bar has dimensions, number of spatial dimensions times number of nodes in the problem minus the script, N D, right, those many rows, okay. Times the full number of columns, okay? So K bar is not a square, is not a square ma, matrix, right? It, it, it's a rectangular matrix, okay? Right, so the, the situation that we have now is in c transpose K bar d bar equals c transpose F bar internal plus c transpose Fj, right? Well, some of the degrees of freedom in d bar are already known, right? >> Hm. >> Those are the ones that are known, okay? So what we do is we simply move those, you know, the columns from the K bar matrix, right? That correspond to the known degrees of freedom from the d bar matrix, sorry, from the d bar array over to the right-hand side, okay? So what we will do now is, is the following, right? So what we say is that c transpose times a reduced matrix, K, times a reduced vector d, okay. Equals, c transpose F bar internal, plus F j, minus c transpose times K bar. Now, for K bar you will have the A column for the A bar column, all right, times the dA bar degree of freedom. The dA bar degree of freedom here is just a scalar, all right? Likewise, you will also have c transpose, times the K bar B bar column, times the scalar degree of freedom, d B bar, okay? Now you specify that this as well as that are A bar and B bar columns of. The full K bar matrix, okay? Likewise, the d matrix now, is obtained by, removing. A bar and B bar degrees of freedom, right? And any others, degrees of freedom from d bar, okay? All right, now this is done for all A bar, B bar. Belonging to our set, A bar D, okay? All right, so what are we left with then? We are left with a system, right, in which we have a system of the form c transpose K d, right? Where the dimensions of K now is N s d times number of nodes minus the size of this set of Dirichlet degrees of freedom, essentially squared, 'kay? And so we'll be using notation a little here. And of course c transpose and d are vectors, also, of the size N s d times number of nodes minus N D, right? Okay, this minus F bar internal, minus F j, minus K bar, the A bar column of the original full K bar matrix. Multiplying d A bar minus the B bar column of the original rectangular matrix, K bar. Multiplying the d B bar degree of freedom, okay? All right. And this entire vector, now, right, also, is a dimension, N sd times number of nodes minus N D, right? It has to be, right? And, indeed, the way we've set up things, it is that, right. It does have that dimension, okay. So all of this has to be equal to 0, all right. All right, so then, this has to be equal to 0, and this has to be equal to 0 for all c. Sorry. All right, let me write it down here. This has to hold for all c belonging to R with that dimension, right? Number of spatial dimensions times number of nodes, minus ND, okay? And you remember where that requirement comes from, all right. It comes from the requirement that c degrees of freedom are the ones that interpolate our weighting function, okay? Since this must hold for all c transpose in that space, the only way this can work out is if we have K d equals everything on the right-hand side, everything, that makes up, that vector, right. F bar internal minus F j, minus K bar A bar d A bar, minus K bar B bar and d B bar, okay? And now, of course, we're going to call all of this our final forcing vector, okay? So, we've arrived at the same matrix vector form of our final finite element equations, right? K d equals F for linear problems, okay? So this is our final matrix vector equation. All right, we are going to stop this segment here.

\section*{ID: 1G6uOV5SY6c}
There were a couple of errors in sign that crept into this slide. The first error appears at the matrix vector equation at the top of the slide and the error, errors are that both of these last two signs should be plus. And then, when you come to the very last equation, this one, which, as you will recall, was obtained by just moving the terms that are braced. This equation to the right hand side. When we do that, everything is correct, except for this term, which should also be plus. With that the equations are all consistent and everything works out just right.



\end{document}
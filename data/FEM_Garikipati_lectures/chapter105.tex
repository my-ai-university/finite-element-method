\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}

\section*{ID: lb5gmDZbPD4}
Welcome back. Today we are going to start and hopefully get through most of a new unit. And that unit is going to address basis function in an in, in an, in, in a unified manner, okay? The idea is that the most standard basis functions, which are based upon Lagrange polynomials. Are constructed in two and three dimensions. Essentially through a process that is called a tensor product, representation. Right? And this is done by first constructing basis functions in 1 D and then essentially extending them to multiple dimensions. So, we're going to take that approach, and, and, and doing so present a hopefully unified picture of basis functions that we have already been using. So, all right. So we're going to look at a I call it just that, we're going to look at a unified view Of basis functions. In 1 to 3 dimensions. All right. Okay. The the approach is to start with 1 D and and, and work up from there. I should also mention that we'll start the segment by looking at this approach for Lagrange polynomials. Okay, so we're going to look at Lagrange polynomials and let's start by recalling 1 D right. In 1 D let's look at what our linear basis functions look like. And that's our element. In 1 D, we're talking about linears, so this is what we have. Of course, we construct everything in our bi-unit domain, okay? So, in this bi-unit domain we have C. I'm going to call it C 1, just to prepare ourselves for smoothly moving on to two and three dimensions, okay? So this is C 1 equals minus 1. C 1 equals 1. OK, and of course this is 0. All right? In this setting we know very well what our linear functions are. Okay. That's meant to be linear okay? So this is N 1, and we have N 2 Okay and of course they satisfy the usual properties Kronecker delta and the fact that they evaluate to one when added up at any point C 1 in the domain, okay? These are linears and you recall how they were written? They were written as N 1. Of C 1 equals 1 minus C over 2, and N 2 C 1 equals one plus C 1 over 2. Okay and then we went on to develop quadratics. Right. We have our element. C 1 is minus 1. C 1 is 0. C 1 is 1. Okay? And in this case. Our first basis function looks like This. Right? That is N 1. N 2 looks like. That, right. That's N 2 and N 3 as you will recall is that. Okay. Allright, and the expressions for them were N 1 function of xi 1 is 1 minus xi 1 times xi 1 over 2. N 2 equals 1 minus. C 1 squared, it's just one minus, C 1 squared, and then N 3, is 1 plus C 1 times C 1, divided by two. Okay all right, so we know these ve, well enough, and, the reason I put them down here is just to get us started. You recall that when we developed the finite element formulation in one d, we also presented the general Lagrange polynomial formula. Right, for, arbitrary order, right? And that formula was the following, right? So. Right? So when we develop Lagrange polynomials of order. So we going to write this out for Lagrange polynomials of order k Right? And you will recall from our treatment of problems up to now and actually from just looking at this at the sketches of our basis functions, that the order of the polynomials bears a relation to the number of degrees of freedom on the element, right? And what is that relation? Relation that the, that the order of the polynomial is the number of degrees of freedom minus 1. So, we're going to write that out. And, we, we previously called this number of nodes in the element. I'm going to use that sort of, idea. Except, I'm going to call this number of nodes in 1 D. Okay. So it's number of nodes in 1 D minus 1, which gives us the order of the polynomial. The reason I am bringing in this reference to 1 D is as you will see, we will use the one d construction to build our constructions in two and three dimensions, okay? All right. So so the polynomials of order k are the following, right? So we know by looking that at, at, at what we've written out there that we can, we have N A, right, function of C 1. Equals the product index b, running from 1 to number of nodes in 1 D, okay? Except that B. Is not equal to A. Okay. Now the product that we need to have here is the following, C 1 plus C 1 at A, divided by Z 1 at A, minus Z 1 at B. All right, and if you test this out for any of these functions I put up there for linears of quadratics, you should see that it works. Okay? So this is the setting we have. Now, we are going to generalize this right? Essentially what we are going to see and what we have seen already is that when we go to two and three dimensions, and we did look at two dimensions briefly, we're going to see that we essentially construct those bi-linear and tri-linear or bi-quadratic and tri-quadratic functions as we will also see, by forming what we call tensor products of these functions in 1 D. Okay, also what I'm going to do is that because I'm going to now use this to construct our functions in, in higher dimensions, I am going to put a tilder on everything I have written here. Okay? This is just so that I can use N itself for the actual functions in higher dimensions. Okay? Same function. I'm just putting tildes on all of it. All right, so this is what we have. So let's go, let's move on now to tensor product functions. In let's look at 2 D. Okay. Now, in 2 D, we already did see the bi-linear shape functions. Okay, so recall the bi-linears All right, the bi-linears were the following. I'll sketch one of them all right? So because we want to make use of this perspective view. C 1, C 2, okay? And we label the nodes here. Or, degrees of freedom here as A equals 1, 2, 3, 4. And, I will draw just a, and 4. Okay? And you recall that is bi-linear, so it, it, it slopes down linearly to 0 along each coordinate direction, okay? And, it also slopes down to 0 in this dimension, all right? And this dependents actually, not quite linear but looks something like that. Okay? All right, so, so this is N 4. Okay so this shape, this basis function I am showing you is N 4. Okay. And, okay, so this is what we have. I'm going to directly just write out the, the, general Lagrange polynomial formula using the stance of product, using the denser product approach. Okay. In order to do this let's note that the kind of bi-linear shape function we are constructing here has n n e degrees of freedom. Right, we're constructing four basis functions because, because this element has four degrees of freedom. And in this case, n n e equals 4. Okay? Note of course that n n e equals 4 is a number of nodes in one dimension squared for, the linear case, right? So if you take the linear basis functions, right? And well, the, the two more degrees of freedom, the bi-linears in for, for the 2 D case involve four degrees of freedom, right? So that's n n 1 D squared. What we are going to do here is, is observe that we always have some NA, Z 1, Z 2, right, are functional Z 1 and Z 2, is going to be written as a product. Of N tilde B. C 1 times N tilde C, C 2, okay? For B comma C equals 1 up to number of nodes in 1 D. Okay? This is the general formula, alright. And A of course here, A which is on the left-hand side equals 1, 2, number of nodes in the element in 2 D. Okay. It's really as simple as that. Right, and it is, and, and this sort of formula where we're really taking those basis functions we developed on the previous slide that we labeled N tilde, right? We're taking them in each dimension, right? Along each of the dimensions the, the C 1 and C 2 dimensions and simply forming a product with it, right? This sort of a product is what we mean by the denser product formula. Okay, with this sort of view, what we observe now is that N 1, C 1, C 2 is a product of N 1 tilde. C 1 N, I'm sorry, N tilde and N It's a product of N, N 1 tilde in the C 1 direction and and, two, two three four. N 1 tilde again in the C 2 direction. All right. And that becomes clear by just looking at it. So this goes on. Well actually let me write all of them. N 2. N 2 is what? In the N 1 for, in the C 1 direction. Look C 1 dimension it is N 2 tilde C 1, N 1 tilde C 2, right? This goes on and we finally write N 4, okay? N 4, C 1. C 2 is, can you work it out? It's N 2 tilde, sorry, it's N 1 tilde again. N 1 tilde, C 1. And. 2 tilde. C2. Okay, it's as simple as that. Right? Okay and you recall we did indeed look at the bi-linear case, right? And now if, when we go on to higher dim, when we go on to basis functions of higher order, all we need to do is to go back to the polynomial sorry, to the 1 D. Construction. And, figure out what order we need to order. And it send the order in 1 D that we need in order to construct these functions in 2 D is simply the square root of the degrees of freedom that we want to have, right, in 2 D. Okay, so we construct those 1 D basis functions, and then form the sort of products with it.

\section*{ID: MNonArg5EZs}
So I introduced a couple of errors in the formulas on the segment, and both errors appear in the very first slide. So, if we go to the bottom of the slide, and look at the numerator here. If you try to evaluate it and see whether it gave you the sorts of basis functions you'd expect, it doesn't because it's not right. Instead of C1 plus C1 sub A, we should have C1 minus C one sub B. The numerator's all right, but with this correction things work out. However, if you look to the right, there is another error. And that is fixed by just introducing a minor sign on the right hand side of the expression for N1 tilde. With those corrections, everything should work out.

\section*{ID: Pu-E1i2S3Ro}
And now you immediately know how we are going to for our basic functions in 3D, right? Already written about previously, you written out our trials. At three forward to just write out the formula now, all right? So we say likewise. In 3D, right? In this case we have number of nodes in the element equals number of nodes in 1D cubed, okay so it won't definitely be constructed down the trilineals we observed that we had eight degrees of freedom. Right? Two cubed. Okay? So it is the same approach. So, we have you recall that the tri-lineals, right? The tri-lineals were. Right, for the trials and errors we had this sort of element. In the by unit domain. Okay. Right. So in the setting once again, we simply have N A phi 1 xi2, xi3 equals N-tilde B C1, N-tilde C C2 N to the D C3, right? Where B, C, and D belong to 1 up to number of nodes in the 1D element. And A belongs to the set 1 up to number of nodes in the element. Okay? So it's completely straightforward to see how we construct these, and then it's just a matter of figuring out the numbering. Okay? The numbering in this case, of course, is A = 1, 2, 3, 4, 5, 6, 7, 8. Okay, if we were to go to triquadratics, there are various numbering systems that I use, depending upon the particular preference of the people consulting these functions. But typically for traffics, let me show you what sort of numbering is followed. Okay, and we're going to draw that. Again, and I'll try to draw it a little bigger. Okay. So these would be our vertex nodes. Or vertex degrees of freedom. I'll switch to a different color to draw the mid-side nodes. The mid-side nodes would be these ones. Okay? And then there are the mid-face nodes, which are these. Let's see, one, two, three. I guess I need to have one on and that sort of does it okay. And there's one final which is a mid body node for which I'm sort of running out of colors here, but maybe I'll use black. Okay, this one would be in the very center of the cube. All right so the numbering that's typically used here is the following. Often the vertex nodes are numbered one through eight. And then the mid-side nodes are numbered next. Sorry, I began numbering the, some of the mid face nodes instead. So you have nine, ten, eleven, twelve. Need to erase this one, as well. Okay, 9, 10, 11, 12, actually, maybe I won't use that number in system. What I'm going to do, instead, is this one, 1, 2, 3, 4, 5, 6, 7, 8. Okay. Okay, so I have one, two, three, four. Sorry. One, two, three, four, five, six. Seven, eight, nine, and then continuing on, we get 10, 11, 12, 13, 14, 15, 16, 17, 18 and we get the mid body node. 19, 20, 21, 22, 23, 24, 25, 26, and 27, and with the mid face node on the top. Okay. So there we have it and other numbering schemes are possible, now what one has to do in constructing this triquadratic basis functions is pick any one of these nodes. So let's suppose we decide to construct a basis function for node five, all right a degree of freedom five. Okay, all we have to do here is observe that that basis function could be constructed from the n third of By choosing for the xi1 direction, something, something else for the C2 direction, and something for the C3 direction. What I'll do now is to come back and say which ones. All right, let's look at this. The xi1 direction is this. That is the C2 direction, and that is C3. All right, so that's now, that is the five, right? That is the basic function we want to control. So, in the C1 direction, right? When we look back at the way we number, the basis functions in 1D. And you see in one direction we are using the N2, N2 tilde. Okay, in the C2 direction we are using N1 tilde and in the C3 direction also we're using N1 tilde. Okay? So that's the sort of check one would make in writing out these basic functions in three dimensions, and of course the same approach would be used in 2D as well. Okay? The nice thing to observe is that it is just the very simple product formula that one has to use. So we construct these basic functions and then generalize them to two and three dimensions. Of course, this works out as long as we are looking at elements that exploit the product instruction. Right? So it is indeed that when one can go from simple 1D segment in 1D to quadrilaterals in 2D and hexahedron in 3D. Okay. In a couple of segments we will look at a variation on that idea. All right, but we'll end this segment here.

\section*{ID: uIRkSoONw-g}
Hi. In this segment, we'll begin looking at the coding template for the second coding assignment which is homework three. All right? So, to begin with, let's look at the source file, main.cc. All right? Now, I've actually included two sets. We have a source files and header files for the 2D problem and for the 3D problem. Okay? So, let's look at the 2D problem first. You can see, it's basically the same structure as the previous homework assignment. One difference is that now I'm setting the dimension equal to 2, which we're then inputting as the template parameter for our finite element method class. The other difference is that, when we're generating the mesh, instead of passing in a single integer, since it's 2D, we're passing in a vector with two integers that defines the number of elements in each direction. So here, the name of the vector is num of elements, or num-of-elems. And for an example, I've set up four elements in the x direction and eight elements in the y direction. A four by eight element mesh. Of course, you can change that as you're doing your programming, but that's the set up. Okay? Other than that, it's pretty much the same. We have generate mesh, set up system, assemble system, solve and output results. We aren't calculating the L2-norm of these problems. It's just you can, you can do it for a 2D problem, if you have the exact solution. However, for the problems we give you, we aren't giving you the exact solutions as well. So, you don't need to bother calculating the L2-norm. If you did however, you would still be able to look at the rate of convergence as you increase the number of elements. Another difference or two other differences are that in this case, our constructor for problem object for a finite element class. We don't have any constructor. We don't have any inputs to the constructor because we are only doing linear basis function, so we don't have to input the function order. And since they've already split up the problem into a 2D and a 3D problem, there aren't any further sub-problems after that. There's only one 2D problem, and one 3D problem on this homework assignment. So, there are no inputs to the constructor, okay? So, let's go over to the template. This is FEM homework 3a template. We have the same header files here. Again, using namespace deal two. And if we scroll down to the declaration of the functions and objects, let's look at that. All right. You'll notice that I didn't include the function C at node, whatever. Because since we're doing linear basis functions, it's a lot easier to keep track of what the value of C is at, whatever node you happen to be at, okay? So I deleted that. If it was useful to you, you're free to go ahead and write such a function yourself. Now let's look at basis function and basis gradient. There are a couple differences here. First off, we have more inputs. We still have the node number, but now we have C1 and C2. Obviously we have two Coordinates since it's a 2D problem. Also, with basis gradient instead of returning just a double, since we are 2D the gradient is a 2D vector. And so I'm returning the standard vector of doubles. Okay? The solution steps however are the same generate mesh to find boundary condition setup system. Assemble systems solve an output results. Except of course that generate mesh accepts as a vector of integers instead of just the single integer. The D02 Class objects are the same. The Gaussian quadrature rule is the same. All the data structures in solutioning those are all the same in this assignment. All right, so let's scroll down here, to the constructor. Again we still have the same method of calling the constructor of our class variables fe and dof-handler just as before. But since we don't have problem and order as an input, we don't need to worry about storing those. We have the same destructors before as well. All right? Moving on to the basis function and the basis gradient. Again, we talked about the differences in input, where I have node xi-1 and xi-2 node. But you'll still do it in the same way. And probably in this case it wouldn't make sense to do a generalized functional, though you can certainly go ahead and do that. Since it's a lot simpler. Since we're just doing linear basis functions and that end. So we don't necessarily have so many notes as before. You can just use a simple if statements. If the note equals such and such. Evaluate the basis functions using C1 and C2. All right? And then again, you'll return value. Similar with basis gradient. You notice here, when I declared this standard vector values, which is what will be returned, I've already set the length, or the size of values to be dim. Of course, dim we've defined in main.cc, we've defined it to be two in this case. So this creates a vector of length two. If it initialized the contents to be 0, of course, they would have been 0 automatically, anyway. But you will need to go ahead and evaluate the basis gradients at the given nodes for both components of this gradient vector. All right? In some ways, simpler than before because we're only dealing with linear basis functions or rather, bilinear basis functions I should say, because it's 2D. But of course since they are in two directions, that is something else for you to consider. All right? At this point I should point out that DL2 again had some different node numbering from the class. All right? So let's go to the board here. In the class, or rather in the lectures.  We went around the nodes, since we're only dealing with bilinear, we only have nodes that have the vertices here. But there's something like this, one, two, three, four. All right? In deal two, it's a little bit different. Again as before as with the linear case. We'll be starting at 0 in our numbering but we're always going left to right. We go left to right, bottom to top. So 0, 1 and then up here it's 2, 3. Okay, so that's something for you to consider in your if statements. What would've been basis function one according to the lectures is now basis function zero according to DL2. What would've been basis function two according to the lectures is now basis function one according to DL2 and so on. Okay. Okay, so in this case remember we need to use the DL2 basis function node numbering. All right? Because we are using D02s mesh. We're using D02s connectivity, matrices, and so on. So remember to use D02s known numbering as you're creating your basis function and the gradients of the basis function. All right. So moving on to generate mesh. Again, it's very simple, there's not much for you to do. You just have to define the limits of your domain, the left and right, top and bottom. So x min, x max, y min, y max. And again, we'll use DL2s functions to create that mesh. So not a lot for you to edit there. And now we come to define boundary conditions. Again, this is defining the Dirichlet boundary conditions. In this homework, homework three, you won't have any knowing boundary conditions, and we actually don't even have a forcing function or a body force at all. All right, sorry, in this case a forcing function since it's a temperature problem. And so these Dirichlet boundary conditions are the only Dirichlet, are the only boundary conditions we're applying. Of course there are Neumann boundary conditions of zero, which we don't have to do anything about in our coding. Now I have, you actually have to create this function yourself but it's going to be very, very similar to this same function, to find conditions in the previous homework, alright. You'll be looping over all of your notes and you'll have an if statement to check to see what boundary you're on. Now the difference is, that now you have both x and y coordinates. And so, you'll need to check. In order to check your boundary out, you'll have to specify, are you checking the x value, or the y value? The other difference comes in that, node locations whereas before it was a vector, now it's a table. Let me scroll back up to the top and so you can see, table is a DL2 function or a DL2 data object. And the two here just specifies that it's a two dimensional table. All right? And it holds doubles, okay? So let me just sketch out what that would look like. Two dimensional table, the row index, is going to be the global degree of the global node number. So it's just like the index and the vector of node location before. So this will go up to, just the largest node number, global node number. Okay? So that specifies which node you're at. The column just tells you which component of the position looking at, you want to know. Really it's telling you, do you want the x coordinate or the y coordinate. Okay? So for example if this is our mesh. And I'm actually, not exactly sure how DL2 does the global node numbering in 2D. So I'll just do it this way, but again, DL2 does it in this, and they do it in a different way, so. Anytime you want the global node number, you'll actually need to use local DUF indices to go from a local to a global node number. But let's say the value here is, say this coordinate is 0,0 and just for simplicity I'll make this 1,0, 2,0. This would be 2,1. This would be 2,2. And so on. Okay, so this table over here would look like this. At Node zero, the x-coordinate is zero, the y-coordinate is zero. At Node one, x is one, y is zero. At Node two, x is two, y is zero again. If we come down to Node five, for example, x is still two, the y has a value of one and so on. Okay. So if I wanted the x coordinate at node five, I would use node location. At node five. If I wanted the X location I would take 0. And then that would return to me 2. Okay? So again, this is the global node number. And the second component, the row, or sorry, the column number, is the component. Again, essentially X or Y or in the case of the 3D problem that we'll look at, Z. Okay? All right. So with that, you should be able to fill in this defined boundary conditions function pretty well. Now, let's stop there in this segment and in the next segment, we'll move on to looking at setup system and assemble system.

\section*{ID: jrrrApF4s-0}
Alright, we'll continue with our development of basis functions in multiple dimensions. What we did in the previous segment was to, observe that the stensor product of idea allows us to very conveniently construct, the higher order, the functions in, in higher dimensions, using the functions in one D. Okay, in order to continue and, and provide us with, ourselves with everything we need to complete formulations in two and three dimensions, there is one thing we haven't yet done. Even for our 3D formulation of the linear elliptic PDE for the scaler variable, and that thing is integration. All right, we haven't talked about how we do numerical integration in multiple dimensions, all right, that's what we will aim to do in this segment. So the topic of this segment is numerical integration. N again. One through three dimensions. Well, let me write out the word. The approach we will use is conceptually similar to what we did in previous segment. Right? Side of forming the extensive product formulas. Okay, and to start out let's go back to one d. Okay, in one d this sort of situation we are confronted with is the following. We need to integrate from minus one to one. Some function, say G of C1, D, C1. Okay. Every single integral that we need to evaluate in our finite element formation reduces to this. Okay. And of course this is the form that's applicable to every component of, of a matrix. Whether it's our stiffness matrix, or conductivity matrix, or, or our forcing function. And so on. Okay? And you recall how we did this in one d? Right? How did we go about doing numerical integration in one d? Remember? Yes we did it with numerical quadrature, right? Which basically says that okay let me just do a sum instead of the integral. Okay? I'm going to do a sum with l going from one to number of integration points Okay? I am going to evaluate g xi 1 at all these different values of l, all right? And once I do that. I am going to multiply each of those function evaluations with the, a weight, w L, okay? So, what we have here are quadrature points. P-T is short for point and that is a weight. Okay, that's the general approach for numerical quadrature, of course we use Gaussian quadrature. Okay? And Garrison Quarters said that well if you choose to do it with a single integration point. Okay? You would pick c l equals zero and w l equals two. Right. Okay and the reason for that weight of w l equals two is something that we understood from how we would integrate constants. All right? And this went on, right? So, for instance, if you went as far as n int equals 3. What we would see, it. What we would observe is that we would get sorry. I should have paid attention to my notation here, this will be c one, here right? Okay. So if we were doing an n equals three, then c one, one at n equals one would be minus square root of three over five. W one would be, five over nine. C one two would, by symmetry, be the point zero. W two would be eight over nine. And, c one three would be the point, root three over five. And the weight, you may remember, or you may figure it out from symmetry, would, again, be five over nine. Okay? And we saw that you know, Gaussian quadrature we said that a rule of that n int rule, right, or n int point rule integrates.  A polynomial of order two, n, int, minus one exactly. Okay? So, we'll recall all this. What we're going to see is, that, when we go to multiple dimensions, it is essentially a tens of product idea. Right? And already, tens of product rule that calls on this basic idea. All right. So let's go ahead and see how that works out. Okay. All right, so in 2D. In 2D as you may imagine you need to integrate terms of the following type. Need to integrate, okay? Minus 1 to 1. Minus 1 to 1. G, generally a function of c1 and c2. d c 1 d c 2. Okay, and these limits, let's say, for c 2, and this is for c 1. All right. It's really very straightforward. Let's suppose we first do the integral over c 1, okay, and we're going to do numerical quadrature, okay? So, right, so I have numerical integration or quadrature. Right? So what numerical integration says is that well I can just break this up. I can write this as an integral c2 equals minus 1 to 1 and now for the inner integral. Right? Over c1 I will first use quadrature. Okay? So what this says is that I'm now going to talk of doing a sum let's say L1 equals to number of integration points along that dimension. Okay? All right, so we have L1 to number of integration points. We have G, C-1, L-1. Alright, you pick quadricant points only, along the C-1 coordinate. And, you leave the C-2 as the continuous coordinate. All right. So, you, you retain, therefore, the elemental d x e 2, okay? Actually, let me write that. We're first doing the integration for expectancy 1, right? So then, we just multiply this by w and 1, okay? So, what I have here in parentheses is the result of having it integrated over C 1. Right, yah we did numerical integration over at c 1, but nevertheless there we have it. Okay, d c 2. Right. And then we come back and do the same thing over at c 2. All right. So numerical integration would give us this, and then as another step we would get now Sum l 2 equals 1 to n int. Sum l 1 equals 1 to n int. G c l one. Sorry. C 1 l 1, c 2 l 2, w l 1. Wl2, and we're done. Okay? Now, of course, this is, based on an assumption of symmetry in c1 and in c2, right? Symmetry in the sense that we are, considering the case where we have a polynomial of, perhaps, the same order in c1 and c2. All right, that's why we've chosen the same number of integration points along the two directions. Okay, of course, this does not have to be the case, right. We are always free to choose different number of integration points in the different directions. Right, if we know something about our polygon, right? So I, I'm going to make this more particular by changing this to n one int. Right, and this to n 2 right, it just says that we can use different number of integration points in the different directions, right, so the remark here Can use different number of integration points. Along c1 and c2. Right, if you know something about the polynomials that we are working with, okay. And of course we would be, we could use Gaussian quadrature in each direction, right? And everything would work out. The, the integration points along c 1 and c 2 would be the same as the Gaussian quadrature points and so would the weights, all right? All right okay, and here, what you will see here, of course, I'm not, I'm not going to write it down, it's something that you can conclude easily by going back and looking at the formulas. What you will see is that, in, in the two d case, the weights add up to what? Consider what would happen if you had to integrate a constant. Okay, so what happens here is that I just write it out as a sentence, here. Sum of. Weights. Equals 4. Okay and this comes simply because we're integrating over bi-unit domain, right? And the area of the bi-unit square in 2D is of course 4. Okay, so that's where that's from. Alright, now we can extend this to 3D, right? And this would be completely clear how to do it in 3D. Right? In 3D we're trying to integrate something of this form, right? We're trying integrate c 3 equals minus 1 to 1. C 2 equals minus 1 to 1. C1 equal minus one to one G of C1 C2 C3, D c1, D c 2 d c 3. All right? I'm straight away going to just write out the formula. Our numerical quadrature formula is sum l three equals one to n three int, right? Number of integration points in that c 3 direction. L-2 equals one to number of integration points in the, C-2 direction. L-1 equals one to number of integration points in the C-1 direction. G, C-1. l 1 xi 2 l, sorry, xi 2 l 2, xi 3 l 3, right? Times. wL1, wL2, wL3. Okay? And again, we could be using Gaussian quadrature points and weights where the points are again simply determined as the For the points as chosen in the one d case. So this is how we would construct our, we would, we would actually evaluate all our, our integrals. Alright, so, so this is for the generally case, sorry, sorry, not the generally case but the case where we are looking at either a bi unit domain in one d. Two d or in 3 d. Okay, and that bi unitness is reflected in the limits on these integrals. All right. We'll end this segment here. When we come back, we will look at a, different type of basis function.

\section*{ID: LVbv05-l-0Y}
Hi, now we'll move on to looking at set-up system and then assemble system. And we'll probably finish up with the homework 3 template in 2D in this segment. All right, so lets look at the code. Here in setup system it looks very, very similar to problem one, or rather to homework two, which was our first coding assignment. Slight differences here, is that no location, again we have two indices. Again the first is your global node number, just as before. But again, the column number now is which component of the position vector you want. Do you want the x-component or the y-component, or in 3D, the z-component? Okay, we again call the define boundary conditions function. Resize our vectors and matrices, we define the quadrature rule. Again, you'll need to decide what quadrature rule you need. Is two enough, do you need three? Or more, or less? Not a lot to do here in setup system, not much different from before. But now we'll move into assemble system Okay we have the same setup before k=0, f=0, same constants, data types. We set up our element loop the same way. Again we update the local DOF indices vector with element. So that we can again relate our local degree of freedom numbering, or local node numbering with the global node number. But, now we have introduced these other objects. We have a full matrix called Jacobian, and then a double detJ, which will hold the determinant of the Jacobian. The Jacobian you can see is of dimension dim by dim, or in this case a two by two matrix. It's the Jacobian for the mapping between the real domain and the bi-unit domain. Remember, in the 1D case we actually did have this Jacobian, but it was just a 1 by 1 matrix. Essentially was H, E over two. All right, so that's how that showed up there. In this case we have to create the matrix itself. Now in the general case, elements aren't always going to be the same. And they aren't always going to be square. Or have nice right angle corners like they are in this problem. And so, I've kept the Jacobian general for that case. However in our case, as you're checking your code you may notice that the Jacobian is actually the same at every quadrature point in every element. And that it's actually a diagonal matrix. That's because of the uniformity of our mesh. Okay, but in general with an unstructured mesh, you wouldn't have the Jacobian being the same at each quadrature point. And it wouldn't necessarily be a diagonal matrix. So on this first part, I have the same structure for defining Flocal. Again, we don't have Neumann boundary conditions. Or at least, we don't have non-zero Neumann boundary conditions. And we don't have any forcing function. So Flocal is actually zero for this problem. However I've left this structure there in case you wanted to include a forcing function. So it's there for you if you'd like. Again, all the structures there you would still have to define what the eighth component of Flocal was. Just as you did in the 1D problem. All right. Now for Klocal, I also created a full matrix called in the Jacobian, which will store the inverse of the Jacobian. I'll go into that a little bit more on the board in a second. I've also created this full matrix Kappa, which will be our connectivity tensor. In the assignment, it's already defined for you that it's a diagonal matrix with the value of 385. Okay so now let's look at K local itself. And actually before we get into that. Again Klocal used Jacobean, uses the determent of the Jacobian. Which actually comes into play with the quadrature, numerical quadrature. And So that's where it will come into play. So let me look at the Jacobian for a second. Look at it in 2D, but it easily expands to 3D. Okay. So here's the Jacobian, it's in 2D. It's just the partial of x 1 with respect to dPsi 1. Partial of x 1 with respect to dPsi 2. Here of course x 1 is x, x 2 is y, if you're thinking of it that way. Partial of x 2 with respect to x dPsi 1, partial of x 2, with respect to dPsi 2. Another way to put a more generally. We could say that the Jacobian of  is equal to the partial of x, sub i with respect to dPsi sub j. And, of course, that easily transfers over to the 3D problem. Okay, now where do we actually need this? Well of course, we need the determinant of the Jacobian to do our numerical quadrature. Again we saw the HE over 2 and we did quadrature in 1D. The HE over 2 was the determinative of the Jacobian. So that will show up again, and we'll see that in a second. Now the other place it will show up is when we're doing our basis function gradients. Remember when we define the function basis gradient. It was the derivative with respect to dPsi, okay. Now for Klocal, let me just look at Klocal . Something to point out, first is we have this negative sign. I include the negative sign in the Klocal. So that when we're solving the actual system we can still do, D is equal to K inverse, F, okay? So, I include that, minus sign here. We have this integral over dPsi domain. In this case it's a double integral, so I'll just include that here. We have our basis function A. Now it's the derivative, I'm going to write it out here. The derivative with respect to X, I. Times kappa, I, J. And then a partial of B with respect with X,J Determinant of j. Sorry, the determinant of the j isn't necessarily with the numerical quadrature. It's with the change of domain. The change of indices from our real domain to our bi-unit domain, just to clarify that. dPsi 1, dPsi 2. And I think this notation we used in the lecture. When you have this repeated index there's an implied summation over I and J. Over I and J. And it's going from using our C++ notation would be 0 up to dim-1, okay? So in this case, I will do a summation with I equals 0,1. And a summation with J equals 0, 1. All right, but now this term here, or rather both of these terms. These are the gradients with respect to the real domain, but we only have the gradients with respect to the bi-unit domain. So if we look at this first term, of course it applies to the second as well. Partial of NA with respect to dPsi i Is equal to, using the chain rule, partial of NA with respect to dPsi 1 times the partial of dPsi 1. With respect to xi plus- Partial of Na with respect to dPsi 2. Times the partial of dPsi 2 with respect to, again, xi. So you can see that these are very close to what we have up here. Of course the little I and the big I is just an index. Except that these terms come from the inverse. So, this term here would be the Jacobian Inverse. And it would be component  for example. Okay? So that's why in the code over here, after I've created, after we found the Jacobian I. And you can see that we've done that using an interpolation here of the node locations of x, times the basis-gradients. Once we've defined the Jacobian, we come out. Notice this is all within our quadrature loop here. Okay, we are at, since it's 2-D, we're looping over the quadrule in two directions. And at each quadrature point, we evaluate the Jacobian, then take the determinant. This Jacobian.determinant, that's a function related to the DL2 full matrix class. So we can do that. And also associated with it, there's this function dot invert, so we can get our inverse Jacobean. Okay? Now this term here I could actually write as a summation as well. Let me write that on the next slide, just so we have a little more room. So the partial of A, N A with respect to X I. Actually be, the product N A with respect to dPsi. I'll do a little i. Times the partial of dPsi little i with respecting XI. And again, there is an implied summation here. It's the same as summing over I equals zero up to less than ten. Okay. Writing that summation in again wasn't necessary, because there is the repeated index here. There's an implied simation over the index i. Alright, so now if we look back at our code here, I've set up several different for loops. We have the for loops over A and B. Which of course correspond to the components of Klocal. We have a for loop over capital I and capital J. Which of course are these capital I's and capital J's. And then, a for loop over the little i and little j's. Which are the i's and j's that we saw in this notation here. But now again, you will have to decide what is integral going to be, so you'll set that up. Using, of course, the quadrature rules. Including the quadrature weights. And, of course, we're using the inverse Jacobian and the determinant of the Jacobian. Because of the bi-unit domain. Okay? You will still need to do the assemble system portion of this function. But it will be exactly the same as assembling the system in the previous homework template. We apply Dirichlet boundary conditions the exact same way DLT will do it. Solve is exactly the same as is output results. Again, the biggest portion that you'll have to be working on in this template, is again with an assemble system. And the basis functions themselves, as well as defining your boundary conditions. Okay, so that wraps it up for the 2 D template. In the next segment we will quickly go over the coding template for the 3 D problem on homework 3.

\section*{ID: -AhH81aVr1Y}
In this next segment, we will quickly go over the coding template for the 3D problem on Homework Three. So we'll go straight to the code, there actually isn't a lot It's different. Just a few small changes. We'll start with the source file. You can see, of course, we've changed dimension to equal three as you would expect. Now the vector defining the mesh size, so the number of elements in each direction. Of course, we added on a third dimension. So in the z direction, you can specify the number of elements. So here, for this example, it's a four by eight by two element mesh in 3D. I've also included the .h file, the header file for the 3D problem. So just those three small problems. Lets go over to the template file itself. No changes in the hair file. Here in the declaration of our objects and functions, you can see that for the basis function and basis gradient. We now accept as in input C three, since we have the first dimension now. Same for both. Of course, the outputs are still the same, double and vector of doubles. And the rest of these functions, the declarations are exactly the same and the same with all these dead objects. So just that one small change in the function declarations. We'll scroll down. And the constructor, you can see is the same. I don't know if I pointed this out though in the TD problem. Here, this is one, because we're using linear basis functions always. This is one, because we're using linear baseless functions always. And this is one, because there is one degree of freedom per node. The destructor is also listening. Moving back down to the baseless functions and baseless gradient, we've already covered the difference here. It's just that you have a third input. However, again, we have to watch out for the deal to know the numbering. So let's go to the board quickly. In the lectures, we then went around in a more sequential manner. For Deal II, we follow the same method of going in the same direction. So let me write out the, get some axis here. So this is C1, C2 is going back into the screen. I think C3 is pointing up. If you were to look at just the bottom half, so just looking in the C1 and C2 plane, you would see that Deal II is consistent going from 2D to 3D. We go zero, one, two, three. You can see that's the same as the numbering in the 2D problem. Now we shift up to the plane XC3 equals 1. We just continue going left to right. So we get four, five, six and seven. Again, you'll need to use this node numbering as you're defining your basis functions and your basis gradients. And again, you can do those with the next statement or you can even set it up as a tensor product of the 1D basis functions. That's up to you.  Now again, for the basis function gradient, we are storing the value in this values vector. The declaration is the same as 2D. But again, here, dim is three instead of two. So that automatically updates for the new dimension. I scroll down to generate mesh. Again, very similar as before to what we had before, except now we have z-min and z-max that you'll be defining. The top and bottom boundaries of your domain. Defined boundary conditions will be essentially the same as the 2D problem. Note the node locations in this case is still a vector, where the row index corresponds to the global node number and the second index corresponds to the component of the vector. So is it your x, y or z. So in this case, there are three columns in this table and 0, 1, and 2 correspond to the x, y and z components of the location of each node. Setup system is exactly the same, any changes that would need to be made in setup system are taken care of by the fact that it's a template class and so we use dim a lot. For example, in this forward, we use dim. And so now, we're leaping up to three instead of two. Assemble system is again, very similar. The main difference here is that since it's 3D, we have three loops over the quadrature rule. So again, this is that sort of tensor product idea with the quadrature points. Each quadrature point, technically now becomes a 3D object. But by leaping over the 1D rule three times, we find all those combinations of quadriga points. Now remember, since we are dealing with, in the 2D problem two quadriga points and in the 3D problem three quadriga points. We also need to multiply by respectively two quadriga weights or three quadriga weights, depending on the dimension. When you're doing your numerical integration. Kappa, I've also updated to be a 3D or a three by three tensor, which we'll use in creating Klocal. However, other than that, everything else is the same. So, it should be very straightforward to go from the 2D problem to the 3D problem. The main challenge will be creating your 3D basis functions and basis function gradients. Going to 3D means you have twice as many options there. Twice as many nodes, so twice as many if statements, if you're doing it that method. So, hat should cover it for the Homework Three, coding templates both 2D and 3D.

\section*{ID: NvlTcojBFck}
Welcome back. In order to wrap up this unit on basis functions, and as it turned out, on integration as well, I would like to say a little bit about slightly different basis functions that are still polynomials but are maybe not quite as clean and as, as the ones that we've been working with so far. Okay? So the, the, the motivation for these types of basis functions comes from the fact that, especially in two dimensions and three dimensions the quadrilateral in two dimensions and the hexahedron in three dimensions are actually not the simplest space filling figures, right. In 2D, there's a simpler figure that fills space or simpler sort of structure that fills space, and do you recall what it is? The triangle, right? And in 3D, it's a tetrahedron, right. In fact for that reason, partly for that reason they're also called Simplices in the corresponding spaces. Okay, so what we are going to look at here for completeness is what I may call, in general simplex elements. Right, and this, sort of by definition is in 2D and 3D. Okay? So let's look at 2D first, right. So what we look at here are triangular elements. Triangular elements. With linear basis functions. Okay? Now everything is exactly the same except that you know, I'll find, find element formulation and all of that is exactly the same except that, we just don't work with quadrilaterals, right. Instead, we're interested in looking at subdomains omega e which, in general, has that sort of arbitrary scalene triangle shape. Okay? So this is our omega e. We could have A equals 1, 2, and 3. Okay? And everything's exactly the same. So we have three degrees of freedom here if we you know, think of something like our heat conductivity problem, right or our mass diffusion problem, right. Essentially linear elliptic PDE in three dimensions with scaler variables, right? So we'd have in this case, have three degrees of freedom, we'd have n n e equals 3. All right, and we take the same approach that we took in the case of quadrilateral elements, right. Where this triangle is thought of as being constructed as or, or as being obtained as a mapping from a parent domain. Right. So, since it's a triangle we, we obtain it from a triangle also in this parent domain. Okay, it's also two dimensional so we have our coordinates there. As before we have c1 and c2, okay? Except well here's one of the differences. In the case of triangular elements, this parent domain is not bi-unit. It is simply a unit domain, okay? So then this is the point 0 comma 0. This is the point 1 comma 0, right. C1 equals 1, c2 equals 0, and that's the point 0 comma 1. All right? It's a unit domain. Okay? Now, all we really need to do is define the basis functions. Once we've define the basis functions, everything proceeds just as before. The way this is done is, it is conventional to label this posi, this node in the parent domain as node 1. This is node 2. And the origin is node 3. Okay, right? So we have three basis functions, right? And the way these things are set up, is that although we have a two dimensional domain and so we truly need, of course, only two coordinates in that two dimensional domain, we do go ahead, and define a third coordinate. Okay? And the third coordinate is the following, right. So what we do is also define. C3, which is simply 1 minus c1 minus c2, all right? The reason for doing this as well simply because one can, one doesn't really need to do it. One can work without it, but once one does this, the definition of basis functions becomes very convenient. Okay? When we do this, we what is done is that the, the collection of coordinates nodes c1, c2, and c3 are called are often called area coordinates. Okay? The reason is that if, I redraw this triangle here. Okay? And you recall that those are our 3 degrees of freedom, our 3 nodes and recall that these are the points 1 comma 0, 0 comma 0, and 0 comma 1, okay? And okay. If you look at let's consider c1, okay? If you now draw little strips, right. Parallel to the c1 equal zero axis, right? That c1 equals 0 axis, you remember that this is c2, and that is c1, but c1 equals 0 axis is, is, is indeed the c2 axis, right. So if you draw these little strips parallel to that axis, what you see is that as we draw large as, as we come closer to the point c1 equals 1, c2 equals 0. Right? And if we just add up the area included in all of these strips right? As we, as we move towards c1, we essentially come closer to the area of the triangle. Okay, all right. So it's as if, the area of you know at any time, if I now look at at the area. Enclosed by these strips say, all the way up to here. Okay? So area of the strip. Okay? Is is essentially equal to c1, okay. That's the basic idea here, right? Right. Sorry, it's equal to one-half c1. Right. One-half of c1. Okay. Right. So it's essentially for this reason that that we take this sort of approach. And you know, of course the area of this entire triangle is a half, and so on, that explains why we call these, why we call these coordinates also, area coordinates. Okay, but that's just a detail. The fact is it's convenient to use c1, c2, and c3.

\section*{ID: 21xRLnJcXhQ}
Okay, so with this in hand, how do we sh, how do we write out the shape function, the, the basis functions, right? So again, let me and, let me now draw this. And, again, because I'm going to draw the basis functions here, I'm going to draw this triangle. Though, it is of course, the same triangle I had on the previous slide. I'm drawing it now, in perspective view. Okay, all right, so in this in this view. Okay, I'm gon, I'm going to first write out the basis functions and then sketch them out. Remember this A equals 1, that is A equals 2, and this is A equals 3, okay? So, in that setting, we, it's, it's, it's really easy. N1, re, is the function of xi 1, xi 2, xi 3 is xi 1, N2 is xi 2, N3 is xi 3, okay? And in particular it is the convenience of writing out these basis functions in this manner, in this so to speak completely symmetric manner that also motivates the definition of the code of these coordinates, xi 3 even though the span of the space is just two, right, consider this with the rank of this the, the dimensionality of the space, sorry, is, is just two. Okay right, so, so let's sketch these, right? So I'll sketch just I'll sketch one of them to begin with. And so I'm going to sketch N1, okay? As you can see from that, N1 goes to 1, right, at xi 1 equals 1, right, at xi 1 equals 0, which is the xi 2 axis, it drops to 0, right? So to 0 all along the xi 2 axis, right, and it goes down linearly, okay, along this, along the, the line, right, along this line here. Right, along that line, right, the N1, of course is always equal to xi 1. Right, along that line it slopes down linearly from 1 to 0, okay? That is xi 1, and likewise, we can construct xi 2 and xi 3. Xi 2 is one there slopes down linearly to 0 at the other two, at the other two nodes, right? That goes down to 0. Right, and likewise xi 3 or, or N3, sorry, is one at that node goes down to 0, linearly, okay? So that is N3.  we should write, okay. N2 is that basis function, and N1, N1, we've, we've already  and one and two in green. Okay? So those are are basis functions and one thing to note, of course, is that. You will have observed that I referred to these basis functions as being linear rather than bilinear, 'kay? And it should be completely clear why, right? They are indeed just linear not bilinear. Okay, so that's it. Once we have these basis functions, everything proceeds just as before. We just need to pay attention to the fact that the number of nodes in our element is not four, 'kay, even for the simplest of these of this class of elements, it's not four, it's three, and we can go ahead and construct our, our entire finite element formulation. One can go on to higher order triangular elements also. There are quadratic triangular elements which involve midside nodes as well and, and it's in ex, one, one can define the, the basis functions for them also and as, as, as an extension of what we've done here. I'm not going to get into it because really if you're, if you do need quadratic triangles, you, you are probably better off just using quadrilateral, bilinear quadrilaterals anyway, okay? So, so try thi, this, these elements have certain advantages. The, the primary advantage of, of, of them is of course, simplicity, okay? So simplex elements in general. They're, they're simpler than quadrilaterals in and hexahedra, right? Of course I haven't put down the simplex element in 3D but I'll do that pretty soon. They're simpler but there's a drawback, right? There is an obvious drawback. And what do you think that could be? Just the basis, right, I mean, you're using a lower order basis, the, the, you, you start out with the linear basis instead of a bilinear basis. Your representation of functions is already a little poo, is, is a little poorer. They also are there, there, there are, there are issues of incompleteness of polynomials and so on which, which arise, so in general they don't work as well as the corresponding bilinear, biquadratic elements and so on. Okay, but nevertheless they're simple to use and they are convenient they also of course, for certain problems of generating meshes they do present an easier task, 'kay? But that is largely been been overcome in the finite element community. All right, so we have this, right? Let me just put down the simplex element in 3D and then we'll be, we'll, we'll have got this covered as well, 'kay? So, in 3D, right, so we have tetrahedra. 'Kay? So a general element in the physical domain may look like that. Right, it'll have four nodes, right? The simplest linear tetrahedron. Right, I'm showing you the simplest linear tetrahedron. So this would be our omega e and it would be constructed as you may imagine from a parent domain, right, which is also in 3D, but it is a tetrahedron in 3D, right? This sort of a right tetrahedron in 3D. So it is. Okay, and oops. Okay, I guess I, in order to make it completely clear I should make some of these lines dashed. Yep, that's really the only line I needed to make dashed, okay? So, so the, the coordinate directions would be xi 1. Xi 2. Now we would properly have a xi 3 coordinate dimension, right? Our nodes, and here in, in the physical domain we may have numbered them as whatever, A equals 1, 2, maybe 3 and 4, right? What happens here is that we have four nodes of course. Right, so we will label them here as A equals 1, A equals 2, A equals 3 and here we'll get A equals 4, okay? For convenience again we will define now, xi 4 equals 1 minus xi 1 minus xi 2, minus xi 3, okay? Right, using the same sort of idea, in that case, we use this idea of area coordinates. Now we can I guess effectively call them volume coordinates, right? But, but the idea here is this, is the same. Now all we have is N1 function of xi 1, xi 2, xi 3, and xi 4, right? It is just xi 1, right, until we come down to N4. And that's it, right? We just go ahead, we construct our, we now know that the simplest of our basis functions is, is linear. There are four nodes to the element, four degrees of freedom. We just go, go ahead and construct our entire finite element formulation just as before. Okay, calculation of derivatives and everything is straightforward. One remark I  I could make is that now that when we work with linears linear triangles and or linear simplex elements. Lead to what is the order of any gradient now of our basis functions, and therefore the order of our representation of any gradient field? It's constant, right? Right, so if we take any of these linear basis functions and compute its derivative with respect to any physical coordinate, right, what we will see is that it leads to constant gradients. All right, and this in some cases can be an advantage, for, for simplicity of calculation, but of course if, if your best representation of gradients within the element is a constant, you are obviously losing some fidelity of representation of functions and so forth, okay? You're just not you're just not able to represent higher order functions. Right, so, so there are, so there are those sorts of drawbacks as well. So, so, there, there's also a, when it comes to integrating of with with, with these type of simplex elements of course there are special, the, there are special numerical quadrature rule, numerical integration rules. Just as we did in the case of triangles, we can also define higher order tetrahedra. Okay just as we do for for triangles, right? And one can construct the triquadratic so sorry, not triquadratic, quadratic tetrahedral, cubic tetrahedral and so on. One can also define the, the, there are numerical quadrature or numerical integration rules defined for them. The optimal the optimality of Gaussian quadrature however does not hold for triangular elements, right? There are other numerical integration tools that are, that are used. So Gauss quadrage, Gaussian quadrature is no longer optimal. Nevertheless, these can be defined, these have been defined, and they're actually  available in module, the finite element literature. Okay, but we can stop this segment here, and indeed, this unit. Hopefully, it's allowed us a unified representation of basis functions of definitely of the linears, bilinears and trilinear and therefore, you know, quadratics, biquadratics and triquadratics and so on, based on Lagrange polynomial functions. And we've also seen how simplex elements are defined, the simplest versions of simplex elements, the linears. We also saw in the course of this unit the extension of quadrature to three dimensions of, two, two, two and three dimensions for basis functions that are constructed from Lagrange polynomials. All right, we'll end this segment here. When we return, we'll start up a new unit.

\section*{ID: 8vV-ENhex-c}
Welcome back. What I'm going to do in this segment and maybe one or two more is  almost take a step back. And outline for you the main steps that are involved. Only slightly different steps that are involved in setting up two-dimensional problems. Okay? So, we're following somewhat of a deductive approach. Where we go from the more general 3D structure of, of the problem, to a 2D structure which is, of course, is a particularization of a 3D. Okay, we're going to do this in the context of the linear, lytic problem in two dimensions now, with a scalar variable. Okay? So we will call this, of course two dimensional. Linear. Elliptic. PDEs in scale, in a scalar variable. All right. So, since we are talking of linear elliptic PDEs with a scalar variable. The physical problems that this is applied to remain, heat conduction and mass diffusion in two dimensions. That's all. Okay. There's actually very little to do. Almost nothing to do. And I'm going to do just that, all right? So, here is my version of doing nothing. So, I'm going to first, we, we start out with a strong form of the problem. And, and really, all we have to observe is that everywhere in the 3D formulation where we observe that the spatial dimensions could the spatial dimension index ran from one to three. It now runs from one to two. And we need to recall for ourselves how we would construct the basis functions from over, over two dimensional elements. All right, okay. So, I'm, for, just for completeness, I'm going to put down fairly quickly, the strong form and the weak form. Okay? And in fact because I'm going to do that fairly quickly, let me straight away say that we're given the data. Okay? We're given u g. We are given j sub n. We are given f. And we are also given the constitutive Relation  -j i. Sorry, +j i = -Kappa i j. u, j and here, we have i, j = 1, 2. Okay? This is what we're given. So, with this background, here is the strong form. Okay, the strong form is. Find u such that j i, i =, now we'd use f in our first all sort of problem. And later on, you will recall that I said, that there was reason to replace that with minus f bar. Okay? Right? This is our PDE. All right? In omega. And, and we remember that now omega is a, sorry, it's a subset. Omega is a subset of R 2. Okay? So, the domain that we're looking at would be this. Right? With the difference that instead of drawing a three dimensional basis, or three basis vectors here, I have just two. V-1, V-2. Okay. So, this is our PDE. With. Boundary conditions. U = u g on what we understand to be the Dirichlet boundary. Okay? And -j, sorry. I'm doing this all with coordinate notation. So,- j i n i = j sub n for the influx condition on, and now, I'm in boundary. And n, now, is a unit vector in two dimensions. All right? Okay, that's it. That's our strong form of the problem and I've already stated up here that i and j run over one and two. Okay? I'll go ahead and write the weak form and I will in fact write the finite dimensional weak form, right, essentially skipping a step here. So, I will right now the finite dimensional weak form. Okay. So that is find u h. Sub, sorry, it's not sub i. Find u h belonging to s h subset s where s h consists of functions u h, belonging to h 1, over the two dimensional domain omega, and satisfying the traditionally boundary condition. Okay? So find of, of this type such that for all wh belonging to Vh subset of V, okay, where now Vh consists of all weighting functions. Also, to be picked from H1 functions, and satisfying the homogeneous Dirichlet boundary condition. Okay, find such that for all wh of this type, the following holds, right? Integral over omega wh,i, jhi. Now we are in two dimensions, right? So, what we are working with here is is an integral over an area, okay? So we get dA, right? So, wh,i ji jhi equals integral over omega wh f bar dA, okay? And what we note here is that okay, that in order to get things right we will pick up a minus sign there, okay? This is, I'm just working through all those changes I made with the signs later on in the 3D problem, okay so we have this. Plus integral over the Neumann boundary wh j sub nd. Okay, now we need to be a little careful here. In two dimensions the boundary is a curve, okay? So the boundary integral here I'm going to write as dS, all right, where for us S is now a curve. Okay, and dS is the elemental curve, okay? All right, so this is what we have. And the only thing we need to recall, if you're, is as we did on the previous slide, just remember, here, that i, remember that i equals 1,2, okay? And in this goes jhi, we also know will eventually be written as capital kappa ij sorry, minus kappa ijuh,j, okay? So j and i both run over 1 and 2. All right, and you see that now those signs will cancel out, right, plus, minus and minus will cancel out and we get back everything as positive, right, the way we write it down. All right good, so, so this is what we have and you know how everything works from here, right? We are going to use elements which are subdomains. I'm not going to write all the technical stuff about the, the union of elements you know, with closure applied to them giving us the total domain and all that, right? We don't need all of that here. Let's just look at the element sub-domains, right? So, we may work with quadrilaterals here, right, of that type, okay? And just to fix idea, let's suppose that we're working with bilinear quads. All right, so this would be omega e, right? We would have A equals 1, 2, 3, 4, which we know would be constructed from a mapping from a bi-unit domain, and we studied this process. Okay, we could have then, have it constructed from a bi-unit domain, which is now expressed in terms of coordinates xi 1, xi 2, all right? And here our nodes are we, we know exactly how, what these are, right, these are minus 1, minus 1; 1, minus 1; 1, 1; and minus 1, 1, okay? Those are our nodes in the bi-unit domain, okay? Now we also remember that this mapping gives us a way to write out the geometry, okay? So we know that the geometry in a the physical domain is a function can, or can be parameterized by our coordinates in this bi-unit domain, okay? All right, so we may have this or we may choose to be working with triangles, right? And we do that or. Triangular elements. In the case of triangular elements, I just drew the very simple so then, it's the one that we looked at in a previous segment. All right, that's omega e. Once again we have nodes at the vertices, and I'm going to label them as A equals 1, 2, 3. We know this can be constructed as a mapping from a unit domain, which we also express in terms of coordinates xi 1, xi 2, and in this setting We get that point is 0, 0; 1, 0 and 0, 1, okay? And right, we also recall that in this setting our no, our degrees of freedom here are labeled 1, 2. And three. And that reminds me that I did not label the degrees off freedom in the bi-unit domain here. So let me call this one, two, three, four. Okay, but we now that already very well. Okay. And, again, we will get from here a mapping of the geometry as well xi 1, xi 2. Okay? All right, so in this setting we will get our basis functions, right? Right. There's two ways in which we would write our basis functions, depending upon whether we were working with the bilinear quad or the linear triangle. Right? In the case of the bilinear quad right? We would get bilinear basis functions right? Bilinear polynomials. Right? These would be written as n a parametrize by c1 and c2. And we know what the formula for them is right, I'm not going to rewrite it. We've written it a couple of times at least now these are the standard bilinears. We've also looked in one of the previous segments at the Generalized Lagrange polynomial formula for this, right? So these are bilinear Lagrange polynomials, right? In this case, A runs over 1, to number of nodes in the element and their number of nodes in the element is 4. Right? Okay, or we would have linear polynomials. All right? If we were working with the triangles. Right? Linear polynomials would again we written as a n parameterized now by c1, c2, and remember the way that these things are written often Using area coordinates, we would write Z, include 8, Z3. Okay. And remember that Z3 is defined, in the case of triangles as 1 minus Z1 minus Z2. Right. And we saw this as recently in E segment. In this case again A equals from 1 to number of nodes in the element. And for the case that we're considering up here on this very slide. Number of nodes on the element is equal to 3. All right? So this is what we have and and with this we go ahead and of course construct our finite dimensional functions, right? So we will go ahead and write u h over e is now sum over the number of nodes in the element. N A, right? I'm not going to write the parameters of N A because they could be different depending upon whether we are working with bilinear or linear polynomials, all right, but we're working with bilinear quads or with triangles. So I'm just going to say it's n a times d a e. And for the waiting function, right, we have w h of e equals the same sort of expansion. Okay. All right, we just need to remember that in the two cases, the number of nodes in the element could be different. All right.

\section*{ID: P3NHmrT9EC4}
And next we have to construct our gradients, right? So, for gradients of our trial solution and weighting function, we have u h comma i in general, 'kay? This is now again, sum over A 1 to number of nodes in the element, NA. Okay, now we're going to write N A as a we're going to write the gradient of N A, right? And the way we can do it here is using the chain rule, right? So, what I'm going to do here is write it as N A comma xi1, okay? Xi1, oh, I'm sorry. N A comma xi1 times derivative of xi1 with respect to x i. Plus N A comma xi2, derivative of xi2 with respect to x i, okay? Again, remembering that here that i equals 1, 2, all right. Now something to observe here is that I claim that this way of writing it works for bilinear quads or for linear triangles, okay? Even though we said linear triangles have an additional, can be written as, depending upon an additional coordinate, xi3, right? Because, of course, what we're doing in here is sort of rewriting, xi3 as depending upon xi1 and xi2, right? That's what's going on here, okay? So let me say here that rewrite. NA xi1, xi2, xi3. Using xi3 equals 1 minus xi1 minus xi2 for triangles. Okay? All right, basically there are only independent coordinates, of course in 2D, so that's all that we need to compute. All right, so that should be straightforward. Okay, that brings us then to to the question of how do we get how do we compute those derivatives of xi1 and xi2 with respect to x i? All right? How do we do it? By exploiting the mapping of the geometry, right? So what we do is we use the, the fact that we have an isoparametric mapping. All right. And you recall what that means, right. That means that x, i in element e, all right, can be written as a function of xi1 and xi2. Always by writing it as sum over A, NA x A i e, all right. Just as we did before, okay. And in fact we've even drawn on the, in the first or second slide of this, well on the first and second slides of, of this segment, we've, we've shown that mapping, okay? All right, what this lets us do then is write out partial of x i with respect to xi1, and partial. All right, so, well, let me write it in general, write it in this fashion. Partial of x i, with respect to xi capitalize I, all right. And we know exactly how that works. It's now sum over A, running from 1 to number of nodes in the element, NA comma xi I, x A i e, okay? And here, too, we have i equals 1, 2, and capital I also equals 1, 2. Here, too, I've used the fact that even if we were working with triangles, xi3 would be written in terms of xi1 and xi2, right? There, there are only two independent coordinates. All right, and this is what then allows us to write J, all right, the Jacobean of the mapping, okay? So J is now, of course, just partial of x1 with respect to xi1, partial of x 1, with respect to xi2, partial of x 2 with respect to xi1, partial of x 2 with respect to xi2, all right? So, now this is a little different from 3D. Well, it's, it's different from 3D simply because we work in two dimensions, right? But it is what you would expect, which is that the Jacobean of the mapping is a. Tensor that takes two-dimensional vectors to two-dimensional vectors. Alternatively viewed as a matrix, it is a two-by-two matrix, okay? And this implies for us, then, that J inverse, which is actually very easy to write because it is just a two-by-two. And we know there exists an analytic formula which is actually very easy to write up, right. We know that we can, we cannot therefore explicitly compute the inverse, right. But that inverse by definition is this. All right. And, why this matters is that these are the terms that are going to be used in our chain rule to compute derivatives, right, of our functions. Right. I just rewrote the formula for computing the gradient of u h. There is a sum implied over capital I, and you remember the capital I runs over 1 and 2, okay? And having written out the inverse of the Jacobean gives us those terms, okay? So that's it. Now we can go back to assembling all the integrals that we need to compute, right? Now what we can do is compute. Element integrals. Okay? All right. And remember that the way we go about that is to say that integral wh comma i jh i with a minus sign d A is sum over e, integral over omega e w h comma i j h i d A, right? Well, we know how to compute everything inside there, okay? When we write this out in detail, what we get is integral over omega e, w h comma i, j h i, dA. In one fell swoop I am going to write the, the final expression, okay? So, we know that this can be written as minus sum over A and B, okay? A and B running over 1 to number of nodes in the element in the general case. All right. We get here c A e. We get an integral. Now, I'm going to straight away write this integral as an integral over our parent domain, okay? So, it will be xi1 or, or, I guess xi2 equals minus 1 to 1, xi1 equals minus 1 to 1, okay. Inside here I'm going to get NA, all right? I will get NA comma xi I partial of, well actually, we don't even need to write it that way. We can write this directly as N A comma i, right? Where N A comma little i simply means that we know how to compute the gradient with respect to physical coordinates, right? We looked at that on the previous slide, right? We know how to compute those guys, right? That times right, we get a minus kappa i j. Now to compute u h comma j we will pick up an N B comma j here, okay? We will be integrating this over d A where if you look out here, we have d A as the elemental area as it, it is an element of omega e in the physical domain. But here because we're integrating over the parent domain, we pick up here, determinate of J, right, which also depends upon xi, upon the vector xi, right, upon both coordinates. This now with an, multiplied by our elemental area in the parent domain, which is that one. Okay, I'll put parentheses here, and coming out here is d A e, sorry d B e, all right?  Simple as that, okay? Okay, we know now that this is going, when we carry out this integral we know how to carry out this integral. We've looked at numerical quadrature, okay? So we know how to carry out this integral. It is going to give us, finally, we know it is going to give us K bar A B. The AB entry of the K bar matrix for element e, right? And if we were doing heat conduction, we would call this the conductivity matrix. Okay? Just like that, we go ahead, to compute now integral over, omega of, w h f bar, okay? Integral over, integral over omega of w h f bar d A, right? Okay, integral over w h f bar of d A, which is again, sum over e, integral over omega e, w h, f bar, d A. Okay, where, now, integral over omega e, w h f bar d A equals sum over A, c A e, integral minus 1 to 1, minus 1 to 1. We may call this one xi2. Call this one xi1, okay? And here we get N A, no derivatives f bar. Now we need to integrate over d A but over the elemental area in the parent domain. So once again we just get the determinant of J. D xi1, d xi2, all right? Okay. So, one thing that I should point out perhaps on the previous slide. Our, our tier is that when we look at these at the summations over i and j, just remember that i comma j here equals 1 comma 2, all right? Okay. Okay, so we come back here. So that's how we know, that's how we need to compute this particular term. All right, and what we will, what we know also is that when we look at that term, it is what we will call, believe we call this F bar internal A for element e, okay? And the very last one is the now, the, the integral over the Neumann condit, over the Neumann boundary, all right. So we have integral over partial omega sub j, w h, j sub n, d little s, okay? This one actually takes a little work simply because we need to be careful about how we do the how we pick the degrees of freedom that contribute to that particular boundary here, right, from each element. And also just recalling for ourselves what we will do with doing the integral along the curve that ds implies, okay? So we'll come back and do that in the next segment.

\section*{ID: naLrrzB2Np0}
All right. We will continue with, just looking at the critical points that go into setting up our equations in two d. And I hope what you're noticing is that there is really nothing to do. Nothing new at all just paying attention to the fact that our spatial dimensions aren't over one and two. Okay? So let's get on with this. All right so we got as far as the boundary integral. Okay, the boundary integral which is this one. Integral over the Neumann boundary. W h, j n, d s, right? And here, we need to pay attention to the fact that d s is a curve, right? An elemental. All right. Elemental curve. Okay, again, this is a sum. Now over e, belonging to the set of elements whose boundaries coincide with a Neumann boundary. Right and we'd use that notation, script e sub n. Integral, now over partial omega e sub j. All right? And then the, the integral over that part of the element boundary that coincides with the Neumann boundary, okay? W h j n d little s. Okay? And then, we said that what we need to do is worry about this integral. Okay? And the way we do this is to once again do a sum over a. And now, we observe that. We, essentially, we recall things that we've noticed before, right? Which, and, and one of the things we'd noticed before, was that, if this is our, domain. And this were the element that we were concerned with. Right? What we note is that only some of the nodes lie on the relevant boundary, right? Of this part of the boundary with the, with the Dirichlet boundary, this part with the Neumann boundary. What we observe it is, is that it's only this elem, this node a or that degree of freedom. Which belongs to the set of Neumann boundary nodes which we call a sub n, okay? And likewise, if right next to it we were to have a triangular element, and this is possible. In a finite element mesh, right? If you had triangular element right here, then it would again be, so that would be a, maybe that would also be b, also belonging to a sub n, okay? All right, so we have a belonging to a sub n, c a e integral over. All right. Now. We need to go to the element boundary but in the parent sub domain. Here we would have n a, j n. Okay. And what we would do is to recall for ourselves that. If our edge here. Right. If our, if our edge here were this one. Right? Partial, omega, e, sub j. Right? We would need to just remember that, well, it always arises as a mapping from one of the. Coordinate plane, coordinate lines now in the parent domain. Right? So what we would observe is that we have this map. Okay? Right? And that map, you remember, is something that we denoted as j s. Okay. Okay, now in this particular case, the way I've written it out, j s is the function only of c two. All right, so okay? Given that we know the positions of all the relevant degrees of freedom. And actually, the nodal coordinates in the physical domain. The relevant nodal coordinates in the physical domain are that one, and this one. Okay? And the relevant, relevant ones in the parent domain are minus one comma minus one. And one comma minus one. Okay. So we know very well how to construct that map j s. All right? In particular we did talk about, in, in, in the case of the three d problem we said that if the edge were not in, in the physical domain if the edge were not aligned with one of the, coordinate directions. Right. One of the basis vectors in Two dimensional space, such as this case, right. That edge, partial omega, e sub j, is not quite aligned with the e one vector, okay. And I suggested back then, when we were doing this in three d, that alright, maybe we need to find a new direction there, right? Right, we said that maybe we need to define a new vector along that direction. Okay? If, if that were the case, it would take a little bit of an effort to construct that, but, but not too much. If it did align perfectly with a coordinate direction, it would be a much simpler construction, right? To get this math. J sub s. Okay? All right. So, what one would get in this case is here we would get the determinant of j s, right? And we would be then integrating along for c two as shown in that particular, in this particular example. Okay? And the same sort of thing would happen if we had a triangular element. Right? And that with the triangular element. Right? And that were the direction that we needed to integrate along. Right? If this were, sorry the edge that we needed to integrate along. It would come from a regular unit domain. Okay. Those will be the nodes that will be relevant in this domain. If that were a, b, and if a were the first order in the physical element and so on. Right? So then it would be mapped along zero comma zero, and one comma zero. Okay? Just realized that with the way I've drawn things this, this elemental curve length is d x c one, not d x c two. All right so this what we would have to construct. Right in either case we would note that that determinant, determinant of j s is simply. Derivative of x tilde with respect to c one. Okay. If x tilde were one of the coordinate directions only, it would be a very simple calculation. It would be a little more complicated if it were aligned in some general direction.  Right, in some intermediate direction. Okay, but nevertheless, it can be constructed. All right. Okay. And then this figure is pretty, is quite busy now, but let me just do one more thing here. That integral is what I believe we were calling f a sub j. Sorry, F j sub a. F j a element e, okay? All right, so we have all of these, and what we would then have to do is put these things together, okay. One thing to point out is one useful thing to point out is, that I will do in a remark here. Okay. Is that for the case of linear triangles. Okay? The matrix components of j c, right. J x c is for the case of linear triangles is the map that allows us to go from, go to the physical domain from the parent domain, right? And if you're working with linear triangles. J x c takes on a very special form. All right, so, the map itself is x, of c. And j x c is the gradient of that map, right. So, the matrix j x c here, which is now, x one comma c one, x one comma c two, x two comma c one, x two comma c two. If you go ahead and compute this for linear triangles, what you find is that this matrix is a constant, right? The components of that matrix are constant because of the linearity of the map, right? There's no bi-linearity in the independence of x one and x two point in, in c one and c two. So when you take those derivatives you get constants. Right? And in fact, in fact not only that but a further, simplification is the fact that the determinant of j x c in this special case, right, only for linear triangles turns out to be. Can you deduce what it's going to be? It's always twice the area of omega e. Okay, the measure of omega e is the area of omega e in this case. Okay, so that's a special case for linear triangles. It's something that's useful to know and to be able to use when one is developing these formulations. Okay. All right. So let's return then, to assembly. All right. For assembly, here's what we have. Sum over e, of, let me see, we have here. Sum over a comma b. C a e. K bar, a b. And d b e equals sum over e. Sum over a. C a e, f bar internal a e, right this is what we get from the forcing. Plus sum e belonging to the set of elements that include. Whose boundaries coincide with the Neumann boundary. Sum a belonging to a sub n. Right, the set of nodes within each of those elements that also lie on the Neumann boundary, c a e, f bar, sorry f j a e. There's no bar on that one. Okay? Assembly is what lets us go from here to a single matrix, right? And now, actually, everything's just the same. Nothing is any different. We first of all can get rid of the sum over degrees of freedom over in each, each element by essentially writing each of these sums using matrix vector notation, right? So we here introduced the idea of the row vector of waiting function degrees of freedom. Right? We get k bar, the rectangular conductivity matrix for this element. Multiplying d bar, and I don't need that superscript. Multiplying d bar e. Right? And you remember the reason that I'm using a k bar and a d bar is recognizing that, Dirichlet boundary conditions have a say here. Right? And the K bar is a rectangular matrix because of the fact that the, the existence of Dirichlet boundary conditions says that the size of the vector c e, right? Is not the same as the size of the vector d bar e in every single case, right? For the elements lying on the Dirichlet boundary they are not the same, right. And then like Y is for this we get c e, transpose f bar internal e. And here we get, the same thing. Right? Remember, in this case, I'm going to the full, row vector c e. We simply say that we construct f j e, such that the degrees of freedom that don't lie on the Neumann boundary from that element e. We'll simply have zero entries in the corresponding f j vector. Okay. This is exactly the same, all right. We don't, oh, the way we did this in three d and the way we would do this in two d is exactly the same. In the case of two d, we wo-, like in the case of three d, the, the sum over the degrees of freedom a and b. Right? Would run over the number of nodes in the element, right, or the number of degrees of freedom that were used to interpolate each field in that case. The fact that that number is greater in three d and lesser in two d has no effect. Right? So we just run over that sum. Okay, so that process is no different okay, in constructing these element matrices and vectors.

\section*{ID: M96mlUdkaCk}
Okay, so, just recall however that K bar e has dimensions, number of spatial dimensions times number of nodes in, in an element minus ND, which is the number of degrees of freedom with Dirichlet conditions on it, right. This times N s d times N n e, okay? This is no different from before except for the fact that now N s d is 2. That's all. Right, otherwise it's exactly the same thing, okay? All right. And then assembly is what let's us say that assembly over e of K bar e equals K bar, all right? And this assembly process is just like we did in 3D. Again, nothing different, right. We just look at every global degree of freedom, look at every local degree of freedom from any element that has the same global degree of freedom and add up contributions in the matrix. Right, and the same way for our other, for our vectors, F bar, internal e, right? The assembly of all of these gives us an F bar. This assembly process is no different, right? Just look at global degrees of freedom that, correspond to local degrees of freedom in every single element that they belong to, right, and just add up the contributions. Right, and in doing this, we may choose to say that e here runs only over the set of degrees of freedom, that. Sorry, it, it just runs, we need to only run over the elements that have a boundary coinciding with a Neumann boundary, okay? 'Kay, I just realized that what I did here for the, for the dimensions of K bar e actually apply to K bar. This is corresponding the K bar, okay. All right, so we've observe that K bar is a rectangular matrix. It's not square. And we know why it's not square, because there are fewer degrees of freedom corresponding to the trial solution vector, okay? So that last bit is exactly the same. So if you look at K bar, what we will see is that, right? In general, it has fewer rows than columns, right? These are number of spatial dimensions times Nne, right? Whereas, these are number of spatial dimensions times Nne minus ND, right, as we observed above, up there, okay? Now, some of these are some of these degrees of freedom that are in the d bar vector are known, right, okay? We've observed that fact before, maybe some of these degrees of freedom are known, right? And what we do then is go back here and essentially eliminate the corresponding columns right? So for instance if this were an, and if this were A bar and that were B bar, right? This column would be the A bar column here, right? And then you would get the B bar column. Right? And corresponding to it, we, what we would see is that d A bar would be known. And d B bar would also be known, right? And d bar, A bar, B bar, those degrees of freedom would be known. We just moved them over to the right-hand side because they're known degrees of freedom, right? So, that is known. Right? And likewise that one would be known. And why would it be known? Sorry, because it corresponds to a Dirichlet boundary condition. Right, likewise, this one. D-i-r for short, for Dirichlet. Okay? All right. And this column is what I've been calling before K bar, A bar. And this would be K bar B bar. Should be using that as a super script. Okay? And just as before, we move this over to the right-hand side. Okay. So what this gives us then is a, the full c vector here now multiplying K and d where K, where properly now has dimensions of number spatial dimensions times number of nodes in the element minus ND. Square. All right? And therefore c transpose ND are also of the same dimension. Right, N sd times N ne minus ND. This is equal to c transpose F internal plus F j minus dA bar, right. Just that degree of freedom, just that scalar multiplying the K bar A bar column, minus dB bar, multiplying the K bar B bar column. Okay. And this finally is our full F vector. Also with dimensions Nsd times Nne minus ND, okay. And this has to hold for all c belonging to that same, to space of the same dimension. All right. And what that implies again is that K d equals F, okay? All right, so as you see, there's really nothing to do here. Just had, it was just a matter of observing that our spatial dimensions run over 1 and 2. Just recalling for ourselves how we would construct the relevant gradient fields, using our basis functions. See that, observe that it doesn't matter whether we're working with quadrilaterals or triangles, there's something special in the case of linear triangles which is that that the Jacobian of the mapping is a constant. But other than that, it's, it's, it's all completely standard, especially once we form the elemental matrices and vectors. And in fact the process of forming the elemental matrices and vectors itself is exactly as we followed in 3D. And then final global assembly also follows the same process, including the treatment of Dirichlet boundary conditions. Okay, so we'll end this segment, in fact, this this treatment of two dimensional problems here, 'kay?

\section*{ID: nK0sQuS3s5A}
So, there is an error in writing out the dimensions of vectors and matrices in two slides of the segment. Starting out with the slide that I have before me here, if you look at what I have up here. Let me get my pen working. I have n s d times n n e here. That should be replaced with just n, sub n p for number of nodes in the problem. That, however, has to be applied over the rest of this slide, as well as the next one. So let me put a line through that and call it n sub n p. Let's see, where else do I do it? Here n sub n p. Here, too, n sub n p. I should emphasize that the n, n sub d is correct, that does stay. That looks like it for this slide. Let me advance to the next slide. And here again, we are. That should be n sub n p as should that n sub np and that n sub np. With that, we have corrected the slide, or the segment.

\section*{ID: ew2w-UzSQbg}
Welcome back, at this point we are ready to take a fairly significant step forward. We're going to begin a new unit where we look at a linear elliptic PDE but with a vector variable in three dimensions. And a model problem for this is elasticity, specifically we're going to look at linearized elasicity, okay? Because there are some technical issues well, not just technical issues but it turns out that non linear elasticity is not necessarily elliptical. In case, we'll do linearized elasticity. Okay, so, we begin now with the linear elliptic PDE in three dimensions. For a vector variable. And the model, like I said is linearized elasticity in 3D. Okay, I'm going to start by writing out the strong form problem. So the setting is one of 3D again, so we have our basis, our three dimensional basis set. Okay, we have our domain. We have let me just show the position vector of some point here. Okay, that point is x. Okay, so maybe I should write it on the, at the point. This is x. 'Kay, so that's the position vector of x. Our body as before will again be denoted omega. And okay. Let me now start out by fine. Setting up the problem, okay? Let me first state what data we have. Were given some data, now remember, were doing a vector problems so all the data actually are vectors, okay? Or vectors are, are tensors in fact. Okay, so what is the, what, what are the data we have? We are given we are given a vector set of vectors and allow me to put the g as a super script here, okay? Unlike the subscript that we were doing earlier because I need to use a subscript to denote quadrants, okay? So, that little i there signifies our spatial dimensions, right? Okay, so we're given this as part of our data. Given u g sub i we're also given, what I'm going to denote as t bar sub i, okay? We are given f sub i, okay? These are all data and right here phonetically what I'm going to say is that what this really means is that u g, t bar, f bar, now using direct notation. All belong to R3, okay? So truly all I'm saying is that those data that we, that I just spoke about are all vector data, okay? So when I use back here I've used coordinate notation and back here I've used direct notation. Okay, so these are the, the, the data we have. Furthermore, we are given a constitutive relation. We're given a constitutive relation which I'm going to write first in coordinate notation, okay? The consideraitve relation we are given is for the stress. A quantity that we call the stress, okay? That will be denoted by sigma. It is a tensor so it has, in coordinate notation, it has units, it has it has two indices, i and g, okay? I'm going to put down here stress. Okay? The stress is given to us as a, through a tensor con, constitutive relation as equal to c i j k l epsilon, okay? And what we have out here is the strain, okay? I'll just state here that because we're doing linearized elasticity the strain here. I'll write it out actually, the string is what we call the infinitecimal strain, okay? Infinitesimal strain, okay? Sometimes people like to specify that the stress that we call, that they are using here is the Cauchy stress. But it turns out that if you're doing linearized elasticity in infinitecimal strain. There is only the Caushy stress this out in it, okay? This quantity here is what is called the elasticity tensor, 'kay? It's a fourth order tensor and it is what we call the elasticity tensor. Okay? Now all of this, I, I'm not going to write it but since I'm using indices this is coordinate notation. Direct notation for the same thing, which I'll write in parentheses like I did above. Direct notation would be sigma as a tensor and like I said once before, we will do, I'm using an under bar for tensors and vectors. We just distinguish them by context, okay? Sigma in the direct notion is c elasticity tensor, okay? And the fact that two in the c's are being contracted out here. K and l with k and l is represented by a colon or two dots, 'kay? Just think of it as a generalization of a dot product at symbol is sometimes called a contraction symbol. Okay, an epsilon, all right, in direct notation also, okay? So let me just state here that, that symbol is sometimes called is sometimes called a contraction. I suppose the idea comes from the fact that you're taking a second order tensor and which is, which is, epsilon. Epsilon so, so the stress and the strain in our second order tensor try to, and to take second order tensor which is a strain and sort of contracting it down to a different second order tensor. Or alternatively taking the fourth order elasticity tensor and getting rid, rid of two of it indices led a contracting down its the order of the resulting tensor, okay? So that's the contraction symbol. All right, so we have we have all these things here. So to this, the constitutive relation, okay? We need more.

\section*{ID: mw1wsbSm-vc}
We also need, some something called Kinematics, all right? We need the kinematic relation Again, with coordinate notation. This kinematic relation for the stream. Epsilon kl is one half derivative of another vector, u with respect to x plus the symmeterizing term. I should mention that, I should make it clear that that is l, not i. Okay? Right. So we have this kinematic relation. In direct notation this would be epsilon equals the symmetric part of the gradient of u, okay? All right. So we have these components of our problem, okay? So let me just go back here for a second. So we're given data in terms of these vectors ug, t bar, and f. We're given a constitutive relation, relating the stress to the strain through the elasticity tensor. Okay? And we're given this kinematic relation. Okay, so given all this, what we are faced with is finding u. Okay? Which is, I'm sorry. We need to stick with coordinate notation here. Find ui, okay. And again, ui is a vector, right? It's a component of a vector, so this really is saying that u belongs to R 3. Okay again, parenthetically, we have the direct notation there. Okay? Find ui, right? Which, s, so ui is our solution field and you probably know if you, you know some elasticity, you know that this is the displacement field. Okay, so, find ui such that. The following hold, okay? Let's first write out the PDE itself. The PDE is the following. Sigma i j, sorry, sigma i j, j plus f i equals zero in omicron. Okay? And, if you all ready know it that simply means that the stress, the stress divergence, the divergence of the stress plus f equals zero, okay. This is sometimes called the stress equilibrium equation. Okay, so. Sometimes called the quasi-static stress equilibrium relation. All right? Okay. So that's the PDE. Boundary conditions. U i equals u given sub i on the Dirichlet boundary. But here comes the rub We have a vector problem, right? So if you look at the equation that we put down the PDE, this PDE. What we need to recall is that in all of this, we need to recall that we need to recall that i, j equals 1, 2, and 3, right? So when we look at the PDE, we actually have three PDEs there, right? For each value of phi. Not a, not a big surprise, because we are talking of finding a field which is itself a vector, right? Which has three components. So we're really solving three PDEs. Okay? So vector PDE. What this means is that we need boundary conditions for each component of our field, of our solution field, all right? But then each component of our solution field could have a Dirichlet boundary condition defined on a different Dirichlet boundary. Okay, so on this Dirichlet boundary itself we can have a subscript. What this means, when I go back, oh, well, l, let me draw it right here. What this means is that we have our basis We have our domain. It means that for each component, right, I could have a certain tertiary boundary. Okay. Right? Let me actually give it, give this a specific component. Then we suppose that this is the Dirichlet boundary for the component one. All right. However, when I come to component two and three, I am free to take on, to, to give them different Dirichlet boundaries, right? So it, and these could overlap or not. All right, they, they can be completely arbitrary all right? No implied relation with each other. This simply comes about from the fact that we're solving a vector problem. All right, and of course, it's elliptic so we do need boundary conditions everywhere. Okay? So that's for the Dirichlet boundary. Well, the same sort of thing happens for the Neumann boundary. Because each of those Dirichlet boundaries has as its compliment a Neumann boundary. And, I think I'm going to denote this as omega. Well, it's obvious, right? Omega, partial of omega t bar sub 1. We have partial of omega t bar sub 2 and Partial of omega t bar sub 3, okay? Of course, in each case, each set of Dirichlet and Neumann boundary conditions are complements of each other, okay? So, here we have, okay. So where does the Neumann boundary condition come in? It comes in to say that sigma ij, nj equals t bar i on partial omega t bar sub i, okay? What we have is that the boundary, okay? Is always equal to, partial omega u sub i, union partial omega t bar sub i. And that each Dirichlet boundary for a given component of the solution is, disjoint from its Neumann boundary. Okay? This holds for i equals 1, 2, 3. Okay? Here's what this means.  Three dimensions. This is our continuum football now, all right? When it comes to boundary conditions, what we're seeing is that for each component of space, right? For each compo, for each, f, for each, direction as defined by our basis vectors, we have, a Dirichlet condition for the corresponding component, so u 1! So maybe u 1 is specified on the maze bar of this football And the corresponding traction component, right? Which would be obtained through this relation. All right? The corresponding value t bar 1. Right? When you're talking of this. Okay. So if you want to specify on the maize part t1 is specified on the blue part, all right. And then again if you too maybe specified on the part of the football that you can see, right? And t2 is specified on the opposite part. Okay. You u 3 maybe specified maybe on some I don't know, some combination of the maize and blue. Maybe that maize part, this blue part. On it's compliment is where u3 is specified. Okay, what this corresponds to, is if you've studied problems of elasticity or mechanics is the fact that on a given boundary we may often not be controlling all components of the displacement. Okay? So it's common in, problems of elasticity to do, so called union axial tension problems, right? So supposing this were my, this were a bar and at this end, I decide to hold things fixed. So there are different ways to do this. I may choose to specify that on, that on this surface, on this edge of the, this sort of LEGO pieces, I control all displacement components, right? U1, u2, u3, I may choose to say all of them are zero. All right. And then I may choose to load this thing up. Right? Alternately, there is another version of ten, of tension. That it, what, what I've just described is not true in the axillary tension. It turns out it develops stresses at this end in, in, in, in directions other than along the longitudinal direction. Instead, I may choose that at on this boundary I specify only this component, let's call it the e 1 component, the u 1 component, to be zero. Okay? And the other two components are allowed to move around, to relax, so that now, when I pull this bar. Because of something called the Poisson ratio, in linearized elasticity, this bar will tend to shrink in size. It will tend to contract in, in, in, in the other two dimension I pull it one way. Then I'm allowing points on this surface to also move in the, the vertical direction and the direction towards you. Okay? So elasticity gives us the freedom to do that on a single boundary. There are many ways in which we can combine the Dirichlet boundary conditions on, on the, any given surface and indeed for the traction conditions too. All right, so in the problems I just described here, in this uni axial tension, often the lateral boundaries are free of traction. Okay? Which means there are traction on those surfaces is zero.  Sorry, zero. Now when I pull on this, on this end of the bar I may choose that but also specifying the Dirichlet condition, right? I may specify that I'm, I'm, I'm going to pull this end. There we go. I'm going to pull this end to some distance, right? That's a Dirichlet condition. Or I may choose to say that on this surface I'm specifying only that the traction along this direction, this longitude, this, this longitudinal direction is controlled, only that component of the vector. Okay? I'm a, I'm free to actually to specify the displacement components in the other two directions. That's they're all kinds of ways in which we can combine them. All right? And, and these are actually highly relevant for physical situations and lead, they're, they're completely different boundary value problems. They lead to solutions that look That look, that are different in, in important ways, okay? But the important condition to always be satisfied, is that the union of the Dirichlet and Neumann condition, boundaries for each component, always essentially, give a union that gives us the entire boundary. And that each Dirichlet boundary and Neumann boundary for a given component direction are disjoint. Okay so this is just a beginning step an early step towards the strong form of elasticity. I'm going to end the segment here, but when we return, we are going to say a little more about this. Because it's important to understand what elasticity's trying to do here. I will also write up the direct notation for this, problem. And say much more, actually about, many of the, the ingredients of, this problem. And it's important to understand that, as we are, setting up numerical methods for it. The comp, the complexity comes mainly, entirely from the fact that it's a vector problem. All right, we stop here.

\section*{ID: 7sY8JnZHeko}
I neglected to write something down on this slide and it happened when, I was talking about, the nature of the boundaries here. In particular we talked about how we decomposed the boundary into a and a subset for each spatial dimension. And we also mentioned there that in each spatial direction, the and subsets are mutually exclusive, right. So their intersection is the empty set. I said that, I didn't quite write it. So, let me put it down here. Let me try and squeeze it off to the right here. So that would be, partial omega U, sub I, intersection partial Omega T sub I = the empty set. All right so with that everything works out.

\section*{ID: FA4VWUXqOu4}
In the last segment we've seen 3D linearized elasticity using coordinate notation, in strong form. What I'd like to do to start off this segment is for completeness put down the direct notation version of the strong form of 3D linearized elasticity. Okay? So, let's start then with linearized elasticity In three dimensions. Okay. Direct notation Okay, the setting is as before, we have our bases, e1 e2, e3. We have our body, right? Omega. We have our position vector of a point there. Right? And as we've done before, we observe that for boundary conditions, we have Dirichlet boundary conditions for each spatial dimension, right? So this is where we have u sub i equals u. G sub i for a specific i, right? And in the complimentary part, we have attraction boundary condition, right, or the Neumann boundary condition for a specific i. Okay, and remember that the whole business with the fact that we are now looking at a vector problem, right. Our, our solution the solution that we're looking for is a vector solution with three components. This is what specifies that when we look at the boundary of the body, we need to specify boundary conditions separately for each component. So, it being linearized elasticity, being an elliptic problem, it means that we do need to have boundary conditions at every point of the boundary but what can happen is that perhaps over the maze part of this boundary we specified maybe the u1 component of the, of displacement over the blue part of the boundary, which is complement to the maize part. We may be specifying therefore, the T1 component of the traction. All right? However over a different part, maybe this part of the boundary, we specify the u2 component of the, of the displacement, and on its compliment, which is facing me, we specify the t2 component of contraction. Observe that the way we've partitioned the boundary for the two different cases for spatial dimension 1 and 2 are overlapping, all right? Okay? And, and likewise for, for spatial dimension three, it may be a completely different sort of partitioning. Okay. Whoever, on the partitioning for each spatial dimension must satisfy the requirement that the Neumann and Dirichlet boundaries are disjoint. Okay, as I've indicated in the figure here. Okay. So we have, of course, that that the boundary is equal to the Dirichlet boundary union, the traction bond, the Neumann boundary and furthermore ,for each such decomposition, the intersection is disjoint, right? Sorry, the intersection is empty, right? Those boundary subsets are disjoined. Okay. Given all this, here is what we want to do. What we want to say, is, given, ug sub i, t bar sub i, the vector f. Now, I'm using direct notation for the vector f, okay? And the cosiderative relation sigma equals c, contracted with epsilon. Note that I'm using direct notation here before. The contraction of, the fourth order tensor, e is c, and there is, and there, the stream tensor epsilon. Okay, given all this, and the kinematic condition the kinematic relation. Epsilon equals the symmetric part of the gradient of u, okay? Given all of this, find. U, okay. Such that. Okay we have divergence of sigma plus f equals zero vector in omega, okay? And then we also have, e sub i equals ug sub i on that Dirichlet boundary, okay? Note that we cannot write out the Dirichlet boundary condition in direct notation simply because the boundary subsets are different and each component is specified. Possibly in a different boundary subset. Right, and the same thing happens for the traction condition. For the traction condition, we can however write sigma n, which is a vector. We're seeing that its ith component equals t bar i on partial omega t bar i, okay? All right one thing to note here is that This notation, delta sigma, is sometimes written as divergent of sigma, right? And, it is, sometimes also interpreted as taking this operator, the gradient operator, and dotting it with sigma. Okay, which is also the same thing that's expressed here. Okay. Alright, this is the end as far as direct notation is concerned. Perhaps the one other thing to remember, is that, when we write in direct notation, C contracted with epsilon, this is, a second order tensor, okay? This is a second order tensor with free indices i j, okay? And this is, by definition, this is C. I j k l epsilon k l. Okay. All right. So this is direct notation. This is an explanation of direct notation using coordinate notation. All right. Now, however, in order to continue, in order to develop our, formulation, I'm going to prefer to use coordinate notation because it makes all the manipulation of indices much more transparent. Right. Long before we get into that however, we need to see a little more about the constitutive relations. Okay, so let's do that. We have sigma, I'll, I'll use coordinate notation. Sigma i j equals C i j k l epsilon k l. Okay. Let's say a few things about this. C i j k l is a fourth-order tensor, it's called a fourth-order elasticity tensor. Okay, for linearized elasticity, it is constant with respect to. Wrt is short for with respect to. Right. C i j k l is constant with respect to epsilon. All right? With respect of the epsilon tensor. All right? So this is what we mean by linearized elasticity. And the context of linearized elasticity, this is what we get. Essentially what that means is that the relation between sigma and epsilon is a linear one. Okay? The relation on this, in this equation is a linear one. When we have linearized elasticity, and that's translated by the fact that C is a constant with retrospect to epsilon. Okay. I want to say a few more things about the properties of C. C has what we call as major symmetry. 'Kay? And it's important that we understand these properties betw, because we are going to use them we, develop our formulation. Okay. Major symmetry. What this means is that C i j k l equals C k l i j. Okay, and here's why. It follows. From. The fact. That. When one is dealing with linearized elasticity, linearized elasticity itself is built upon the idea that, there exists a function. That backward looking E is, the symbol for there exists, okay? There exists a function, psi, okay, which is a mapping from S3. S3 being the space of symmetric, second order tensor. Okay. There's a mapping from that space S3 to the space of positive release. Okay? All right? So S3 is symmetric second order tensors. It's, it's that space. Okay. Now, there exists a function psi which is mapping from S3 to R, it's in fact call, written as psi function of epsilon. Okay? And this is the strain energy density function. All right. We saw an explicit form of strain energy density function when we developed our variational methods for linear elasticity in 1D. All right, you consider it a quadratic form, there. We'll get into, into the particular form for psi in a, in a little bit, but let's just, for now, all we need to know is that this exists, okay? It can be shown then, that, right, one can show that C i j k l is the second derivative of psi with respect to epsilon i j, and epsilon k i. Okay? In fact, linearized elasticity, since it says that C is a constant, also says that, since C is constant, with respect to Epsilon, okay? It implies that psi is, at most, what? In fact, psi has to have a certain form. Can you think of it? Psi of epsilon is quadratic. Okay? So this second derivative of, psi with respect with epsilon, can be taken and, in general, it's not zero. Okay. Because C i j k l has that relation to the second derivative of psi with respect to epsilon, it follows that. C, I, J, K, L, which is equal to second derivative of psi with respect to epsilon I, J, epsilon K, L Is also secondary root of psi with respect of epsilon kl epsilon ij. What condition do we need to have that to be true? It is that in general psi is, let's just say here smooth enough. Okay, with respect to Epsilon. It can be more precise but for our purposes that's all we need to do. All right, if psi is smooth enough we can interchange the order of derivatives of epsilon, I, G and epsilon, K, L, right? But then if you look at the second, or rather the third member of this equation, this by definition is just Cklig. Right, and what we've called our sum major symmetry has just been proven. Okay? So, so the fact that C has major symmetry, it really follows from the existence of the strain energy function, strain energy density function, all right? It emerges however, that C has further symmetries, okay? C also has minor symmetries. Okay, which is that C, ij kl equals C, j i k l. Okay? And this follows because the stress Is symmetric. Okay, it turns out that the stress that we are dealing with here is properly called the Cauchy stress, but actually that's a detail that is somewhat lost in the context on linearized elasticity. Okay, it gets more important if we we're doing non-linear elasticity which we are not. Anyway, the Cauchy stress is symmetric. Okay? Which is that sigma i j equals sigma j i. Can you think of why that last relations holds? It follows from the balance of angular momentum, for this version of theory of elasticity. Okay? Well, if that is the case what we have is sigma ig which from our constitutive relation to cig kl epsilon kl and note that the kl and the Cs have been contracted out with the kl and the Cs of epsilon leaving free in the Cs ig. Okay? But this is also sigma ji which from definition is C ji kl, epsilon kl. Okay? These two conditions then imply that C ijkl equals C jikl, right, which is this condition of minor symmetry that we'd put forth, okay? C has the other kind of minor symmetry as well. Okay? So the other kind of minor symmetry is that C ijkl equals C ijlk. Okay? And why does this come about? This comes about because epsilon kl is one half, by definition, it's one half uk comma l plus ul comma k, right, and remember what that is, that is just one half derivative of uk with respect to xl, which is the first element of the parenthesis, plus derivative of ul with respect to xk, okay? Well, if this is the case, it follows that epsilon kl equals epsilon lk, okay? But then what that says is that sigma ij, which is C ig kl epsilon k l, is also C ij kl epsilon lk. Right? Okay? Now, if you look at, the order of the indices here. Because of this contraction, right? Okay? And recognizing that l and k are just dummy indices. Okay, it follows that this last term can equally be written as C ij, okay, lk, epsilon kl, all I've done there is to switch the order of the, switch the position of the key and l indices. Okay, it doesn't matter because they are dummy indices and they are being contracted out anyway. But then if you compare this and that, okay, it implies that C ij kl equals C, ij lk. Okay. Which is what we'd set out to prove for the second of our minor symmetries, right, this one. Okay. That sort of sums up what we needed to see about the symmetries of our elasticity tensor c, we will use, these symmetries in a very important way when we develop our formulations for finite elements. All right, we'll end the segment here.

\section*{ID: TNpV5IJy5Bc}
Welcome back. We continue with our study of the constitutive relations of linearized elasticity. What we've seen so far is the fact that our elasticity tensor c is symmetric, right, it has major and minor symmetries. We saw what these mean and why they hold. Continuing. Right. In the context of linearized elasticity, there is a, another very important property that c owes, okay. So C is, positive definite. Okay? And, here is what we mean by saying it's positive definite. For all tensors and, I'm thinking of what I should use as a general tensor up here. Let me use theta, okay. I hope that doesn't disturb you, any of you too much if you're, gotten used to seeing theta as a, as temperature, or angle or something. Theta is a tensor for me. So theta is a general tensor, so it doesn't need to be symmetric, so I'm going to say it belongs to that space. That's not notation I've used before, GL3 simply means the group of general linear, I guess I should capitalize them, right, so, okay. General linear transformations. In R3, 'kay, that's what GL 3 is. It's just a fancy way of saying matrices in 3D, if you choose to look at heavy tensor as being a square matrix, okay. Anyway, so supposing we have this, right? Then what we're saying is that for all theta belonging to this space, we have the following result. Theta contracted with C, contracted with theta again, right? So we form this quadratic product of C with theta. Okay? This is greater than or equal to 0, right, the scalar, for all theta. Okay? In coordinate notation, this is what it means. Theta ij, Cijkl, theta kl is greater than or equal to 0. Okay, that's what we're seeing. It's useful to write this in coordinate notation just to be sure that this quadratic product is indeed a scalar. Okay, so it's greater than the real number 0. Okay. This holds for all theta belonging to gl3. In fact, what we can also show is that actually, the equality, right, so theta contracted with C contracted with theta, equal to 0, right, which is one of the cases of the, of the inequality on the previous line. The, the, this special case, the equality holds if, and only if, iff to mean if and only if, theta itself is the 0 tensor. Okay? All right. This is the condition that holds, okay. Now this is very important in the context of linearized elasticity. It essentially translates to the condition that linear elasticity or linear rise elasticity talks about materials that are fundamentally, not subject to material instabilities, okay. And this is important if you can imagine as you may imagine if you're going to compute with this model, right, we're going to put stresses and strains. We're going to put loads on it. It's important to know that material instabilities within the context of linearized elasticity are inadmissible, okay? So what this means is that linearized  elasticity theory,  has no material instabilities. Okay. If you're wondering what a material instability could possibly be, fracture is a material instability, that's not covered by linearized elasticity. Okay, you need to do special things beyond linearized elasticity to admit fracture. Likewise slip bands or shear bands, which form in the context of, of extreme plastic deformation are not covered by linearized elasticity well. You have to include plasticity theory for that, okay. That's always the same. It's important, however. It's actually very important, and we're going to see another result which , which, which brings out that importance. Okay.  what I'm going to do next is, actually give you a very actually, a very special form of the elasticity tensor, but one that is used very widely, okay. So we are going to look at, we're going to write out C, for materials that are isotropic. For isotropic materials. 'Kay, of course, this is for isotropic materials, long as we want to treat them, with, linearized elasticity. And of course, we're talking mechanical isotropy, right? Okay. Right, let me first write it in coordinate notation. Okay. In the context of linearized elasticity, the requirement of material isotropy also translates to the fact, and this can be proven, that it, that the material can be entirely specified elastically with just two constants. All right, and those constants are what go into C So with that being stated, here is what C looks like for an isotropic okay? One of those constants is lambda, I'm going to denote it as lambda. So, C i j k l is lambda times delta i j delta k l, where, can you image, can you guess what the delta i j refers to? Kronecker deltas. Right, and of course this one too. Okay? So we have that plus, 2 mu times one half delta i k delta j l plus delta i l delta j k, all right? If you wonder why I put, put in a 2 mu and then reintroduced the half, it is because what I have here is, also often recognized as a fourth order tensor, okay? This is often denoted as sort of a bold faced I with indices i, j, k, l, okay? And this is what we call the fourth order. Symmetric identity tensor. Okay? What it does is the following. If you take I as defined there, okay, and, feed to it any second order tensor, right? So let's use theta which we used in the previous, slide, right? And remember, theta is just any old second order tensor. Doesn't need to be symmetric or anything, right? It turns out that I acting on theta spits back for us theta, but the symmetric part of theta, all right? Which is, since I'm using coordinate notation, that is properly written as one half theta i j plus theta j i. Okay. So it sort of gives us back theta, but the symmetric part of it. All right, so if theta were already symmetric, well it would just give us back the same tensor. However, in general, when we feed it a non-symmetric tensor, a non-symmetric tensor, this identity gives us back just the symmetric part of that tensor. Okay. All right. Right. Now, in writing direct notation, so I'm going to use this I in writing the direct notation version of this relation, in writing the direct version notation of this relation, delta i j, the Kronecker delta, I like to write as, what I call a bold faced one. Indices IG. Okay? If you come from the world of matrixes, that's just your three by three identity matrix. Okay? All right. So, direct notation for the same relation is the following. C equals lambda: one dyadic one, okay, that's the dyadic product. Again, to know what it is, well just look at the direct notation, that's what it means, okay? I often call this the tensor product, plus 2 mu times the fourth order identity, symmetric identity. Okay? Note that because each of these, tensors, each of these ones is a second other tensor then they're dyadic or their cancel product gives us the for, a fourth order tensor. Okay? So this is direct notation. All right. What about these constants? Lambda and mu are what are called Lame constants. OK? And if they seem unfamiliar to you, they will become familiar in just, a few seconds. And here's how they're related, right. If now, you use something that you're probably really more familiar with if E is the Young's modulus. And if nu is the Poisson ratio. Okay? Then, the relations are the following. The Lam constants can be related to these guys. So Lambda is E nu over 1 plus nu times 1 minus 2 nu, okay? And mu is E divided by twice of 1 plus nu, okay? Chances are the Young's modulus and Poisson ratio are something that you've encountered almost certainly in, in your, second year of, your study of mechanics or, or, or, or engineering or something like that. Do, do you recognize what this is? Do you recognize what mu as defined as that also gives you? That should be a little more familiar if you recall these sorts of relations. It is the Shear modulus, right, all, of course, for isotropic materials. Okay? Lambda is not related to Shear or anything. It's just often just called along a parameter. A third, yet another modulus that you may be familiar with is kappa, which is the bulk modulus. Right, so kappa is related again to E and nu as E divided by 3 times 1 minus 2 nu. Okay? All right.  So, there are limits on, these, constants, and I'll talk about them in just a bit. It, it emerges that, mu can lie between minus one. And half.  All right, for the linearized theory of elasticity. Okay? Do you recall what this limit Corresponds to. No, this corresponds to elastic incompressibility. Okay? And as you may imagine from here, if you approach the limit mu equals minus 1, you get a material that is shear unstable. All right, so actually well, actually to use this theory properly, I guess I should, let's just do that, okay? All right, because otherwise, linearized elasticity tends to break down in those limits. So let's, let's just leave those actual limits on, okay. Right? Also from here you, you may be able to see why mu equals half is the limit of incompressibility because it makes the bulk modulus get unbounded, okay? So you have a material that's very, very stiff to, to compression, right, like that. Okay, fine. So, let's get back to talking about, positive definiteness, all right? So having put down this form of the elasticity tensor for an isotropic material, the, the condition of positive definiteness. 'Kay, it translates to a requirement on our on the elastic constants that we were working with on the previous slide, okay. In particular, one can demonstrate the following. Positive definiteness implies the following conditions. It implies that lambda plus 2 mu has to be greater than 0, okay? It also implies that mu itself has to be greater than 0, okay? If you come from the world of structural dynamics or acoustics in elastic materials, these conditions may mean something to you, okay? You recall it. No? Okay, well, let me see. Well, maybe some of you do know. Well, the reasons these are, these are important conditions is the following. In elastic materials waves propagate in at least two ways. In the bulk of a material, as longitudinal or sheer waves. It emerges that the longitudinal wave speed, is lambda plus, sorry, that lambda looks bad. The longitudinal wave speed is lambda plus 2 mu divided by the mass density under the square root, okay. So when we say that lambda plus 2 mu is great than 0, what we are saying is that we have real wave speeds. It allows longitudinal waves to actually propagate through the material, okay. And, the sheer wave speed, is square root of mu divided by rho. Okay, so these conditions on lambda plus 2 mu and mu essentially allow us to have, propagating longitude and shear waves. In the elastic material. In the elastic material that we are describing with this linearized theory of elasticity. Okay? Let me see, now. Now, it turns out that in most materials of interest, the longitudinal wave speed is greater than the shear wave speed. Okay, and that's a fact that is sometimes used in geophysics too, when people are, are dealing with earthquakes. Anyway, all right, that is probably the basic material we need to know about our constitutive relations for elasticity, for linearized elasticity. And there is more of course that can be said, but it's not, but, but we won't, we don't really need to get to it, I think. Okay, so we're going to end the segment here. When we return, we will work with the weak form.

\section*{ID: gJZg7YLUw-c}
All right, welcome back. We continue and we will get down to business now. And we so by writing out the weak form . The weak form for linearized elasticity. Right so. I'm going to take the approach we, adopted when we worked with linear elliptic PDs with scalar variables in 3D. Okay, which is, and I'm going to put down the weak form. And then, I will demonstrate how we obtain it from the strong form. I won't go back the other way to just avoid just, because those, those arguments are, identical to what we did in 1-B. But they are, you know, they're more subtle and, and so on. Okay, but however we know how they, how they operate and, and they hold. Okay, weak form of linearized elasticity. And, and, and from here on, what I'm going to use is called notation only, okay? Because there are details, as you can see, which become most transparent when we use coordinate notation. Okay, so given the usual data. Given ug sub i t-bar: i, f, i, the constitutive relation. Sigma: i, j Equals cijkl, epsilon kl, and the kinematic relation, Epsilon of kl, equals one half. Partial of u k with respect to x l plus partial of u l with respect to x k. Okay? Given all of this, find ui, OK? Which, belongs to some space S, OK? Which consists of all u i such that u i equals the Dirichlet boundary data on the corresponding Dirichlet boundary reserved for that particular component. Right? Of course what's implied here is i equals 1, 2, 3. Okay? Find u i belonging to s in this, of, of this type. Such that. For all wi belonging to v which now consists of all functions, which satisfy the homogeneous boundary condition on the corresponding Dirichlet boundary, and corresponding Dirichlet boundary, right? Here, too, we imply that i equals 1, 2, 3, okay? Okay. So let me read this all over. Given the data u, t-bar, u g, t-bar, f and constitutive relation. The constitutive relation and the kinematic relation, epsilon equals that turtle's derivatives of u. Find u belonging to s such that for all w belonging to v, the following holds, right? Alright, so now let's write out what holds. Integral over omega, wi,j, sigma, ij, dv equals integral over omega w i f i d v. Plus, now, here comes the rough. Because of the fact that our traction boundary condition needs to be specified individually for each component, we need to straight away here have a sum i going from 1 to number of spatial dimensions. And of course, we're doing this in three dimensions, right? Of integral over the corresponding traction boundary. Okay? Of wi t bar i ds. I want to emphasize here that, do we imply a sum in that integrand over i? Think about it. Look at the last sum. There is in fact no sum. Ok, a sum is not implied there. A sum is explicitly being carried out here. But importantly we don't do that over the integrant, because, why do we not do it over the integrand? It's because that domain of integration could be different for each component i. Okay. So we can't quite do it as an implied sum. What we need to do instead is compute that product for each value of i w i t-bar i. Integrated over the corresponding Neumann boundary, right? Which could be different for each component, right? We get a scaler, right? And then, we sum up those scales, right? Over to three special dimensions. All over the special dimensions. Okay? That's important. So let me state here. No sum implied for, for just that term, wi t bar i, okay? Instead, we have that explicit sum. All right, this is our weak form. What I will do know is derive it from the strong form. OK? And I, I'm going to take a bit of a short cut. I'm not going to re, well, OK, maybe I will restart all the data very quickly. So, what we, what am, I'm going to state here first that the strong form Implies the weak form. And the weak form also implies the strong form, but like we did for the case of the scalar 3D elliptic linear equations, We'll do only the rightward implication here, right? The rightward equivalence just, just to see a demonstration. All right, and that is the following, right, once again. Given u g i, t bar i, f i the constitutive relation sigma ij = Cijkl Epsilon k l. The kinematic relation. Epsilon i, epsilon, let me write this as epsilon k l equals This. Okay. What we want to do is find ui such that sigma ij comma j plus fi equals 0 in omega and boundary conditions. U i equals u g i on the relevant tertiary boundary. And sigma i j nj equals. T bar i on. Okay, so this has wrong form. Alright, yet again. Alright, we start out from here and now we introduce as we did before, the waiting function, right? So what we start off by saying is now let us consider. W i, belonging to v which has the usual property right? Which is the v consists of all wi such that wi vanishes on that Dirichlet boundary. Okay. What we will do now, and this is something you may recall from our previous repeated development into weak form. Actually, think about, what do we do? How do we proceed now? Alright we multiply that pde with wi and integrate. Okay, so, what we do is, multiply the pde by wi, and integrate over our domain omicron. Right? Which is an open domain in r, r3. All right, fine. When we do this, here's what we get. We get integral over omega wi sigma ij, comma j, db. Plus integral over omega, Wi Fi d V equals zero. Okay, here sums are implied over i. Okay, so, I'm not going to state it which means sum is implied, right. Okay. So, do you recall what comes next? Right, we observe that this, divergence, right, we get rid of by, invoking integration by parts. Right, and integration by parts, you may recall, is nothing but a combination of, the product rule of differentiation and Gauss's Theorem of Divergence. All right, apply to tensors now. So, what we do here is integrate by parts. Right? And we integrate by parts in that term, okay? And in integrating that parts, remember that this is basically product rule of differentiation. Plus the divergence theorem. Okay? And the way we do that is the following, right? We observe first that that derivative on sigma can be rewritten if we just consider it to be the following. Integral over omega, a derivative of the whole product. Okay, minus an integral over omega, wi comma j sigma ij dv. And what I've done here is applied the first of my to two techniques here, right? I've applied the product rule here. Okay. And of course, along for the ride is, W, I, F-I, D-V, integrated over omega. The whole thing is equal to zero. Okay. The next step, we invoke off the divergence theorem on the first integral. And in doing so, we observe that since the i index has been contracted out here, sum being applied, that term in parentheses is essentially a vector, right? A vector with free index j. We recognize that we have properly here the divergence of that vector, right. That vector for our purposes is something that you may probably write as w dot sigma, okay. It means we just have the divergence of that vector. Right, so now we apply the divergence theorem on. That term, right? And in doing so, what we get is integral over the boundary, okay? Wi sigma ij mj ds minus, the other terms don't get don't change. Minus w i comma j sigma i j d V plus integral over omega w i f i d V equals 0. Okay. We have this. And, what we are going to do now is, is two things. I'm going to rearrange things a little. And the way I'm going to rearrange this is by observing that this term has a negative sign. So I'm going to move it to the other side. But I'm going to change my left and right hand sides.

\section*{ID: vWcNQ5r5KJ8}
So there was a question on how the summations over spatial dimensions are handled and to answer it, it's probably best to start out with the strong form, okay? So I'm going to just talk about. Over i equals 1, 2, 3, right? Spatial dimension, three. Okay. So this is the question that I'm going to try and address. The, the way to start off with it, probably the best place to look at it is considered the, the strong form. Again, I'm, and I'm not going to write out the entire strong form because I think we are all a little tired of  seeing it too many times. Okay, strong form, just the PD, right? Sigma ij, j plus fi equals 0. Now, remember this holds for i equals 1, 2, 3, so really what this means is that what does it mean? It means that there three equations, right? Three separate equations, and those equations, I'm going to write them out explicitly, are sigma 1j, j plus f1 equals 0. Sigma 2j, j plus f2, equals 0. Sigma 3j,j j plus f3 equals 0. Now in each of these, when I say j comma j, there is a sum implied on j, right? Sum implied on j. That statement has got to be an irony because the moment you put it down it's used the implication is no longer implied, right? It becomes an explication. Anyway. All right, so this is what we have okay? So, so when we look at the strong form, we really have three, three PDs, right? Three equations that are being solved independently, right? Each of them have to hold. The weak form, however, is different in, and, and you remember how we get to the weak form, right? Now, now in order, in getting to the weak form, we say how do we get to the weak form? All right, we take that same equation, sigma ij, j plus fi, right, equals 0. We multiply it through by wi, okay? And now we're seeing there's a sum on i and j. Okay? In so doing, we have essentially collapsed the three, the three equations that we started out with into one. Right? The explicit form of that is I'll write it out just once, w1 sigma 1 j, j plus w2 sigma 2 j, j plus w3 sigma 3 j, j plus w1f1 plus w2f2 plus w3f3 equals 0, right? That's what we're doing and then we integrate this over the volume. Right? So here too, sum over j. Okay? And then, of course, we do integration by parts and so on, but observe that here too, even though we started out with three PDs, we have collapsed them all down to a single weak form, right? A single equation, right? A single scalar equation really, okay? And then, of course, from here when we go on, you know, we do integration by parts and so on. What we end up is a term where we have integral over omega wi, j sigma ij dv equals integral over omega wifi dv plus the sum over i, integral over this, wi, t bar i. Okay? Now, the form of this is, is the following, the explicit form. It is integral over omega w1, 1 sigma 11, plus w1, 2 sigma 12.  Right? And somewhere along the line, we come a lot, come upon w3, 1 sigma 31 plus, until finally we come to w3, 3 sigma 33. All right, there are nine terms in here. Sorry. Yeah. Nine. Okay? Nine terms in here, dv, okay, equals the first term on the right hand side has how many? Three, w1f1, it goes until w3f3. Okay, and then this meddlesome term involving the sum, right, over the Neumann boundaries. I'm going to, I'm going to write it over here, because I need a little room. Okay, so that is integral over partial omega t bar 1 w1 t bar 1 dS plus integral over omega t bar 2 w2 t bar 2 dS plus integral over, the third Neumann boundary subset, w3 t bar 3 dS. Okay, and the key in this case, that each of those boundaries' subsets could be different. Okay, so over each of them, I mean clearly there's no sum implied, right, because I've written everything explicitly. Okay, so over each boundary subset, you're only integrating its corresponding components multiplied. Okay? So, all right, so that, that's how it works the weak form is a single equation, unlike the strong form, which is three.

\section*{ID: QtwjiOnHICE}
So, essentially, what I'm going to do in this process is the following, integral over omega wi comma j sigma ij dv equals integral over omega wi fi dV plus integral over the, the entire volume, right. Wi sigma ij nj dS, all right. Now, here comes a step that is a little different from what we've done in our previous application to a three dimensional problem, okay. What we are going to use is this special decomposition of omega into. Into subset spatial omega ui, which is tertiary boundary and, Neumann boundary, okay. This holds for i equals 1, 2, 3. Okay. So what this says is that the way we're going to apply this is by regarding this product as a vector with index i. And then observing that we essentially have a, in that integrant, a dot product of w with that vector sigma n, okay? So, the way I am going to write that is the following. And we right that as integral over omega, okay, so let me write everything here. Integral over omega on the left hand side, wi comma j sigma ij db equals integral over omega wi fi dv plus. Now, because that integrand when viewed as a final, dot product of w with the vector sigma n. Right? Because that integrand has three terms in it, right? For i equal 1, 2, 3. We're going to write each one of those contributions of the dot product, right, separately, all right? So I'm going to write that as each, so each, of these decompositions of, of the boundary, all right, are going to be used, to give us the domain of integration for each integrand. Sorry, for each contribution to this integrand, right. Okay. All right, so, so the idea is you fix i, maybe i equals one, right. So you do the integral by viewing our boundary as a union of these two subsets, boundary subsets, right, but in computing that integrand you compute only for a fixed i, okay. So you fix these guys, right, these are fixed. Okay, fix everything, right. So fix all those i's. Now that gives you, the integral with just one of those components. You need to have all the components, and you do the sum. Right? So far, we've just written that integrand differently, okay? That's all we've done. Sorry, all we've done is write that integral, differently. Okay, but now we can do something nice, right? And what is that? We invoke our boundary conditions, right? Now we invoke. Boundary conditions on wi and sigma ij nj, okay. All right. And remember the way we wrote those boundary conditions, okay? Essentially, the way we wrote those boundary conditions, say that, now that we're integrating over each, special decomposition of the boundary, what we can do is this, right. So let me actually go to the next slide. So, what we can say here is the following, right. So we have integral over Omega, wi comma j Sigma ij dv equals integral over omega wi fi dv. That term has got to, got to be the most tame term we've ever encountered, right? The, the forcing function pretty much does nothing. Okay, plus. Sum i equals 1 to number of spatial dimensions integral. Now, I'm going to break up that integral, right? Because we've written it on the previous slide as an integral over the union of tertiary and Neumann boundary subsets, right? So I'm going to write it as integral over partial of omega ui of wi sigma ij nj ds plus integral over partial of omega t bar i, wi sigma ij nj ds. And I close my parentheses here, 'kay? Actually, looking at, what I've saved here from my previous slide. I'm going to go back for just a second because I realized that I did not put my elemental area, that integral, okay, but you may have caught it. All right. Here we are. All right, now is when we invoke the boundary conditions on, on wi. And what is that boundary condition on wi over the corresponding tertiary boundary? It is that it, vanishes. Right. And what is our boundary condition on sigma ij nj over that part of the Neumann boundary? It is that sigma ij nj equals at t bar i component, okay? And so we have it. We have integral over omega, wi comma j sigma ij dv equals Integral over omega w i f i d V, plus sum i going from 1, to a number of spacial dimensions. Integral over the corresponding Neumann boundary only, wi t bar i dS, okay? This is our weak form, all right? Which we've derived from the strong form. Like I said, we can go back the other way. But, it's not something we get into here. I will make one remark though. Which is that, just as we did in our little treatment of variational methods in the context of the linear, linearized elasticity problem in one leap. All right? We demonstrated there that the weak form could be obtained through these variational arguments of extremization of a free energy functional in one day. Exactly the same things holds in three dimensions as well, okay? So, the weak form can be obtained as this so-called Euler-Lagrange conditions The Euler-Lagrange conditions of a variational principle Right? And what is the variational principle? On well it's not minimization necessarily but it's really, it's extremization, it's on extremization. Of a free energy functional in three dimensions. The problem we treated a few segments back, quite a few segments back, quite a few units back, was in 1D, but essentially all those arguments hold in three dimensions. And actually, this is a very powerful approach. While in the case of the simplif, the simplest stuff, formulation of linearized elasticity, it doesn't give us anything that we can't get through other methods. Something you may be aware of is that there are so called constraint problems, right, and a common constraint on linearized elasticity is incompressibility. So if you're dealing with a linear, with a linear elastic material that's also incompressible. It emerges that the methods we work with, the standard finite element methods we work with, and that we will be developing now, do not work well. Okay? And there are fundamental pathologies which arise, and these are very well understood. And methods to circumvent that difficulty, right, are based upon defining more generalized versions of these free-energy functionals, introducing more fields and working with them. It's the domain of what are called mixed-methods, okay, and it's a very powerful approach and which, which arises from this, from the variational basis of the method. Okay, but we won't be getting into that in this entry level series of lectures. Okay, the very last thing I want to do is just go from there to putting down the finite dimensional weak form. Okay, so what we have here is will be just derived is the infinite dimensional the weak form.  Okay. The finite dimensional weak form. Follows. All right, and now of course you're quite expert at doing this, and you know exactly what that's, what going to happen, is going to look no different except for this pacification spaces, right? But let's just put it down so that we will be able to move on with the formulation. Okay, the finite-dimensional weak form. As before, given Ugi, t bar i, fi, the constitutive relation sigma ij equals Cij KL epsilon KL. Right? That's the form of the constitutive relation. Let me state it That's a constitutive relation and the kinematic relation epsilon KL equals the neutral stuff. Right? Given all these, find, now, now here's where things become different, right? So actually let me put it on the next slide, so that we can, give it its due attention. Find sub i, right? And as, as always that superscript h reminds us that we're talking of approximations and h is going to be related to the element size. Okay,that's all it denotes. Okay, to find of i, belonging to sh, which is a subset of s. Okay. And what do we mean by sh here? sh consists of all functions. Now, we further require as we've done before that we are interested in functions that are H1 over the domain. Okay? And, of course, they must satisfy in this class of finite element methods that we're working with throughout the series of lectures. We do require that they satisfy the Dirichlet boundary condition on the relevant subset of the boundary. Okay? So, find your weight of this type such that for all wh sub i belonging to Vh subset of V, okay? And what is Vh? Vh consists of all waiting functions, also H1. Such that, Wh of i equals zero on. The corresponding Dirichlet boundary. Okay, so find Belonging to Sh such that for Wh belonging to Vh. What all is well, the weak form that we wrote before, except its called the relevant fills, replaced with their finite dimensional versions. Whi,J, sigma hij, dV. Right? And we recall here that sigma h i j is simply computed by a C i j k l epsilon h k l. Okay? And what is epsilon h k l? Well, it just obtained from this relation. With u h. Okay? All right. This is equal to integral over omega. Sorry this is over omega too. w h i f i Dv, okay? Remember that, as before, we do not consider finite dimension inversions of, f, right? The, the f is part of the data, and we take it as given. Plus sum over the spatial dimensions. Of the integral over the Neumann Boundary, of Whi, t bar i, ds. Again, because d bar is part of the data we don't try finite dimensional representations of it, okay? This is it. Just for completeness, let me squeeze in here that epsilon hkl equals that. This parentheses closes that one. And this closes that entire node. Okay. All right. We are done with this segment. When we come back we will just work with this to develop our finite element formulation

\section*{ID: kDfW0vYt--g}
Welcome back. We have got as far as deriving the finite dimensional weak form for 3D linearized elasticity. What we're going to do in this segment is essentially press ahead with the formulation. And of course you know how this proceeds. We essentially have to define basis functions, construct, representations for the fields that we care about, and go ahead and compute the integrals that arise in the weak form. Okay, so I'm going to call this segment the finite dimension weak form and basis functions. Okay, and remember just the integral equation we are working with is the following. I am not going to put down the whole finite-dimensional weak form in all its glory, but just remind ourselves what we are trying to accomplish here. We are trying to solve the following equation. Integral over omega Whi, j sigma h ij dV equals integral over omega. Whi fi dV plus this curious sum of integrals which has this form. Sorry a w bar is not required. Whi, t bar i, dS. And I'm not going to state it now but just recall that on the last interbrand we don't have a sum implied. Okay we have this and of course there are constitutive relations and we've studied those and summed and summed it. As you imagine now, we, we essentially have to define basis functions, right? And, and in defining basis functions as before, we will do this by constructing a partition of our domain, right? We partition our domain as follows. Basis our, domain of interest which is really the body that we want to solve and the linearized elasticity problem over that's our domain Omega. Actually, let me leave that outside of the body because we will confuse it with other things, that's Omega, we have the usual decomposition of the boundary subsets. And right, the, the way we do the decomposition is essentially the way we did it in the case of the 3D elliptic problem with scalar variables. All right. ,the kinds of elements we considered there are admissible here as well. Okay? And to fix ideas, let's look at hexahedron. Okay, so that is an element omegas, omega e and, that part, the partition of the domain is as always given to us as follows. Union over e, of all these open element sub domains, closed gives us the closure of the body, okay. As always, we have omega e1 intersection omega e2 is the empty set, right there at this joint. Okay, we have this and we're going to go ahead and construct our basis functions using this sort of decomposition. Equivalently now that we've talked about it, we could also have had a tetrahedra, ED composition into tetrahedra. Okay. Alright. The way we are going to go ahead and construct our, basis functions is as we know, very well is to construe, to consider nodes on the element. And again, just a fixed idea as I'm drawing things out here, I'm going to consider bilinear, sorry, trilinear hexahedra. So those are our knowns. We only have knowns at the vertices. Ok and we know that Okay, we know that digit. Right. So now let me suppose that this is element omega e, right? This is the element that we started out with which is now being essentially expand out here, okay, that's element omega e, and that is our node A equals 1 for the element, right? I'm following here, the local numbering of nodes. Okay, that's two, three, four, and so on. We are quite expert at this by now. All right and we're going to construct our basis functions from these from these nodes. The way we do this is to, to essentially define these basis functions on the nodes and we are going to define, write the basis functions as before as NA. Okay, our basis functions. This is our basis function at node A, okay? Let me say here, local node A. Now, something you may have noticed is over the past minute or two, as I've introduced, the nodes and the numbering. I have not been calling them I've not yet started calling them degrees of freedom. Okay, whereas earlier, I was using the term, I was using A more often for, in the context of degrees of freedom. But here, I'm being more careful to call them nodes and, and, and here is why. Okay? The reason is the following. We're going to construct a, representation for our, trial solution, all right? And remember our trial solution here is the, displacement field. That's component i over element e. All right? Now, we are going to use a representation where the basis functions will be exactly the same basis functions that we are so familiar with, right? If we are doing trilinears, we know what these are, right? And they're the same basis functions. However, we, and the sum of course is over A going from one to number of nodes in the element. Okay? However each one of these basis functions is multiplied by a degree of freedom vector. Okay? What this means is that for component U hi I have dA, ie. Okay, and we need to recall, here are the i, runs over one, to number of spatial dimensions. Okay. So this is a little different because now if you think about it, how many degrees of freedom does this element have? Okay? So the number of degrees of freedom In omega e, right, or on omega e, right, is. What is it, is it just the number of nodes as it was in the previous problems we were doing. No, correct. So it is actually, in the general case, it's number of nodes in the element times number of spacial dimensions, okay? Whereas, earlier the number of nodes on the element was, same as the number of degrees of freedom in the element. Things are a little different now. Okay, so what we have here is, another way to write this is the following, we use direct notation, right? So now that is the displacement field over element e and this is sum over A NA dA e, as before, except that that d A e, whereas it was a scalar for our problems of scalar variables is now a vector, right, for our problem of vector variables. So and, and what we have here is u h e, and d A e both belong to R3, right, the three dimensional vectors, okay? So sorry for seeming to flog a dead horse, but what we have here are vector degrees of freedom at each node. Okay, and this is a particular approach that we take for this problem, it is not necessarily universal, okay. So we have here a representation for a vector field and what we're seeing is that the basis function is here scalar, just as we've been doing all along. Degrees of freedom here carry the vector information. All right, and this is common for the kinds problems we're doing. Definitely for, for elasticity of, of, of any kind, of mechanics of any kind. It is not the same, for instance however in the case of the problem of electromagnetics, 'kay? It's common in that case to construct finite element formulations where the basis functions are vectors and the degrees of freedom are scalars. Okay? It's, it's just a difference. It's, it's a, a difference into, to do with the requirements of each problem and so on, the mathematical requirements. All right, so, so that's something to know too. And, and then of course it's the same thing for the, for the waiting function. We have wh ie equals sum over A NA CA, ie. Using coordinate notation. Right. Where we know that i Equals 1 to number of spacial dimensions. Right, and using, direct notation that would be wh e equals sum over A NA cA e, okay? As we noted above, Whe and CAe are 3D vectors, vectors in R3. Okay. The, the next thing we need to do if we are happy with this is to just remind ourselves of what the basis functions are. And these are constructed in exactly the same way that we know. Alright, so what we are seeing here is that as before if we have an arbitrary hexahedral element. Okay? That's omega e. We constructed as, as always, really from a bi-unit domain. All right, and everything, everything proceeds just as before. Nothing new here. Okay, so, in this setting we would have, in this domain again, we would have A equals 1, 2, 3, 4 would be in the back, 5, 6, 7, 8, right, and this is a bi-unit domain, so this point is as always, minus 1, minus 1, minus 1, right? And so on that point, our number 7 would be 1, 1, 1, and so on, right? We know this very well now, okay? This is how we construct our basis functions having defined these coordinates, c1, c2, c3. What we see is that as always NA c1, c2, c3 will be constructed from you know, these, these one dimensional Lagrange polynomials, right? And NA tilde, and I forget what I call them, let me suppose I call them A bar c1 N tilde, E bar, c2, N tilde, c bar, c3, I may have used different Our notation for the, for the indices a bar, b bar, c bar before, but, but I think by now we understand this very well, right, each of these as we recall is a 1D Lagrange polynomial Okay? And we know exactly how to construct these, in the case of bilinear, sorry, trilinears, triquadratics, and so on. Okay, and since we are so good at this now, I, I don't need to belabor the points, so I'm, I'm going to tell you right away that as always, we also use the the same sort of basis functions to interpolate the geometry, right? That part, of course, continues exactly the same because even when we were solving the linear elliptic problem in 3D with scalar variables. The geometry still was fully 3D and the geometry was indeed defined by by position vectors and so on. Right? So that bit is exactly the same. Okay, so we have x e, as a function of the C vector. All right? That's exactly the same map that we had before. Okay?


\end{document}
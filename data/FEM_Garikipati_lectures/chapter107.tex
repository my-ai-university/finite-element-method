\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{arydshln}
\graphicspath{ {./images/} }

\title{Transcripts}

\date{}

%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}

\section*{ID: BnlrtvFcJRg}
Kay? And in particular let's suppose that we have let's, let's suppose that we have bi-linear elements, right? Here. It, it, it really doesn't matter whether our elements are bi-linear or not. It will come in only in a few more minutes, but but, but whatever it is, once we know that we have this partition into elements of domains, we can then go ahead and construct our basis functions, right? So, just as we did before we will construct basis functions. Okay. And, and here's where things start getting a little interesting. So, let's construct basis functions. We know that the, the way that we set it up is to, to construct basis functions which are, defined over every element, right, that's how we do it with this idea of compact support of the, of the underlining basis functions that we are going to, that we are going to use. And I will specify that here is, is independent upon position, and time, okay? So we want to construct a representation for this sort of function, using our basis functions, alright? So we will as before expand over the nodes in the element, NA, okay? Now, here is the, here is where we need to really start paying attention. So, any of the func, is, is parametrized by x because after all it is a, spatial basis function. But we already know all about that. We know how we're going to construct our basis functions from our, bi-unit domain right? Which is parametrized by c. All of that is exactly the same. So, this is where the spatial dependence of Is, taken care of. The time dependents of Right, is done in, in this, in the sort of formulation that I am describing here. It is done simply by having the coefficients here. Right? Which we know to be the nodal values of our trial solution field. Right? DAe all right? We just make these guys time dependent. Okay? In this sense, once we've done this what we are doing is a spatial discretization. Right? So this thing, this takes care of our spatial discretization. Okay, we haven't done anything to discretize time, yet. For this reason, the type of formulation I'm describing where the time dependence is held in the coefficients. Right? This type of a formulation, this type of a finite element formulation is often called a semidiscrete finite element formulation. All right? So this thing is often called a Right. Okay. Because there is as yet no discretization of time. All right. Interestingly, for w h, let's try to understand what we have. We know that w h is a function of position. Look back at our weak form, and think about whether or not we actually need any time dependence in our waiting functions. Is there anything in that weak form, you could look at the finite dimensional or the infinite dimensional weak form, it doesn't matter. Let's look at the finite dimensional, because that's the one that we're, we are actually working with. Is there any need for time dependence in our waiting functions there? All right, and indeed there isn't any. Okay. Because, it doesn't, there's no time derivative on it. There also is no integration over time, right? So there is really no nn, you know, w is something we specified. We don't really need to worry about any time dependence there. So w h is only a function of position, right? And this we, we already know, right? We've got this one very nicely covered. All right, we have our basis functions which depend on x through c and the coefficients here are c, A, e. So this is no different in any manner from what we did with the weighting functions for the steady-state problems. Okay, all right with this in mind, observe that if you are still looking at, at if you still have in front of you the finite dimensional weak form right, I'll pull it up again, here we, here you have it. If you look at it, you will, if you look at the second term of the left hand side and the two terms on the right hand side, it should be b fairly clear to you that through the entire process of our finite element formulation, right, setting up the element integral, well, first of all calculating gradients of fields as needed, setting up the element integrals going to matrix vector forms, first over elements then doing assembly over the whole domain, none of that is any different. Right? For the second term on the left-hand side and the two terms on the right-hand side. Okay. So, what this leads to, what this allows us to do is save about three seconds worth of me talking, and we can simply say from here, it should be clear that from here integral over omega w h, i kappa i j u h, j dv, right, at the end of the whole process, right, when everything is said and done. It's going to be essentially a c transpose K d. All right, we know how this all works already. And nothing is different. Except, except for one thing. What is different here? The c is just as before, the K is just as before, the d coefficients are time dependent, okay? All right, we have that, integral over omega, w h f dV plus integral over the Neumann boundary wh jn dS is c transposed f, okay? Just as before. Right? Because here there is no time dependence in in c nothing is different,. Note, however, that, right one could allow a time dependence in the forcing term. Okay, and in fact even in our boundary terms. Okay, so let me make a remark here. Okay, one can have ug equals ug function of time, right? So the Dirichlet function that we use, the function that we use on the Dirichlet boundary, could be a function of time. In fact, so could your influx. Right? There is nothing preventing this. Right? And indeed, the formulation can take full account of it. It could also have f, our forcing term could be, of course, a function of position, but also of time. Okay? And so in general we could have f equals some function of time. All right. Okay, if that's all, good, really the only term we need to worry about, when you go back and look at your week form for this problem is the very first term. Okay. It's only this term that we need to worry about now. Let me go to a different color. Right, this is the only term that we need to really focus on here. Okay? And so we will when we return in the next segment.

\section*{ID: MeRwljkxtIM}
So, we will continue, and what we observed at the end of the last segment is that the only contribution we need to really worry about and, and, and expect it to be any different is the new one, right? And, so we consider the time-dependent term, right? Right? And the time-dependent term is this one. Integral over omega, w h comma, sorry, there is no comma here, that's the whole point. W h rho partial time derivative of u, d v. Okay, this is the one we need to consider, sorry u h here. Okay. It should be completely straightforward now because, we, we already have our, expansions for w h n u h, right? And, in fact, in fact, let's just make one observation. In order to consider this, what we need to do is use derivative of u h with respect to time, the partial time derivative is after all nothing. So, and, and there, furthermore, restricted over a particular element, right? That partial time derivative in an element e is nothing other than sum over a N a, we know all about those, d A e dot, right? Where the dot denotes a time derivative. Okay? All right. Well then. The way this works out then is that, integral over omega w h rho. Multiplying the time derivative. Is. Sum over the elements. Right. Integral over each element of the following. Right? Now, let's, let's write out the, the usual sort of summation that we have. Sum over a N a c a e, right? And we know that this first parenthesis is what gives us the w h term, multiplied by rho, and for the next term, we have now sum over B, N B, d B, e dot d V. Okay? Simple as that. All right, so I'm just going to carry along this sum over e, as well all right, because we know, we've done this sort of thing so often in the past that we know how it all works out, okay, so let's just do that. So, this is a sum over e right, I'm, as before now I'm going to pull out the sums over A com, A and B. We have here c A e, integral over omega e, N A rho N B, d V. We have there the integral, and we get here d B e dot, okay? All right, so, if you stare at what we have here for the integral, that I've put in parentheses, it should be clear that for a given element, when you carry out that integral for a combination of, basis functions on node A and B, you get a scalar, right, in every case. Okay. This, resulting matrix, or, or, or these resulting terms, are going to be of a form that we've not, encountered yet, before, okay? We have encountered related, not really related terms, but if you think about the term that gives rise to our, conductivity tense matrix or our diffusivity matrix for this sort of problem, it involves derivatives on the N A and N B, right? It involves spatial derivatives, right, we don't have any of those in this term. Right? So, this is a different type of term. I am going to denote this integral M A B sub e. Okay? All right. And this term that I have just written out, M A B sub e can be further assembled into a, a matrix vector form, right? And we know how that happens, right. So then from here, if we just use, c e equals c 1 e up to c number of nodes in the element e. Right? And the same thing for d. E. D e we know is just d 1 e, up to d number of nodes in the element sub e, okay? However, we also know that in the particular form that we are dealing with here, we have a d e dot, right? Because it, we, we get time derivatives of each one of these terms. I'll make this dot a little bigger than the dots of the ellipses, okay? So, when we get all of that, what we observe is that, this integral that we started out with, this, mystery new integral, essentially reduces to, integral over omega w h rho. Right, so this integral that we're trying to evaluate is essentially now sum over e, c e transpose, right? By assembling all those you know, by replacing the explicit sum over nodes A and B, right, the explicit, replacing that explicit summation, which we had on the previous, slide here, right, this explicit summation. What we're able to do by defining these element level vectors we know is get rid of that explicit summation, right, and we are masters of this, as well. Okay, what that means is that our M A B scalar terms, slot themselves nicely into a matrix, right? They slot themselves into a matrix that I am going to call the M. Bar. A B matrix. Sorry, it's not A B. I, I just got rid of the A B. The M bar e matrix. Okay? And that is multiplying a d bar dot e, vector. Okay, you also know why I am using bars here, right? Rather than the final forms, because we know that we can have this issue of Dirichlet conditions there. Okay, so just because, after we apply the Dirichlet conditions the final matrix, matrices that we have are going to be reduced in dimensions.  Also, the final d vector, or, or the d dot vector that we have is going to be reduced in dimensions. Through exactly the process we've observed before, for handling Dirichlet conditions, okay? It's, it's, it's in, it's in sort of preparation for that, and in anticipation for that, that I'm calling these, this matrix M bar, and M bar e, and that, that vector d bar e, okay? That's all. Now, this matrix is what is called often the mass matrix, okay? In the context of finite element methods, any such matrix that's obtained by directly multiplying the basis functions, no derivatives, right, no spatial derivatives on the basis functions. Directly multiply them and integrate over the domain, maybe multiplying with rho, right, and you've that in some cases that rho could be 1, so that case is also covered, right? The important thing is that it is when we form a matrix of this type which involves a direct multiplication of the basis, functions over an element, right, and their integration, it's called a mass matrix, right, and in this case, of course, the element mass matrix, okay? All right. It's, it's a new matrix that we've not previously encountered, okay? Now, I should make a remark here. Which is, that if we have a general element that is far, that, that is not, that doesn't have one of its surfaces coinciding with the Dirichlet boundary, then of course we know that the, the, the c e vector for that element is going to be is going to be full. It's going to have as many entries as the number, as the nodes in the element, and likewise the d bar vector, right? For that case, all right, for that case what sort of a matrix is M bar? 'Kay. So. For a general element. Omega e, such that. The boundary of omega e intersection Dirichlet boundary is the empty set, right? So an element that does not have one of its surfaces coinciding with the Dirichlet boundary. For such an element we observe that the dimension of c e equals n n e, okay? Right? Right? Therefore, what that implies, that for such a, for such an element, what can you tell me about the M bar matrix? Okay, recall that the M bar matrix is now going to have this form. M 1 1 e up to M 1 n n e, e. M n n e, n n e, e, and here we will have M, n n e 1 e right, it should be obvious that this is a square matrix with, dimension n n e times n n e. What more can we say about it? If I have a general term here that is, M A B e and, we have a term here which is the. Through the transposed position in the matrix. What can we say about M A B, and M B A? Right? That's right. They're equal. All right, it just follows from the definition of one of those components, right? Each of them is just a mu, just the integral of n A times n B multiplied by rho integrated. All right, so they're symmetric, right? So basically we have M, M bar is a symmetric matrix. Okay. All right. One can also show using the properties of the of our basis functions that the M bar matrix is positive definite, okay? Right, and you recall what that means, right? So if we have, some general vector. Well actually, let well, it, it's true for M bar e, as well, but, but, hm. Yeah, that's okay. So M bar e is positive definite, so what we see is that, what that means as you will recall is that if we took any vector c, right, and we formed this product c transpose M bar e, sorry, if you too any c e transpose c e, right? And remember, c e can be arbitrary, right, could be any vector. This is, greater than or equal to 0 for all c e being in any dimensional vectors, right? In fact, it's equal to 0 only if c, the c e vector itself is equal to 0. Okay? This is what we mean by positive definite matrix, and indeed, M bar e is positive definite. Okay, all right, so this is one remark about properties of the matrix. There is another remark, which I am going to make, which is that M bar, as computed here, is what we call the consistent mass matrix. Okay. And. Okay. One can also do a lumping, okay, and, what a lumping involves is computing the consistent mass matrix and then summing up all the elements along a row, and putting that sum on the diagonal. Okay, so it's, it's, it's an approximation, it's not a, that's why it's not called a consistent matrix, right? So a lumped element mass matrix. Is the following, right, M e, say, tilde. Okay? Which is obtained as follows. On each diagonal, use sum over A. Sorry, sum over B M, in this case use sum over B, M 1 B e, okay? And put it there, and make the other 0. Okay. Likewise, you do that for all the diagonal elements. Okay. And, and every other element is 0. Okay. Right, which basically is telling you that M tilde e A B, right the A B component of the M tilde matrix, is simply sum over B. M A B e. Right. If, sorry. A equals B. If this is to be the case, sorry, I need to make a correction here. I need to have a new index for the sum. Okay, so if A equals B, which makes it a diagonal element, then you simply sum up over, you sum up the elements in that row. Okay? And put that, sum on the diagonal. Okay? And, it is equal to zero, otherwise. All right. It's just, it's just a sort of, quick and dirty way of diagonalizing the matrix, right. It's not a proper diagonalization through defining orthogonal basis functions or anything like that, right? It's just, just a way to simply lump it and definite a diagonalized matrix. And you also see why it's now called a lumped mat, element mass matrix, right? Because you're lumping all of the mass up at the, at the nodes. All right.

\section*{ID: rbpJXpAoizA}
There remained a minor error in board work on the slide that I have before me and that you have in front of you on the screen. It appeared in the way I wrote out mass lumping for that diagonal component. It's an error that you may have picked up on yourself, especially by comparison with this other term, which is correct. The error here is that I wrote that sum as a sum over e. It should properly be a sum over B. So what we want to do is draw an arrow right through that e and replace it with a clear B. With that, everything's consistent.

\section*{ID: cBN8hI2Cb-8}
Okay, so, so this is something to know, and we will also see what happens when we, you know, if we, we. From, from here we, I'm not going to consider the lumped mass, element mass matrix. I'm going to go back to the consistent one, when we finish the, the process, we will then redefine at the global level also, a lumped mass matrix, okay? But this, this, this was a very useful place to introduce it. Okay, so, with those two remarks in hand, let's just return to our formulation, which is the following, right? So we have we have now this, the following representation. Integral over omega w h rho Right? We've got as far as this. Consistent mass matrix. Okay? We're going to go from here to assembly. All right? Now we do just the same things the, the same things that we know from before, right? We say that okay, my c vector is you know, going back to our global numbering. Both nodes at c1, c2, up to c number of nodes in the problem. Okay? And this is. Global degree of freedom numbering. Okay, and the same holds for the d bar matrix, right, d bar again is d bar 1, d bar 2. D bar number of nodes. Okay? All right? Okay, oh, I, I should be a little careful here. This is not just number of nodes, right? I should be careful here. The size of the C matrix is less than number of nodes. It's number of nodes minus number of Dirichlet degrees of freedom, right? Number of degrees of freedom that have Dirichlet conditions on them, right? Whereas d bar is the full one. Okay, so we have this, what that let's us do is that we, we can go from here to saying all right if we have these vectors c trans, c and d bar defined. Then what we have here is c transpose n bar d bar, right, where n bar is the assembly overall elements of D n bar e matrix. Right, of the consistent element mass matrices. Now, this assembly process works just as before, right, just in the way we, just in the way we assembled our conductivity matrix or the diffusivity matrix, right, which is that we go, we look at every single degree of freedom, that global degree of freedom, that shows up on any element, right, and add the contribution from that element into the, into the global mass matrix. Okay, so that's it. So let's just recall with a very quick example how that works. Right, it's really no different from, it's identical to what happens with a K matrix. But it's probably worthwhile to just recall. Okay? All right, so as before element omega e1, omega e2. Let me label the global nodes as maybe A, B, C and D, and go to the local node numbering. Right, the local node numbering may be 1, 2, 3, 4, 5, 6, 7, 8 and on element e2 we have 1, 2, 3, 4, 5, 6, 7, 8. Okay, so with this setting in hand. Let's go ahead and look at how the m bar or, the global n bar matrix would be formed right? For just, for those nodes A, B, maybe AB, maybe CD as well. Let's see, okay, so So now we go to n bar, right, the global matrix, and let's suppose that the nodes we are interested in are A, B C and D. And those show up here as maybe A, B, C and D. Okay, so let's look at the contributions to the m, to the a a, position right, right, out here. So, from omega e1, we would get a cont, contribution which would be M e 1, 11, sorry, 22,  22 plus from m, from e2, we get indeed 11. Okay, let's do another one, let's do cc. Cc would be from e1, n e 1, we get 77, and ne 2, 88. Right? And of course, because node A, global node A and global node C will be shared by other elements besides e1 and e2, this, there could be more contributions to both of these, right? Okay. Let's do the AB term. Yeah, let's do the AB term. So, for the AAB term, what we see is that from e1, right? From e1, we get the 26 component, and from e2 we get the 15 component, okay, and maybe other ones too. Other ones from other, other elements that share nodes A and B. Just to demonstrate the, the symmetry let's do the B A term. So this was the, actually I think I already did the B A term. Let's do the A B term now properly. The AB term would be the a row and the b column. It would be the one here. Okay, so let's see. So, if we now we look at oh, no. I, I just realized that I, I actually did make a mistake here. Everything that I wrote here actually goes to the goes properly to the AB term. Right, so I was talking of the BA term but then I wrote all the AB contributions here. Right, so one needs to be careful about this, always, of course, okay. So the AB term means row A, column B. Okay, so let's do that. Row A, column B would be here M e1 26 plus M e2 15. Okay, and then let's do the row B, column A down which would be the one here. Okay? So row B, column A 10 would be from Me1 BA, right? So it would be 62 plus Me2, 51. Okay. All right. And so now we realize that because we've observed already that the element, the consistent element, of this matrix is symmetric. In fact, the lumped one is also symmetric. Anyway, because a consistent one is symmetric, we realize that the, that the global consistent mass matrix is all the symmetric, provided it is square, right? But, in general it is not yet square. We need to make it square, right? Which we will. Okay? All right. Now, let's, let's, let's go ahead with this with this right? And so, so, essentially this is how things will work. And, and what you're observing, is that the way you slot in the components of the same bar matrices are, is identical to the way we do it for the, for, for the, for the conductivity of the diffusive, diffusivity matrix. Okay? That assembly process is exactly the same. Okay this is a useful place to stop because all we do, really need to do when we return in the next segment is talk about how Dirichlet boundary conditions have an effect upon this term, this new term that we are considering. All right.

\section*{ID: -ChO6q-8Udk}
So, I just noticed that when I wrote out the globally assembled form of this new time dependent term, I missed a critical dot on it, right? So this is what we have before us. And when I wrote out this formula. Right, what I missed was the fact that there is a time derivative on the d. And that dot is what I missed out. So let me make a nice big green dot on it, okay. And that carries over here. Okay, with that correction, we are essentially ready to move on.

\section*{ID: Eza34lQzpK8}
Welcome back. What we did in the last segment, what we accomplish was a global matrix vector representation of the new time dependent term that arises in the unsteady form of the linear parabolic equation in three dimensions with scalar, with a scalar unknown. So let's so to speak, finish the job now. And we'll do it by carrying out the last remaining step in our finite element formulation. Which is accounting for our Dirichlet boundary conditions, okay? So the topic of this segment is, Dirichlet boundary conditions. Okay. So, for completeness, let's just and, and for connect, and for connections let's just write out our finite dimensional weak form, just, just the integral form. And then immediately after that, our matrix vector form for it, okay? So, what we have is the following. Integral over omega w h row, partial time derivative. Plus integral over omega. Wh,i, kappa ij. j sorry. U h comma j d V equals integral over omega Wh f dv, plus integral over the, the influx boundary, wh jn ds. Okay, these are the terms we have. Now, we've already written, so we've already written out the matrix vector form coming from the second term on the left hand side and the two terms on the right hand side. And we know very well that after accounting for the Dirichlet conditions on those terms we have a form that c transposed k d equals c transposed f. Okay, and this is all we would have if we were working with the steady state problem. What we've added onto this is the understanding that this term can be rewritten as c transposed M bar, the consistent mass matrix, d bar dot. All right, now, let's talk the Dirichlet boundary conditions on the Stein dependent term, only understanding that the Dirichlet boundary condition have already been accounted for on the remaining terms. Okay, so. So what we're seeing is that this form follows if the Dirichlet boundary conditions From the. From the integrals to be really precise about this, okay? For if this follows, if the Dirichlet boundary from the, integrals. Without time derivatives, Have already been accounted for. In the, in this matrix vector form.  Okay? In particular, in particular, note that that is what allows us to say that the K matrix is square. All right, otherwise the presence of Dirichlet boundary conditions makes K a, would, would make the corresponding matrix there a rectangular matrix. And that's what allows us to say that we have d here and not d bar. Right, the d here is only the final set of unknown Dirichlet conditions, and in fact, what I am saying here, what we are saying, is that F already has the Dirichlet conditions accounted for in there. Okay? So I'm sort of jumping ahead to that final step. All right. So this thing already has. The Dirichlet boundary conditions, from, the non time derivative term, right? The non time derivative integrals. Okay, it may seem like a piecemeal way of doing it, but I think we understand why we're doing that because we've already been over that part of the steady state problem. Yes in fact, you could view this as taking the steady state problem and adding on the time dependent term, right? How would you do it? Well, this is how. Okay, right, if that's what we were doing, then already, the Dirichlet boundary conditions would be accounted for in the f vector. Okay, so then we only have to worry about the Dirichlet conditions which are reflected in here. Okay, so let's do that. Okay, so, let's note, then, that C transpose. M bar, d bar dot, okay? Can be, or, or, or is indeed written as a C vector, right? C 1 up to some, C N N E minus N D. Right? We have that, multiplying Our m bar matrix and here on the right we have our big D bar vector. D bar dot vector in fact. Okay? So here we have, d bar 1 dot. Right? And let's suppose that here we have a d bar, d bar dot. Okay? Maybe I should make that dot in a different color. Well, never mind. That big dot is for, for the time derivative, right? The little dots are ellipses. Okay. So, it goes on and let's suppose that we have here a d bar, with a big dot, on a, on the B bar degree of freedom. And we, and with our, d bar n n e. Big dot. Okay? Now we know how this works. What we're talking about is that those degrees of freedom, have a Dirichlet boundary conditions. Okay? And we completed the Dirichlet boundary condition on degree of freedom a bar. Global degree of freedom Right on d bar, okay. And we know how that works out, because what, what we're seeing is that is that the, that this column, okay, that I'm going, that we denote as a, M bar, A bar. Right? That's a column. Right? And likewise I guess we'd get N, M bar, B bar column here. Right? And we know how that, what that implies, it just says that every element in that col, every component in that column is multiplied by the d a bar dot com-, component. And likewise, everything in the M bar B bar column is multiplied by the d B bar, sorry, t bar b bar dot, right. Degree of freedom okay, but those are known, right. So these are known, right. Not only are those, are those degrees of freedom known, but since they are time dependent, if they're known with every, at every instant in time, we can indeed compute their time derivatives as well, okay. So those components also are normal, all right. Those time derivative components, all those degrees of freedom, the Dirichlet degrees of freedom are known. So what do we do? Well, we take it, take them to the right hand side because the idea is that, that time dependence of those Dirichlet boundary conditions also drives the problems that we're looking at, okay? It's a time dependent driving of the problem with Dirichlet conditions. Okay? Right. So may, maybe I should just state that as a remark here. Right? So in the form that I've written things up in the previous slide. D bar, a bar, and d bar dot b bar drive the problem actually, let, let me qualify this further, drive the initial and boundary value problem Via time-dependent, Dirichlet boundary conditions. Okay? All right so well, what do we do? We know very well now, right? So, what this lets us do is to rewrite the whole problem now as C transpose M, d dot, plus C transpose K D equals C transpose F. Minus d A bar, dot M, M bar A bar, that column. Minus, d bar dot B bar, just a component, right? A time dependent or a degree of freedom, right, whose time derivative now, is dr, continuing to drive the problem, multiplying this column. Okay? Unfortunately, I started out by calling the F to the root earlier to pF instead of calling it F bar or F tilde or something. So, allow me now to simply redefine this is also F. Okay? So, redefine as F. Okay. So whatever F we caught from the earlier steady state problem has sort of been updated to account for the time-dependent driving with Dirichlet conditions, okay. All right. So what that lets us say then, is that we finally have C transposed M d dot plus k d, minus F with our redefined F, including these Dirichlet conditions, right. All of this equals 0 for all C now belonging to R of dimension n n e minus N d. Okay. And that of course comes from our, a specification in the weak form holding for all waiting functions. All right, and then we impose this, we see that the final matrix vector equations we impose to solve this linear parabolic problem in a scalar unknown in 3 D is this. Okay. So this is our semi discrete matrix vector problem. Semi discrete because we've really truly discretized only in space. All right. And what we really need to do now is account for how we, deal with the time dependent term here. And note that the all the time dependence has now been sort of put into this time dependent vector. Of degrees of freedom, right? That we need to solve for. So, so we'll end the segment here. When we come back, we will focus on the time integration.

\section*{ID: -CA1T2d97JI}
Welcome back. We'll continue with our development of the finite element method for these linear parabolic problems in three dimensions where we have a single scalar unknown. What we've managed to do in the last couple of segments is starting from the weak form we've written out the stro, sorry, starting from the strong form we've written out the weak form and arrived as far as the matrix vector equations, okay? And if you have your notes in front of you, you can go back and observe that the form of these final matrix vector equations was the following. Right, the matrix vector equations. Right, it takes on the following form. We have this new matrix, the mass matrix Md dot plus Kd equals F. Now in this form of the equations, we know that because of the, there is no methods being employed. The methods are on the weak form. The the boundary conditions are already embedded in this set of matrix equations, right? What we do need to count for, however, is the, the initial condition. Okay, so whereas we have this linear system of equations, right, essentially an a, a first order of ODE, right? So this is a first order ODE. ODE being ordinary differential equation, so let me write that out. Right, in our vector unknown, d, okay? Now, because we've dealt with all this business about, you know, number of nodes in the problem and the number of Dirichlet boundary conditions and so on, I'm going to, as we proceed from here on just say that d is a vector living in an ns ndf dimensional space, right? NDF is simply number of degrees of freedom, okay? So, those are the number of degrees of freedom we are served. NDF is the number of degrees of freedom we are actually solving for. Now, this is the first order ODE, and what that implies is that we do need an initial condition which is something we had stated in the strong form of the problem. We just need to incorporate it here, and that ODE that we're working with i, is the following, sorry, that dif, i, initial condition that we need to work with is the following. d at time to equals 0 equals the vector of conditions which are obtained by taking u not at the particular location of the corresponding node, okay? And this is all the way down to u naught at x, now written in ndf, okay? Right, instead of continuing to number by node, we've forgotten all about nodes. Now we're just looking at this linear system of equations where d as an R belongs to to an ndf lucrative space and this is what we have for the initial condition, right? It's convenient sometimes to also offer, to, to refer to this vector as just d naught, okay? Right, so this is the setting we have. One thing I should mention is that whereas we sort of carried through our development of the method by considering a consistent mass matrix, one can also do global lumping, okay? So a globally lumped matrix, lumped mass matrix. All right, let's denote it as M sub l for lumped, okay? It can be defined as this thing can be defined. Okay? And the way to do that is to simply say that MAB lumped is equal to the sum over c of MAC, okay?. If A equals B, right, and it is equal to 0, otherwise. All right? Okay, and in some cases the problem is often solved with the globally lumped mass matrix. Okay, so this is this is what we needed to complete from the previous from, from the work, we set, set out in the previous segment. We move on from here and begin looking at the integration algorithms for this class of problems, okay? And this is going to depend upon time discretization, okay? The reason for it is that the the form of the matrix vector equation we're working with is what we call a semi-discrete formulation. And I explained why it is semi-discrete because we've gone through the discretization in space, but don't get in time. Okay, so now we do time discretization. Now, in a move that may seem a little disappointing to some of you the time discretization for this sort of problem is done with finite difference methods, okay? However, the, the nature, the structure of the matrices M, K, and and the vector F depend upon the finite element method. So even though we're going to use finite difference methods in this series of lectures for integration of time dependent equations, time integration of time dependent equations. Those methods are, do continue to be affected by the, by the underlying finite element formulation for the spacial part of b d e, okay. So this part is going to be finite difference. Finite Difference methods, 'kay? With that, that's what we're going to use. Now, I'll make a remark here, which is that a fully, space-time, Galerkin finite element method does exist. Okay. So, space-time. Finite element methods do exist. And they have remarkably good properties as well. These types of methods are based on, carrying out integration. Over, omega and over the interval 0, T. Okay? So these are based actually on a formulation which is now going to look, well, I, I, I, I probably shouldn't since, begin writing this formulation because one, we'll get sucked into the rest of the development, but anyway, they're based upon integrating over omega and over, the time integral. And, they, they tend to have very good properties. For instance, they have properties of the type where, the accuracy. The accuracy with respect to time. Is of higher order. Then what we will be developing here. Right? Accuracy is of higher order than, with finite difference methods, F D for finite difference. They do come with a, they do come at a cost, though. In particular, one has to develop a finite element mesh over space and time. And this can pose a challenge when one is doing problems that are 3D in space, right. Because then you have a forth dimension in time, which can get to be a bit of a challenge to visualize, definitely. At any rate the methods exist and they're actually very sophisticated, okay. So we get back now to our finite difference methods, right. So the nature of the time discretization that we are, we are seeking carry out is the following. We are going to divide our time interval into sub-intervals. So divide our time integral 0,T into sub-integrals. Okay. And these sub-intervals are, 0, let me do it this way. These sub-integrals are t0 to t1, t1 to t2. So on until we come finally to. TN minus 1, to t N, okay. So we have all of these sub-intervals, and what we also imply here, of course. Is that t0, or t nought is equal to 0, and tN equals capital T. Okay. So, if you look at this what we've implied is that we've divided our total time interval into capital N sub intervals, right. So clearly here we have, N sub-intervals. Okay? The basis of our, of the methods that we will develop is the following, okay. So what we are going to do here is consider. A, consider an interval. T sub little n, to t sub n plus 1. Okay? All right. Where, where, of course, little n belongs to the integer integral 1, 2, N minus 1. Sorry, 0 to N minus 1. Okay. So we consider an integral of this type. The whole, basis of our methods is going to be what is sometimes called time stepping, okay. So, time stepping implies. Right. Time stepping implies that we know the solution at tn, and we want to get to tn plus 1, okay. Time stepping is essentially, is the following. Knowing, the solution at t sub n, find it at tn plus 1. Okay. All right, so we're really, sort of, hopping along in time. Okay, so if you think about it as, if you think about it as follows. That's the time axis, right. We've started here at 0, we want to get as far as t, right. We have t0, t1, right. We have tn, and we will have tn, t little n plus 1, and here, of course, we have t capital N, okay. So the whole idea is that well, I know what, what, I know everything at tn. How do I get to tn plus 1? Okay, in order to proceed, and also actually in order to Later on carry out some analysis, we need to lay down some more notation. Okay? And here is what we will do. So some more notation. Okay? When I write d, at some time t n. What I implied by this notation is the. Solution vector d, if one were able to do exact integration in time. Okay. So this is the time exact solution, or the time continuous solutions. All right. At time at t equals t n. I am going to stick with the term time exactly. Time continuous gets, gets a little bit more, ambiguous. The time exact solution. Okay? And this simply means if we were able to integrate our ODE exactly. Right? This is our matrix of vector ODE. Right? If we were this with, with bound, with initial conditions. If we add a method to exactly integrate this, then the solution that we would get at a given time t n would be this, okay. In contrast, I will use d sub n, okay, to denote the, the algorithmic, solution at t n. Okay? So this is the algorithmic. Solution, okay? By which mean the solution that's obtained by applying some sort of discretized time integration algorithm based upon the discretization that we've just spoken of. Right? The algorithmic solution obtained. By the method to discrete, to integrate the discretized, the, the time discretized ODE, okay. All right, so we have this notation. What we need to do is actually first of all say what we mean by the time discretized ODE. And here's what we mean, okay? Now if I were to simply re write out our ODE in the time-exact form, here's what I, I, I would write it as m just as I have it up there, but let me, explicitly put in the time. Okay. Supposing we were looking at it time t n. Okay? This would be m d dot at time t n plus k d at time t n equals f which potentially could also depend upon time. All right. Now when go to a time discretized version of this ODE, what we will write here is the following. We will write this out in the canonical form m. Now instead of d dot t n, we will adopt the notation that, you know, that well if d is our solution, we can think of d dot as sort of the velocity of the solution, right. Just, just as a term, it's really just the rate of the solution, but, but we use the, the sort of canonical or the generalized velocity for a rate here. And so we would write this as v, okay? Indicating a velocity. Now, this is not a time-exact quantity. So we are going to denote it as v sub n, okay? Using this sort of idea. Okay? Plus k d n equals f at n. Okay. This is what we mean by saying that well here, v at n is an, a discretize approximation, It's a time discretized approximation again. Time discretized, discretized approximation of d dot right. D n is of course just as we've defined above. Okay, so this is what we call what I will refer to as the discretized version of the ODE, okay. So this is the time discretized Are the time discrete, ODE. All right? Okay.

\section*{ID: 1lT0ssVpNzs}
Now, in order to proceed from here, let me actually write this time discrete ODE. In ord, for, for, for, for purposes of argument let me write it at time, tn plus one. Right, so this of course is the time discretize ODE at tn. Right, because everything is at n. Okay. Let me know write this at time tn plus one. Right? So, at T n plus one, that becomes in canonical form, it becomes N V and N plus one plus K, d at n plus one equals F at n plus 1. All right. All I've done is, is write out the ODE at two different. Times okay two different times and as well. All right so this is generally what we would call the the time discrete version of the ODE. And, and when we write it in terms of the philosophy it's often sort of canonical All right now, this is where we need to invoke special integration methods, right? Our integration methods are based upon what we call the same inte- it's a family of integration algorithms. Okay? That are sometimes called the Euler family for first-order ODE's. Okay. And here is how they are posed. Supposing we're considering the following ODE. Suppose we're looking at, y dot equals f of y. Okay? This is the, algorithm we're looking at. So, what we will see here is that the time discretized version of this or the ARD or the, the, the, the solution as post by the Euler family of algorithms is the following. All right, the algorithm then is Is the following. It is that y n plus 1 minus y n over delta t, okay? Is equal to f at y n plus alpha. All right? This is the algorithm as defined by the Euler family. I'll have some explanation to do here of course right? So what we have is the following. Just the algorithm where, delta t is simply the what is called the time stamp. It's d n plus 1 minus t n. Okay? And this is the time stamp Okay? Alpha is a real number. And alpha belongs to the closed interval 0 to 1. All right, okay. So. We have the setting, and, since we're talking about the Euler family for the very first time let me, so to speak, introduce you to the members of the Euler family. So, we have the following sort of setting. Four alpha equals 0. Right, this is called the forward Euler method. Okay alpha equals 1, is called backward Euler. And alpha equals one-half is called various things. I will tend to call it the Midpoint Rule. Okay? It's also sometimes called the Crank-Nicolson method. Okay, so those are special members of the Euler family. Of course as as indicated back here you can one can design algorithms. One can develop algorithms and use them for any value of between zero and one. Okay, so, one one final step I want to take in this segment is to note that the way this can be sometimes rewritten is as simply. All right, this, this think can, is sometimes rewritten as y n plus 1 equals y n. Plus delta t, times f at y n plus alpha. Okay? Now, in order to complete this description I need to tell you what y n plus alpha is. Okay? Y n + alpha is constructed simply as a linear interpolation between values at alpha equals zero and alpha equals one. Okay? And that tells you that, that you construct it as Y N plus 1, right? Let me see, alpha times y n+1 plus 1 minus alpha times y n. Okay? All right. So really so when you sit back and look at these Euler family of At the Euler family of algorithms, what it is doing is to approximate our time derivative, right so we approximate the time derivative as a linear, as a linearly varying quantity over every time interval. Right? Why don't, we'll just sayinh well,let me just take a linear, approximation of it over the sub, over the time stamp of the time derivative. This is our approximation for the time derivative, right? And by saying that we are requiring it to be equal f at n plus alpha, this is where the design of our algorithm also comes in over the various possibilities in the Euler family because if we are trying to go from tn to tn plus one. Right? We're seeing that the rate in the average rate across the entity of tn and tn plus one depends upon f and we could, we are free to evaluate f or we're leaving ourselves the flexibility to evaluate f at any value inside here. Right, so at, at any point inside this. So that point is a gen, is generically n plus alpha, t at n plus alpha. Okay. By choosing different values of alpha, including alpha equals 0, or alpha equals 1, or alpha equals one-half, or, or really anything else, we get particular properties for our method. Right? So, so this is where the approximation of the time derivative comes in. We have this side of it, which is the linear approximation, and here we are approximating the ODE by evaluating the right hand side at some T equals alpha. Okay? So this is the basis of the Euler method and we are going to apply it to our time discretized ODE when we return in the next segment.

\section*{ID: Eyh2ws5uZO0}
Welcome back. So at the end of the last segment, we wrote out our ODE in its discretized form. And we've also introduced the the Euler family. All right, we've met the Euler family, so to speak. All right. What you're going to do now is go ahead and apply it to our ODE. And the approach we are going to take is one of revisiting this discretized form of our ODE, okay? So go back to our time-discretized ODE. All right. We write it out as M v at n plus 1 plus K d at n plus 1 equals F at n plus one, right? Where what we can say now is that we can invoke our Euler family as applied to our definition of v, okay? So what this means is that we're saying here that d at n plus one equals d at n plus delta t v at n plus alpha, okay? Where v at n plus alpha equals v n plus 1, multiplied by alpha, plus 1 minus alpha times v at n, okay? And in writing out these two equations you can see that we have essentially applied this idea of the Euler family in there, right? Okay? All of this, of course, to complete our specification of an, of even our time-discretized form of the ODE. What we are saying is that of course with all this, we are given the initial condition, all right? And that initial condition is d 0. Okay? We know what we mean by d 0. Right, in the previous segment we explicitly said what d 0 is. All right? So now with you know, in general you, you really see this idea of time stepping. The idea is that given things at d n, right? Given quantities at t n, sorry. Given quantities d n and v n at t n this is a scheme for going from n to n plus 1. Okay. And it utilizes, it uses the, the initial condition here. All right. Now, there are two sort of canonical approaches to solving this time-discrete, discretized ODE. And we'll, we'll look at both of them in turn. The first is what is called the v-Method. 'Kay? Somewhat unimaginatively. All right. Here is the v-Method. In order to app, apply the v-Method and actually to apply the other method as well, we go back and look at this. Form of the problem and look at these two equations and do the following. Okay? We first note that. Then we can rewrite now d n plus 1 as d n plus delta t, times v n plus alpha but v n plus alpha is also written up for us here. So we'll write that as alpha v n plus 1, plus 1 minus alpha, v n. Okay, all right, so that is the first step, combining the two sort of the equations that define the Euler family. Now, in the next step what we're going to do is to rewrite this as rewrite the right-hand side as d n plus 1 minus alpha delta t. V n, plus alpha delta t v n plus one. Okay. Now what we've done here, if you observe, is rewrite d n plus 1, in terms of quantities that depend only on quantities at n. Okay? So let's denote this as d tilde at n plus 1. Okay? Now, we are getting into the realm of what are called predictor-corrector methods. 'Kay? And the idea is that if we want to calculate d n plus 1, right? We have this formula for it. We predict this, right? We say that well this is our guess for what it is, okay? And in general that will not be correct and we need to correct it with this term which is the corrector. Okay? So, written differently, d n plus 1, actually not so differently, d n plus 1 is equal to the predictor d n first 1 tilde plus the corrector alpha, delta t v n plus 1. Okay, so this is a predictor-corrector method. Okay. This v-Method that I'm talking about is one where what we do is return to our time-discretized ODE. Right, which is this one, Mvn plus 1 plus Kdn plus 1 equals F at n plus 1, right? We return to this equation, and we try to rewrite it only in terms of v, okay, and the way we can do that is to essentially use for dn plus 1 here, this predictor-corrector form. Okay, so substitute. Predictor-corrector form for dn plus 1, all right? What that does then is leave us with M v at n plus 1 plus Kdn plus 1 tilde plus alpha delta t Kv at n plus 1, okay? All of this equals F at n plus 1, all right? Now, I said that the reason we call this the v method is because we're going to try and rewrite our discretized ODE entirely in terms of v. Right, entirely in terms of vn plus 1. So that is indeed what we do, okay? So we get here this implies M plus alpha delta tK vn plus 1 equals Fn plus 1 minus Kd tilde at n plus 1, okay? We solve this equation for v, for vn plus 1 and we now have the corrector, right? Because then we can add vn plus 1 from here, up there, and we get the corrected state, all right? Right, so, so basically we solve. It's this inverse acting on Fn plus 1, minus Kd tilde n plus 1, okay? Right, now I'm going to make a couple of remarks. Okay, and those remarks are the following, right? Now, the first remark is that if, instead of M, we used the, the lumped-mass, okay, and, furthermore, we use alpha equals 0, okay, this will be forward Euler. Okay? 'Kay, have this particular combination, and what we see is that for vn plus 1, which is lumped-mass inverse Fn plus 1 minus Kd tilde n plus 1, okay? Because the lumped-mass is a diagonal matrix, its inverse is trivial, right? So as a result we don't really solve equations here, right? We don't solve, we don't have to solve with a, a, a linear system of equations by, by somehow effectively generating that inverse, okay? So this is what we call an explicit method. Okay? In contrast we have if alpha is not equal to 0, okay, all right, then we have in that case even if our mass is lumped, we still have equations to solve, okay? So this, this now an implicit method. All right? Okay, so these are two, two remarks to make and what we will do is move on and proceed directly to the other canonical solution method, which is the d-method. Okay? And the d-method once again, we could look at our predictor-corrector form. But now, we sort of turn this around to eliminate v from our discretized ODE, all right? What that implies then is we write here we get vn plus 1 equals dn plus 1 minus dn plus 1, tilde, okay, and divided by alpha delta t, okay? This of course, holds only if alpha is not equal to 0, okay? All right and in this case we get v from here and then we substitute it back into our discretized form of the ODE, okay? And. Okay? So, when we substitute in our discretized form of the ODE, M will be n plus 1, we had dn plus 1 minus dn plus 1, tilde, divided by alpha delta t, right? That is our approximation for vn plus 1. Well that's our expression for vn plus 1 using the Euler family. And here we have Kdn plus 1 equals F at n plus 1. And this again we reorganize by simply multiplying through by alpha delta t, okay? So we get M plus alpha delta tK. D at n plus 1, equals alpha delta t. F at n plus 1 minus Md tilde at n plus 1, okay? Now in this form of course you realize if we can get to the final form it doesn't matter if alpha is equal to 0. Therefore this works ev, even though I, I, I wrote that in, in, in defining vn plus 1, right? Alpha even though I wrote that alpha is set equal that, alpha cannot be equal to 0, right? We still get back our we will still able to actually solve the equation and move on, okay? So this would be the v-form and now what we get here is dn plus 1 equals M plus alpha delta tK, dn plus 1, sorry. Inverse here times alpha delta t Fn plus 1 minus Md tilde, n plus 1, okay? For this sort of method if we use a lumped-mass, right, we use a lumped-mass here. What is seen is that we require less effort to set up the right-hand side, okay? All right, if we use a lumped-mass here, there are fewer operations, not less, less that limit that fewer operations. Fewer operations to form the right-hand side. Okay? But the actual equation solving effort in the two methods, the v-method and the d-method is the same, all right? So here, here we have it our you know, this, the, these are the two methods by which we can set up the the, the solution of this problem you observe that you're, you're still solving exactly the same, sol, the same serof, the same set of equations so the methods are, are, are, are really equivalent. It's just a matter of which, which variable you attempt to eliminate first, okay?  right, so what we can begin to do now is to proceed to the to actual solution and well, sorry, we, we've already looked at the, at the solution techniques. What we can do now is to, is to set ourselves up to carry out an analysis of these methods, okay? And that may perhaps be done best in a separate segment. So when we return, we'll take up the analysis.

\section*{ID: nL6u-s460qM}
In this segment, we'll start looking at the coding template for homework five. So let's turn and look at the code now. Again, homework five is the transient heat conduction problem. And you'll notice some differences here in the main.cc file. One is that I've created this constant alpha, which is an input for the constructor for our FEM problem object. Alpha is to specify what Euler method we're using. Again, alpha can range anywhere from zero to one. For the homework, you'll be doing it for alpha equals zero, one half, and one. But, other than that, the mesh generation is very similar, and setup-system and assemble-system. However, now we also have to solve functions. We have solve-steady, which solves our steady state problem, and then solve-trans which, obviously, is the transient solution. In your header file you'll have a function that calculates the L2 norm of the difference between the steady state solution and the transient solution at a given time. And so I'm also outputting to the screen here, some of those results of the L2 norms, there will be four L2 norms, It will calculate the L2 norm at time equal 0, 1,000, 2,000, and 3,000. Those are some of the results that we'll be using in the grading. Let's shift over to the header file now. Just as in homework four, I've defined order and quadRule as global variables at the top here. Again, when you turn in your assignment, use order equals one and quadRule equals two. But for your own edification, you can  change that order and quadRule if you'd like. The L2, again, will take care of any any order our quadrature rule. We'll scroll down and look at the declaration of functions and objects. The first four solution steps are the same, but then we have this solve-steady and we have apply-initial-conditions, since it is a transient problem or includes a transient problem, we have to include initial conditions. So you'll be defining those within this function. That function is called within the solve-trans function itself. I now have two output results functions, one for the steady state result and one for the transient results. Notice for the output-trans-results, I have as an input an index. So that's just to distinguish between the output results for, say time equal zero and time 1,000, it just puts an integer onto the file name, so that it doesn't overwrite the solution file. And again now we have another L2 norm function again, so if you've missed that from homework two, it's back. You'll notice here we only have the quadrature formula for volume integral; since we don't have any Neumann conditions, we aren't doing any surface integrals on this problem. You'll notice I have more matrices and vectors here than before. We have capitol M, which is our global mass matrix. There is again K and then we have a generic system matrix. That's because when you're doing the transient problem, you have to do a couple of matrix inversions to solve, for example, the VM plus one. And so, system matrix is a generic matrix that you'll be defining later on, in the solve-trans function. We also have D-steady, which is the steady state temperature profile. D-trans, which would be the transient temperature profile at Tn or Tn plus one. V-trans would also again be similar. It's the transient time derivative of the temperature at each node. Either for time Tn or Tn + 1, depending on what part of solve-trans you're in. And then again our vector F and RHS. RHS is a generic right hand side vector, similar to system matrix that you'll be populating within solve-trans to solve for your Vn + 1 vector. Okay? We also have this standard vector of doubles, called L2norm-results. This is going to store the L2norm at various time steps. And then alpha. Alpha's just the input, from our constructor, ok? Specifying again what Euler method we're using. So if we scroll down to the constructor, again you can see we're inputting alpha and storing that within our class and very well alpha And if we scroll down to generate-mesh, this again is exactly the same as previous assignments where again, you just need to define the limits of your domain. Define boundary conditions, would be very similar to previous assignments. Very similar to homework three, which was your previous heat conduction, your study state heat conduction problem, except now you'll notice that I have two maps here. Boundary-values of D and boundary-values of V. I actually skipped over those higher up. They are class variables, class objects. And since we have these two fields that we're dealing with, we're dealing with the temperature distribution and we're dealing with the field of the time derivative of the temperature. And so we have boundary condition on both of those. The boundary conditions on D are 300 at one end and 310 at the other. They are fixed temperatures, so what does that tell you about the boundary conditions on V? Which is the time derivative of the temperature. Tells you, of course, that the derivative of D would be zero at those boundaries, where temperature is fixed, okay? So you'll be doing a similar loop to check where your node is to see if it is at an Dirichlet boundary. If it is at a boundary, you'll put that information into these two maps, using the globalNodeIndex, which is the same as degree of freedom index for this scalar problem. And you'll passing the prescribed temperature value and the temperature time derivative, which again we said would be zero. When you have, at fixed temperature, alright? So that will take care of your boundary conditions. Again set up system is very similar to previous problems, there's nothing for you to change here. Let's look at a symbol system very quickly, before we end this segment. This is going to be sort of a mixture of the previous two homework assignments, your heat conduction problem and your, and using FEvalues in the linear elasticity problem. Again, we have FEvalues here, we're going to update the values, update the gradients, and the Jacobean determinative of the Jacobean times the quad digital weight values. I set up this variable row and that's the specific heat per unit volume. It's a constant that we'll be giving you in your homework assignment, that you'll use to define M local, and then to get your global mass matrix. All right so let's move inside the element loop. And first we're going to be populating M local. Let me write out that integral for you here on the board. MlocalAB is equal to the integral over the element's domain, of row, which again is your specific heat pre-unit volume. Now, potentially that could vary across the domain. For this homework problem we're keeping it as a constant, so I can pull that out of the integral. But then, it's simply the basis function BA times the basis function B. Integrated. Okay, so this is fairly straight forward, and from your experience doing Klocal and Flocal this should be pretty straight forward for you. Pretty simple. Okay, so you'll edit that there to find that here. We scroll down and we have to create Klocal. This will be the same Klocal as you've created in homework three. The only difference is you don't have to worry about the inverse Jacobian since, using deal two's basis function gradients. We already have the gradient with respect to the real domain. So that simplifies our quadrature loops a little bit. All right so we have looping over AB. The quadrature points, which again all three quadrature loops are grouped into a single quadrature loop with. Then we do i and j to perform the multiplication with 0ur conductivity tensor, kappa. Alright? We don't have a foreseen function in this problem, so Flocal will be zero, and so we don't even bother in this assembly to assemble F, our global vector. But you will be assembling K and M very similar to previous assignments. Right. The solve-steady function acts very similar as the solve functions in previous assignments. The small difference that I've done, is I've Instead of applying boundary values, instead of applying our boundary conditions within the assembled system function, I've shifted it down to solve-steady. Okay. And within this function, we'll solve for the steady state solution, and we store that result within the vector D-steady. Okay, so that's nothing new. Up to this point we haven't solved for anything more than what you already solved for in Homework 3. Which is just the steady state heat conduction problem. In the next segment we'll move on to looking at solving the transient solution as the time progresses

\section*{ID: iM-Ht1x7e2M}
Now in this segment, we'll move on to looking at solving the transient heat conduction problem in the hallmark five template. If we look over here, now that we're solving the transient problem, we have to plan initial conditions. And I've actually set up most of this for you. What we need to do is we loop through all of our nodes, and we check the position of each node. And, depending on the position of the node we'll define the initial temperature at that node. Pretty straightforward. You'll just need to insert those values, in some cases it actually depends on the x value of the note itself, as a function okay? All that information is given in the homework, you just need to calculate the value. Now that takes care of d naught. Storing the values of the initial values of the temperatures in D-trans. But now we will also need the initial values for v. We need v naught. And so to do that, we will solve a matrix vector system, okay. Now remember, this problem is based on this equation. We have M times V plus K times D is equal to F. F in this problem is of course zero, but in general, it wouldn't and it could be changing with respect to time. Actually, all of these objects could be changing with respect to time, potentially. So, we can also put the initial values in this equation, and if we need to solve for V naught, well first we move KD over to the right hand side This becomes our right hand side vector. And then simply do a matrix inversion to solve for v not. Now in this problem the mass matrix and the stiffness matrix don't change with respect to time, which is why I can get away with calling the symbol system only once. Okay, so let's look in the code at how I'm doing this matrix inversion, and how I'm doing these matrix vector operations. Okay, I've done all of these for you, because you'll be doing several of these on your own within the solve trans function. Okay, so first, I set RHS equal to K times D trans. And to do that I used this dot vmult function. Now with sparse matrices it will do a matrix vector multiplication. I pass in RHS and D trans. And it's acting on my sparse matrix K. But what it does is it stores within RHS the values of K times D-trans, okay? In the next step, I just take RHS times equals minus 1. Essentially I'm just changing the sign on all the components in my RHS vector. Okay, so this point RHS is equal to minus K times D trans or minus K times D naught. And then finally, even though F is equal to 0 and in this case I've included it for generality, I do RHS add 1 times F When is the scalar coefficient to F. And so that takes RHS and adds on to it one times F, which is, now, gives us a right-hand side vector of F minus K times D trans, or F minus K times D naught. Okay, so now my right hand side vector is set up. In this case, I'm only doing a matrix inversion on M, so I don't have to do any matrix operations there. But I did, up here, copy M into the system matrix, okay? So that's the same as saying system matrix equals M. Now that all these data objects are set up, I can apply my boundary values. Now note that, since I'm solving for V trans in this case, I will apply boundary values of V. The matrix that I'm inverting is system matrix, the solution is v trans through an outside vector is RHS. I create this sparse direct umf pack A, which copies over from system matrix. And then I do my matrix inversion. Now it may be a little confusing. I do A dot vmult here, but you notice it's doing a matrix inversion and then multiplying it by the right hand side vector. It's just the way the functions were named and so you just need to remember that with the sparse direct GMF pack, sparse matrix dot vmult does the matrix inversion. However, when you're dealing with a sparse matrix and a vector dot v mult, just as a simple matrix vector multiplication without any matrix inversion. Okay? All right, so with that background, let's scroll down to the solve trans function. Okay so the first thing we do in solve trans, is apply the initial conditions. So that's the function that we are just looking at. And the next is to define delta-t. I have delta-t set to equal one and that will give you a stable algorithm for all alphas, alphas = 0,1/2, and 1. You're free to change that on your own however, when you turn in your assignment, leave it at delta t = 1, okay. I declared this vector D-tilde, which you remember we use as we're updating. From DN to DN+1. And then we go into our time loop. Now I'm going to point out here that we are using the V method here. Okay. And so we'll look over those equations as we go through this code. All right, so the first step is to find detailed. Let me write that out for us here. So detailed, n plus 1, again this is a global vector, is equal to d at time n plus delta t times 1 minus alpha v n, right. These are equations taken from the lectures so you can look it up there. So this is step one. Step two is to use d n plus one tilde to find v n plus one. And we have this system maybe, equations here. We have M plus alpha delta tK, and I'm not going to re-derive these during the lectures, but here's the result. Sorry, we haven't quite inverted that yet. going to delete that. Okay, so M + alpha delta t K times vn + 1 is equal to Fn + 1, which in our case is just 0. Minus k times d tilde n plus 1. Which of course you would then to solve for vn plus 1 you would do a matrix inversion. So we take m plus alpha delta tk, inverse times fn plus 1 minus k. Detailed m + 1. This will be referring to as our system matrix in the code. And this we'll set up as the right hand side, the RHS factor. Okay, once we have the n plus one, step three is to find d n plus one, which is d tilde n plus one, plus alpha delta t, v n plus one. Okay? Note in the code dn at this point is d trans, vn is also v trans. Here we update v trans to be Vn+1, and at this step we're updating d trans Sorry that's a capital V in our code. Updating D-trans to be dn plus 1. Okay. So we'll be following these three steps in the code itself. Okay, so let's go back to the code. You'll notice I've given a list here. Of how to perform these matrix vector operations. A lot of these we've already seen above in the apply initial conditions function. You can use this same .add function with the system matrix, like we did with the vector. Again dot v mult will do a matrix factor multiplication. If you want to set m equal to or system matrix equal to a vector or to a matrix excuse me, you would use copy from. However with a vector you can just say RHS equals f for example. We already saw this. I'm operator times equals with a vector to change the sign. Okay, so that should be enough for you to be able to do all of these matrix vector operations. Okay, so the first step again was to find D tilde. And again, at this point D trans it does hold the values of D so then nv trans holds the value of V sub N. Okay so once you found D trans you will then create system matrix, as we wrote on the board and RHS. Again as we wrote on the board. Once you've done that, we apply boundary values of V, again because V trans or VM + 1 is the solution vector we're solving for. So we apply boundary values of V using the system matrix that you've defined and the RHS factor. Okay, and that matrix inversion happens here with these three lines. Again using the sparse strict plume of pact matrix, A. Okay, once you've done that matrix inversion and V trans is now equal to V n+1, you can use D tilde and V m+1, V trans, to solve for D m+1. Okay and there you have the solution for this time step. The last thing to do is to output results. Now I have it set to output results every 1,000 time steps. You'll notice I'm using this modular operator. So every time the current time step index, divided by a 1,000 gives a remainder of 0. Then we'll output the transient results as a VTK file. And also store at the L2 norm of the results in this L2 norm results. Or, sorry, I'll calculate the L2 norm of the difference between the transient solution at this current time step and the steady state solution. I'll store that within this vector here, okay? Alright. So the last step, after outputting results, or the last function I should say, after the output results functions, is to calculate the L2 norm. This is going to be very similar to the L2 norm that you calculated in Homework Two, except of course you can now use dl2 basis functions. You will still need to interpolate to find U steady and U transient in exactly the same way you interpolated U H, you will find that element solution in the torched point. And you will construct the intergrand of you know, the square of u-trans minus U study. Okay, and then we we return the square route of that value to actually calculate the l2 norm itself. Alright, so that should be pretty straight forward with what you've seen before, and that concludes our discussion of the template for homework five.

\section*{ID: NLQd-oA5qqY}
Welcome back. We are going to take the first steps towards analyzing our ODE as written out using the Euler family. Okay, and in order to carry out this analysis there are couple of things that we have to do. The first is the following, let us go back to our discretized ODE and write it out in a slightly different form. Okay, the forms are completely equivalent to what we've seen before, the v and the d form. But, but, for, for the purpose of analysis, let's do the following. Okay. So, the, the title of this segment is analysis. Okay? And in particular, we are going to look at what I will call model decomposition. Right. In order to get there, let's do the formula. Let's write out the equation to these formulas, right? We're going to write it out as M. Okay? We are going to directly write a time discretized approximation, a finite difference approximation if you will, of our time exact time derivative, right? So, we're going to write d dot here as approximated as d n plus 1 minus d at n divided by delta t, okay? So that's our approximation of M d dot. Now the, where the Euler family comes in is when we say that everything else is evaluated at n plus alpha. All right? This is how we want to view it. Now, one more thing. We are interested in understanding what the basic properties of our integration algorithms are, what the basic properties are, and as often happens in the study of ODEs, we will take advantage of what is sometimes called a homogeneous form of the ODE, which is obtained by, by, by setting the forcing equal to 0. Okay so, so what we need to analyze here is we want to analyze the stability, stability and what we will define as the consistency of the time integration algorithms. Right? We will define what we mean by their stability and consistency. But in order to do this and especially with things like the notion of stability it makes sense to actually set the forcing equal to 0. Okay? Think about why this may be. Why may it make sense to set the forcing equal to 0 if we want to start out by looking at stability? Okay? Now, it's, it's simply because well, you know, how a solution evolves in time can depend upon the forcing, right? You, you give it a forcing which makes it keep growing in time, well it will keep growing. Right? So we want to get that out of the out of the picture. Right? And instead we want to understand you know, the rigorous way to pose it is to just ask the question of, ask the question that if there were no forcing, how does my time discretization and my, my introduction of this, of this Euler, Euler family of algorithms affect the evolution of the problem, okay? So we, what we will look at is the, we will consider the homogeneous ODE. Okay? Now, the time exact version of the homogeneous ODE that we are working with is this. M d dot plus K d equals 0, okay? With the initial condition d at 0 equals d naught, right? And the time discretized version of it is what we've written up here M d n plus 1 minus d n divided by delta t plus K d n plus alpha, okay, equals 0, with the 0 being given. Okay. So the first of those equations that I just wrote is the time exact homogeneous ODE and the next one is the time discretized version using the Euler family. Okay one thing you will recognize here is that in all of this what we're seeing is that dn plus alpha equals alpha and dn plus 1 minus alpha. Sorry, alpha dn plus 1 plus 1 minus alpha dn. Okay? That's what we have. All right. Now, in order to proceed, we are going to do the following. We will, we're first going to study the question of, we're, we're first going to look at stability. But even before that what we want to do is carry out what we will call a modal decomposition of the problem. All right. In order to carry out a modal decomposition, we are going to take a step which is to invoke a related eigenvalue problem. Okay. We will invoke the related, generalized eigenvalue problem. Okay, and the generalized eigenvalue problem we want to invoke is the following. I'm going to write it as M for a vector here. Let me not use d but let me use phi, okay? The, right, so the problem we want to look at is M phi equals lambda K phi. Okay. Sorry, let me turn that around. Let me write it as, sorry, let me write it as this. Let me write it as lambda M phi equals K phi. Okay, this is the generalized eigenvalue problem. And, and perhaps you know, in a very obvious step, let me just change the left hand side and the right hand side. So we have K phi equals lambda M phi. Okay? All right. Where, so when you look at it in this form, it if you haven't seen a generalized eigenvalue problem in the context of linear algebra before. You will, you have seen a standard eigenvalue problem probably where a standard eigenvalue problem is obtained by just setting N equal to the to what you would call the identity matrix in the corresponding dimension. Okay, so let me just state this. So a re, remark is the standard eigenvalue problem. Is, a standard eigenvalue problem would be of the form K phi equals lambda phi. Okay? And all of this of course, phi belongs to the Euclidean space or real space with number of, of dimension ndf. Okay so, what I've written down here will be a standard eigenvalue problem which underlies eigenvalue problem is where instead of having the the corresponding identity matrix here, right? We have sorry, that's the isotropic denser. Instead of having the, the identity matrix here, right, we would have the we have some other matrix here. Right, and in our case it's a matrix n which we know to be symmetric and positive definitely. All right? Okay, so this is the generalized eigenvalue problem we want to consider, right? So it is this one that we will be working with. Okay? Now what one can do from here is the following, okay? What one can see is that for that problem let say, phi sub m, right? Where m equals 1 to n d f. Okay? Let these be the eigenvectors. Okay, and let lambda m be the corresponding eigenvalue. Okay. All right so, so lambda m is the corresponding eigenvalue. Now what one can do is one can show that it's possible to con, to construct a so-called orthonormalization of the of the eigenvectors, okay? So, the eigen-vectors and eigenvalues satisfy the following equation. The eigenvectors and eigenvalues of course satisfy K phi m equals lambda m times M phi m. Right? Little m going from 1 to number of degrees of freedom. All right? Okay. So given this, it is possible to construct a, a, it's, it's possible to choose the phi n such that they satisfy a certain orthonormalize a, a, a certain property of orthonormality, okay? In particular the property for orthonormality that they satisfy is the following. The phi m, right, the set of eigenvectors, m equals 1 to ndf, right? This set of eigenvectors can be orthonormalized. Okay? They can be orthonormalized to a different set, psi sub m, okay? M again equal to 1 to ndf. Okay, they can be orthonormalized to a set psi m which are such that they are, this orthonormal, this orthonormality property is an orthonormality with respect to m. Okay, what that means is the following. If we take psi m and dot it with M mass matrix, psi k. All right. This product is delta m k. Okay? Where delta is our Kronecker delta, all right? Okay? So this is the Kronecker delta. Okay, and furthermore the set psi m. Right? That set of eigenvectors. I mean, I see they're orthonormalized. What I can further see is that they are M orthonormalized. Right, it's simply, it can be interpreted as being orthonormal with respect to M. Okay?

\section*{ID: DhhryQHWoLI}
The type of autonormalization we need here it can be constructed by what is called the GramSchmidt process, okay? It can be constructed by the GramSchmidt method I guess. Okay? And what this does is it lets us start out with a linearly independent set of eigenvectors which would, which maybe the ones that you originally solved for by solving that generalized eigenvalue problem. Okay? So this is a linearly independent set. Okay? Why do we know that the eigenvectors that we get by solving that generalized eigenvalue problem are going to be linearly independent? That's right. It's because k and m are both positive definite. Okay, that's what guarantees the existence of linearly independent eigenvectors. All right. In some situations, we may have repeated eigenvalues, in which case the eigenvectors are not uniquely defined, but nonetheless they can be picked to be linearly independent. Right? Okay, so so we, we are assured of linearly independent eigenvectors phi and the GramSchmidt process, GS is short for GramSchmidt essentially takes us to a si sub m. Right, which are now M orthonormal as we have defined. This is a completely standard process procedure that is available in many books on linear algebra or, or sometimes even books on partial differential equations because the method does exist to, does extend even to continues eigen functions. All right, so, so we have that. Well, what does that then say for our, for, for, for our problem, okay? Note that, what this tells us is the following. Right? So now, because we have the set psi M we can construct the following sort of the product right? Supposing we take M times, M acting on psi K and we dot it from the left with psi M. Okay. All right. Now. Okay. And then we also have a lambda k here. Okay. The reason we have a lambda k, is because I started out by writing M psi K. Okay, so what we get on the right hand side is psI M dot K psI K. All right. Okay, when we get, when we look at this sort of product now what we observe is that because we have this M orthonormal property for our autonormalized set of eigenvectors. We have here lambda K, delta mk. Right, on the left hand side. Okay? And here on the right hand side we have now a result for forming the sort of quadratic product between psi M, K and Psi K. Okay, so alternately what this implies for us is that if we form just as we form for psi M a product, if we do the same thing for, if we do the same thing with by, by using K in it, right. Right, this is equal to lambda m, okay? We get lambda M because of the fact that we have this Kronecker delta product acting here. All right. So we have this additional property. Okay? All right. Okay. So, with this background in hand to let us now move on to trying to understand how we analyze this how we apply it to analyze our problem. Okay? In order to proceed there, let us first use the, another property that is given to us by orthonormalization. Now, because we started out with a set of linearly independent vectors phi m, okay, and we proceeded to orthonormalize them. We also know that this set is linearly independent, okay, therefore it spans the space that we are working in Okay so, let's state that, all right now the set psi m, m equals 1 to Ndf forms a basis in R Ndf. Okay? What that means then that any vector, okay, say d, right? Any vector d can be written as sum over N running from one to Ndf of d sub m. Psi sub m, okay? We have this expansion of d in the basis. Okay, all right. Okay, and now if we ask, well how do we compute this, how do we compute such a, a representation? We can compute such a representation if we have the dm's. Okay? And in order to know what those dm's are, we observe that we can simply multiply this, equation from the left by our matrix m. Okay, so. To get the coefficients d sub m, okay, what we do is that we first full construct Md equals sum m equals 1 to Ndf. Dm, M, psi m. Okay? Just using linearity. All right, okay, now, once we have this we can now dot this vector equation on the left by psi k, right, our eigenvalue, okay, and we can do the same out here. Okay. Now, using the linearity of the dot product, what we get is the following. Okay? What we get on the right hand side is sum m equals 1, to Ndf. dm psi k dot M psi m. All right, but then what is this? This is just delta km. Where did that come from? From the fact that we've constructed the psi m's, that entire set of psi's to be m orthonormal, okay, but then this explicitly when we compute the sum knowing the delta km is the chronicle delta, we get d sub k. All right, so here we have a definition for what our coefficients are. These coefficients, dk, are uniquely defined, okay? And therefore this expansion, right, of d in terms of it's, of that basis is also unique. Okay. So, let me just write a few comments about this, about this decomposition, okay. So, when we say that d can be written as sum over m, going from 1 to Ndf. D sub m psi sub m. This is what we mean by the modal decomposition of d. Okay? Each psi m is what we will call a mode, right, so psi m is the mth mode. Okay, and then d sub m is the corresponding modal coefficient of d. Okay? And, because we have a, representation for d for, for the dm's or the dk's in general as we derived on the previous slide. This representation, okay, what is this representation? It is that the dm, let me get rid of this because it might look like a minus sign. The dm the dm's are defined as psi m dotted with M d. Okay? So we have a unique representation for the dm's. Okay right. We are going to end this segment here, thanks.

\section*{ID: C6o1k5ZLDF4}
Welcome back. We have just embarked upon the analysis of our time integration algorithms for parabolic equations, right, as of, of, first starter in time. What we've accomplished along that direction in the last segment is a recognition that there exists this generalized eigenvalue problem that one can identify, which provides for us a basis on which to carry out a modal analysis of the problem. Okay, so let's just recall that aspect and charge ahead. Okay so we're engaged here in the analysis of our, of the time integration algorithms. All right for linear parabolic systems. Right and you will recall that this is based upon the generalized eigenvalue problem. Right, and that generalized eigenvalue problem takes on the form K, which is our stiffness matrix. Sorry, our conductivity or diffusivity matrix. Acting on a vector, that is psi M right? K psi M equals lambda M, M which is our mass matrix psi M, okay? This is the basis for it and we went ahead with this, right? What we found and I'm not going to go through the entire analysis I'm just going to recall the critical parts of it, is that one can orthonormalize the, the eigen vectors psi with respect to M. Correct, so we found that psi L dot M, psi M equals delta LM, chronicle delta, and, because of that, this result also lets us say that psi L dot K psi M equals lambda L okay? All right actually properly equals lambda M multiplied with delta LM. Okay, that's what it actually is. Okay, so it is zero, unless M equals L. Okay, the second equation, the right hand side of the second equation. Okay, and if L equals M, it's equal to psi L, which is the same as psi M. All right, and you recall that in this lambda M is our eigenvalue, right, corresponding to the eigen vector psi M. Okay, so this is orthonormalization. Okay. All right, so with this in hand, what we observed I think at the very end of the segment is that since these vectors psi form and actually span the space, they serve as a basis. Okay. So, what we have is an expansion In the psi m basis. Right, where m equals 1 up to ndf, okay? All right, and this letter say that a vector d, right? Such as the vector of global degrees of freedom that we're now working with can be expanded as sum over M d sub m psi sub m, okay? Right, and we saw that each of the d n's, the modal degrees of the, the, the modal coefficients each, right? Each d n is given by right it's equal to psi M dotted with M d. Right? This is everything that we need to know. Right and these are the modal coefficients. All right. What we are going to do now is with a summary of our analysis, of sorry, with the summary of our modal decomposition of the problem, right. Right, so this, this, this result is the modal decomposition. The modal decomposition of d. All right, so, with this, with these results, with this summary of results, we can go ahead, all right. And, what we are going to do now is, extend this idea of the modal decomposition to, the equations we are working with. All right. So, we're going to start here with a modal decomposition. Of. The time exact O D E. Right. Right, and remember everything that we were working with homogeneous equations, okay. So which equation is that? Think about it. It's this one. M d dot plus K d equals not the forcing term on the right, or well, the forcing term is equal to 0. All right we have this with this initial condition that d at times 0 equals the d 0 vector. Okay, so now we're going to do a modal decomposition of this equation itself. The way that works is the following. We already know the modal decomposition of d. Okay. That modal decomposition I'm going to write in slightly different notation, the notations not terribly different. I just want to move the m from being a subscript of the coefficient to a superscript. Okay, so that's not a power, that's a superscript. All right, and you'll see just in a bit what I want to do with the subscript. I need them in there, okay. This times psi. Let me move this also to the after the superscript. Right, the m there is also a superscript. Now remember that this d vector of ours is time dependent. Right, that's why we're able to take a time derivative. Well, we could always take a time derivative, but that's what makes sure that the time derivative isn't doesn't vanish, right. Okay, so d is a function of time. Okay, my question is where on the right hand side does that time dependence go? Does it go into the coefficient? Right, the d n's or the sines, the eigen vectors? Right, they go the time dependence shows up in the d n's. Okay, and more than just a convenience that it has to be that way. Why is it that the sines don't change with time? It's because the sines are, are the eigenvectors of the generalized eigenvalue problem. But the matrices that define the generalized eigenvalue problem, the K and the M matrices are fixed in time, okay, because we are doing a linear problem. Okay, so let's just state that, that's useful to remember, okay. So, this what, what, what's behind this particular decomposition, right, where the time go, where the time dependence is held in the, in the coefficients, right, is the fact that the set of psi m. Right? Are fixed in time. Because, our conductivity matrix K and mass matrix M are also fixed in time. Okay, that's what allows us to do this. Okay, what you see, what that also allows us to do, what that immediately lets us see is that. D dot, right? D dot is now sum over n, d dot n, function of time, psi. All right? Some of cos running from one to number of degrees of freedom. Okay? And we are going to make these, and you are going to substitute both these decompositions into our time exact O D E, right? So what this implies then is let me just say this as substituting. Okay? It implies now that M multiplying the sum over little m d dot m, psi m plus K sum over m d m, psi m equals 0. Okay what I'm going to do next is is apply linearity, right? So what this then apply, what this implies is that because M and L are matrices and because we're dealing with linear algebra there, what we can write here is that sum over M, d dot m mass matrix acting on psi m plus sum over m, d m, conductivity matrix acting on psi m equals 0. All right. Now I am going to dot this entire equation on the left, well it doesn't matter with left or right, I'm going to dot it because it's a vector equation finally. I'm going to dot it with a specific eigenvector lets say psi L, okay, so now I'm going to take psi L, okay? Dotted with the sum d m dot, psi m. Plus psi L dotted with the sum over m, d m, K psi m equals 0. Okay? Note that something has changed between those two equations, right? Something very obvious has changed. 0 here is no longer a 0 vector, it's a scalar 0. Right? Because I'm after all taking a dot product off a vector equation here, this was a vector equation, and I've taken the dot product of it with another vector of the same dimension right, psi L. Okay, but now we know how this all works out, right? Because we know exactly what happened with psi L oh I'm sorry I am missing here, sorr,y I'm missing an m here, so let me just clean this up. I'm missing here. That matrix okay, all right. Now its all right, okay. Well let's carry out these dot products right and and we know what we get from these.

\section*{ID: kMwd1mVNq1I}
Kay, so we know how this works out. We get in the, in the, for the first term, sum over m, dm, dot psi L dot M psi m, plus sum over m dm psi L dot K psi m, equals 0. All right, now this we know from the orthonormality condition is delta lm, okay? And this we know also from the fact that we have the generalized eigenvalue problem at hand. This turns out to be lambda m delta lm, okay? And then when we account for the fact that there is a sum over m in each case, the Kronecker delta does its job and leaves us with the following, okay? What it leaves us with is on the, for the first term, we get dl dot plus, from the second term we will get dl, well let me put the lambda in front, we get lambda l dl equals 0, okay? All right, and this came around, came about just by applying the properties of the Kronecker delta and using that sum, right, over m, okay? Now, we did this for an arbitrary mode, or an ar, an arbitrary eigenvector psi l, 'kay? So this holds for all, well, this holds for all l actually, where all l equals 1 to ndf, okay? All right, this is what we will call our single degree of freedom modal equation. Okay, produces to a really simple form. All right, okay so we have this and what I'm going to do is very systematically do essentially the same thing for our time discrete homogeneous ODE, okay? Okay let's first write out that ODE, right? We agreed that as far as analysis was concerned, we were going to work with it in the following form. Okay, this is the form that we're going to work. We're going to rewrite this by just expanding out the n plus alpha, and also multiplying through by delta t, 'kay? So in one step, I'm going to say that I have M dn plus 1 minus dn plus delta tK times alpha d at n plus 1, plus 1 minus alpha, d at n equals 0. Okay, just multiplying through by delta t and expanding our dn plus alpha, okay, so that in itself. Right? Okay, and let's go ahead from here and essentially introduce modal decompositions of dn plus 1 and dn, okay? All right, so the modal decompositions we are going to apply here are. D at n plus 1 equals sum over m, the modal coefficient, m, evaluated at n plus 1. Okay, this is the modal coefficient the nth modal coefficient of the d vector at n plus 1, right, the time tn plus 1, right? Psi m, okay, I don't need to really to write the same thing for n, because it's just replacing n plus 1 with n. Okay, right, so likewise with dn, 'kay? We do carry our those modal decompositions and then we are to substitute them back. All right, so now when we make we we go ahead and, and plug them into the last equation from the previous slide, we have M, okay let me do two things in one step here again if you don't mind. I am going to write, I'm going to combine the matrices multiplying dn plus 1 and the matrices multiplying dn, okay, in a single step, okay? So for dn plus 1, I have M plus alpha delta tK, right, multiplying dn plus 1, but dn plus 1, I'm going to write using this modal decomposition sum over m, d, right, that coefficient, the dm coefficient at n plus 1, right psi m, 'kay? And the thing's multiplying dn, I'm going to write as follows. Okay, I'm going to write this as minus M minus 1 minus alpha, delta tK, and hopefully I caught everything I need to, yes I did dn, but dn again I'm going to write using the modal decomposition. Right, all of this equals 0. Okay, so I've taken two steps here relative to the last equation that I wrote on the previous slide. I've combined the matrices that multiplied with dn plus 1 and dn, okay? The dn plus 1 and dn vectors as well as using the modal decomposition for dn plus 1 and dn, right? So this is And this is that modal decomposition, okay, that's what we have, right. And, you know what I'm going to do next, right? What I need to do is go from here, this is a vector equation of course, go from here to a scalar equation for a single degree of freedom, and we're going to do that by simply dotting this on the left, by dotting this with psi l, right, so I'm going to do psi l, dot it with all of this, 'kay? Likewise here, I have psi l, dotting everything, okay? And then using linearity and so on, what we get here is the following. We get sum over m, dm, n plus 1, multiply psi l dot m psi m, okay, all right, plus alpha delta t psi l dot K psi m, 'kay? All right, all of this minus sum over m, dnm, we open parenthesis here, psi l dot M psy n, sorry, M psy, M psy m minus 1 minus alpha delta t psi l dot K psi m, okay? All of this equals now, what is it, vector or scalar 0, scalar 0, okay? And then, everything works out just as expected. This thing gives us a delta lm. From here, we get a lambda m delta lm, okay? Here again, we get delta lm. And from here, we get lambda m delta lm, okay, all for orthonormality and from the eigenvalue problem, right? And then we account for the fact that we do indeed have a sum over m and what that lets us say then is that because of the action of the Kronecker delta we are left here with essentially the action the, the what the Kronecker delta does because of the sum over m, is that it effectively converts that index m into l, 'kay? So we have delta l n plus 1, okay, from the first term here, 'kay, we get 1 plus alpha delta t. Here again, because it's Kronecker delta acting he, acting on, on, multiplying the lambda m, we are left with alpha delta t lambda l, okay? Minus d at n for the l'th mode times 1 minus 1 minus alpha delta t lambda l. Again that lambda l comes from here in the action of the Kronecker delta, right, all of this equals 0, okay? This is our equivalent for the time discretized ODE of the single degree of freedom modal equation for the time exact ODE, okay? So this is the single degree of freedom modal equation, right, for the time discrete problem. All right? 'Kay, it is to be compared with the single degree of freedom modal equation that we obtained for the time exact problem, okay? All right, this is actually a great place to stop. When we return, we are going to analyze the two single degree of freedom modal equations. Oh, just one, one more thing. I should mention that this holds for all l equals 1 to ndf, 'kay? All right.

\section*{ID: PGw-wgZgTn0}
All right we are ready to move on, and were ready to move on with the analysis of our single degree of freedom model equations. For both time exact as well as the time discrete problem. Ok? So, we are going to look at at these two model equations, right, single degree of freedom modal equations, okay. So, we have here our single degree of freedom. Model equations. Right, I've used an abbreviation for equations here. All right, we have for the time exact case. We have d dot l plus lambda l d l equals 0. Okay? Now an o d e is not complete without its without what? Initial conditions, right? So the initial condition that we have here is the following. Right? This is after all and o d e for our L coefficient to the model expansion. Right? So the initial condition that we have is simply the following. It is that dL at 0, okay, is obtained as. Psi L dotted with Md at 0. Okay? Which is of course psi L, dotted with Md not, okay? And just for convention let's call this, let's call this quantity, right? Let's call that quantity d0L. Okay? That's the initial condition. Okay? All right, time discrete. And time discrete case is the following. It is d L n plus 1, times 1 plus alpha delta t, lambda L, minus d l n, 1 minus 1 minus alpha delta t lambda L equals 0. Okay? And again, this holds how do we bring in the initial condition in this case? We just say that we're given that that quantity we had identified for the time exact problem. All right? Okay. Now, one thing I want you to note is that the way we write at thi, at this time discreet single degree of freedom model equation is by starting from the, from the right. From the time discrete matrix vector equation, and applying model analys, modal decomposition to it. Right? We could equivalently have gone the other way around, right? So, we could have equivalently taken the, the model single degree of freedom model equation for the time exact case, right, which is this one, and then time discretize this. Okay, if we do this, and it's a very s, very useful exercise, it's actually a very simple exercise, you would end up with this. Okay, so this is your mark, I'm just going to give it to, give you in the form of a, sort of flowchart. Okay? So, what we have done here is the following, right, we have at one end, the time exact. Matrix. Vector equation. Right? Where this is Md dot plus Kd equals zero. Okay and I'm going to put a box on it here. Here that is Md dot. Plus Kd equals 0. Okay? Now one way of looking at what we did was, we went from here down to our time discretize case, right. Right, and this is the following. M d at N plus 1, minus d at N. Okay. Divided by delta t. Alright. Plus K dn plus alpha. Right? Equals 0. Okay? So this is the time discrete. Matrix vector equation. Okay? What we did then, is two things. From Md dot plus Kd equals 0, we went to our single degree of freedom SDOF, for single degree of freedom, model equation. Alright, this is the time exact version. Alright. Alright, which is essentially Dl dot plus lambda L, dl equals 0 plus initial conditions, right? Everywhere we have initial conditions. Okay? And then what we also demonstrated is how we can go from here to our single degree of freedom model equation. Right, but this is the time discrete case. All right? Which takes on the form that we've written just above here d L n plus 1, 1 plus alpha delta d. Lambda L minus dn at L, 1 minus 1 minus alpha delta d. Lambda l equals 0. Okay? That's where we are. The one step we haven't taken and which is very easy to take actually is this one. You could start out with our single degree of freedom model equation, the exact form and essentially do a time time discretization of it. And indeed you do get this. Right? Okay. We've done everything but that last dashed the step cor, corresponding to that l, last dashed arrow. Okay its very obvious why this works do you know why this do you recognize why this works? That's right because the process of time discretization, as well as the process of model decomposition are essentially linear operations, okay. And, and therefore in this sort of setting they commute, right, those two operations commute. Okay. All right. Okay. It, it, it of course not all linear operations commute. But, but because of the nature of the sort of operations we're carrying out here, they do indeed commute. All right.



\end{document}
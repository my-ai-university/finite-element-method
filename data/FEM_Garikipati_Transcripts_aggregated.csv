video_id,text
U65GK1vVw4o,"Hello, and welcome to the series of lectures on finite element methods. What we will be doing here is developing the basic introductory finite element methods applied to a certain number of problems in physics. For those of you who are going to be taking the series of lectures as a MOOC from somewhere in cyberspace hopefully this experiment will be an interesting and more importantly useful one. For the others of you who also have access to real lectures in class, this should all serve as a as an enhancement to the in class experience. Okay, so let's get started and what I'm going to do is give you some introduction to what we will be doing during these lectures. The point here is to introduce you to the development of finite element methods, or the mathematical background to them. And to, how to code them up, right? And eventually, and, and then use those, use that code to solve problems. This series of lectures is not about a specific software, whether that it's a commercial software or something that's open source. We will be using something like that, but this is not the, the point of these lectures is not so much learn the software, as to learn the mathematics and the, the computation algorithms that are required to develop finite element methods. As far as background is concerned, I would expect that this should be accessible. These classes should be accessible to the advanced undergraduate student with an understanding of differential equations, but perhaps more importantly, a grasp of linear algebra. So, it's expected that you know what matrices and vectors are and how to multiply matrices and vectors, maybe compute inverses of them, and so on. With regard to differential equations we will use the terminology of differential equations right through this series of lectures, but again, you're not expected to know classical solution methods for differential equations. You're not expected to recall things like methods of characteristics or separation of variables, asymptotic methods or anything like that, okay? We will refer to differential equations. We will refer to some of the, some of the machinery that goes with them, but we will mostly be developing everything that we need. Let's see, and yes, this is going to be a series of lectures with units, and the units are, are, are, are, are all ready laid out, I will be filling them in with segments. And we'll see how it goes. We'll start right away now and one thing I want to point out is that we are not going to develop these lectures as addressed to specific problems only. And in order to do that, we have to go back and recall what the underlying differential equations are for each particular phenomenon that we want to address, okay? I will focus more upon the nature of the differential equation for which we're developing a particular finite element method. And along the way, wherever appropriate and perhaps also often for the development, I will say that while this particular set of methods that we are developing has a particular phenomenon as a canonical example. So we will often refer to elasticity, or linear elasticity in one dimension or multiple dimensions. We will refer to transport problems like the heat transport problem, and, and so on. And right, the other important thing to note is that because this is meant to be an introductory level class and to finite element methods, it will focus on linear problems only, 'kay? I will try to state that as often as possible, but occasionally I may forget to do so. But you, it will, it will be pretty clear that we're not looking at non-linear problems, okay? I think that's about it and we will just get started now. So, to begin, we are going to  consider a particular differential equation, and this is the set of we, we, we're, we're going to start with things in 1D. And we're going to look at a type of differential equation that we will, that I will refer to as linear elliptical differential equations in one dimension. There are at least a couple of examples of phenomena that are governed by this particular differential equation. And, let me straightaway put those down, so that it gives us something to something more concrete to think about as we're developing these methods. Probably the most common one is 1D heat conduction. At steady state. So when we talk of the 1D heat conduction equation at steady state it is actually the same mathematical equation as one-dimensional diffusion at steady state. Okay, so this would be mass diffusion. So we also have, one-dimensional mass diffusion. At steady state. So, whe, when we talk of this, what we have in mind is the following. So, this is my little prop for a one-dimensional domain, all right? So, we're talking of how heat is conducted along this in this case, set of Lego blocks. Or, alternately, if, if this were a one-dimension domain and we were talking of mass transport or mass diffusion along this deme, domain, we may consider maybe introducing a drop of dye at one end and watch as it makes its way by diffusion through the bar or, or through the, through this one-dimensional domain, okay? So those are the two, the two types of problems that I've written down here. In addition, there is also the problem of one-dimensional elasticity, all right? And depending upon where you come from, you may think that is more canonical than either one-dimensional heat conduction or one-dimensional mass diffusion. So, in the case of one-dimensional elasticity, we make, we may look at this as a, as, as a, as representing a bar. And talk of maybe holding it at one end keeping the displacement fixed equal to zero at one end or at where I'm holding it with my right hand. And either apply a load at the other end or specify the displacement of the other end. Okay, and then we would have the problem of solving for the displacement field over the bar, okay? In both cases so whether, whether it's heat conduction or mass diffusion or, or, or elasticity there are other fields that we also need to talk about and which we will here as, as we start developing the, the material. So let me write down also one-dimensional, one-dimensional elasticity, also at steady state. Okay, and as I said just a few minutes ago, all these problems are, that we will consider will be linear problems. In fact, in the series of lectures, except perhaps at the very end, we are not truly going to consider non-linear problems at all. Okay let's actually dive into it and lay down our differential equation, and begin thinking about what it takes to solve it, okay? So. All right, so as, as I develop it just in order to have something to, to, to, to hold on to that's a little more concrete, let's just think of ma, maybe you want to think of one of these three problems in your in your mind, as, as we develop it. I, and, and I will try to refer to these problems as we develop the equations and see what each of these you know, as, at, see what the quantities would work out to in each of these cases. Okay, so let's let's start with this, and, and do the following. Okay, I'm going to develop this first by thinking of a problem in one dimension elasticity for my own purposes, 'kay? So let's suppose we have a bar, okay? And let's say that this is our x-direction, okay? At this end we have 0 and the bar extends up to the point L, okay? Let us suppose that because the way I've drawn it here you know immediately that at the left end I am seeing that the bar is fixed. All right, it's built into some sort of wall here. So, getting back to this, at, at, at well the right end for me, the left end for you, the, the, the bar is fixed, okay? So, so this, this, this what we mean by the support. At the right end, we may either specify the displacement, and I will denote that as U sub g, okay, for given displacement. Or we may do something else with it. We may specify the force at this end. And for various reasons that have to do with how we're going to write things out as we go deeper into this into the, into this series of lectures, I'm going to denote it as t. Okay, so what we have here is U sub g is the specified. Displacement. At x equals L, sorry, x equals capital L or t is the specified. We, we could call it the specified force. But, again, for reasons of generality, with what we're, we will develop as we go deeper into the series of lectures, I'm going to call it the specified traction, 'kay, at x equals L, okay? In addition, what we may also have is a distributed force over this, over this bar. And I will denote that force by just a series of arrows here. And I'm going to label that as f. So what we want to think about here is the foll, is, is, the following idea. As far as this distributed force is concerned we have our bar in one dimension. And what the force is doing is, effect, is, is, it's a force acting on every little volume or mass element of the, of the bar, okay? So, let me write that down as well. F is a distributed Body force as we often call it. Okay, all right, so, so this is the setting and I, I, I'm now going to state our problem in more mathematical terms."
kk_D65zDSBE,"Okay, our problem is the following. We want to find the displacement field which we are going to denote as u. Find u. And we expect it to be a function of position. We do not expect it to be a function of time, force, rather obvious reasons, right? And what are those reasons? Think about it for a couple of seconds. Or you probably, see it already, right? We're talking about steady state drop, that's right. So things are not a function of time. They're only a function of position. Okay, so, we want to find u as a function of x. And now, I'm going to use, introduce some more mathematical notation that will be useful for us. We want to think of u as function, as being a mapping, right, from a certain domain in one dimensional space, right? We're thinking of it as being in mapping from the open interval 0, L, okay? Into a certain range, right? Now, the displacement field, because we're doing a problem in one dimension is for our purposes just a scalar. Okay? So, it's, so, when we find the displacement field, we expect it to be nothing more than a real number, right? Apart from that, we really don't want to say anything more about it at this point. So, I'm going to denote that as R, where R denotes real numbers, okay? Okay? Also to prepare us for what comes later, I'm going to denote this as R1, although in one dimension, specifying that one is really superfluous because if even if we wrote just R, we would know that it is was one dimensional space, okay? So, the way you would read this is, that user mapping from the do, from the domain 0, L, it's an open interval onto one dimensional space of real numbers, okay? It's important to note that we are talking of an open interval here, not a closed interval. The reasons for this will become clear as, as, as we develop that, I'll, I'll come back to this, and to say, why it is that we said, it's an open interval. Okay, so, if we want to find u of this type given, 'kay? Given u at x equals 0, equals u0, okay? We're also given ug, right? The, the quantity that I talked about on the previous slide, where I talked about, when, when I mentioned that we may have a displacement specified at the right end, or t, okay? So, we know u naught, we know either ug, or t, okay? We are also given f, this distributed body force as a function of position, okay? In addition, we are going to say that we have what is called a constitutive relation, okay? So, we also have the constitutive relation, 'kay? Which we are going to write now as sigma equals E times u, x, okay? So, we're given all these quantities, and, and this, this last constitutive relation, okay? And we want to find u, such that. The following holds. The derivative of sigma respect to x, plus f, equals 0, okay? In the open interval 0, L, okay? So, this, of course, is our differential equation, right? So, this is our differential equation. 'Kay? But this is not all. We need to specify other details about, about our problem, okay? And in particular in this case, we need to specify also the boundary conditions. So, we want this differential equation to be satisfied with. The boundary conditions. U at 0, that's u at x equals 0, is equal to u naught, right? And really here, we're just recalling what we already wrote in further up in the problem definition, okay? So, we have this condition, and either u at x equals L, equals u given, or. Sigma evaluated at L equals t, okay? Where, what do we mean by sigma? Well, we need for, in order to get sigma, we go back to our constitutive relation, which we have back here, and evaluate it at x equals 0, okay? Right? So, so these are the boundary conditions. And, and it's important to note that for the way we've set up the problem this boundary condition at x equals 0, always holds. However, at x equals L, we have either this condition, or that one, okay? So, let me say, just one, or two more things about these boundary conditions. So, of the boundary conditions. U at x equals 0 equals u0. And if we have the displacement boundary condition at x equals L, these together constitute what we call Dirichlet boundary conditions. Okay? Dirichlet boundary conditions are boundary conditions which are applied to the primal field that we are solving for, in a partial differential equation, okay? So, let me state that, as well. These are boundary conditions. On the primal field, 'kay? And by primal, we simply mean sort of, you know, the primary field, right? What is the pre, what is the field in terms of which we are posing the problem? Well, we are posing the problem in terms of our displacement field u. And we apply boundary conditions on that field, we call them typically Dirichlet boundary conditions. And this terminology is quite uniform throughout mathematics, right? Through, throughout the field of partial differential equations. Okay. There's another type of boundary condition which is the following, right? In this case, we have sigma at x equals L is equal to t. This is what we call a Neumann boundary condition. All right. Now, if you look at where, what we've written out for sigma. In this particular case, we have, using our constitutive relation for, for, for, for, for sigma. We have E u, x, the whole thing evaluated at x equals L, equals t, right? Just from our constitutive relations, right? So, this thing would follow from. Right? So, we have this from our constitutive relation, and typically, in a fairly vast range of problems, whenever you have a boundary condition that is applied to a derivative, a spatial derivative of your primal field, tends to get called a Neumann boundary condition. That's not uniformly true, for certain high order problems, the situation gets a little more complex, which we won't get into in these series of lectures at least, okay? Alternately, in the context of of the displacement problem, the, the, the Dirichlet boundary condition is often also called the, the displacement boundary condition for obvious reasons. Whereas the Neumann boundary condition often gets called the traction boundary condition, okay? So, let me just state that for elasticity. The Dirichlet. Boundary condition, is called the displacement boundary condition. Okay? And. The Neumann boundary condition for obvious reasons. Is called the traction boundary condition. Okay. At this point, let's stop here from this segment. We will pick it up when we come back."
9GFtoFRXwPw,">> Welcome back. We'll continue with our development of, our one dimensional elliptic partial differential equation and we have been doing it so far using elasticity as a canonical physical problem, right, to guide us through this process. So we had got as far as writing out the differential equation and we were talking about boundary conditions. So let's continue talking about, about boundary conditions, okay? So let me actually draw our domain once again. We have the bar. We have our coordinate x. We have x equals 0 and we have x equals L. Now, At the left end, x equals 0. We have u 0 equals u naught. Right? Specified as a condition. And at the right end, we have either u at L equals u sub g for given, or we talked about how we have sigma at L equals t, okay? And at the very end of the previous segment, we had talked about you know, the nomenclature that we employ for these different boundary conditions. Now I want to point out something that's actually quite important both from a mathematical and a physical standpoint for this problem. Which is the following, observe that we always have this the Dirichlet boundary condition at x equals 0. However, at x equals L, we have either a Dirichlet boundary condition or a Neumann boundary condition. Okay? In, in particular we do not have a situation where at both ends we may have Neumann boundary conditions. Okay. So let me state that. Okay. So in terms of boundary conditions >> Okay? We have either >> case a, being Dirichlet boundary conditions. As we go on, I'll be offering you as BCS short for boundary conditions. So either way, we have Dirichlet boundary conditions at x equals 0 and x equals L, right? Or b. Dirichlet boundary condition at x equals 0 and Neumann boundary condition at x equals L. It turns out that in case b we, we could actually of flipped things around. Which is that we could have had Dirichlet boundary condition x equals l, and a Neumann boundary condition at x equals zero that would not pose a problem. Okay, what's important is to note that there is a particular combination that we are not considering. Okay? It's very obvious but I'll let you think about it for a second or two, to make sure that you are indeed with me. This. Okay. The condition we are not considering is Neumann at both ends. Okay? So, we do not consider Neumann boundary conditions at x equals zero and x equals L. All right? There's a very good reason for this. Okay? Let me try and motivate the reason first from a, from a physical standpoint, okay? What will, why this is important is the following, is, is, is the following. For the kind of problem we are considering, we're saying that we always need to have the displacement specified in one end or the other, at least at one end, okay? So we may have the dis, the displacement specified at this end of where I'm holding it and on, on the other end we may either specify the displacement or a force. That's okay. Right? Or attraction, sorry. That's okay. Okay? Or what we are not allowed to do is to say that we have the traction specified at one end as well at, as, as at the other. Okay? So we can't have Neumann boundary conditions on both sides, okay? And now you may wonder why this is the case. In particular you may ask the question, but surely that leaves out a very important class of problems. In particular, that does leave, it seems to leave out the class of problems where I mean I'll take this bar, right? Remember it's after all an, an elasticity problem, right. You may say that it, we're leaving out the pro, the class of problems where we take this bar and sort of throw it one way. Remember we're doing it one way, so we're assuming that it moves only one direction along the x direction. If we, if we, if we toss this bar and it moves along the x direction, we may say well, we've ruled out the ability to solve that problem because we have stated, so far in our development, that we always have the displacement specified at one end or the other. Okay? If we were to toss it and just watch it, watch it evolve in time, we would not be specifying the displacement at one end, right? It would just be, we would be throwing it and let it evolve in, in space and time, but we would not have specified the displacement at one end. Okay? The reason is that, the reason we, we, we have this restriction is because we are considering steady state problems. The problem where we actually throw the bar and watch it evolve in, in, in space and time would be a time dependent problem. Okay? And in particular in the context of elasticity it would no longer be an elliptic problem, it would be a hyperbolic problem. There are other ways in which that problem is treated. In particular, we would need to specify an additional set of conditions. Can you think of what those may be? We would need initial conditions as well. Okay? What does that mean mathematically, 'kay? What it means mathematically is the following, okay? The reason we do not consider Neuman boundary conditions in, in this description of the problem is because of the following, okay? Supposing you consider, let, let's suppose we do try to specify non boundary conditions at both ends. Okay? So consider, Neumann boundary conditions at x equals 0 and x equals L, okay? So what this may mean is that we are saying that sigma at 0 which we know to be E u,x evaluated at x equals 0 equals let's say. T, 0 right for traction at zero? And sigma at L which is what we have indeed considered is E times u, x evaluated at x equals L now equal to T sub L. Okay? I'm distinguishing T0, T not and T L just by the positions, okay? Now you observe that this poses a certain problem because supposing so, so if you had these two conditions Our, problem would be one of finding this basement field which, in addition to satisfying these two boundary conditions also, of course, needs to satisfy the differential equation. Right? So we had this and now we say consider U of X satisfying these Neumann boundary conditions and the differential equation. Right and the differential equation remember is the sigma d x plus f equals 0. Right? In 0, L. Okay. Now, if you had this sort of situation, making our substitution come from the constitutive relation tells us that now d dx of E times u,x, where I've simply substituted our constitutive relation for this stress, plus f equals 0, okay? Now. This is the problem we would be solving, right? We, we would be solving a problem with these two boundary conditions and this, differential equation. Now observe that any solution we get to this problem would be non unique up to a constant displacement field. Okay? Because supposing you did have a u that satisfied these conditions. Right? Alright. What we could do is that u of x plus some other u which I will denote as u bar. Okay? Which is a constant. With respect to wrt being short for with respect to. X, okay? This field is also a solution. Right? Clearly if u, if u bar is a constant with respect to x, it would satisfy our two Neumann boundary conditions. It would satisfy that. And it would satisfy that because derivatives with respect to u, sorry, derivatives with respect to x of u bar would be zero. It would not contribute to that boundary condition. Likewise to the differential equation, right? Because the differential equation actually involves two derivatives on the displacement field. Right? If, if, if E were a constant. Or even if E were not a constant it would involve at least one derivative on, on, on the displacement field. Right? So therefore adding on U bar equals constant would still satisfy that, that equation as well. Okay? So what we see is that If we had Neumann the boundary conditions only on our problem the solution u of x is non unique.  Up to a constant displacement field and in the context of elasticity, a constant displacement field is also called a rigid body motion. Right? Because remember, this is our one dimensional domain over which we're trying to solve this problem. What is a displacement field that is constant with respect to position? It's one in which we seeing, we're seeing that the entire bar has a translation, right? Every point in the bar has the same displacement, u bar, added on to any solution we specify to it. Okay? So we say that the solution u x is non-unique up to a rigid body motion. U bar, okay, which is a constant. Okay? In this particular say, case, saying a rigid body motion and saying u bar is a constant are actually superfluous. They both mean the same thing. Okay? So it is that for these kinds of problems that we are looking at, in particular elliptic differential equations in one dimension, okay? We cannot have Neumann boundary conditions, only. We do need a traditionally boundary condition and it is exactly to make sure that our solution is unique with respect to the solution, with respect to the primary field. Okay? And also remember that I said that, that, we did talk about how the problem of actually tossing this bar and watching it move in space and time is a different problem. Okay it's no longer an elliptic problem. In particular, it's no longer a steady state problem. Because we would be looking at how a solution evolves in time by not assuming that things are constant In time. Okay. So maybe I should state that as well. Neumann boundary conditions alone can be specified. For the time-dependent. Elasticity problem. Okay? And I'll just state here that though we won't be coming back to this time-dependent elasticity problem. I will just state for now that this would then become what is called a hyperbolic differential equation. The hyperbolic partial differential equation. Okay? We are not doing that right now. We are looking at elliptic partial differential equations, which for the kinds of things we are looking at also means that we have steady state problems, okay? So those are the important things we need to say about the boundary conditions. What I will also do now is spend a few minutes talking about what I've been calling our body force, okay? So let us now recall. The body force that we've written as f has a function of x. Okay, and the role it plays in our problem is the following. It gives us d sigma dx plus f, and here I'll make it explicit that it depends upon position, is equal to 0 in. Okay? In this case, what I'm calling the body force, of course is a term that is applicable only when we're doing elasticity. Okay? In more general problems, in more general PTEs, this would just be called the forcing function. Okay? So this is the. Is the force and function in other pdes probably not other pdes. Maybe I should just call this in general pdes, all right, simply because the term body force may not have any significance in other pdes. All right. It's just, it's just the forcing function. Nevertheless, since we are talking in the context of, we are talking of elasticity let me state what this forcing function is. Okay. Give you, give you an example of the forcing function. So, as an example, let's suppose that we have again this sort of situation. We have the bar. Kay. It extends along the x direction. 0 and l, and that, I had this sort of picture that I drew early on, right? Just say that that is our distribution of the body force, right? F. So a specific example of this would be the following. Supposing that although I've drawn this body as extending in the horizontal direction at least relative to the, this particular screen let's also suppose that, interestingly enough in this situation, this was the direction of the action of the acceleration due to gravity. Okay? I just happened to turn my coordinate system around so that it turned out to be horizontal but that's the, the, the direction of gravity. Okay? So G is D. Acceleration- Due to- Gravity, okay? In that situation, f of x would be the mass density of the body, which, you know, could be a function of position, right? Maybe this bar does not have a constant density with respect to position times g. So is this a specific example of the body force. And know, perhaps make it a little more physical maybe we should say that this bar, you know, is actually turned around this way. So, downward is the x direction, and now you can see very clearly how that would be the body force, right? It could simply be the the distributed force. Distributed because it's distributed at every point in the, every, every, every, every little mass element of our bar. And it's the force that's distributed in every mass element of our bar due to gravity. Okay, so that's another example of it. Okay. So, I think we can stop this segment here. When we come back, we will continue with, saying a little more about this, differentiation."
JfdcpgVTrco,"Okay, we will continue with laying out the elements of our linear elliptic differential equation in one dimension. Okay? Let's, let's look at our differential equation again, so. We've written it in the form d sigma dx plus f function of x as we discussed, is equal to 0 in 0, L. Right. Well, one thing you note of course is that I'm using total derivatives here. I'm not using partial derivatives, that's because when we're doing one dimension, doing things in one dimension, we don't even have time. There are, there are no other variables, right? So we're free do to this. Of course when we go into multi, multi-dimensional problems we will have to use proper partial derivatives. Right, so though I've been calling it a partial differential equation sometimes and a differential equation, really they are one and the same thing for this one dimensional problem in a single variable. Okay? Just a few more things I need to state here. One thing you will note is that I've been careful to say that we have an open interval over which the differential equation is specified. Okay? So, that is an open interval. Right, and you recall what this means? It says that we are interested in looking at this differential equation over the range 0 to L, right. So, we have our domain here. That is 0, that is L. This is the domain over which we are interested in looking at this differential equation. But importantly we are seeing that the differential equation does not apply at the point x equals 0 and x equals f, right. So open interval remember that this implies the domain between. X equals 0 and x equals L, but importantly excluding the end points. X equals 0 and x equals L themselves, right? This is important. And what I want you to do is think of why that maybe, why is it that we specify the differential equation, the partial differential equation or just the differential equation, in this case excluding the point 0 and L. And to help you think about the answer, recall that we do need to know something about 0 and L. If we don't have the differential equation specified at those points we must have something else. All right? Well, you probably get the answer. The an, the, the answer is that we do already have boundary conditions specified at x equals 0 and x equals L. If we were to insist that the differential equation also held at the boundary points, we would actually have a, an over-constrained system. Okay and this will often manifest itself in the mathematical or numerical solution of problems where you by, by, by essentially developing instabilities in your solution mattered or, or, or situations where you just can't find a solution, okay? So let me just state that. This is because we have boundary conditions. Right. And those boundary conditions are either on the primal field itself, or on effectively on u, x, okay, at. The boundary points. Okay? And this is why the partial differential equation is always specified over an open interval. This, by the way, is the case for any differential equation, right? And any, any, any, any partial differential equation, is not specified at its boundary points. Likewise, if you had an ordinary differential equation it would not be specified at the end points of the interval of interest. Okay? This is an important thing to note and, and to recognize that it's not purely, matter of pace. It really means something. Okay? It means something mathematically and physically as well. Okay. That's one thing I wanted to state. The other thing I also want to state is I also, I also want to talk about is the constitutive relations, right. Constitutive relation. And in this problem, we specify the constitutive relation as being sigma equals E times u, x. All right, in constitutive relation as, the term suggests, it tells us something more about the constitution of the domain of the problem, okay? In this particular case, we started out with a primal field u, but then we, introduced another field sigma. Okay? And, what we are seeing, is that we know something more about the domain of interest, and in our case the domain of interest is this one. Sorry. That is our domain of interest, right? We're seeing that, sorry, it's not x here, it's 0, that is x, okay. We are saying we know something more about the domain of this bar, over the domain 0 and L, by specifying this particular relation. Now we are trying to set this problem up using elasticity as a, as an economical physical phenomenon, right? In that context sigma is the stress okay and. And we rewrite it here. Sigma is the stress. And it is related, not to the displacement itself, but to the gradient of the displacement, which in this setting of linear or linearized elasticity is the strain, okay? The strain is sometime given its own symbol. And sometimes, or I guess pretty often is denoted by epsilon, which is u comma x. Okay, so in the context of this particular problem that we're looking at, the elasticity problem, we are seeing that there is a relation between the stress and this which is this field signal and the strain, which is the displacement reading. Okay? And that relation as you can see here is a linear one. Okay, so when we see sigma equals E times u comma x what we have here is linear, linearized elasticity. Right, and it's obvious why we say it's linear or linearized because that's a linear relation between the stress and strain. In this setting, E is a physical constant, that often gets called the Young's modulus, or you may simply call it the modulus in a one dimensional setting. Right, in, in a one dimensional setting, there is only one modulus, might as well call it the Young's modulus. Okay? So this is the other bit that I wanted to state about this, about the physical phenomenon that we're using for now as canonical setting for the problem we're considering. The probably very last thing I will do is restate essentially the same problem but in the context of a different physical phenomenon. Okay? So let me state it here the 1D scalar, because remember we said that in 1D everything is essentially scalar. So the 1D scalar 1D scalar linear elliptic problem. Also, models are heat or diffusive mass transport. Okay? Let's suppose we are talking about heat transport, okay? In that case we would be, we would state the problem as following. We would say find u function of X, which is a mapping, again, from this domain 0, L, open interval to the one dimensional real space. Okay? Given, I, I will attempt to use different symbols here, okay? And I, I, I'm going to use different symbols. But because we're talking about the heat condition of, of the heat transport problem we, I, I will also state here that what we're talking about when we talk about u, the primary field, we, we have the temperature in mind, okay, for, for heat transport. Okay, so define the temperature U, which is mapping from 0,L open interval to R1, given the following, okay, given, u0 and ug or. Now, I want to reserve t for traction. So for the other sort of boundary condition that we could have in the context of heat conduction, I'm going to use j, j bar. Okay? We're given these. We have our forcing function, f, and the constitutive relation. Now in this case the constitutive relation that I want to use is going to be for the heat flux. And want to use a different symbol instead of sigma here. Okay. I'm going to use the, the symbol j for the heat flux. J equals minus kappa u comma x. Okay, all right? Such that. We have the following, okay? Minus d, dx of j equals f of x in (0,L), okay? All right. Now let, let me put down the boundary conditions. And the boundary conditions are. 'Kay, u at zero equals u naught and either u at L equals ug. Right? Now both these, u at 0 being u naught, and u at L being ug would be given temperature conditions, all right? Or at X equals L, we may also have the condition that j at L, okay, equals minus j bar. Okay? All right. Right, so this is how we may specify the heat conduction problem. Now in doing this observe that what we have here is the, is to be be thought of as the divergence of, of, of the heat flux. Okay, and in fact G. In this case, would be the heat flux. Okay? What I've circled here would be the divergence of the heat flux. Okay? In the con, in the context of heat conduction, what we would be seeing would be that the, so, sorry this would be the negative diversions of the heat flux. Okay. So the minus diversions will be heat flux. What we would be seeing is that the negative of the diversion of the heat flux in the context of heat conduction is the amount of heat you're supplying to a particular little volume element in, in this domain of interest. Okay? I already said that you, you, that the conditions on you would be, would be temperature boundary conditions. So let me specify that also, okay? So the Dirichlet boundary conditions would imply in this case, temperature on great conditions. Okay? And this, and finally, the Neumann boundary condition would imply a heat flux on recondition. Okay? In particular, this condition that j at L equals minus j bar. Okay? This would have the following significance that j bar, as I've written it here would be the heat influx at x equals L. Okay? So, if you look at this problem that we've just described here rather quickly and you just make substitutions, right? You just make substitutions instead of you know, instead of j, you go back to sigma instead of kappa, you go back to e. And instead of j bar, you go back to t. Okay? You will see that the problems are in fact very similar. Mathematically, it is exactly the same problem. But we know that this is a problem of steady state heat transport or diffusive mass transport. So, so, so this is important to, to note that there is a multitude of problems that are described by the same differential equation or the same partial differential equation. It's just a matter of redefining our fields of interest boundary conditions of interest and giving them different ascribing to them different physical interpretations. Okay? And because we're interested in developing in this class finite element methods that apply to a range of problems. We will tend to focus more upon the underlying differential equation of interest. And, of, we will repeatedly make connections with the physical problems but we will also bear in mind that rather than thinking of the method as a problem to be developed as a problem applicable. Only to structure mechanics and only to heat conduction, it truly is a lot more general than that. All right? Okay. So this is where we were and this is where we are going to stop for this segment."
A_PWKnOzxX4,"Welcome back, we continue with our preamble into the finite element method. And you will recall that in the previous segment, we had started working with this linear, one dimensional, partial differential equation, which was of elliptic type. Okay? And, we, we, we'd written out this differential equation. We had, also motivated it in the context of elasticity. And also looked at how essentially the same partial differential equation arises in other problems. Right? We'd looked at heat transport as well as mass diffusion. Okay? We'll pick up now, but we'll focus on the PDE itself. All right? So, the topic of this segment is what we call the strong form of the p, of the partial differential equation. Okay? So. So the strong form of a linear PDE, or partial differential equation, of elliptic type. Right? In one dimension. Okay? Now let me write the, the PDE and this is the same PDE that we were working with in the previous segment, but let me put it down now. Okay? So what you want to do is the following. Right? We've set ourselves the task of finding U, function of X, given certain data. Right? We're given Ug, U not. We're given Ug, or we're given t, right? And remember how we qualify the difference between Ug and tUg would be the Dirichlet boundary data, and t would be the Neumann boundary data. Okay? We're given these. We're also given the function f, function of x. And the constitutive relation. And that constitutive relation is sigma equals E. U, X. Right? Given all of this, what we want to do is sorry, we already said that we wanted to find U. So we want to find U given all of this information, such that the spatial gradient of sigma plus f equals 0 in the open interval 0, L. Okay? With boundary conditions, Boundary conditions being u(0) equals U not. And, either U at L equals U given or sigma at L equals T. Okay? This is our specification of of the linear partial differential equation, of elliptic type in 1D. Now, we call this a strong form of the equation, right? That's new terminology that I will introduce. We call it the strong form for two reasons, one is that if you look at the PDE itself, you will observe that the PDE has been specified to hold essentially at every point in the interval of interest, right? When we say that d segment dx plus f equals 0 in the open interval 0,L, we're specifying that that PDE has to hold at every point. Right? So, in a sense, that is a strong condition, we've seen that at every point we want it to hold. Additionally, you observe that when we account for this, constitutive relation, right? What this implies is that, if we substitute that considerative relationship for sigma in the partial differential equation, what we get is the following form. Right? We get d, d, x of sigma. But sigma itself is E du, dx plus f equals 0 in the open integral. Now, if you observe that term, what we see is that there are two derivatives effectively on the field of interest, right? There are two derivatives implied here on the prime of field U. And that is a slightly strong condition, because really for this equation to make sense, classically, we do require that U should be a function of a type that allows us to take two derivatives okay. As you may imagine if U is not a terribly smooth function, if it has discontinuities and so on. If it, if, if even worse, right, if it has delta functions, right? If there are delta functions in it. There are certain difficulties we are going to encounter with taking two derivatives of it. Okay? All right? And so, what, what we're doing here is w, w, we are, we, we require so called strong conditions of what I will loosely call smoothness. Okay? On U. Right? And why is this? Because the strong form, and I will capitalize it here to emphasize that, for now, it's something special to us, right? We require strong conditions of smoothness on U effects, because the strong form has to Spatial derivatives. 'Kay, and there's also this idea that we require the whole point wise, 'kay, so we require The partial differential equation to hold pointwise In the interval of interest. Okay, these are the reasons for which we call this form of the equation the strong form. All right, and this is of course is the classical form in which you have seen partial differential equations posed up to this point, assuming of course that you do not already know something about the mathematical background to finite element methods. Okay, all right, so, so this is the strong form with a PDE of interest and, and, of course, we observed in the previous segment that it, it models elasticity, it models heat transport, mass diffusion and so on. What I'm going to do now in the next few minutes is sketch out for us the approach to an analytic solution, 'kay? And I say just sketch out because I'm not actually going to produce an analytical an, an analytic solution. But I will point us in the direction of of how we would proceed, okay? Of course I'll be using the fact that this is all in 1D, so it makes life a little easier for me, okay? So so let's like now look at an analytic, analytic solution. 'Kay? The approach is very straightforward and you probably already know how I'm going to approach it. It is to simply integrate them, okay? So, we have d/dx of sigma equals minus f, all right, and I did that by just moving f over to the other side, it's just integrate, okay? So we integrate this now. I'm going to integrate this from 0 not all the way up to L, 'kay, but I'm going to integrate this from 0 to say y, okay? And here the the dx, of course. And here I have integral of f. Also over the same limits, right? 0 to y, dx, right? Where y belongs to the open integral, sorry, it belongs now to the closed integral. Right, y belongs to the closed interval, 0 to L. Importantly, we do not necessarily, we do not actually in so, in solving this PDE, we do not want to integrate all the way up to L, right, because we want to maintain a general solution applicable at any point, right, and that's why we don't take the upper limit all the way up to L. Okay this again is completely straight forward, right, so what we, what we know we get from here is we get sigma at y minus sigma at 0 equals minus integral of fdx over the limit 0 to y. F in general is at this point it's, it's not quite unknown. It's data, which we would know if we set out to actually solve the problem. But I want to leave it general. I do not want to assume a particular form of f or anything, right? All right. So we have this. But then we realize that what this implies is, of course this, right, it is that E u, x, evaluated at y minus E, u comma x evaluated at 0 equals minus integral 0 to y, fdx, okay? because that's what it implies for us, okay? And you, and you see how I get this last line by simply making the substitution for the constitutive relation for sigma, all right, at the two limit points. Now in in sa, in what is actually some what of an abuse of notation, let me simplify this to write as let me simplify and write this as E du/dy equals minus integral 0 to y fdx plus E du/dx evaluated at x equals 0, okay? Right? Okay, so and now you know what, what the next step is. Well, we need to do one more integration, right? So what that does for us then is the forage now we'll integrate this again now let's integrate from 0 to z, right? Or 0 to zed depending on which part of the world you come from. Okay, so that becomes E that the, the integral here is E, du/dy dy equals minus integral 0 to y of fdx, and then we're doing another integration, right, 0 to z dy. Okay, and just to make it clear, let's do this, okay? And then we have another term here, which is plus E du/dx evaluated at 0, okay? But this term also gets integrated from 0 to z, dy, all right? Okay again, straightforward enough what, what we get here, now, now note that of course E could be a function of position. Right, so, so, you're, you're allowed to make E a function of position. So E could also depend upon y. All right, so now when you do that integral we know what we get, we, we essentially get to make things simpler, let me forget about that particular dependent, okay? Let me, let me leave things as they are. Okay, so let's suppose E's a constant and we go ahead with the integration what we get here is now E. U at z minus E. Again, u at zero equals everything that we have here on the right hand side, correct? So we have integral zero two z, integral zero two y f d x, d y plus integral zero to z. Well actually this last integral can be made very simple now, right? Right, because it is just E. D u, d x evaluated at zero, right? Times z. Okay. So this is the final form that we get, and since we want to write this all as a, as a result for u of z. We write it as one over e. Of integral zero two z. Integral zero two y, f, d x, d y, plus e, d u, d x. Evaluated at zero times z + E multiplying u at zero and close the brackets, ok? So this is the final, the general form of our equation. Of course I made things even a little simpler for us by saying that lets just assume that E is not a function of position. Okay, and if E were a function of position, of course, would involve a little more. But just for the point of fixing ideas, I want to take this approach. Okay. Now, you observed that we have some unknowns here. Would, we know how to apply them, them, right. We would apply the boundary conditioning, we would apply the differential boundary condition here for u of zero. That is equal to u not, okay? That leaves us however with one more unknown, which is this okay? And this would be obtained by applying the boundary condition that we have at l. Okay? This term which is determined. By applying the given boundary condition. BC for boundary condition at X. Equals L. Right? X equals l or in this case z equals l because we are finally expressing this as, a function of z. Okay? Alright. Now. If the bound recondition that we have z equals l is a displacement boundary condition, if its a directional boundary condition it could be straight forward to have to apply it. What if it were a Neuman boundary condition? Think about it. Straight forward of course its strictly almost if it were a Neuman boundary condition we would simply carry out a derivative. A few right and thereby allow us. That would allow us to apply the boundary condition right. So if boundary condition at x equals l is a Neuman boundary condition, we first calculate, calculate or go to u comma z. Using that form of the function, okay? And that will then allow us to apply the Neuman boundary condition, right? And then apply The boundary condition, e u comma z at l. Okay? That will then allow us to determine this unknown. Okay? I also note now, that I have just 1 sign off in this, slide. I have a minus sign here, which I forgot to carry over. So let me just put it in here. Okay? That takes care of things. So, this would be the. General approach to getting an analytic solution of the strong form of the pde that we are working with. Okay. The trouble with this of course is that we can do this only for very special forms, or for the forcing function f. All right? And furthermore, we did observe during the process that I simplified things by saying that E is a constant. Okay? I'm assuming that E is a constant. It does not always have to be the case. I assumed E were a constant to go through the process. If E is not a constant it makes the process a little more complicated right? And likewise if F were some sort of complicated function. Furthermore this sort of simple minded approach or simplistic approach works only in one dimension right? As we go up to, to higher dimensions integrating up the equations of, of this form. This, this particular linear elliptic PDE. It is not at all a trivial matter, alright? Exact solutions or analytic solutions exist only for very special cases. And what we are trying develop here is really an approach to solve general boundary value problems. All right? With general data, general data in the form of F and E. So that's what we're going to go on to. Shortly, but we'll do that in the next segment, and we'll end this segment here."
qPjJoKwvYEk,"All right, welcome back. We'll continue, where we left off with the previous segment, was in identifying the strong form of the linear elliptic PDE, in one dimension. We sort of sketched out, an approach to generating analytic solutions to the strong form. And then, I made the point that this is not a very general approach, simply because it's not a very useful approach I should say. Simply because as, as you know, the given data, and as boundary conditions get more complex, in particular in multiple dimensions, as you can imagine. Domains of interest in the context of many problems are not just a simple regular domains, right? And that introduces a further element of complexity, to solving partial differential equations, in strong form. Okay? So, analytic solutions are limited, and one tries to find approximate solutions. Okay? The first obvious way to approximate any differential equation is to go to something like, a finite difference method, right? Where a finite difference methods simply take any derivative, and replace it with a difference. Okay, and this has spawned a whole class a whole field, of finite difference methods. What we try to do with finite element methods is, is a little different. Okay, we take if a mathematical approach that is fundamentally, quite different. And the basis of that difference in approaches, is to go from the strong form to you guessed it, the weak form of the PDE. Okay? So, that is the topic of this segment. All right? The weak form of a linear Elliptic. PDE in One dimension. All right. What I'm going to do is, first give you the weak form, and then, develop more ideas about it. Okay, so, the first part of this segment, the next few minutes may seem a little formal, but don't worry, by, by the end of before too long actually, you will be masters of it. Okay, so, here is the weak form. The weak form is the following. It is to find U, okay? U of x, belonging to S. Now, S for us is a space of functions. We talked about finding a function u. And when we see that it belongs to S we are thinking of S as some sort of collection of functions, right? Some sort of space of functions, from which we expect to draw our actual solution u. Okay? When I say space of functions, you may think of you know, any class of function just to fix the ideas of polynomials, or, you know, specific types of polynomials. Legendre polynomials, or Lagrange polynomials, or you may not want polynomials. You may want to have a harmonic functions, right? Or, or exponential functions, or something. Okay? This is the sort of thing we, we have in mind when we say, that u belongs to a space of functions S. Okay? All right. We want to see a little more about, what the space of function is? Okay? And we say that the space of functions u,  S, okay? Consists of all u, right? Such that u at 0 equals u not. Okay? All right. And. So, what I'm doing here is sort of building in the Dirichlet boundary condition, okay? Into this sub-space of functions. What we're saying here is that, we're only interested in solutions you would satisfy the Dirichlet boundary condition. Now of course, there is a possibility of having two Dirichlet boundary conditions. When we saw the strong form, we observed that on the right-end of, of the domain, at x equals L. You could have either another Dirichlet boundary condition, or a Neumann boundary condition. It turns out to be a little cumbersome to develop the weak form, for both cases. And so, I'm going to develop the weak form, for a single Dirichlet boundary condition. Okay? At x equals 0. Later on, we will see what happens, when we have Dirichlet boundary conditions at x equals 0 and x equals f, okay? All right. If, if we did have that, we would build that condition also into the space S, okay? For now, we're just saying, that we have a single Dirichlet boundary condition, okay? So what, so, let me write that. We're assuming, we're, we're considering a case where there Dirichlet boundary condition at holds, at x equals 0 only. Okay? Let me see actually, let's consider this case. Okay? Just, just, because otherwise, it just gets a little cumbersome to develop, in most general form. We'll come back to it, all right. So, this is what  we want to do. We want to find u, belonging to this particular space S. Which for now, is completely general. All we're saying is that, it needs to satisfy the Dirichlet boundary conditions. Okay? So, find u, belonging to space S given all the other data that we have, right? Given u 0, t because we're developing it, with the single boundary condition, right? So, we are not considering u g, now. Okay. So, we're given u 0 t. We're given the function f, right? Which is our forcing function, our body force in the context of the elasticity problem. And, the constitutive relation. Sigma equals E U comma x, okay? So, we're given all this data. Which, which was the same as, as the case, for the strong form. Okay? However, there is more, okay? Such that. For all the w belonging to V. Okay? Now, this is new. All right? This symbol is for all, okay? Right? So now, we've done something new. We've introduced a new function w, which was not in the mix at all. We are saying that it, is a function belonging to some space V. Okay, think of V as the same sort of concept our S is, right? If you think of S being some plane kind of polynomials, maybe V is the same sort of class of functions. But we want to say a little more about V, okay? So V, consists of all functions w such that, w And 0 equals 0. Okay? So w, also satisfies Dirichlet boundary condition except, that it is homogenous. Okay? It is a homogeneous Dirichlet boundary condition. Okay? If we had a we, if we were considering a boundary value problem, with two Dirichlet boundary conditions on u, at x equals 0 and x equals L. We would likewise, have w at L also, equal to 0, okay? Note that so far, w is something we've cooked up, we've just, we've just conjured it up. So, we are allowed to say, what we want about w. Okay? All right. So, let me read what we have, so far. We're not yet done. Find u belonging to S, where S is that, given u not t f of x and, and the constitutive relation, sigma equals E u comma x such that, for all w belonging to V, for V is as specified the following holds now.  What holds? Integral 0 to L. W comma x, sigma d x Equals integral 0 to L w f d x plus W at L. T, all right? Essentially, what I've done is, go to an integral form. This is our weak form, okay? I want to do, just one more thing here. Observe that I'm integrating over x, right? And x really x I mean, integrating over x going from 0 to L. Which is effectively our volume in one dimension, right? Now, recognizing that though we're working in one d, we're really thinking of problems that you know, the canonical problem that we are considering here is that of elasticity. Which actually has some cross sectional area, right? A, right? Which could potentially be the function of x, okay? In order to make connection, with what we're were going to do when we go to multiple dimensions, I want you to take the step of simply multiplying everything here, through by A, okay? Okay? Because what that does is A d x then, becomes A volume element, right? Okay? Likewise here. All right. And what we get here, is like a boundary force. 'Kay? This one made the, the ultimate transition to three dimensions completely seamless. Okay? All right. And, and, A is a lot to be a function of x. Okay, so, this really is our weak form of the equation. Okay? It's considerably different, if you don't have experience with this type of equation, it may look as though it bears no relation at all, to what we started with. Right? To the strong form. It does bear a relation, right? In fact, we're going to show in a short time that it is completely equivalent to the strong form. This weak form by the way, is the basis for approximations that leads to the finite element method. It's a, it is, it is it is a fundamental aspect of a class of approximation, or a, or a class of methods, which are called variational methods"
WKdrBI8Lw9M,"Okay? So let me, let me just tape that. The, the weak form is the basis of the finite element method. And because this is sort of the first time we're writing the term, I will capitalize it. Okay? It's the basis of the Finite Element Method and of other variationally based numerical methods. The term variational may be new to you. You will, before too long know what that means. Okay? Just take it from now for now. Okay? So this is doing the basis of the finite element method. Okay? The very first thing I'm going to do is demonstrate to you, the equivalence between the strong form and the weak form. Okay? So let me state that here. The claim is that the strong form, the strong form and weak form are equivalent. Okay? What that means is that one implies the other. So the strong form implies the weak form, and importantly the weak form implies the strong form. Okay? So let me, I'll write that out, strong form implies the weak form and importantly, the weak form also implies the strong form. Okay? We're going to set out to show this and it turns out that the first step is actually quite easy to show. The first step being the strong form implying the weak form. Okay? So we'll take that approach. Okay? So, consider the strong form. All right? We know the strong forms, I'm not going to write it out in great detail. I'm only going to star write out the essential ingredients of it, right? And the most essential ingredient is the PDE itself, right? So that is d dx of sigma plus f equals 0 in 0,L with the boundary conditions, B.C. for short. Now, the boundary conditions we have are the following, U at 0 equals U not, right? Now because we've, we've written out the weak form for a single Dirichlet boundary condition, only at x equals zero, that's what we have to assume for the strong form as well, right? We need to consider that strong from, we can't have different boundary conditions in the strong form and expect it to lead to the weak form that we showed. Okay? So the boundary condition at x equals L is sigma at L equals t. Right? And we need to remind ourselves that we also have the constitutive relation, right? Sigma equals EU, x. Okay? So, this is where we start out. What we are going to demonstrate now is that this leads to the weak form. And to do that, we will proceed as follows. What we are going to do, is introduce W, just like we did back there, right? W belonging to V, right? The space V, which consists of all functions W, such that W at 0 equals 0. All right? As we discussed previously, W satisfies a homogeneous Dirichlet boundary condition, right? Any place we have a Dirichlet boundary condition, W must satisfy a homogeneous Dirichlet boundary condition. Now, what we will do is the following, we're going to treat W as a waiting function, right? In fact W does get called In the context that we are developing here, W is indeed called the waiting function. Okay? And I'm going to use it indeed like a weighting function. I'm going to multiply W in to our strong form. Okay? So what I'm doing here is multiply strong form by W, and now integrate. Right? And integrate over the interval 0 to L. Okay? So integrate 0 to L, 0 to L. The right hand side, of course, stays equal to 0. Okay? Also, because, we have this we introduce the area, right? So I'm going to sneak the area also into this, multiplication here. Okay? And then I integrated it over the domain of interest, okay? Because the right hand side of zero, of course, it stays at 0. Okay? So, we have this form and now this would work. Okay? But really we haven't done anything special. What we are going to do next is use a very common technique in calculus, in integral and differential calculus, which is called integration by parts. Okay? So I'm going to say that here, integrate, by, parts. Now, in integrate by parts, what we're going to do is act upon that derivative. All right? When we do this, we get the following form. Right? When we apply integration by parts we get minus integral 0 to L, W, x sigma Adx plus w sigma A evaluated at 0 and L plus integral 0 to L, WAfdx equals 0. Okay? And integrating by parts, if it's a step that's probably familiar to all of you, we have done that. Okay? That is the common integration by parts step. Okay? And if this is something that doesn't seem terribly familiar to you, recall that integration by parts involves two steps, right? It involves, the product rule of differentiation. Right? And what is referred to in higher dimensions as the Gals divergent theorem, but in lower dimensions, it is simply the fundamental theorem of calculus. Right? All right, it's not a difficult exercise to do, it's actually quite trivial. Okay, so, we have this form now. Let's focus attention upon that term. Okay? All right? Let me re-write this, this follows let me write it now as let me do two things. Let me move this stone to the other side of the equation. Okay, right? But then let me write the other side of the equation first. So effectively what I'm getting here is the following. I get integral 0 to L W, x sigma A dx equals integral 0 to L, WfAdx plus W sigma A at 0 and L. Okay? All right? Okay, let me go to the next slide and simplify it. For doing that, let me just go back here for a second. If you look at the very last term of the last equation on this slide, we observe that it involves that term being evaluated at two limits, 0 and l. Okay? And so that gives us integral 0 to l, W, x sigma Adx equals integral 0 to L WfAdx plus W at L sigma at L A minus W at 0 sigma at 0 A. I simply expanded out that last term. Okay? Now, if we stare at this very carefully, one should be able to say something special about this very last term here. What can we say? Think about it for a few seconds. In particular what do we know about W at 0? Remember we have a homogeneous Dirichlet boundary condition on the waiting function. Right? So that factor is equal to 0. Okay? Which gives us then our final weak form, which when we also observe that in addition, we know something about sigma at L. Okay? We choose that sigma at L is equal to t. Okay? The fact that w and zero is equal to zero comes from the homogeneous Dirichlet boundary recondition on W. Okay? D-I-R is short Dirichlet. The fact that equals t comes from the Neumann boundary condition. Right? On the strong form. Okay? So, when we do all of this we get our final weak form. Integral 0 to L, W, x sigma Adx equals integral 0 to L WfAdx plus w at L t times a. Okay? This is our weak form of the linear partial differential equation of elliptic form in one dimension. Okay? And you observed that we started with the strong form and essentially obtained this weak form. Okay? It is indeed the weak form we wrote out because W, with this waiting function that we'd introduced in the weak form, does indeed satisfy all the conditions we'd assumed of it when we specified the weak form. In particular we have this homogeneous boundary condition of W. Right? And the fact that we started from the strong form, allowed us to bring in the Neumann boundary condition on sigma. Right? Okay. We'll stop the segment here."
7SUuQQxW2y8,"Okay, we'll continue. In the previous segment, we did two things. We introduced the weak form of the partial differential equation for linear elliptic problems in one dimension. And we also did took the important step of demonstrating that the strong form of the same equation actually leads to this weak form, okay? So we demonstrated the equivalence between the strong and weak forms in one direction by first considering the form, strong form in demonstrating that it does, indeed lead, lead to the weak form that we'd specified. We are going to complete that exercise now by starting with the weak form and demonstrating that it too does lead to the strong form. Okay, so, we do the following, right so. What we're attempting to do here is show that the weak form Implies the strong form. Okay? Well, we start with the weak form then. And again, I would not write the weak form in complete gory detail except to except for the important components, okay? So the, so the weak form is the following, right. It's find u belonging to S which satisfies our Dirichlet boundary condition. Such that. You know, given all the data, and I'm not going to specify the data again in this in, in writing it here, okay? So assume that we have all the data. We have our knowledge about the boundary conditions, and constitutive relations, okay. So find u belonging to S such that for all w belonging to V, right, all waiting functions, w belonging to the space V, which consists of functions w satisfying the homogeneous Dirichlet boundary condition. Okay? The following equation holds. Integral 0 to L, w, x sigma A dx equals integral 0 to L, w A f dx plus w at L, sigma at L A. That is our weak form. All right, so we start from here and we want to get to the strong form. The approach that we take is essentially to step backwards, relative to the approach we took in the previous segment, okay? So, that means we apply integration by parts to this term. Okay? And the reason that you may already see that that should be the approach to take is that you recall that integration by parts essentially transfers derivatives from one field to another, right? And so we want to transfer the derivative from this, from w comma x onto sigma. Okay? That's the approach that we want to take here. All right, so, let's get going. Let's do that now. When apply integration by parts to that term, recalling again that integration by parts is just a combination of the product rule of differentiation and the fundamental theorem of calculus. When one is working in 1D, we get the following. We get minus integral 0 to L, w sigma comma x, which you remember this as d sigma dx, right. A dx plus w comma X, sorry it's not w comma x here. Plus w sigma A evaluated at the limits 0 and L equals, everything on the right-hand side remains the same. Because there are no, essentially because there are no derivatives on the right-hand side that we want to transfer on to other, other functions. Okay. In the next step, we are going to split out this term. Okay. We're going to split out this term, okay, into the two limits that it implies. So we get minus integral 0 to L, w sigma comma x, A dx plus w at L, sigma at L A minus w at 0, sigma at 0, A. All right, and you observe that that is essentially what we've done here, right? We've split out w sigma A, evaluated at 0 and L into the two limits. All of this is equal to, integral 0 to L w A f dx plus w at L, sigma at L, A. Okay? And now since you are already getting to be quite expert at invoking our conditions on the problem, especially our boundary conditions, it should be pretty obvious what to do in the next step, right. Think about it for a couple of seconds if you don't already know it. Paying particular focus upon the term that, I've applied an upper brace, bracket to. Right. So, what we do here is to first observe that, that goes to zero, right? And that is so because w does indeed belong to the space V, right? And we have that homogeneous Dirichlet boundary condition upon it. So that last term disappears, okay. And what I'm going to do now, is to essentially combine what remains into integrals, right, here and there, and to terms multiplying w of L here and there, okay? When I do this, I'm giving the following, I, I arrive at the following form. When I combine the integrals, right, because those integrals are over the same domain I have the following, I have w times minus sigma, x minus f. Because what I've done here, is to bring f, which was on the right-hand side in the previous equation, right? In the last line of the previous slide, right? The term, the integral involving f, I've brought to the left-hand side, okay? So, that flips its sign. I have A d x. And I'm simply going to bring everything over to the left-hand side here, okay? So, I have w at L, let me keep A here. Multiplying sigma at L, this was all the stuff on the left-hand side, and the term that I need to bring over from the right-hand side is the minus t, okay? All of this equals 0, okay? Let me just remind ourselves of what we had here. Oh, I, I noticed that when I, when I wrote this Weak Form I actually, sort of, jumped ahead of myself by writing something that I ought to have done a little differently. The sigma at L in the Weak Form, actually appears as t, right? So, I need to correct all the sigma at Ls, and replace them with t, right? So, and, that is the Weak Form. I was jumping ahead of myself by already implying the Neumann boundary conditions, but that is not how we write the Weak Form. That is how we write the Weak Form, okay? So, now, things are, things, everything's all right. Now, when I come back here, I do indeed, have, in this very last te, term sigma minus t, sig my tail minus t, okay? And this is equal to 0. Now, let's recall the conditions under which this holds, okay? This holds For all w belonging to our space V, which consists of functions w such that, w and 0 equals 0, okay. So, the statement here, the mathematical statement is that, that equation in the first line of this slide holds for all w, long as w vanishes at x equals 0, okay? Now, in particular, I'm going to claim that if this holds, the, the condition I'm going to use is that if this holds, I can consider a particular, right? So, what this implies is that it also holds for w of x equals phi of x times minus sigma, x minus f, 'kay? Where, phi of x is greater than 0 for x belonging to the open interval 0, L, okay? And phi of x equals 0 at x equals the points 0 and L, okay? So, really what we're talking about is a function of this type. If this is 0, and that is L, right? And this is our x-axis. The function phi is one that, that looks something like this, right? It may be as smooth as this, or it may be something else, 'kay? Doesn't matter, right? It's positive, right? And it right, it's positive, and it vanishes at 0 and L, okay? What this does then, is two things, right? What this implies for us is that then, it ensures that therefore, w at L equals 0, right? It also ensures Dirichlet boundary condition that w needs to satisfy, right? The fact that w has to vanish at 0 comes from the space d, right? That's, that's the condition that we're assuming on the waiting function. In addition, we've assumed a very special one, right? It's not like we've assumed them, we're saying that because this condition has to hold from all w belonging to phi. It also holds that this particular form of w, right? This one, where phi has the form that we've assumed here, okay? Well, what is that do for us now? Because we've taken a form that makes sure that w at L goes to 0, the second term in this expression here for the Weak Form vanishes, okay? That leaves us with integral 0 to L. Now, for w, we have phi of x times minus sigma, x minus f, right? All of this is our W. But then, this multiplied again, minus sigma, x minus f d x equals 0. The other term involving the boundary terms vanished by our choice of phi, okay? So, what we get from here, is now integral 0 to L phi of x minus, well, minus sigma, x minus f. The whole square d x equals 0, and I realize I'm missing an A here. Okay? Well, but if this is the case, what do we know here? We know that phi, x is greater than, or equal to 0, everywhere, right? Or 0 to L, right? In fact, its, in fact, over the, over the domain, over the interior of the domain, right? This term is indeed equal to 0. Sorry, over the interior of the domain, this term is indeed, created in 0, okay? This being the square of an expression, is also greater than, or equal to 0, okay? So, the only way this whole expression can be equal to 0, right? As an integral over the domain, is if, minus sigma, x minus f is itself equal to 0, right? In the open integral of interest 0, L, okay? But this, of course, is what? This is nothing but our, our, our the p d e, which goes into a strong form, okay? This is simply, the p d e in In the strong form. Okay? Let's continue then. So, so, what we demonstrated that having when we, when we considered the Weak form. It, since it holds for all of w, it also holds for a certain choice of w which reveals to us that particular choice of w reveals to us that the pde of the Strong form must hold. Okay? Let's continue with that exercise now. So now, let's return to this other form that we sort of massaged our weak form into, and that was integral from 0 to L, W minus sigma comma x minus f, Adx plus w with L A sigma at L minus t equals 0. Okay, for all w belonging to v. Okay, now what we've demonstrated just above in the slide, by a particular choice of w. What we've demonstrated by choosing a particular w? Is that this term actually vanishes. The pde of our Strong form must hold. That leaves us with only this last term to worry about. Okay. Okay, and this has to hold for all w belonging to v. So, in particular. It also holds. For a w of x, which is such that w at 0 equals 0, which is required by our specification of the space v, and w at L, not equal to 0, right? It also has to hold for this particular choice of w. Well, if that is the case, the only way the condition at the top of the slide can hold is, if sigma at L minus t equals 0. Okay? But after all this is nothing but the Neumann boundary condition of the Strong form. Okay so we've done two things here, we've demonstrated by starting from the Weak form that, since the Weak form has to hold for all waiting functions belong to our space v it also must hold for certain special choices of w. And from the special choices of w, we are able to demonstrate that one, the pde must hold, right? And secondly, the Neumann boundary condition must hold. Right? The pde of the strong form and the Neumann boundary condition of the strong form. It would appear that we left out one component of the Strong form here. What about the Dirichlet boundary condition on the Strong form? Where did that go? How do we make sure that that is satisfied? Think about it for a couple seconds. My claim is that it is already satisfied. Okay, so it is indeed already satisfied. That the Dirichlet boundary condition on the Strong form is already satisfied, simply from the fact that we have our Weak form here, right? On this slide, at the top of the slide, on the second line of the slide you will observe, that when I stated the Weak form, I said that we wanted to find u, belonging to S, where the space S consists of all functions that actually satisfy the specified Dirichlet boundary condition, right? Which, for this, boundary value problem that we're considering to fix ideas, and to develop our approach. The Dirichlet boundary condition in this case is that u vanishes at x equals 0. This condition, right? It's already in our Weak form. Okay? So our Weak form already has that condition, right? Let me just state that. Weak form already has the Dirichlet boundary condition on u. Okay. And that comes about because we say that u belongs to space S which of consists of all the functions u such that u at 0 equals u not. Okay the Dirichlet boundary condition so to speak is built into our Weak form. Okay? So, we've demonstrated that the Weak form implies our pde of the Strong form, it implies the Neumann boundary condition of the Strong form and it already contains the Dirichlet boundary condition of the Strong form, right? That essentially completes it, okay. So what this implies is that, the Weak form implies the pde, Neumann boundary conditions of the Strong form, and contains. Dirichlet boundary condition. Okay. So, indeed, the Weak form is completely equivalent to the Strong form. Okay, so what we demonstrated in the segment before this is that the Strong Form implies the Weak form. And we did this by starting out with the Strong form, multiplying it by the weighting function, and integrating by parts. What we've done here is start out with the Weak form, pose certain arguments makes, make the observation that since the Weak form holds for all weighting functions w belonging to the space v, it also hol, holds for certain specific forms. Right? And for it to hold those specific forms, what we've demonstrated is that certain conditions must hold. In particular, we demonstrated that the pde must hold. Right, the Strong form of the pde must hold. Our Neumann boundary must hold and the Dirichlet boundary condition is built in. So, we have that the Weak form implies the Strong form, okay? All right, so we'll stop here for this segment."
EN3kzZKhctc,"Hi, I'm Greg Teichert. I'm a Ph.D student who's working on Finite Elements with Krishna Garikipati, and for the next few segments I'll be introducing you to programming in C ++. Now this isn't meant to be comprehensive overview of C++, it's really just to give you the basics, the basic understanding so that you're able to do the coding assignments that are associated with this course. All right, and so let's begin. I'll write down an overview of what we'll be talking about in this first segment. First, I will talk about how to run your code, your C++ code using CMake. Then we'll move onto talking about the basic structure of a C++ program. And from there, we will move on to talk about the basic number types, or a few of the basic number types. In C++. And then we'll move on and talk about C++ vector, a standard vector. All right, so when we're running the code using the deal.II finite element library, we'll be using a building system, a build system called CMake. And CMake takes your C++ code and it creates a Makefile which will then be used to run the code. So we'll move over here to the desktop screen. And you can see here I have two files. I have this CMakeLists.txt and main.cc. .cc is the file extension that specifies that this is a C++ source code file. The CMakeList.txt is what CMake uses to identify the source code. All right, so this is the file that we'll include with the coding templates, with your assignments. It's also in with, there's also a CMakeList.txt file included with all of the deal.II examples. All right. So here on line number 6, it says set target and a high of, in parentheses, main. So that specifies the name of the file of this first code that we'll be looking at. That's main, of course. main.cc. The .cc is specified here in line 10 as the file extension. All right? If your source code had another file name, you'd simply change that here. For example, for the examples in deal.II, they're all labeled step-1 or step-2 or so on. But I'll leave it as main for now. And from there, in my terminal, I'll go to the directory where those files are held. So they're on my Desktop in a file called Example, okay? If I list of contents of that file, you can see there are CMakeLists.txt as well as main.cc. Okay? Now, the very first time we want to run the code, we need to run CMake. And CMake will create the Makefile that we need. So, I type in CMake and then CMakeLists.txt. And there, if I open up the folder again, you can see it's created several different files, one of which is this Makefile. Okay, now we only have to do this once. The only time we need to run CMake again is if you needed a new Makefile. All right, so from here on we'll refer to the Makefile. And the Makefile will link to the source code. And we type in make. Make builds the executable. If we want to run it as well, we type in make run. Okay, and there you can see it. Built the target and ran the target. Okay? So that's the way that we are going to be writing our C++ code both in these tutorials in the beginning and also for the templates for your assignments. All right? Now we'll move on to discussing the basic structure of the C++ program. So I'll open up this main.cc file, and you can see it's pretty sparse, there's not much in it. And these are the basics of what you actually need in your C++ program. And this actually is in the same structure as a C++ function. And we'll talk more about functions later on in one of the following segments. You can see here each function has a name, in this case the name is main. It has, or can have, inputs. The inputs would come here between the parentheses. As you can see, this function main doesn't have any inputs. The outputs are described right before the function name. In this case it's int. Int stands for integer. So that means this function main, doesn't need any inputs but it has one output, an integer, Now down at the bottom it says return 0. Here we're specifying what specific integer we'll be returning. And we return 0 to let the computer know that the program ran correctly. For example, if you had come across an error you might do return 1 and so the operating system would know that you had come across a processing error and that was why you were terminating the program. Okay? So that's the basic structure of the C ++ program. Of course, it's not doing anything. And in order to add some more functionality, we add header files. Header files include additional functions and data types that we can use. So let's add one of those and we add header files using #include. And I'm going to add a file called iostream and iostream allows you to print statements to the terminal, to the screen. It also allows you to input statements from the terminal. So for now, I will just be printing a statement to the screen. One of the functions included in IOstream is cout. I do this standard std::cout to specify that cout is a standard C++ function that we're using. Okay. And so we want to print out to this screen. I'll print out the canonical example, Hello, world! Okay, the quotes specify that it's a string that we're printing out words. And standard endline, endline, just ends the line on the screen. Get carriage return, okay. So, we save that. Again, we don't need to run CMake again, because. We are still using the same make file even though the source code itself has changed. All right, so I'll type in make run, and there you can see on the screen we have, hello, world, okay. Now most likely we want our program to do more than just print words to the screen, right. We will be using numbers of various types, and so let me talk about what some of those types are. I won't be discussing all of them, but just a few. Okay. So there are actually mostly just three types of numbers that we'll be using in the templates. The first is int and we solved that already in the function. We were returning an int. An int is an integer. Okay. So that zero one two minus one minus two and so on. The second number type that we use quite a bit is an unsigned int. And an unsigned int is just what you'd expect. It's an integer without a sign, so it's non-negative, we can't store sin with that. The third number type that we use quite frequently, is called a double. A double stores real numbers, and I'm not going to get into the specifics of how it's storing these numbers. I will explain though, that it's called a double because there's actually another number type called a float. Which also stores real numbers. A float uses 32 bits per real number. And can you guess how much a double uses? 64 bits. OK, double is short for double precision. We use more memory to store the number so we can get a greater precision with those numbers, all right? So, let me show you here in the source code how we would declare these integers and real numbers, okay? So for an int, I type int. We have to let the program know how much memory to store for each of these variables that we're declaring here, okay? So I can declare int a, and you notice I put a semicolon at the end there. I can also declare multiple ints at the same time, separated by a comma. I can specify the value while I'm declaring it. So, I could do d equals 1, e equals 3, and maybe f without actually specifying anything there. Okay, so I can do all of that. Similar with an unsigned int. Specify a number there, and same with the doubles. 1.3, for example, i. Okay, so all of those are valid declarations of an integer, unsigned int or a double. Now there's one thing to be careful of, and this is a bug that can show up in, that will probably show up in your code at one point or another, it's certainly shown up in my code. And that's the fact that, okay we understand that an integer plus an integer equals an integer. Or an int times an int equals an int. Int minus and int equals an integer. In C++ an integer divided by an integer is also an integer. And so for example, if we print to the screen, 3 divided by 4. What would you expect it to print to the screen? We would or I would have expected it to print 0.75. Right? But if you look here. It printed to zero. That's because three is an integer, four is an integer. When you divide it, C++ wants an integer, and so it truncates any decimal values and takes only the integer in front of the decimal. Okay. So if you want C++ to recognize that three divided by four is .75, you need to put a decimal on either the three or the four or both. So no if we run this, you can see it prints to screen .75. Alright. So that is something to watch out for, too. So if you want your division, your quotient to give you a double, make sure that the numbers involved are also doubles. Okay. All right. So for the last topic in this segment, I want to talk about standard C++ vector. And these are very useful objects in C++. A vector, as you might expect from mathematics, is an array or a range of elements. However, as opposed to in mathematics where we talk about vectors and a vector would be an array of numbers. In C++ you can have a vector of any other C++ object, so you can have the vector of doubles, you can have the vector of integers, a vector of unsignance. You can have a vector of characters, of letters. Or you could even do a vector of vectors. So they're really quite handy, it's not just a mathematical object, but you can also think of it as a storage device. Now, over here on the, in our code, we'll look, but to use vectors, I actually have to include another header file. Okay. So I pound include vector. And when we declare a vector, we use standard vector, again because it is a standard C++ object. We have to declare what this vector will be storing in it. It can only store one data type, all right. So for the example here, I will store a vector of doubles. And again, as with integers and doubles and so on, there's more than one way to initialize a vector. So we can declare its name. Without saying anything about it. When we declare simply the name, then the vector has size of zero, there are no entries, there are no spots for any numbers, it has a length of zero. Okay. Let's declare another vector here. Make it a vector doubles as well. And here, I want to specify the size, and so in parenthesis I'll put three, for example. So now, we've declared a vector of size three, of length three. However, the doubles that are stored inside of it have a default value of zero. Now, if I want to make a vector of length three where all the entries are one, for example. I would again put in a three for the first entry and separated by a comma, I would put one. Okay. And so that will make every entry initialize as one. And as before, we can initialize more than one vector at a time Just like with doubles or integers. Okay? Now, a vector differs from the integers and the doubles in one way, in that a vector has several functions associated with it. And, okay, now I'm saying function but, again, function in C++ doesn't mean the same thing as a function in mathematics. We're not necessarily taking a number or a range of numbers, performing some mathematical operation to produce another number. A function in C++ performs some operation, but it's not necessarily a numerical operation, okay. So, one common function for vectors is this resize function. So, when I declared vec1, it was empty. I want to resize it, so it now can contain five elements. Okay? So now let me demonstrate another function. If you want to seal it as the length of the vector, you can do vec1.size. And I'll print that to screen, and we'll see that. It should print out five, which is the length of vec1, okay? Now we can access any component of the vector using square brackets. So let's look at the first component. Notice indices in C++ start at zero, all right? So vec1 should print out the first element of the vector vec1, okay? Okay, so here on our screen, we printed out the size, which is five and the first element, which is zero, which is what we'd expect. Because when we resized the vector, we didn't specify the values, and so the default value was zero. We can also resize vec2 and vec3. Even though they've already been initialized, we can resize those. Another function that is Is handy is the push_back function. So right now, vec1 has a size of five, and all the elements in vec1 are zero. Now let's say I want to add another component to the end of vec1. And I don't want it to be zero, I want it to be, for example, three, maybe 3.2. Okay. So this adds another element on to the end of vec1. And so what that does. Is it increases the size by one, and we'll be able to see that here. As I print this to screen, okay? So the original size was five. The first component is 0. The size after column pushback is now six. We've added another element onto that vector. And element contains the value 3.2. All right, now there are many other functions related to vector and one valuable resource for looking at these functions, looking at other header files in C++ and even a very informative tutorial series is found on the website, cplusplus.com. I'll write that here. Plus is spelled out. And so if you have further questions on something we've talked about it in this segment or any of the future segments You can refer to cplusplus.com and, in their search, you can search, for example, vector or double or any of these other data objects or functions that we're talking about. All right, in the next segment, we'll move on to talking about conditional statements and for loops and what the scope in C++ means."
tDeMawGbov4,"Hi, in this segment we will be talking about using pointers in C++ and functions. All right. So for pointers, I'm going to talk a little bit, briefly, about how C++ is storing the values of these numbers that we're working with. Okay, I won't get into a lot of detail but I'll draw a quick picture here. So, for each, each number is stored in memory, all right? And there are two components, two aspects to each number. One is the value of the number, whether it's an integer or a double. And that's as we'd expect. However, the operating system needs to know where to look to retrieve that value when we are referring to that variable. And so we have an address as well. So each variable has an address, and it has a value. Okay, and just for simplicity I'm going to label the addresses 1, 2, 3, 4 and so on. Okay, so let's say that we have declared an integer a. And we're going to give it a value of 3. Okay? We can also declare what's called a pointer. And a pointer doesn't actually store a value. It stores an address. So here, I'm going to declare a pointer. We declare it with the asterisk. And I'll call it b. So the value of a pointer is the actually the address of another variable. Okay, so here I might store as the value of my pointer, the address of int a okay. So let's look at how that works in the code. I create an integer a, and I'm also going to create a pointer b. Now, b by itself at this point is uninitialized. It doesn't have a value. The way we initialize the value of a pointer Is through a none pointer element of the same type. Okay, so I will say that b is equal to the address of a. So if we want to get the address of a, we actually use the ampersand. So let me write that here. So the address of a = &a. Okay? So I'll do that here. B is equal to &a. So now we were to print to the screen, actually let me give me a a value. I'll say a = 4. Okay, now one other thing before I print to screen. If we want the value of a pointer or rather the value that a pointer points to, the value pointed to. We can use the asterisk. So *b, would give you in this case, that I've shown here, 3. Okay? And so here I've simply set b to store the address of a and yet when I print out the value of b, it will give me the value stored at the address that it contains. So it should print out three, exactly. Now, what happens if I change the value of a? Say a is now equal to 4. What will that do to b? Will b still be three or will it now be four? Let's look at that. You can see that the value pointed to by b has changed to four. Why? Because b is only storing the address. And so here, when we've changed the value of 3 to 4, b is still pointing to that address in the memory, okay? Now, while I've shown this demonstration with integers, you can actually have pointers of any other object in C++. You can have pointers of vectors, okay? So let me do that. Let's create a vector, standard vector. Of type double and I'll create a regular vector here, vec1 and I will create a pointer to a vector, vec2. And, let me initialize vec1 to have length 2 and values of 3.1 as an example. And in order to initialize vec2, again as before say vec2 is equal to the address using ampersand of vec1. All right. Now, vectors have functions, we've talked about before. All right, we can look at the size of the function. So how does that work with a pointer to a vector? Now, with a regular vector, with as is the case with deck one, we would just do .size. Right? And. As we can see here. Okay, gave me an error. Oh, this is important for you to recognize. Okay, I forgot my semicolon. And it gives a helpful hint here, the expected semicolon before return zero, okay. So now we should be able to run it. And again, I forgot to save it. So now we should be able to run it. Okay. So, it's given us the value of 2, the size of vec1. Now in order to do that same thing with the pointer, we actually don't use the dot anymore. We de-reference the vector. So, we, instead of looking at the reference that the pointer's pointing to, we look at the value. And we do that through this error, this arrow, excuse me, through the arrow. And then we can use the same function name, size. Okay, and now, as we run it, you can see it's printed the same size. We can, as before, if we change something with vec1. So I'm going to resize vec1 and maybe make it 3. And even though we only changed it on vec1, the same change goes throughout in vec2, because, again, it points to that address. And I have saved my program and so now I can run it. You can see they were both 2 2 in the beginning when we resized Vec 1. Both Vec 1 and Vec 2 have changed too, a size of 3. Okay. Now, I want to change gears a little bit and talk about iterators. So, with the for loop that we talked about before, we used an integer to increment from one element to another or to iterate the for loop. There's another way to do it and iterators are related to pointers, and so that's why I waited to talk about it until now. In the case of a vector, there's what's called an iterator, and an iterator is simply an object that points to a single element in an array of elements, or a range of elements. And it also knows where either the next element in that range is, or where the previous element in the range is, depending on which direction the iterator is going. Okay, and the reason I'm brining this up is because we do use an iterator In the coding assignments, and I'll point that out to you as we go over the template file. All right, and so I wanted you to understand what's going on there. Okay, so in this case I will create a vector, and I'll iterate over this vector with an a for loop, okay? So I'll create a standard vector of doubles, call it vec. And I'll do a for loop. So I will declare the iterator here. So it's a standard vector of doubles. So it's an iterator over a standard vector of doubles. All right. And I will call it, IT and so a vector, the vector class has an object that is a beginning iterator and an ending iterator. And so I'll set the sequel to the beginning iterator. And it's the function to retrieve that iterator is begin. Okay, and the conditional in this case won't be a less than, it'll actually be not equal to. So as long as the iterator is not equal to, so the not equal condition you'll use with the exclamation point and the equal sign. So if the iterator is not equal to vec.end. Then we will go through the for loop into iterate. We actually have the same syntax here, it++. Of course, in this case it's not mathematically adding one to the iterator, because the iterator isn't a number. It's just iterating from the current element in this range of elements to the next element. All right. And so just to demonstrate what's going on, I'll print to the screen. I'll print to the screen the values at each point. Okay. Actually I'll print to the screen the values of the index of the iterator. And so you can see this is similar to the pointer in that we're using this star, or *it to sort of de-reference the iterator, it gets some information out of the iterator, essentially. Okay, and so now if I run that. It gives us the value of the vector at each of those points. So we only had two components and so it gave us the first and second components. Just to illustrate what's actually going on I will change the value of the second component so vec I'll set equal to 4.3. So now as you run it you'll see that it's, it outputs. So it* points to the first element in the vector and it prints out 3.1. And then on the second loop, *it points to the second element in the vector, which has a value of 4.3. Okay. So, again, it's pointers and iterators aren't always the same thing, but they are related. And a pointer can be used as an iterator at times, and an iterator can act like a pointer. Okay, so let's end this segment here, and in the next segment we'll move on to discussing functions."
n-7XJK26n5I,"Well hello, and welcome back. We'll continue with our development of finite element methods, and we're working, as you remember, with a one-dimensional linear elliptic PDE, right? And this zeta, eta, xi model for elasticity, for heat conduction, mass diffusion and so on. So where we are is that we've written out the strong form of this PDE. We've we then introduced the weak form and importantly we demonstrated that the, the strong form and the weak form are essentially completely equivalent. Okay, that's where, as far as we got in the previous segment. And that's where we are going to continue from today. And to get us started I am just going to write out the strong form and the weak form and use that as a point of departure, okay? So. So the strong and weak forms of 1D linear elliptic. PDEs, okay? That's where we are going to start off today. Okay, so I'm going to try and write them in a somewhat unified manner, so I'm going to first write out the data, okay? And that is, given u0 we are working with Neumann conditions, remember? Just in order to not keep things too general while we're developing things right now. Okay, so given u0, t, which is going to be our Neumann boundary precondition data we're given our force in data, f as a function of x and the constitutive relation. Sigma equals Eu,x, 'kay? This is what we're given. All right, so now, let's write out the strong form. Write out the strong form here. And I'm going to use the other side of this the other half of this screen to write out the weak form. Okay, and let me also do the following, okay, do that and that, okay, all right. So the strong form is find u such that d sigma/dx plus f equals 0 in the open integral, and we have the boundary conditions, u. So, u and 0 equals u naught. And, sigma at L equals t. That is our strong form. The weak form is again, given the data find u belonging to our space S, which as you recall from our discussion, the previous segment incorporates the Dirichlet boundary condition. Okay, find u belonging to S such that for all waiting functions w belonging to another space, V, which is characterized by the requirement that all functions in V must satisfy the homogeneous Dirichlet boundary condition, okay? So find u belong to S such that for all w belonging to V, the following holds, right? And, and the important thing is that the weak form is an integral form, right? Integral 0 to L, w,x sigma dx. And in order to be able to make connections later on with multiple dimensions, I'm introducing an area of that, right? A, this is equal to integral 0 to L, wfAdx plus W at LtA, all right, so there we have it. Our strong form and weak form. Now, when we look at this, you remember also that importantly these two are completely equivalent, right? So let me write that here actually. I don't need to use all of this, so let me make this line a little shorter so I can write. I can write at the bottom of it, okay? And recall, importantly, that we demonstrated last time. Recall that the strong form. Is completely equivalent to the weak form. Okay? Now, if the strong form and weak form are completely equivalent you realize that in this sort of presentation, we have not yet done anything that may help us solve the PDE in an, in an easier manner, right? In, in, in, in essence, both these forms, the strong form and weak form are, so to speak, exact statements of the problem. And if you were to try and solve the problem in, in its exact form, there is no reason why either of these forms the strong form and the weak form, should present an easier task. A task can become easier. However, if we resort to approximations, if you choose to work with approximations of the strong form, you would be headed into something like a finite difference method where the obvious approach, would be to represent that derivative, and any other derivative that appears with the difference formulas, okay, and therefore finite differences. With finite elements we take a different approach, we will work off the weak form and introduce approximations there, okay? So the way we do this is is the following. So. Let me state here that the finite element method. Is based on an approximate version Of the weak form. Okay? And in order to understand what sort of approximations we have let me state that when we say that u belongs to s and w belongs to v. What we have in mind here are function spaces that we describe as being infinite dimensional function spaces. Okay? These are infinite-dimensional function spaces. And I'm going to tell you what that means. Okay. Infinite-dimensional is important. What this means is the following. If you are thinking of these spaces s and v as being maybe polynomials, right? We say they're infinite-dimensional in the sense that at this point, we are considering polynomials of all orders. Okay? So, you'd be considering the, constant, linears, quadratics, cubics, quartex, and so on, right? All the way up to infinity, right? What, and, and therefore infinite-dimensional, because we think of each order of polynomial as being a dimension of the space of polynomials. So let me give that to you as an example. So example, if s comma v, are. Let me put it this way. Let me use mathematical notation. So, s comma v belong to the space of polynomials of order n on x, right, where v denotes polynomials. We have in mind that n equals 0,1,2, and so on, okay. Where P denotes Pn denotes polynomials of order n. So it's in the sense that they're infinite-dimensional. Now the problem with infinite-dimensional spaces is that we're really looking for solutions in, in, in a huge space. And it is, it is the fact that we're looking for solutions in such a large space, that makes our task difficult, when we restrict ourselves to the exact statement of the problem. The idea of developing approximation methods based upon the weak form is to say that well, let's make our lives a little easier by restricting the dimensionality of the space in which we are looking for solutions. Okay? So what we're going to do here is construct approximations. In what we would call, you know, instead of infinite-dimensional, we're actually going to look at finite-dimensional spaces. Construct approximations in finite-dimensional. Function spaces. Okay, we construct approximations in finite-dimensional function spaces. All right, and example. Of course an example would be, would be to say that we construct for approximations in Pn of x, where n equals, maybe we want to say maybe we just have zero and one. Ok, and this would mean that we are considering approximations in polynomials of up to first order, right, up to linear polynomials. Okay? So this is the approach we take and, and as you can imagine now this, this is, this is likely to make our life a little easier. Because we're saying straight away that, yes I know that this the problem of interest as driven by the data that we have, is, you know, is likely to have polynomial solutions of some arbitrary order. But I'm going to try and approximate that those polynomials of arbitrary order with linear polynomials. Okay? And because it, it, because of the properties of these function spaces those approximations would be good or bad. And actually it is, it is it is addressing that question that financial element error analysis is occupied with. Okay but, we'll get to that later. All right, so, so, the way we formalize this is to do the following, okay. So, we restrict, okay, we restrict the solution space. All right, and importantly we need to restrict the space of weighting functions as well. Restrict the solutions space and weighting function space. And the solution space and the weighting function space as well. Okay, this is what we're aiming to do. And the way we write this formally is the following. We say now, now we're working with the weak form only. Okay, we say the following, we say want to find, first of all, we want to find not u itself any longer. Because we've already given up on the prospect of finding the exact solution. And u denotes the exact solution for us. Okay? So we want to say that we want to find an approximate solution, and we are going to denote the approximate solution by u sup h. That's not u, u raised to the power of h but u sup h. Okay, so you want to find u h function of x, which now belongs to a function space. But this function space is a finite dimension of function space, okay it's also an approximation. And that too is denoted by h, I'll tell you in a little bit why h. Anyway, we do the sketch. All right. So we want to find belonging to Sh. However, we want to allow Sh to lie only within the larger space of, of solutions, which also contains the exact solution. Okay? So we want Sh to be a subset of S, okay? Now, apart from that, we say that Sh consists of all function. Now we want to say something more about right? And actually, about hs itself and one of typical ways in which to do this in the context of finite elements is to say something to make a statement about the so, so to speak the regularity of our function space. Okay? I'm going to use certain notation and we'll come back and clarify this notation in a little bit. Okay? The way we'd introduce this notion of regularity is to say that belongs to a space that I am going to denote as H1 on 0, L. Okay? Apart from the fact that it belongs to the space H1, I'm going to tell you what H1 is in just a little bit. Part from the fact that belongs to H1 we do require that it satisfies the Dirichlet boundary condition. Okay? Right? So straight away, we've gone to this finite dimensional space Sh. And not only have we done that, but we've also said something more about we've, we've set ourselves up to say something more about what the space Sh is like. Okay? All right. So you want to find living in, in this finite dimensional space Sh such that. For all wh, okay? Again, the weighting functions, we are not, we are no longer pulling out of this big space V. We're taking this also from a reduced dimensional space, a finite dimensional space. Wherever we like for S, we do want Vh to be a subset of our original space V. Okay? And Vh, we will tend to say consists of all functions wh. Also belonging to the space, H1 on 0, L. Such that wh at the richly boundary Vanishes, okay? All right. So, at this point, this is purely cosmetic. Right? All all we've really done is to, is just to say that okay, we're thinking of finite dimensional spaces. I've introduced some cryptic notation by talking of H1 and I haven't yet told you what H1 is, but I will. And furthermore, I'm denoting all our approximations, all our finite dimensional functions and finite dimensional spaces by this sop H. Okay. So we're doing this, what do we want? We, we want that the weak form still be satisfied, except that now the weak form is an approximate weak form. It is a finite dimensional weak form. Okay? And it's finite dimensional, because it is computed with these finite dimensional functions. Okay? The first stone looks just as before, except that all functions are evaluated with wh, sorry, not with wh, but with the finite dimensional law versions. Okay. On the right-hand side, we have wh times f A dx plus wh at L t A. Okay? This is it. This constitutes our finite dimensional, our finite dimensional weak form. Now because in the mathematical setting this manner of introducing finite dimensional spaces is ascribed to a Russian mathematician called Galerkin. This is also called the Galerkin weak form. Okay? I'll, I will tell you in a little bit, what H is. Not right away in this segment, but in a little bit. H, H as you can ima, as you may imagine or as some of you may know bears a relation to to our finite elements when we introduced it. Okay? So we'll talk about that in a little bit. Let me see. Is there anything else I need to say about this right now? Oh, I believe not. So, so this is essentially the form in which we are this is essentially how we're going to proceed. All we've done here to see that is to observe is that when we wrote out the weak form originally the one that's completely equivalent to the strong form. We had in the mind that and we had in mind that infinite dimension of function spaces. And we recognized that can prove to be that can prove to be challenging to find solutions in, because we're looking for solutions in such large spaces. Instead, we now restrict ourselves to finite dimensional space, then saying that, that right there lies our approximation. Okay? However, we are still going to solve the weak form as, as written. Importantly as well, our finite dimensions spaces that we are, that we are setting ourselves up to use are subsets of our original larger spaces. Okay? Okay? These are subsets of our original, larger spaces. Okay. This is a good point to stop for this segment. When we come back, we will expand upon these ideas. All right?"
NGFDgAV86uo,"So there was a question about use of the finite dimensional weak form. And specifically, the question was now that we're talking about using a finite dimensional weak form is it still equivalent to the strong form. The short answer is no and if that causes panic, don't panic. Because after all, we are talking of approximations. All right? So, if we were sticking with something that were completely equivalent to the strong form, we would not have moved on at all from an exact statement of the problem. Okay? And we know that an exact statement of the problem implies an exact solution, which is often difficult to find. The whole business of finite element methods or any other approximate method is to look for approximate solutions. And the way we approach the approximate solutions is by freeing ourselves so to speak, from the, from the tyranny of exact solutions. Okay? There, there, there is more and, and, and, and let, let me state it here. So let me start out by saying that the, the finite dimensional weak form is not equivalent. To the strong form in general. Okay? Now qualify that in general, as well. I will just say that there is more. In order to understand what more there is let's, like, recall the way we proved the equivalence. Okay? So recall, recall the infinite dimensional Weak form. Okay? And I'm not going to write it out in full detail, but, but essentially recall that the way it goes is that we say, find u belonging to S, where we know what S is, such that for all w belonging to v. The following condition holds right, w comma x sigma Adx integral zero to L equals integral zero to L wfA dx plus w at LA. Right? And then we proved that this was equivalent to the strong form. In proving that it was equivalent to the strong form, we very critically used the fact that the weak form holds for all w belonging to v. This was what allowed us to then, see that. Well, if it holds for all w belonging to v, it also holds for a very special w. Right? And if, and for it to hold for that very special w, we realized that the implication was that the strong form had to hold. This le, leads to weak form being equivalent to the strong form. All infinite dimensional, right? And both of these are infinite dimensional. Infinite dimensional. Okay? We've abandoned that now, right? In what manner have we abandoned it? First of all, by saying that now we are looking for solutions belonging to a smaller space, Sh. And furthermore, that the finite dimension the weak form holds for certain weighting functions. Wh belonging to smaller space vh. Okay? Now we're seeing, we're seeing that belongs to Sh and wh belongs to vh, which is a subset of V. Okay? As a result, we, we, we have lost the ability to invoke the argument that the weak form holds for a sufficiently large space of functions. Okay? And therefore, we cannot always make the argument that the strong form is implied by the weak form. Okay? But that is the nature of our approximation, right? So let me just say, therefore the finite. Dimensional weak form does not imply, okay? Does not imply the does not imply the infinite dimensional strong form. Okay? It does not imply the infinite dimensional strong form in general. It could, however, imply the infinite dimensional strong form. If it turns out that the problem that we were looking at in any case, because of the data supplied to us. It did, indeed have solutions that lived in the smaller space, Sh. Okay? In that case, we are, it would work. And in fact, we will see that the finite element method has a very powerful property to make that work. We will see more of that when we understand the, the convergence properties of finite element methods. But for now in general, yes, the, the whole point is that we have chosen to adopt approximations and these approximations imply that our equivalence between the weak and strong forms is lost. But we do need to give up that the, the, the, the insistence upon that equivalence. Because otherwise, we would only be locked into only finding exact solutions."
x8zxnXLhW_0,"Okay. Welcome back. We'll continue with our fleshing out of these ideas of the of the finite dimensional weak form. In particular, what we are going to do is say a little more about these function spaces from which we are drawing the finite dimensional functions. Okay, so what we're going to do here is talk a little bit just a little bit not too much and not at all in a formal way but hopefully in a rigorous manner. We will talk about function spaces, all right? And in particular, the kind of functions spaces we are going to consider are called Hilbert spaces. In the branch of mathematics that is usually referred to as functional analysis, okay. So recall, the form in which we specified And Vh. Right, we said that Belongs to a space which consists of functions which are drawn from a group of functions that I denoted as H1. And apart from that, we said that Just needs to satisfy the boundary condition. What we are going to do in this segment is focus upon Focus upon an introduction to the ideas that are behind these sorts of function cases H1, 'kay? And in order to do that let's do the following. Supposing we consider, The function u which actually, let me not use u just to avoid any confusion with the either the exact solution or approximate solutions. Let me just see, it's considered function v, okay. Consider a function v which is a mapping from the, from the open interval 0,L to- The real space, okay? And if you recall, when we first started talking about the exact solution u this is exacly how, this is indeed how we wrote it, okay? So, If we have a function v I will define the following quantity, 'kay? I will define. I will define the function v to be an L2 function. If the following holds, okay? Supposing we take the function v and we square it, okay? We square the function and integrate over the domain zero to f, okay? Now, if this integral is less than infinity, okay? Then, we say that v belongs to L2 on, the open interval 0,L. Okay? And I realize that what I wrote finally is just sort of restating the first line. As I said define the function v to b and L to function if that is less than infinity. And then, I said, the n v belongs to delta theta, okay? But essentially what we are saying now is that the function squared integrated over the domain is bounded, okay? An alternate way of saying is that the function is squared into , okay? V is L2, if it is square integral, okay? Now, we're working in 1d so things are a little easy in 1d. Let's look at what is allowed in L2, okay? So examples. Okay? V of x equals to a constant is obviously allowed. V of x equals some polynomial, right? So we could look at it as v of x is equal to sum over k. Maybe a sub k, x to the power k, where k is now summed over 0 to n, okay? This is also allowed, v of x equals h of x minus x not where h denotes the heavy side function. Okay, so h of x minus x not is the heavy side function located at x not, 'kay? So this is the Right, so this, this function looks like this. If this is x and this is v and that is the point x not then v is that function, right? Okay, this is also acceptable. And why is the, for, for each of these functions you observe that if you squared them and integrate them, you will essentially end up with a constant, okay? You'll end up with a number which is in fact, which is, which is it's a real number. It's, it's there for , it's less than infinity so it, so the function is L2, okay? Can you think of a function that is not L2? Right, think about it. The delta function, right? The the dirac delta function is not L2. So all of these are L2, okay? If you have the dirac delta function v of x equals delta at x minus x not, okay? This, this function does not belong to L2 on 0,L, okay? Okay, that does not belong to L2. And why does it not belong to L2? For it to belong to L2, you need to be able to square v and integrate it but the square of a delta function is is vacuous, right? It's it's, it's it's meaningless, it's, it's not even defined so the function does not belong in L2, okay? So these are, this gives us some idea of what is implied by, by L2 essentially it is telling us that the function is square integrable 'kay? And it gives us a basic notion of the boundedness of a function importantly it tells us not just how the function behaves at, at in that a particular points over its domain. But it gives us a an, an integrated sense of the behavior of the function over the entire domain, 'kay? And this is the important property that the integral delivers for us, okay? Now in general, one can define Lp functions where p is some arbitrary real number actually, okay? But that gets a lot more complicated and, and involved, 'kay? But let me just state it, one can can in general define Lp functions. Right, where p itself well, p, p could be integers of and actually p could be real numbers. In fact, there even is a, a definition of an L infinity function, 'kay? But we, we don't need those details for our basic introduction to finite elements, 'kay? So, so that is, is one type of function that will be of use to us. And you observed in defining L to functions we just talked about the function itself. However, when I introduced the, the notion of finite dimensional function spaces and I refer to this space H1 which I'm going to define now. And that step will bring us to the notion of regularity of functions. And the notion of regularity again, is just a way to talk about how the function behaves with its derivatives, okay? So, let me just pose this question in a very colloquial manner, okay? So, how about control Over the derivatives of v, okay? And this is this whole notion of regularity. 'Kay? All right, and I, and I say control over, over the functions because now, you know, when, when we've said so far that the function itself is squared integrable, that's an L2 function. In a sense, we are saying that yeah, the function itself is sort of controlled in some manner, right? It doesn't, it's not, it doesn't get too big for instance, right? It doesn't get unboundedly big, okay? So we are controlling the function itself but we haven't said anything about it's derivatives. We're going to do that now, okay? So we say that v of x belongs to H1 on 0,L if our integral, 0 to L, v square plus v comma x the whole square. Now, we need to do something here. As you can imagine, when one gets into physical problems, right? Where v would mean something with physical significance, like it could be a displacement. It could be a temperature, it could be a charged field or something like that, right? A charged distribution or something like that. When we then take spatial gradients of such functions, we know that we, we're changing the units of the functions that we're dealing with, right? The physical units of these functions, so in order to count for that fact and make sure that both, both terms in this integral have the same units. For dimensional purposes, we multiply this by L square, where L is just the measure of the domain, okay? Okay, so we have this and now we're integrating this integrant over the domain zero to F, okay? So v belongs to H1 if the square of the function plus the square of the functions derivatives, 'kay? Or in this case the square of the functions first derivative when integrated over the domain remains bounded, okay? So let me introduce a remark and this just paraphrases what I said about introducing L. L has, or L squared in this case, has been introduced For dimensional purposes. Right? In general, If you don't want to write L squared or if you don't want to write L. It's common to write m for measure of 0,L this equals L, okay? Where m here is the measure of our domain, okay. In one dimension the measure of a domain is simply the length, okay? Well actually I, I need to say it instead of just being the measure it's gotta be the measure to the power one over d where d is the spacial dimension, 'kay? All right, so if you were doing, if you were in 1d, d would be equal to one. If we were in three dimensions, d would be three and then the measure would be actually the volume, right? But the volume to the power of 1 3rds would again give us the notion of it length."
bdU1Bab05Po,"So if we were working in three dimensions, in all three, d equals 3. And then, the measure of our domain, okay. Our domain may be in gentle, be referred to as Omega, okay? Omega to the power of 1 over d would be measure of Omega to the power of 1 over 3. And, this would be a. Length. Okay. So this is how we would extend this sort of idea to multiple dimensions. Okay, but then let's get back to our H1 functions. So, v belongs to H1 on 0 comma L, if the integral of the function square plus now let me write it differently. Measure of 0 comma L square v comma x square, the whole thing, dx less than infinity. So we require that the function itself, and its first derivative squared integrated be bounded, and H1 there simply refers to the fact that we attempting here to control not only the function itself, but also its first derivative. Right. If we go on, and want to control also the second and higher order derivatives, then we would accordingly go to higher orders of H. Right, we go H2, H3, and so on. Okay, so now you observe that when we say that. Now, when we see that belongs to Sh, which where sh is equal to functions belonging to H1. When we say this, what we are saying is that is such that we expect that is such that the function itself be bounded, okay? Because it is square integrable, the function itself as well as its derivatives, okay? And as you can think, now as you can imagine now, we're saying something about, about the nature of the displacement field that we're expecting to find to, to find as our, our approximate solution. Okay? So, what we are saying here is that and comma x are. Square integrable. Okay, let's look at functions that are H1. Okay. Okay, so examples. Okay examples of H1 functions. Okay, so are the pcv of x equals constant, no problem, okay? If we say v of x equals sum k equals 0 to N, a sub k, x to the power k. Okay? This is also all right. There is a third type of function that I want to consider and I could write it out in form, int as it in, in the form that I've written out the first two functions here, but let me instead draw it. Okay? Let me instead sketch it, because it's, it, it, will still convey the same idea. Where as defining it would, would require us to define things that we will need only later. Okay, so let me suppose that v is of the following form linear, maybe quadratic, linear again, some other polynomial, linear again, constant, okay? This sort of function also belongs to H1 Okay. It belongs to each one because clearly, the function itself, if you square it and integrate it, is bounded. At points such as this, you observe that v comma x, okay? Is v comma x suffers a jump, okay. Those double brackets there indicate a jump in a function, okay? We know that there is a jump in the function there. Because if we look at the derivatives, right, right? We know that the derivatives are going to look like that or the derivatives are going to be, are essentially the tangents to the two sides of the curve, right? Those tangents are not the same right? And therefore we, we know that the function itself suffers a jump. However, these continuous functions can still be squared and integrated. Okay, in fact, we consider in exactly such a case in, in the case of L2 functions, right? We did consider the, the Discontinuous function itself, the Heaviside function. And, observe that, yes, to belongs to L2. So, Heaviside function, Discontinuous functions can be squared and integrated. That's what happens with b comma x when it comes to defining a 2v in H1, okay? Right. However, what about a function, which has this form? All right, let's do, let's do something that looks pretty much like what we have there. What about this function? Right, as we saw in the case above we see that yes here, the tangents are different, right? So there is a jump in v comma x. But out here, what we are seeing is that, v itself suffers a jump. Okay, and therefore, what happens when we consider v comma x? V comma x, we know goes as the a delta function, which would be called at this point x not, right. We know that d comma x is, is like e, delta of x minus x not. Okay? And that's where the square root of integ, integrability of the derivative of three breaks down. Okay? So in this case, we have v does not belong to H1. Okay. All right this is probably a good place to end this segment. We've essentially gone over the definition of our very basic function spaces, introduced these ideas that we, that we need here. Ideas of square integrability of the functions, and of their derivatives, and this idea that when we specify these functions to belong to certain spaces, these Hilbert spaces L2 and H1 by the way are Hilbert spaces. We gain control over the functions themselves, and their derivatives."
7N5YSuBZMZM,"Okay, welcome back. We continue now with working with the with the finite dimensional weak form or Galerkin weak form. What we saw in the last segment is was a digression to say more about what we meant by function spaces and to understand why this might be important for us, okay? Because and, and, and, you recall that we defined function spaces as being useful or, or we introduce function basis as being useful in order to give us a sense of control over the functions themselves and over their derivatives. Right? It's when we say that we have control over the functions we are saying that the functions are bounded. When we have control over the derivatives in addition to the functions we have this notion of regularity, right? That the functions are sort of smoothed and so on. And we saw that through examples. Okay, with that as background now we will actually launch into the finite element method for this 1D linear elliptic PDE. Okay? And this would be, an we are going to work off the Galerkin, or the finite dimensional weak form. In this segment, we are finally looking at the finite element method for linear elliptic PDE's in one dimension. >> Now, we're going to work off the Galerkin weak form. Okay? Recall the Galerkin of the finite dimensional weak form. One more time unto the breach. Okay, so one more time we're going to write this out. Find u h belonging to s h which is equal to s h is equal to functions u h. Now we're seeing that they belong to H1 on (0,L). Such that they satisfy our boundary condition. Now gratifyingly, we know what H 1 means. So, we already have an idea of the extent to which we've restricted our, search to solutions for this PDE. Okay? So recall all the functions we talked about in the previous segment. Those are the sorts of functions we are now, considering as candidates, okay? Okay. Actually, for this reason, u h is often also called the space of trial functions. Okay? So, often, u h is also called a trial function. Because we say we're going to try out all these functions living in s h, okay? So maybe that's a useful thing to say here. Okay. Find u h belonging to s h, such that, for all w h belonging to v h which is also drawn from H 1 Okay? Such that, for all w h belonging to v h, the following holds. Integral zero to L, w h, x sigma h A d x equals integral zero to L, w h f a d x plus w h at L, t a. Okay? That is our finite dimensional or Galerkin weak form. Now one thing I should point out here is the following. In our finite dimensional weak form, observe that we have sigma h, all right? In that finite dimensional weak form. What we imply is that sigma h is going to be obtained as E u h, x. Okay? However, f, our forcing function f, is given to us as data. So we assume, and in fact we, we, we will in fact use the fact that, in our implementation of the finite dimensional weak form. We are not going to attempt to, to approximate the, the data, right? We are going to represent the data exactly, exactly as given to us. So f, f of x is not, finite dimensional. Okay? f of x will actually be represented as it is given. Okay? In this sense, you may think of it as it's not exact but, but, but the fact that it is data is why we are not going to represent it as a finite dimensional approximation. All right? It's given to us. Okay? All right. And, and of course in this the case, does the question arise for t, the traction? Think about it. It doesn't in 1D because in 1D the traction is applied at the point. So the question of whether we approximate it, whether we write it as finite dimensional or not doesn't really arise. We go to higher dimensions and we go to three dimensional problems we will see that, there is a question to be answered for the traction. Okay? Because the traction will then be defined on the surface and so on. But, but we get to it when, when we do. Okay? So t, let me just state, is a point value. Okay? And I'm stating all these things for us to understand, why in our finite dimensional weak form, we have finite dimensional versions of w h. We also have a finite dimensional of sigma h, right? And that's this because sigma h is going to be obtained through our constitutive relation applied to the gradient of the finite dimension of trial solution, u h, okay? And on the right-hand side, again, the waiting function is finite dimensional, not so for the forcing function which is data. And it's actually not a relevant question for the traction in this one d setting, okay? So, that's what we have, all right? So that explains why we have final dimensional forms w h,and sigma h. All right, so, we can write out this finite dimensional, or Galerkin weak form, if we say, how we obtain our finite dimensional functions, okay? So, that's really the question. How do we obtain u h and w h, right? Alternately. Alternately, how do we obtain S h, and V h? We need to define those. All right, we need to say, what kinds of restrictions we're using on our finite dimensional functions, okay? In one sense, the finite element method could be thought of, as simply defining what these finite dimensional approximations are, okay? So, here, here's how we do it, okay? The way we do it is to I'll write the statement here, and then I will start sketching things. What we do is partition 0, L into finite elements, okay, which are disjoint. Sub domains of 0, L, all right? Okay. So, here's, here's how it goes. Let me just draw our bar, one more time. And here, we have our x-axis. And that is L, that is 0. Now, for brevity, as well as to make an easy transition to multi-dimensional problems, I am going to introduce notation here for this domain 0 to L. I am going to write omega is our open interval 0, L, okay? So, that is our general domain. Okay, so, the way finite elements proceeds is to partition our domain omega into sub domains, right, which are our elements. And this partitioning is done by nodes that I have, that I'm now marking here, okay? So, we look at these nodes as being defined as x we call this x sub 1, x sub 2, and so on, x sub N, right? Sorry I shouldn't call that N, I'm going to use that for another, for something else. I'm going to call this x sub, call this actually a K, big K, because I am going to call this very last node x sub N, okay? Or I think, I'll even call it, x sub number of nodes, okay? All right, now, each of these sub domains that we've thereby, defined with these nodes. These, these are going to be our elements with the finite element method are omega 1, omega 2. In general we have omega e, right, and so on, okay? Observe that what we've done here is, what the partition that we've introduced. Is the following. We've partitioned omega into sub domains omega e, all right? And the nature of this partition is such that, the total that our entire domain omega is the union of each of these sub domains omega e, all right? Furthermore, each of these sub domains omega e is an open sub domain. Okay? It's an open interval, right? So, the total domain omega is the union e equals 1 to n sub el, which for very obvious reasons is number of elements, okay? So, the domain omega is the union of each of these sub domains, omega sub e, and e here, runs from 1 to n el. Now, because omega e is open, okay, and because we have a very convenient one dimensional setting, what you will note is that each omega e is the open interval of x sub e to x sub e plus 1, right? Clearly, because omega 1 is the Omega 1, which is this one. Is the open interval, x 1 to x 2 and so on, okay? So, now, there's one more thing we need to do here for technicality. Because each of these omega es is an open interval and because we would miss out the, intervening the inter-element nodes, if we just went with this union, we need to say also the following. We need to apply closure to this union. Okay? And say that once we close that union, we also have the closure of our open interval. Okay? So this way we are making sure that we, that we are not missing any points. This is for purely technical purposes. When it comes to computing, it makes no difference because each of those points is what we call a set of zero measure. Okay? So let me just state the, the notation here. So omega bar is the closure of omega. All right? So, it is basically saying that omega bar equals, omega union the boundary points of omega which introduce even more notation is written as that, okay, partial forming, reverse the boundary forming. These are purely technical points, but it's useful to state these now so that there's no confusion later on about what we are doing with nodes, and so on. Okay? All right. So. We have, this, partition. Let me introduce, terminology which have been using already, but let me define the terminology. So. The points x e are the nodes of the partition. Right? Omega e is an element. Okay? Obvious notation, obvious, nomenclature which I'm sure you all either knew, or figured out, or anticipated or whatever, but here it is. Okay? All right, so how do we first, use this partition, how do we begin using it? As the very first step we observe that our weak form has, is stated as an integral. Okay? And what that lets us do is to write that integral as a sum over the elements, right. Over the sub domains of the partition. Okay? So, the weak form or the, the  weak form, what this actually holds for the, for the infinite dimension weak form, as well. All right, the weak form. We will now write as follows right? Instead of writing it as a sum over zero to L, we will first write it as a sum over omega, okay? W h comma x sigma h A d x equals integral over omega, W h f A d x plus W h at, L t A. Okay? So we've gone from writing it as an integral for over zero to L to an integral over omega. Purely notation. The next thing we can do now is to write this as an integral over each element subdomain, okay? Like that, okay? But note that we need to get to all of omega, right? So here we take a sum over e, okay, where e runs over all the elements, one, two and e l. One to number of elements. Okay? And here, too, we have sum e equals one to a number of elements. Integral over omega, omega e W h f A d x plus W h at L t A. Okay? So this is the first that our, partition of the domain into element subdomains lets us . What that, the next step that it also makes possible for us is to now think in terms of defining our, finite dimensional functions. But observe that having reduced those integrals to integrals over smaller subdomains, really, all we need to worry about is how to represent these finite dimensional functions over the subdomains omega e. Okay, so let me just write that here. So, represent. S h and v h over. Each omega sub e. Okay? So we can really afford now to focus on what's happening within each element within each subdomain. Right. With the idea being that the union of all the subdomains are gives us the, the, the complete, the entire domain that we're interested in. And thereby we can also define our finite dimension functions over the entire domain, okay, by going to these sub domains. Okay, let me, it's useful to stop here. When we return, we will think in terms of how we define these functions over the subdomains."
lMj8hv0HIOo,there was a question on apply closure to our full domain Omega and to power up to the union of our subdomains so let me revisit that little bit and clarify something so let's recall let's recall the finite element partition okay so we know that we have these subdomains Omega sub e and each subdomain Omega sub e is the open interval X e 2x e plus 1 ok and here's the picture we have here we have the point 0 that is the point L write x equals 0 and x equals L we denoted this point as X 1 and we have other nodes here ok that is X 2 and so on for a general subdomain which is an element Omega E we have X E and we have X e plus 1 ok and as the manner in which I've written Omega e it is clear that it's an open interval now recall as well that Omega the domain of interest is this open interval okay so where is all mcgurk does contain the nodal points except for X 1 and X believe I call that X n but we know yeah that's X okay what while on the girl contains all the nodal points except the first and the last okay each Omega E does not contain its own nodal points all right so Omega contains the points X 2 X 3 so on up to X N - one and I note that we since we're using any L for a number of elements we also have the following relation right we also know that the total number of nodes is equal to number of elements plus one okay let's just remember that because we made later on we will want to use maybe any l2 number everything ok so this is the important point Omega does contain these nodal points right but it clearly does not contain X 1 and X n okay so as we can see there is you know Omega and the union of all the Omega eases are closely related but they're not quite the same okay in particular what we can see clearly is that Omega itself is not just the Union over e of the Omega is okay why is this because each Omega E does not contain its own nodal points whereas Omega does contain the interior nodes alright the only way we can connect them we the only way we can relate our meager to a union of the Omega YZ is to make sure that we pick up all the nodal points okay the nodal points by the way are more can are otherwise referred to as the limit points of the corresponding open sets okay so the way we can do this however is to see now if we apply closure to Omega what we make sure is that it does indeed pick up the nodes X 1 and X n okay and in order to make sure that it is equal to some other closure what we do is to go back to our union of the element subdomains since each of those subdomains is an open set and these are disjoint open sets we know that this Union on the right hand right of the blast equation this is a secret equation by the way is not yet complete we know that what I have on the right hand side is still missing all the nodal points now when I apply a closure there I make sure that I pick up all the nodal points including X 1 and xn okay so in this form so this contains all nodal points X 1 all the way up to X n okay it's really a technical requirement because it turns out that when you do the integrations the fact that you may be missing just one point does not really make a difference to the type of integrals we will consider in the problems of interest it's a technical point but then it's it's an important one to to recognize just in case it leads to confusion later on ok
m-DqA1jsMUw,"Okay, so we are going to continue with our development of the Galerkin weak form.  For this  one-dimensional linear elliptic PDE. Let's begin by recalling where we were. So we've got as far as here. If this is our domain of interest, or this is the physical problem we're looking at, we have this bar we have our x-axis, positions L, 0, 'kay? And this is our domain omega. Without those endpoints. What we did already is to partition this by the use of points that we're calling nodes. We've partitioned it into subdomains, each of which is denoted as omega sub omega e, 'kay, indicating an element. The nodes that we have used are labeled x1, x2 and so on. In general, this would be the point xe and that could be the point xe plus 1. The point with x-coordinate, x equals L is node, N number of nodes, N nodes and that indicating number of nodes. And we also observe that the number of nodes is equal to our number of elements plus 1 in one dimension. Okay, in one dimension, also, for the very specific manner in which we are partitioning omega for this simplest of finite element examples, 'kay, simplest of finite element cases. Okay, this is as far as we we've got, and what this allows us to do is to write the Galerkin weak form. As follows. We can now write it as a sum over e, going from 1 to a number of elements open integral over omega e of Wh,x sigma hAdx equals sum over e going from 1 to number of elements, integral over omega e WhfAdx plus Wh at L, which of course is our last node, times t, which is the traction times area. So this is our Galerkin weak form. Now observe that each integral really is redefined in this expression as an integral over the relevant element subdomain. What we need to do to proceed is to talk of how we're going to write out e representation for our trial solution. Right, our trial solution of x and the waiting function. Okay? This is what we need to do. Now, in doing this observe that because of the fact that we're able to use this decomposition of our integrals over these element subdomains, we really only need to worry about how to carry out this representation over an element subdomain. Okay? So, let me state that. Need to represent and wh over omega e, right, where, of course, e goes from 1 to number of elements, right? So we have the advantage of needing to focus only upon what often gets called a local representation here, right, local being local to the element, okay, all right? Okay, so let's look at how we can go ahead and do this now. So let's then focus upon a single element. That is xe, that is xe plus 1. And omega e. Okay, all right, so it's all with this subdomain that we need to write out our trial solution and waiting function. The way we can do this now is to define what we will call local basis functions. We define local basis functions on omega e, okay? All right. And what you are going to observe is as you may expect there is going to be a finite number of these local basis functions, okay? So we define local basis functions on omega e. And let me also state that we will define a finite number of basis functions.  Because we have the advantage of working just locally over the element's subdomain, we're going to focus on this element's subdomain, define a finite number of basis functions over that subdomain. But then, if there is a finite number of basis functions over a single subdomain, and there is a finite number of subdomains, what we are going to observe, of course, is that there is a finite number of basis functions over the domain omega. Okay? So, we will have a finite number of basis functions over omega e and therefore. Over omega itself. So, we have a finite number of basis functions. Naturally, of course, our weak form is going to be finite dimensional, right, so that, that's the connection, right? So when it comes down to defining a finite number of basis functions we, we are assured of having a finite dimensional weak form, the Galerkin weak form, 'kay? This is how we want to think about. All right, so let's talk about what these basis functions are going to be. We are going to take in this initial entry-level, very first presentation of a finite element method, we are going to take the simplest possible basis functions, 'kay? We have an element, and it is by choice, by the way, that I have chosen an element to have only two nodes on it. We are going to use those two nodes to write out to expand fields of interest over the element, in terms of just two basis functions, okay? Okay, so what we are going to do is the following. We will define two basis functions. Over omega e, 'kay? These basis functions are going to be polynomials. Okay? And since we are in one dimension, if we have two basis functions and the basis is complete, it follows that we are talking of representing functions as what type of polynomials? Think about that. If you have a complete basis, which you're going to use just two basis functions in one dimension with polynomials, what kind of polynomials are we able to represent? 'Kay, let me just add on that bit about complete basis. Well, you probably got the answer. What this implies is that we are essentially going to be representing functions here as linear polynomials. On omega e, okay? This is the simplest sort of polynomial basis function we can a, adapt, in the most standard type of finite element method. There are finite element method in which we can even use constant polynomials they need a little more work than we need to do, than we should be doing at this early stage of developing the finite method, so, so we go with linear polynomials, 'kay? All right let's actually write, write these down now. So the way this works is, is the follows graphically, this what we will do. We have omega e and I'm going to write the noted coordinates here. Our basis functions are the following. They're linears, right? That one and that one. This is N1 for obvious reasons, that we're going to call N2, okay? So, N1 is a function of position and N2 is also a function of position, okay? With these, what we will be able to do now is to write out our finite dimensional trial solution, okay? And for the very first, just, just for this case, I'm going to specify that this is the finite dimensional trial solution over element e. 'Kay, restricted to element e, and obviously it is restricted to element e, because it is over element e that we've defined our basis functions. Okay? So, the way we write it out in this case is sum A going from 1 to number of nodes in the element. N sub n sub e, is for number of, the capital N is for number, the little n is for nodes and e is elements, okay. So we have two nodes in the element, so n and e is equal to 2, 'kay? So the way that we write this is then NA functional position times some degree of freedom that has been interpolated. And we will write that degree of freedom as d A sub e. Okay, which in detail is simply N1, functional position times a degree of freedom, sorry, that degree of freedom should be 1. Degree of freedom 1 corresponding to element e, plus the basis function 2, function of position times degree of freedom 2 corresponding to element e, okay? So in this what we have is that NA of x is the basis function, all right, and dA of sub e is the degree of freedom, 'kay? 'kay. Also this is NA is basis function A, where A equals 1,2. And dA sub e is degree of freedom A equals 1,2 on element. E, okay?"
96IDHPmiBN4,"All right. So, again, on this light, u h, u h over element e of x is sum A going from 1 to number of nodes in the element. Na functional position d to the a e. Now, because of the way we do this, you are observing that the degrees of freedom and the basis functions which I'm drawing are fundamentally associated with nodes. Right? Right, and then here also we know that we have a degree of freedom, right, corresponding to each node. We have d1e corresponding to that degr, to that node, and we have d2e, okay. And for that reason we have often also call Na x as being the nodal basis functions. Alright and d e as being the nodal. Degrees of freedom. All right. So we've got this far with talking about how we are going to write out this expansion of our finite de, finite-dimensional trial solution over a single element. We do the same thing also for the weighting function. Right? So, similarly. For wh function of position also restrict to element e. Okay, restricted to element e because that is where we are defining it so we do the same thing. Wh of e, x equals summation e going from one to number of nodes in the element. Which in this case, because we are looking at the simplest possible finite element formulation, that number of nodes in that element is two. We have NA function of position. Now, the degrees of freedom for the weighting function will be obviously different from the degrees of freedom for the trial solution, right, these are different functions. So, though we're using the same basis functions for the weighting function and the trial solution, we have to use different degrees of freedom, if not they would exactly the same function, right? so we have c a e, okay. We are in this case again, this is the degree of freedom, dof for short of the waiting function All right, okay, lemme see. There are many things to be said about this, but first of all, this is our most elementary sort of approach to writing out a Galerkin weak form. When we have the same sort of basis function for the weighting function, as well as for the trial solution, right, this is called a Bubnov-Galerkin method. Okay, and what this means, is that, that same basis function. For Of e and Wh of e. There are other, more advanced finite element methods that for good reason, because of certain things to do with the mathematics of the particular equations being solved, do not use the same basis functions for the trial solution and the weighting function. And they're called Petrov Galerkin and we won't get to them in this, series of lectures. Okay, so all right. Let's move on here. Now, now you will note that all I've done so far is to sketch what these functions are going to be. I, I've called them linear function and so you can image what they are. You can very well construct them in terms of position over the, in terms of the x coordinate over the element. But it it turns out that we can do better, we can do something much more systematic. Okay? And here's how we do it. We first take this approach where we say that this element of interest which has coordinates Xe and Xe plus one. We regard this as being the physical representation of the element, right? Or this is the element in the physical domain. Okay? So we call this the physical domain. In order to construct our basis functions we always think of any element of open arbitrarily element has been constructed from a mapping, right, from a different domain. And in this domain the element is going to look similar, okay, except it is that. I'm not going to use X for position along this right, where, whereas I have X for positional in the physical domain, I'm not going to use X in the, in this domain. In this domain I'm going to use z. Right, as a coordinate. And this point here, the node on the right is z equals one. The node on the left is z equals -1. And the mid point is z equals 0. It's important to note that I have not introduced three nodes. The nodes that I think of are still z equals -1 and then z equals 1, okay? I just marked the midpoint, the midpoint is not a node, okay? So these are the nodes. Right in this representation. Now this is called various things. Some people may call it the mathematical domain, I call it the bi-unit domain. Bi-unit because I'm saying the length of my element in this domain is, goes to minus one to one is two, right? So it's two times one, so it's a bi-unit to me. Now we always think of our actual physical element as being constructed from a mapping, as in mapping from our bi-unit domain. okay. Why do we do this? We do this because it becomes very convenient to define things like our basis functions and later on to carry out integrations if we have this idea of the bi-unit domain. Okay, and in fact, here is what we'll do. Our basis functions, N1 of x. We will write as N1 of x, but really x, remembering that x is a mapping from this bi-unit domain, right, x is a function of z. And likewise N2 x equals N2 x mapped from the domain, from the bi-unit domain. Okay? Now, strictly speaking, there is an, an abuse of notation going on here because I cannot have a function parametrized in terms of x and then use the same symbol for the function and parametrize it in terms of z, okay? But I will nevertheless follow that abuse of notation because otherwise our notation just gets much too unwieldy. Okay. Alright, so now that we are in this bi-unit domain, it becomes very convenient to write out our basis functions. Here they are. N1 of z equals 1 minus z divided by 2 and N2 of z equals 1 plus z divided by 2. Okay? Completely clear what's going on here. We have z equals -1, z equals 1. That is our point z equals 0. If you just evaluate 1 minus z over 2 what we, you will observe is that at z equals -1, h takes on the value 1, okay? So, this is the value 1. Okay? And it goes to 0, x equals 1. Okay? This is our function and 1, sorry. That is N1. For N2, let me try and use the ability here to go to a different color. For N2 I have that. Okay? That is N2, that is again the point one, and you observed that when, where at xz equals -1, and 2 drops to 0. Okay, so I can just those facts here, okay. What we observed is that N1 at z equals -1 equals 1. N1xz equals 1 is 0. And 2 at z equals -1 is equal to 0. And, N2 at z equals 1 is 1. Okay? Now if we write these points z equals-1 and z equals 1 as let me call this point z1, and this point is z2, okay? Just corresponding to the idea that the node on the left in this bi-unit domain is node 1 for the element. And the node on the right in the bi-unit domain, this one, is node 2, okay? What that then lets me say, is that Na okay, whichever shape function, whichever basis function you want to use. By the way, basis functions are often called shape function. Okay? I just prefer the term basis function. Okay. If you cha, if you take basis function a and you evaluate it at zb, this, is, the Kronecker delta. Okay? And you know what the Kronecker delta does, right? Kronecker delta is equal to 1 if A equals B, and it's equal to 0 if A is not equal to B. Okay, the basis functions that we have described here, have the so called Kronecker delta. Property. All right? Okay so we've got as far as describing our simplest one of, you know, indeed our simplest basis functions for this finite element formulation. The simplest of finite element formulations. We will continue with this, but this is also a good place to end this segment."
b8cYZqVUjCk,"All right, let's continue. So what we did in the previous segment was introduce the notion of the bi-unit domain and use that to define our basis functions, right? Specific-, and specifically, we are working with linear basis functions in this simplest of, finite element formulations. Right? And we've, we've gone ahead and defined those basis functions. And at the end of the segment, we also talked about. We observed that these basis functions have this delta property. Okay? So, these basis functions also have another property which is useful to note. Also. Right? So, let's do the following. Let's consider N1 at some arbitrary point z in the Bi-Unit domain. Plus N two at Very trivially or very simply, this is one minus xi over two plus one plus xi over two. Which is one. Ok? So at any point in the domain these basis functions add up to one. This is important because it, makes sure that we can at the very least represent constants. Okay? So this, this, this is an important property. The, the other thing I should mention now is that the way we've written these functions can be generalized and we will do this, later. We can generalize these to a higher order Polynomials generalize, generalizable, because I'm not quite sure that's a word. Anyway, it's generalizable to, higher order polynomials Okay. And, in generalizing them to higher order polynomials we are going to do it. The, the time has not come to do it. We'll do it later after quite a few more, segments. But when, when we do, do that. We will do so by observing that these are linear basis functions of a class of polynomials called polynomials, okay. So the generalize-able to higher-order polynomial, and they are drawn from a family of polynomials that are called Lagrange polynomials. Okay? We do that later, not right now. Yet another thing, I want to point out about these basis functions. Let me see. That's a bullet point and this is another bullet point. Okay? Here's another bullet point. If we look at, these basis functions. And for just a minute, I'm going to go back to our physical domain, okay? So, let's suppose we have element here. This has, this is element e. And next to it we have element omega E plus one. Omega E plus two next to it. Right? And so on Now, we focused upon a specific, or and arbitrary element e in order to construct these, basis functions. And we, and we did that by going into the bi-unit domain. First of all, we need to, recognize that each of these elements in the physical domain is hooked in from this by unit domain, c equals minus one c equals one, and in this, this bayou domain we are going to denote as omega c. Okay, for, for obvious reasons. So note that each or, an arbitrary element, omega e or omega e plus one or whichever one we want, is always constructed as a mapping from the same Element in the bi-unit domain, right? The same parent element, as it is sometimes called, in the bi-unit domain, okay? Once we do this, we get our basis functions in the physical domain, and I want to draw them. which i want to do in a different color here. Ok, so in the physical domain, if we look at omega, element omega e, we've had this basis function, right? And that one. Ok, if we look at omega, at at that element omega e plus one We have also similar looking basis functions. But what happens here is that they are defined over element omega e plus one. Right, and so on. Right? Now If we look at, how this basis functions appear together, in, in the physical domain, let's focus on the fact that here we have no dome, x e plus one. We may choose to look at these basis functions in the physical domain as being located at global node X E plus one. When I say global node X E plus one, what I mean is that I'm looking at this node as being pa, one of the nodes in the overall domain omega. Right? So, I may look at x e plus 1 as a global node. Okay? And in fact it has global node number. E plus 1. Right? If you were to look at that same nodal point, in the context of element E. Okay? You would call it, local node number two For element E. Okay? Okay. It is also the local node number 2, 4. Omega E. Okay? And it is local node number one for element. >> Omega E plus one. Right, this is nomenclature that we'll use later on when we, when we, as we advance with our finite element formulation. Nevertheless, the point I wanted to make here is now when we look this collection of basis functions and we focus upon the individual nodes and view them as global no-, nodes on a sort of scale, right? But global node numbers What you observe is that we have for element sorry, for node x e plus 1, we have a basis function which I'm going to draw slightly off. I'm going to sort of emphasize it and I'm, I'm drawing it in red, but it's slightly off the actual line describing the basis functions, right? And, and I'm doing that just so you can see the underlying element. Level basis functions which when put together give us what may be called a, a basis function corresponding to node x e plus one. Okay? Now, the basis function corresponding to node x e plus one, which I will label like this, is the, basis function. For global node e plus 1. Observe that basis function corresponding to global node e plus one has a very local sort of sphere of influence. It is nonzero only on the elements that are immediately adjoining node number e plus one, global node number e plus one. Ok? So basis function for global node e equals one is nonzero. Only in elements, omega e And Omega e plus one. We may choose to regard the basis function correspond to global node number e plus 1, this red hat function I have, outlined here. We may choose to regard it as a globally defined basis function. However, its support is very local. Its support is local to the two elements in this case adjoining the node of interest, okay? So this property is referred to as local support. Sorry not local it's called compact support properly, sorry. Compact support. Compact support of Global basis functions. Okay? The idea here is that our In, in constructing our final element formulation, it is convenient to focus upon our element's subdomains. And by invoking this idea of the bi-unit parent domain, it gets to be very conve, very convenient and very clean to define basis functions over each element. However, we can step back and take the global picture, right? And recognize that, that by putting together these local functions, local basis functions from the elements adjoining each node, we can actually construct these global shape functions. Global basis functions, right? If we do that, what we observe is that the finance, that, that the type of basis functions we are using here are ones with compact support"
gucGXA6qhDw,"All right, so that's word making as a remark. And that remark is that the local definition of basis functions. Leads to global basis functions. Okay? Global basis functions associated With each node Say xe, all right? So, if we look at the global basis function associated with node e, that global basis function has compact support. Right? In elements omega e-1 and omega e, sorry, not omega e+1 it should be just make a e in this case. Okay? Now the fact that this global basis function is associated with the elements, adjoining a given node will be observed in higher dimensions as well. Okay? Because in higher dimensions, it won't be just to the right and left. There would be neighbors in other directions as well. Okay? All right. Fine. So this is another property of the types of basic functions we are defining, so now with all of this in place we have a clean way to write out our trial solution as well as our waiting function, okay. And it's useful now to go back and recall from the types of integrals that we need to evaluate in the Galerkin weak form. Just what we need to do with these finite dimension functions. Okay? So let's recall. So the kinds of integrals we need to assemble come from the fact that we have an integral, sorry, we have a sum over e of an integral over omega e of a function of this type. Right? The gradient of our waiting function x sigma h Adx equals integral, sorry, sum over e, integral over omega e, wh f. It's worthwhile remembering that f is a function of as defined with respect to x, which turned can be parametrized by c, right, using our bi unit dimension. Okay, this x Adx + wh L tA, I'm sorry, there is no dx there. Okay? So these are the integrals that we need to compute. Using these finite dimension functions. Now, one more thing I need to state as we go on, okay. And that is the following, an important note to make is this, okay? Consider element one and omega e, for e=1. Okay. If we have that as our domain and let's suppose that this is omega 1, okay? The very first element. We know that this point here is x1, which in this case, because of the way we are setting up the problem, is the point 0. Right? And this is element, this is global node number x2. Okay. Recall that here we have a Dirichlet boundary condition. Right, at x=0. So out here we know that uh=0. Right? It's equal to u nought but then actually let's just leave it as u nought. I'm sorry, I'm getting ahead of myself by seeing that 0. That's u nought. Okay? And yes we know that if we're thinking of this bar, we're thinking of it as an elastic bar. And if it is fixed on the left that u nought will indeed be 0. But for now let's just leave it as u nought. Okay. So that part is straightforward. What about wh there? What should happen with the waiting function at that point? Because it corresponds to the Dirichlet boundary condition. Right, there's something special that happens to it. Think about it. Yeah, wh goes to 0, okay? Always. Now it does not matter what the actual value of u naught is. I know I got ahead of myself a couple of minutes ago and said that u nought is equal to 0. It often is. Even if it were nought equal to 0, the fact that you have a Dirichlet bonded condition there implies that wh has to be equal to 0. Okay, that's how we've constructed our rating function space. What this tells us is that as far as the field wh is concerned within this element, we only truly need that basis function. Okay. Because we are already going to make sure thus in fact, choosing just a single basis function allows us to ensure that wh does indeed vanish at x=0, okay. And in particular we choose only N2 to make the basis function there. Okay? So, what this says is that now if I just pull out that element, okay, now we go on because we know that at wh at 0, and x=0, has to be equal to 0. What it tells us is that the only basis function we need in that element is N2. Okay, N2 of x is only basis function, fn, short for function for representing the waiting function, okay? It's not the case for the trial solution. Only for the waiting function, right? Do we need a single basis function, so N2 of x is the only basis function needed for wh of x for e=1. Okay? That's something to observe here. All right? However, for any other element, e=2, and so on, right up to number of elements in the problem. We have properly wh of x sub e equals sum A going from 1 to number of nodes in the element, NA (x) dAe. Okay, for wh of x where e=1, we don't need a sum, right, we have a single basis function, we just have that, and I realized that I use d here. We are using c to write out the degrees of freedom, corresponding to the waiting function. Okay? So there's a difference in the way we choose to interpolate, or to represent our waiting function, depending upon the elements. Okay? As something to think about, what if we had another Dirichlet boundary condition in our problem, what if we had a Dirichlet boundary condition on the right end, as well. Then for that element also we would do the same sort of thing right, we would use only a single basis function for that one. Okay, so that's just something to keep in mind, I won't develop that idea right now, we come back to it when we need it. Keep it, that's just something to keep in mind. Okay, what I want to do as well is look at what we need for our finite dimensional functions, right? Let me just do this for you. Just go back to this previous slide and recall that when we look at this finite dimensional weak form, that's written just below the word recall. You observe that we have To compute a gradient of our shape function. Furthermore, you observe that sigma h is defined as e times x. Okay? So we see the need to compute gradients of our functions. Here, however, we do not need anything. We do not need the compute gradients, right? And there, we just need to evaluate the function at L, okay? At any rate, what this tells us is that we need to compute. Compute or calculate wh, x for the weak form. And we also need to compute x and in particular x is needed, for computing the stress sigma h. Okay? So, how do we go about doing this? Easy enough. Let's just recall that, again, taking advantage of the fact that we have a local representation, right? So, we can look at our trial solution in element e, in a general element e. And, so I will write it as, sorry, not a summation over e, but a summation over a. Summation over a, 1 to a number of nodes in the element, NA, I will now write it as it is a function of c, okay? We know that we originally defined it as a function of x, but then, by going to our bi unit domain, we observed that it's more convenient to write it as parametrized by c. Okay, we have this, dAe, and we also have wh of e. Function of c is also = to sum over E. NAc, ce. Right. Well, we also need to compute those components. Right. And the way we can do that is just by computing NA, x in both those expressions. Right, because d, the dAe's and cAe's are just degrees of freedom, right? They do not have any dependence upon the position, right? So we observe that in order to compute these gradients of the trial solution and of the waiting function, we just go to our representation for the corresponding functions. We essentially need compute gradients of our basis functions, okay? So how do we do that? Well, we just invoke the change rule, right, so sum over A Right. So now we write. NA, x as NA,c, c,x dAe, okay? And this quantity is done in exactly the same way sum over A, NA,c, which is easy to compute, because we are indeed actually defining the Na's as being parametrized by c, okay? This time, c,x, cAe, okay? Now this is useful, right, because it tells us how we can compute our gradients of required fields, except for the fact that we may say well, how do I go around actually computing that quantity? Hardware, write the derivative of my position in the bi-unit domain with respect to my physical coordinate x, okay? Think about it. We'll come to it, but we will do it in the next segment."
Wlpa7OvTauc,"Okay, let's continue now. You recall that at the end of the last segment we've reached as far as observing that in order to proceed with our with, with computing the terms needed in our weak form, we needed to figure out a way to write out uh,x, which is sum over A. NA,xi, xi,x, dAe, and the same for wh,x. Okay? And, and in particular the question is how do we do this term, right? Simply because this one is easy, right? This this one is easy. Right, you already know how to compute the derivatives with respect to xi of the basis functions, 'kay? So let's look at how we compute xi,x. And in order to do that, let's just recall that what we've set up for ourselves is a description where omega e, right, and some arbitrary point x in omega e, where xe and xe plus 1 are the nodal positions, right? Some arbitrary point in x is always thought of as being obtained as a mapping from this by unit domain. Okay, now if we could figure out this mapping we may have a chance of figuring out what xi,x is. And what we're going to do now is do exactly that. We're going to define this mapping, okay? So the, we are going to define the mapping. To omega e from omega xi. It's an extremely straightforward manner in which we are going to do it, 'kay? What we are going to do here is the following. We're going to say that any point, x paramaterized by xi over element e is obtained as a, is obtained by representing it in a basis, okay? And I'm not going to complete the limits on that summation sign just yet, 'kay? We're going to write it as some basis function. Right, here, and I'm going to leave myself a blank spot for it times the nodal values, okay? So we're going to write the nodal values here as a let me think, okay, so I'm going to write these nodal values as xeA, okay? Now, we're going to have the sum running over the number of nodes in each element, right? So that is A equals 1 to Nne. When, when I write xAe, right, what I mean here is for A equals 1 I'll get x1e and I get x2e, okay? These are equal to my global nodes, xe and xe plus 1, okay? The coordinate x1e is the same as the coordinate xe. Right, but when I write xe, I'm viewing that as a global node, right, the coordinate is the same. Likewise x2e simply is the second node of element e. Well, globally that is just global node number e plus 1, okay? All right, because of the fact that we need to write this as a summation over the nodes in each element and there are, in this case, two nodes in each element which we're writing generally as N, sub n sub e, 'kay, we need to go to this other sort of numbering for local nodes, okay? So what I have in the brace brackets here are local nodes, and here, the same things appear as global nodes, okay? Those coordinates are the same. It was just that in one case, when we're just viewing a single element, we are regarding those nodes as local nodes, nodes local to that element and numbered 1 and 2 in that element, okay? When we look at them globally, we recognize that they're one of the many nodes in the global domain, and we number them as xe and xe plus 1, okay? So just below this, let me also write the local nodes. So that would be x1 for element e, that would x2 for element e, okay? So the idea there, is that we just want to take our nodes, our two local nodes, describe to them the actual coordinate values, right, that they each possess and essentially interpolate. Now, this blank spot that I've left here in the parenthesis, right, the, the, the parenthesis that I've left unfilled is where we're going to put in our basis function in order to do this interpolation of the geometry. Now, we're free choose whatever basis functions we would want here. It would just mean that we're interpolating the geometry in some way. What is the most straightforward way to do it, the simplest way to do it, one that would probably make our lives the most easy? Yeah, it is to choose the same basis functions as we chose to inter, to, to represent our finite dimensional functions, right? So we'll just use NA, a, A being 1 and 2, right, the corresponding two in fact the same linear Lagrange polynomials that we already know, okay? So, what we are doing here, using the same basis functions. As for representing. And wh, okay? When we do this, when we use the same basis functions to represent our finite dimensional functions as well as to interpolate our geometry, we have what is called an isoparametric formulation, 'kay, isoparametric meaning same sort of parameterization. Okay, so we have here an what we have here is an isoparametric formulation, okay? Isoparametic, but because we're using the same parameterization for our finite dimensional functions as well as for the geometry, okay? Again, it's not necessary, it is just a convenient way to do it. There may be special cases where we don't actually do the same thing, okay, where we don't have an isoparam, parametric formulation. One can design methods of this kind also, right? Finite element methods. Okay, but we will be taking simple approach here, which is to go isoparametric, okay? All right, what that lets us do then, is the following, it lets us say that x,xi is easy to compute, right? x,xi for element e is, now, sum over A, Na,xi xAe, all right? And explicitly, this is just N1,xi, x1 for element e, where x1e is the global node number, right, it's sorry, it's, it's, it's so, I'm sorry, x1e is the local node, right? But the coordinate x1e is the same as xe viewed globally, okay, plus N2,xi x2e, 'kay? And just to make that clear let me state that, that that is just equal to xe viewed globally. That is just equal to xe plus 1, viewed globally. Right, so the coordinates are the same. Okay again, that's easy, easy enough to do. In this particular case, we know exactly what N1 and N2 are. That's easy to compute. So that is just d/d xi of N1, which is 1 minus xi divided by 2 times x1e plus d/d xi of 1 plus xi divided by 2 x to e, okay? Carrying it out, we basically get x 2e minus x1e divided by 2, which is also xe plus 1 minus xe divided by 2, okay? Now, what we will do is observe that xe plus 1 minus xe. For element e is simply the length of that element, right? So we will write this as he over 2, okay? Where 'kay, this is to be viewed as the element link. Importantly what this allows us to do from the ground up, from the, even from the very simplest of finite element formulations is to have elements of unequal lengths, right? So in our partition of the domain omega into subdomains omega e, there was no requirement and indeed there is no requirement that the elements all have to be of the same length, okay? So he can be different for each omega e, 'kay? We don't need a uniform discretization, 'kay? So this is, allows us to have a non-uniform discretization. Okay? And the straight at, at the very outset, it's an important property of finite element methods. Okay let's see how all of this works now. Now so, so we've done very well. We have gone ahead and observed that well where did we start this. Remember we started this calculation all the way up here, okay? So what we've seen is that the derivative of x with respect to xi, which can be thought of as the tangent of this mapping, right, of a tangent of the map that gives us the physical element from this by-unit domain.  Right, the essentially what we've figured out is that we found for this mapping, we know that you know, every little well, well, we know that the scaling of the, the element of the physical domain, relative to the scaling in the parent domain is essentially he over 2, okay? It's a constant because, why, why is it a constant? Because for our geometry we're using linear interpolations, right? We're using linear basis functions, okay? That's what makes it a constant, that's what makes this, this tangent map a cons, a constant."
cjahRaZ-7HQ,"Okay what we also have here is that we have a map that is invertible, okay? This isoparametric map. Is invertible. 'Kay? And the reasons for it being invertible, if you, if you really care about it is because we have a you know, or, or just a different way to state it is that it's a one-to-one unknown to man, okay? Therefore, it's invertible, 'kay? Because it's invertible what this means is that xi,x is just 1 over x,xi. 'Kay, first of all it means that xi,x exists and that can be written as just 1 over x,xi. This stat, this fact, this statement, will become a little more meaningful and significant when we go to multiple dimensions, 'kay? We'll have to have actual denser maps and those will have to be properly inverted and all that. All of that will be assured to, 'kay, but let's just lay the groundwork for, for that discussion when we get to it. Okay, so why do I care about xi,x? Because now what this lets us do is to go back and write uh,x as sum over A, NA,xi, xi,x, dAe, and wh,x, sum over A. NA,xi, xi,x, xi Ae, right? It lets us say that this is just 2 over he. Okay? All right, as the very last thing to do in this segment, I want to just begin assembling some of these integrals that we need to worry about, okay? So, consider The integral Or consider the, the following integrals, okay, which come from our finite dimensional weak form. First, consider this one. Integral over omega e, wh,x sigma h Adx, okay? Putting everything together, we have this as being writ, writ, written as integral over omega e wh,x. Now I'm going to invoke our constitutive equation, 'kay? And I'm actually going to move the, the area into this and write it as, okay, see what I've done? E times uh,x is sigma h from our constitutive relation, okay? And this I'm now going to write by bringing in our expressions on just the line above for uh,x and wh,x, okay? I'm first going to write wh,x. It is sum over A. I won't write the limits on A because it ju, gets too clunky, 'kay? We know now from having looked at it over the past couple of segments that A runs over the limits 1 to number of nodes in the element, which in the simplest of cases is 2, okay, so we, I have that. I just want to write the upper and lower limits. I'll just say that to sum over A. Okay in a,xi times 2 over he, cAe, and we're going to put parentheses here. I have EA, okay, open parenthesis, and here I have another sum but I need to be careful here and use a different index for the sums so I will use the index B, and not index A, 'kay? The sum over B, NB,xi 2 over he, dBe, close parenthesis. And observe that I shouldn't forget the fact that I have a dx here, okay? In writing this, this first set of parentheses is my representation for wh,x. The second parentheses, second set of parentheses give me uh,x, all right? And then I have the other integral, which is actually a somewhat simpler one, integral over omega e whf, a function of x, which could be written as, which could be parameterized by xi, Adx. This is somewhat simple to write. It is just integral over omega e, summation over A. And I will stop writing the limits on that, on those sums NA, no derivatives here, okay? NA CAe, I'll put parentheses on it, f perhaps a function of x through xi Adx, okay? This is what we have, right? As a very last thing, let me just do one more thing here. Observe that essentially everything that in, in these last integrals is parameterized in terms of xi, right, the NA is pa, parameterized in terms of xi and we can also compute the derivatives, right? They turn out to be constants we know, but we can compute the derivatives, can view everything as being parameterized by xi, except that we have this as the of a length element, right? Well, but then we can use the change rule there, right? So we're going to write that as dx/d xi, d xi, all right, and obviously the same there. Okay, what are for, what is dx, d xi? On the previous slide, and actually we have it in some form up here too, right? We see here, that we saw xi,x is 1 over x,xi. Right, and here we have what xi,x is, right? So what we see is we've already figured out that dx d xi is he over 2, okay? The same thing there, okay? What that will let us do is to rewrite these integrals as integrals over the bi-unit domain omega xi, all right? As a very last step that we're going to take in this segment, I'm going to take, I'm going to write that, 'kay? What that lets us do then is write integral over omega e wh,x, EA uh,x. And, and I'm writing this already in the form that we will, where, where we implemented our constitutive relation, okay? This is now, we can write this as an integral over omega xi. We have sum over A NA,xi 2 over he cAe, close those parentheses. EA, open another set of parentheses. This is the sum over B, NB,xi 2 over he, dAe. Now we have d xi, dx, but that we know is he over 2, d xi, okay? And of course what we're going to do when we come back is to observe that these two cancel out. And likewise we have integral over omega e whfAdx, can also be written now as an integral over the bi-unit domain of sum over A, NA cAe, f which we may eventually want to write as a function of xi A, now, we have dx there, but we're going to write that as dx/d xi, which we know again is he over 2 d xi. Okay, that's what we have for the, for the, for the main integrals that go into our finite dimensional weak form. So this is a good place to end the segment. When we come back, we will focus upon evaluating these two integrals."
PJA5F7MVAWg,"Hi, in this segment we will be talking about functions in C++. Now the basic structure. Of a function we actually looked at earlier when we talked about the main function. Okay, but we'll go back to that now. Now, each function has a name. And. There are inputs. Possible inputs I should say. And possible outputs. And then there's the body. And if you do have an output, then you would return. The output at the end of the function. Okay. So let's look at some examples of functions here in the code. Let's say I want a function that will calculate, for example, the area of a triangle. And so, I would have an output, and it would be a double, which would be the area. I will name it triangle_Area. And I'd actually have two inputs that I would need. I would need the base, which is a double. And I'd need the height, which is also a double. Notice I've separated the inputs with a comma. And notice that I've given a name to the inputs, but the output doesn't have a name, okay? It will know, C++ knows the value output because at the end of my function I will again say return whatever value I want it to return. Notice with this format I can only return one output, okay? I have my curly braces which define the body of the function, and as we talked about before, it also defines the scope of any variables that I define within the function itself, okay? So I will create a double, a real number area. And the value of area is one-half. I could do 0.5 or I could do 1./ 2. Remember to put the decimal. Otherwise, 1/2 will give me 0. Right? So watch out for that. So I'll make that 1./ 2*base*height, okay? And then, I just return area. Okay, so now I'll go down into the main function. And. I'll print that to screen. Actually, I'll create a double here. Area. Notice, I've declared area twice, I have a double here in my main function, and I have a double area within my triangle area function. But again, they're both within their own respective scopes, so it's okay. They don't see each other, all right? I've created double area there. I'll say area = triangle_area, and I'll put in, just for an example, 2 as the base and 3 as the height. And I'll print that to screen. So we can see what it gives us, okay? My source code is saved, and so I will make run. And it prints us screen three. One-half the base which is two times height which is three, okay? Gives us three. All right, we can actually. Save some space in this function. Instead of creating the double area, defining what the value of area is in the function, and then returning the value of area, I can do that all within one line. If I want to simply say return, 1./2*base*height. And if we run that, again you see it gives us the same value of three. Now, looking back here at the top, I'm passing in values for base and height. You could consider these as being readonly values, readonly inputs, you could say. What's actually happened is in the function I'm copying the values of my inputs base and height and using that within the function to perform some calculation. Okay, there's another way. That's actually called passing by value, and passing them the value of the variable but not the variable itself. I can't modify based on height outside of the function itself. However, sometimes it's nice to be able to do that. For example, if you have multiple outputs that you want, then you can pass in an input by reference which allows you to change that input, and you can change multiple inputs at the same time. So let me create another function here to demonstrate that what passing by reference involves. So here, instead of returning a double, I'm not going to return anything. So I will put void, and I'll pass in double base, double height. Both of those I'll still keep as readonly, but I will also pass in a double &area, and I'll pass up by value. Remember from our discussion on pointers that the ampersand refers to the address or the reference of a variable, okay? So I'm passing this in by reference. And so instead of saying return 1./2*base*height, I'll just say area = 1./2*base*height, okay? And now the value of that input has actually changed within the main function itself. And now instead of saying area = triangle_area, oh I also need to change the functioning. So I'll call the first one triangle_area1. This one's triangle_area2. So for triangle_area2, I'll still pass in 2 and 3 for base and height. But I'll also pass in a double area. And notice I haven't set anything equal to triangle area because that function is void. It won't return anything. But it will modify the value of area that I'm passing in, okay? So now I can run that. And I still get the same value of three. Okay, now what if, I mentioned before you might want multiple outputs. So. What if, for example. For a square. I just want some generic information. I want to pass in this length. But I want to get out of it both the area and the perimeter. You notice I couldn't do this. As easily. I mean I could maybe do it as an output of vector, a vector of doubles. And the first component of the vector could be the area. The second component of the output could be the perimeter. So that would be one way. This is another way to do it, which I think is a little bit cleaner. Okay, and so here I would say that the area = length*length, and the perimeter = 4*length. Okay. So now I need a double perimeter. Now notice I've, just to make a point, I'll change these names here. They don't have to be the same. My inputs within main don't have to have the same variable name as what's defined in my function inputs, okay? So I'll change these names just to make that point. So I'll input those into square_info. I'll pass in a length of 3, for example. Pass in Area1 and Perimeter1. And print those to screen for us to see. Okay, so my file is saved, and so I can run it. And it gives me an error here because I misspelled length. So these. These error statements are really helpful. And usually they can tell you exactly where you need to go, where the mistake is in your code. Okay, so now that is saved, and. It looks like I also misspelled Perimeter there. There, okay. Now we'll run it again. All right, 9 and 12, so 3 times 3 is 9, and 3 times 4 is 12, giving us the area and the perimeter. So that's an example of what you might use a function for. Now here my functions have all been numerical. But remember, functions in C++ aren't necessarily a mathematical function.  So let's say I could create a function to, that would print a vector, the components of a vector, to my screen. And it would be a void. And I'll just call it print_vector, or print_vec. And I'll take as an input, a standard vector. Of doubles. And I'll call it output, or out. All right, and then within this function I just want to do a for loop, for(int i = 0; i<out.size(); i++. And then I would then print to the screen out. Std::endl;. Okay, so that would be an example. Of a function that's not a mathematical statement. It's just something to make the main body of my function a little bit cleaner, or the body of my main function to be a little cleaner. So down here if I have a vector declared, a vector of doubles. going to notice in this case because the input to my function is a vector of doubles. That's, it won't work on a vector of ints, for example. And I'll do a length 3, components 1.2, okay? And now I just call this function print_Vec. I input. Uses an input test. And as I run it. There you can see it. Use the function to print all the components of the vector. Okay, so that is just to show you that a function doesn't have to be a mathematical statement. We use functions usually to make the body of the main function a little bit cleaner, a little bit easier to read. And, often, if there's something that you're repeatedly doing within your body, you'd probably want to make a function, so that you've only to find the operation once, and you can just refer to it again and again. Okay, so that will conclude our discussion on functions. And in the next segment we'll move on to talking about classes."
krzd3qY68XI,"Hi, for this segment we will be talking about classes in C++. So a class. Is really just a group of data objects and functions. And so we've actually been using classes already in these examples that we've been looking at. The vectors are a class in C++. And you'll notice it's a data object that can store information and it also has several functions that go with it. All right, so what we will be showing you in this segment how to create a class in C++. All right, so there is a basic structure to a class and you would start defining the class in the program before the main function. All right, and here's the basic structure you actually write the word type in the word class and then the name of the class. And then open curly brace and close. Now, as opposed to function we actually put a semicolon at the end of that last curly bracket, okay? Now, within here we're simply declaring what objects and what functions are going to be included in this class, okay? And some of those might be public. So by public, I mean that someone can access those variables from within the main function itself. Some of them could be private which means that you can only access those variables within class functions, all right. So these would be variables, class variables. And functions. Functions can also be public or private. So if a function is private, it means you can only call that function within the class, within other functions of the class, okay. All right, so let's look at that in our code and as an example, so a class like we said a class could be a vectors, it could be matrices could be a class. But it doesn't have to be a mathematical object again. And so in fact it can be pretty abstract. So for an object for an example, class here I'll use the example of triangles, okay. So my class name will be Triangle. And. In my public, some public variables that I would have there or I would have a double defining holding the value of it's space and the value of it's height, okay. Now, what are some functions that I might want like the example we used before, I might want a function that would calculate the area. And so it's the same set appears that I declare the function that here's the output is a double. The function name is area. And as input I actually don't need any inputs because since area is a class function, it already has access to all of the class variables, okay. So it has access to base and height already. All right, so I don't need any inputs. There is a another function that we need, and it's called the class constructor. So a class is just a general description or a group or a type, right? But remember for example, with vectors we would create a particular vector object. So when you actually instantiate an object of a particular class, you call what's called a constructor function. And for the vectors, the constructor it could either take nothing, in which case it created a vector of size zero. The constructor also accepted a size. So we could say vec open parenthesis closed parenthesis of ten and that would create a vector of size ten. The other constrictor for vectors was to specify size and the value in each element, okay. So the constrictor always has the same name as the class itself. So for the constructor for a triangle, we actually want to input the base. And. The height, okay? I'm going to do b and h just to distinguish it from my class variables, base and height, okay? There's also something called a destructor and it has a tilde followed by the class name. And it's used to perform any operations once the class object goes out of scope. So for example, if you declared the object within a four loop at the end of the four loop the destructor will be called. Now, usually we don't need to put anything in the destructor. Occasionally, if you're dealing with pointers you have to make sure all the pointers are cleaned up so you don't have the memory leak. But in this case we're not going to be actually using the destructor, okay. All right, so now there I've declared my class. But now, I still need to define these two functions, triangle, and area. I'm going to rearrange this just a little bit and group all of my functions together just to be clean, get my functions together and the variables together. You'll notice I made them all public. So now, let's declare, let's define these functions, all right? So defining a function for a class is really similar to defining a regular function. But we need to specify that the function belongs to the class that were talking about, okay. So to do that first, we start with the name of the class which is Triangle. Then use the double colon and then the name of the function itself. So the constructor has the same name as the class, we have Triangle. Notice that the constructor doesn't have any return arguments, all right. So that's a little bit different. But we still have all the inputs there when we're defining it. And open and close curly bracket, okay? Now in here, what I want to do is assign the value of b, which is an input, to base so I simply say base = b. B is in the scope because it was one of the inputs. Base is in the scope because it's in the scope of the class, okay? And same with height. So I'll say height is equal to h. Now, the constructor is complete. That is all we needed to construct a triangle and to set up all the variables in the class. Now, I will define. The area function. So again, I do triangle :: area, to specify that this is the area function related to the triangle class. We output a double. Notice that this should match the function declaration that we had earlier, okay? Sorry, this function declaration up here. So I'm returning the double of the functioning of area and no inputs. And as before, I return 0.5 or one half times base times height. Again, base and height are class variables so their scope includes all class functions, all right. So I don't need base and height as an input, okay? So that's enough to demonstrate this simple triangle class. All right? So now, let's actually define a triangle object. And just like with doubles ints and vectors, we can declare more than one at a time. So we have tri1, and I'm going to construct triangle one with a base of 1 and a height of 5. And triangle two, I'll do a base of 3 and the height of 2, as an example. So there I've created two triangle object, they're each instantiation of this triangle class. And now, I can use the functions. How do we use it? We use these class functions in the same way we did with vectors. So if I want to calculate the area of triangle one, I would do tri1.area ( ) to let it know that it's a function. Okay. Standard end line. All right. Now, let's run this and make sure I didn't make any mistakes. Make run. And it says, new types may not be defined in return type. And that's because remember when I was on the slide when I wrote out the template for a class, I stressed that there was a semicolon at the end of the brackets and I hadn't put that in here. Let's see if that picks it up and it does. So again, the class declaration. It's a little bit different from function declarations because we need the semicolon afterwards, all right. And so we see that it did indeed print to the screen, the area of triangle one, half the base times the height is 2.5, all right. Now, what does it mean that the variables are public? How do we access those within the main function? Actually do it in a similar way. We do tri1 for example dot and I'm going to say base now, I'm now going to put the parenthesis on remember I said before the parenthesis showed that it was a function without the parenthesis it's looking for a class variable of that name and so I am going to redefine the base. I will let it be 3, okay. And I will call again, after that the same triangle function, the area function. Let's see if that does what we want. Yup, and so in the first it still gave us the area of 2.5, but then when we change the base of triangle one to be three then of course that changes the area. Now, maybe there is a class variable or a class object that you don't want someone in the main function to be changing. And if that's the case, then you would make those private. So let's say we only want the triangles base and height to be defined when the triangle object itself is defined. We don't want it to be changed later on by a user. So we would make base and height private, okay? And so now, we can still use area. Area is public but base and height are private, so if I try to run this, it should give me an error. And what does it say? It says error within this context with tri1.base. There's an error there and above that it says, triangle base is private. Okay, see we're not allowed to access that within the function itself. Okay, now there are a lot of other topics related with class. There's friendship, there's inheritance, there's in addition to being public and private there's protected. I'm not going to go into all of these details with classes. But if you are interested in that again go to c++.com and their tutorials they have some really good information on classes. Now, in our In our coding template for the coding assignments, we actually will be creating a finite element method class that has specific functions related to the finite element method and data objects. So we would have objects such as a global force factor or global stiffness matrix. We would have Class functions excuse me, such as our basis function. Or you might have a function for the assembly process or a function for outputting results. Okay, so this class structure, the structure itself, will be fairly simple. But we will use several variables and functions that led into the finite variable method. So now, we have our class structure here, all in the same dot cc structure here. We can actually separate the class structure into a separate header file and then just include that header file within our main.cc. So let's do that. Create a new File, and I will save it as triangle.h. And I'll save it in the same folder example. And all I'm going to do is copy this class declaration, including the function descriptions. I'm actually going to cut those from n.cc and paste them into triangle.h and save that. And n.cc I'll just do pound include triangle .h. I use the quotes just to specify it's a header file that I created myself, as opposed to a standard header file. All right, so now we go back to our terminal, and we should be able to make run. And it will run correctly just as before giving the exact same outputs and running in the exact same way. So by doing the pound included as if that whole .h file were just written within the main.cc where I included it. Okay? So this will actually be how we give you the coding templates for the homework assignments. You'll have a main.cc and a fem.h header file for each of your assignments. The bulk of the coding will be within the fem.h where you're defining this fem class structure and all the data objects and functions associated with it. And so we'll stop here and this ends these segments on our review and introduction to C++."
HR9ZGeczCMc,"Great, welcome back. We continue with our development of the finite dimensional weak form. What we have got to doing at the end of the last segment was using the bar unit to mean to not only write out our basis functions. But also to set up the, the integrals that going to the weak form. Okay, so we've written out the integrals that go into the weak form into the finite dimensional weak form for the for the general element by using this this, this approach. And that's where we going to pick up today. All right. So, so the topic of the segment that we are going to work through today is the matrix vector weak form And in order to get to it I, I, essentially I'm going to continue where we were last time, 'kay. Recall that we've written out the form with the following integrals for the, for the case of the general element, 'kay. We've written out this integral. Wh comma x, sigma h Adx for the general element, okay. And it's in your notes, it's in slides corresponding to the last segment. What I'm going to start out with doing is consider this integral for the somewhat special case of element one fueled by the fact that we have a Dirichlet boundary condition on the first node of element one. Okay, so we have this and now let's write con, consider how it works out for e equals 1, okay. Now, for element e equals 1, this is the integral over omega e. W h comma x you recall for element one is written as simply, N 2 comma c, C comma x multiplying c 2 e, where e for this particular case is 1, okay? So this is our expression of W h comma x, all right. It multiplies EA, and then we have our contribution for the gradient of the trial solution. And that takes on the form, sum over B N B comma C, C comma x, d B e, and this is our representation of u h comma x. All right. Integrated over dx, all right. And now of course we take our approach of going to the by unit domain in order to write out this integral. From that we get integral over omega C the first term is, the first factor here is N 2 comma z, we have z comma x, okay. And we recall that, that z comma x is simply 2 over h e, okay. And here we have c 2 e, EA here, and for Comma X we have N B comma C, again, we get C comma X, we use 2 over h e for that as well, we have here d B e. And now, like we did in the previous segment, we're going to rewrite dx as dx we're going to write as d C dx I'm sorry, it is dx, dc, dc. All right now, dx dc we know is he over 2 dC. All right, and as we did in the segment last time, those cancel out. All right. Thus da, a similar sort of exercise for the next integral. Integral over omega e wh f A dx, leads to after going through these sorts of steps we get integral over omega C. Our representation of Wh here is simply N2 times c2e. And remember this is all for e equals 1. Which is why we don't have the contributions from the first basis function, right? We talked about that in the last segment. This multiplying f A, and now in place of dx once again we have dx, dc, right? And we write that as dx dc, dc. And we recall that this is h e divided by 2, okay? So, so we have these two integrals also written out for the special case of the very first element. And you'll recall that what this does is to leave out the use of the first basis function. All right, these integrals here involve only N 2, okay. All right, so we have this in place and what we are going to do now is essentially look at how all these terms contribute, how, how all these simplifications of these, these expansions contribute to setting, setting together the the weak form, okay? So now in order to do that let's consider the general case. Consider for a general element omega e, okay. And let's consider this, the first of these integrals, the integral that comes from the left-hand side, the weak form. That integral is integral na, we now have it written as integral over omega C sum over A, N A comma C, and we write this as C A e, E A sum over B N B comma C D B e. And the term that survives here after cancellations of d x dc, and dc dx type of factors is this one, okay. And we have here dc, all right. Now this needs to be simplified and the way we can carry out the simplification is through several steps. The first of these steps involves us recognizing that the Cs and the ds that are used here are degrees of freedom, right? The Cs is the degree of freedom that is used to interpret the weighting function. The d B sub e is the degree of freedom that's used to interpolate the trial solution, right? So what we do is to recognize that these degrees of freedom are independent. Right? They're independent of the, of the variable of integration, right. They're independent of C, right. Because these are the actual degrees of freedom that are being interpolated by basis functions that are being expressed in terms of c. As a result, we can pull them out of the integral okay, so that allows us to write this out as follows. We can now pull the the summation symbol out. And for the sake of brevity, I'm going to write this is the sum over A and B. Okay? It we get c Ae outside the integral, and within the integral we have a the integrals over omega c. 'Kay, we have here N A comma C EA N B comma C. I'm going to pull the factor of two, which comes here. For the sake of of brevity again, I'm going to put it in here and divide by hE. Okay. So have that factor 2EA over hE sitting inside there. All of this integrated over dc, and that is our integral. Okay. And within those parenthesis, and it is being multiplied by d B e, okay. Recognize any nice, and short succinct form. All right. We have this for this for this integral and let's do the same thing for  the, the integral that involves the forcing function. Okay. That integral is integral over omega C sum over A, N A, C A e, f A, and we get here, H e over 2 dC, all right? Again observing that this is independent of C. Therefore it can be pulled out of the equation. We get sum over A, C A e, integral over omega C, N A f A, and once again I'm going to put the H e here, in this case divided by 2 dc, all right. This is what we have. And now observe that in, in the case of the first element, all right it's going to look very much like this except that the sum over A will not be required, all right. Because A will take on only the value 2. Okay? So, let me just recall this, for e equals 1 there is no sum over A, instead, we have use we, we require to use just the index A equals 2, okay. And why is it that we need to use the index only A equals 2? It's because the first basis function is not used in element one. Recognizing the fact that we have a Dirichlet boundary condition on element one, and therefore, the the weighting contribution from the degree of freedom one is not used, okay? So instead we use A equals 2, because we know that W h at 0 has to be equal to 0, okay. Implying that W h in that element, right? For e equals 1, W h is only written as N 2 c 2 e, okay? All right. So we have this and we're now going to go ahead and construct from this a form of the weak form that we call the matrix vector form. And in order to do that, we need to look at that expression, and that one. The first step to getting to a matrix vector form, of the weak form, is to, is to attempt or, or is to aim to eliminate these sums. Okay? And the way we will eliminate those sums is by writing them out using matrix vector products."
yeV-8fnoSIs,"All right. So so the idea here is use matrix vector product- To eliminate- The sum, sums over A. And sorry. Sums over A and sums over B, okay? Now, allow me just one second to go back here, and do some bookkeeping, okay? All right. This is where we are. Okay, so let's look at the first of those integrals and work with it. So, what we want to do is write sum over A, B. And here let me write the limits, okay? Because it's useful to remember what the limits are. And with the limits on these sums are, are A and B going from 1 to number of nodes in the element, which for this use of linear basis functions. Gives us number of nodes in the element equals 2 okay, all right? So, we have this cAe integral over omega cNA, 2EA over he, and B, C DC, dBe right? This is the one we want to work with. Now, recognizing that number of nodes in the element is equal to 2, right? Because, we are working with linear based functions and therefore, we do indeed have two of these basis functions in each general element, right? To, to represent both the reading function and the trail solution, right? So, what this lets us do is the following. This can be written now as the following. We recall that since A and B run over 1 and 2, we can easily, we can collect the degrees of freedom, see degrees of freedom into a row vector using the notation of linear algebra, okay? And that row vector is c1e, c2e, Okay? Now, what I will also recognize here is something I could have done in an earlier step. Is that each E of course is a constant, because it gives us the length of the element. In many cases the modulus and the area may not be constant within an element, okay? But for the simplest case, of course, they are constant, and let me pull them out of the integral. By seeing that, let us now focus on the simplest case possible where those are constant, okay? So if they are constant, we get 2EA over he, okay? Integral over omega c, okay? Now integral over omega c. We're, we're going to write it out without writing, without explicitly we sorry. Without using the summation, okay? Instead what we will do, is recognizing that A and B run over 1 and 2, for the use of linear basis functions. We get a matrix with entries N1,c and N1,c and N1,c and N2,c. It's symmetric, but let me write it out in detail here N2,c, N1,c N2,c N2,c okay? That's the matrix integrated with respect to c, right? Now, I use this parenthesis to remind us that this is where the, the absolutely clear actually that that's where the integration ends. Now, just as we wrote c in a vector form, right? Recognizing that the index, here, runs over 1 and 2. We can do the same thing with dBe, right? Recognizing that B also runs over 1 and 2. So that gives us a column vector in the notation of linear algebra, which we will write as d1e, d2e. All right, and I should make a remark here or I should make a note That this holds if E and A are uniform. Over the element omega e. Okay, so for this specific element E that we are writing out this integral, if for it holds that modulus and the area are uniform, we can pull the mod of integration sign which is what we've done here, right? We can pull them out of the integral, okay? He, of course, is just the length of the element and so that, that's constant, all right? In the, in the general case the E and A would still be within that integral and, right? And we would've written functions of of, of c through x, right? But let's look at this simple case, just, just for the purpose of, of gravity and getting to something that we can actually integrate, okay? All right? So, so, this is where we have things, and now we recall what happens for the, for the, for the special case of linear basis functions, okay? Now recall that N1 of c equals 1 minus c divided by 2, and 2 of c equals 1 plus c divided by 2. And that gives us then, quite easily that N1 comma c equals minus half, and N2 comma c equals plus half, okay? So, we can use them inside there. And so, what you are see is that in this case the integration is really trivial. Because the integrant is a constant, right? And this comes about from the fact that we have linear basis functions. This will not be the case when, when we extend this to other basis functions but it's a good way to, to, to, to start understanding how the problem works, okay? So what that says that if we look at what we get from that integral that we started out with it reduces to c1e, c2e 2EA over he. Now, again note that that matrix is, that matrix is fixed, right? It's uniform it's constant over the element. So we base it, we just get here one fourth from the first entry which is this entry, right? Recognizing that it is a product of minus half and minus half we get minus 1 over 4 here minus 1 over 4, and 1 over 4 again. This 1 over 4 comes from the product of one half, and one half, which is from that contribution to the matrix. And so then we just have the integral over on the omega c, all right? And over omega c, dc, right? All of this times our. Column vector of degrees of freedom corresponding to the trial solution all right, okay? Now, we, 'kay, so this is where we are, and what we need to do is evaluate that integral, and that integral, we realize that omega c, in this case, is A, recalling that it's a bi-unit domain. So, I'm going to try and make that look like a straight line okay, that's a little better, straight second, okay? Remember that omega c is this domain. Right, c space, that being the .0, okay? So this integral here is integral minus 1, 21 dc which is just 2, okay? Easy enough. So, putting all of this together, what we get is continuing from the previous slide, that integral that we were working with finally turns out to be c1e, c2e if you look back at your notes. You will see that the, the twos in the numerator, and the one over four in the denominator in the matrix cancel out. And we get EA over he. Matrix is a really simple one. It's 1 minus 1 minus 1, 1. And here we have d1e, d2e. Okay? And recall that this integral came, this integral emerged from us considering, this term, this particular integral in the finite dimensional weak form. Okay? Reduce this to this nice, simple, matrix factor form. And now you can see, why we refer to this as the matrix vector, all right? Just to complete this process let's also work out for ourselves how things pan out for the previous for, for, for the other integral, all right? And that other integral is well I shouldn't say likewise, but sort of similarly. Maybe even similarly isn't right? Let's say in a related manner- Okay? The other interval that we had was the following integral over omega c we had, let me see. What form did we have it in red? We had integral over omega c. We had here a we had here a sum over a cAe. Here we had NA, and fA he over 2, dc, all right? Again, recognizing that The degrees of freedom corresponding to the weighting function can be put into a row vector. And assuming as well again, just for the purposes of writing out something that's easy to actually compute. Let's assume that these two are uniform. Over the element of interest, okay? What that lets us do, okay? So let's see, if these two are in uniform over the element of interest, we can write this also using this matrix vector form. And in this case, we actually just have a simple sort of .product of vectors effectively. So we have this representation for our degrees of freedom and we've agreed that we are going to pull f and A out of the integral, assuming that they are uniform over the element of interest. Again, recognize that the, that they are not required to be uniform. Right, if they were not uniform. If they varied with position over this element, they would stay in the integration, okay? However, we are looking for something that's easier to compute in the simplest of entry level cases, okay? So we have this and now we have integral over omega c. But then omega c, we already know has the limits minus 1 and 1, okay? And then we have NA back here. But using this vector representation we write corresponding to N1 and N2. We have N1 here and we have N2 here, okay? That multi, that dc, right? So, that's the integral we, that we need to compute. And now this is easy enough, because we know that N1 is 1 minus c over 2 and N2 is 1 plus c over 2, okay? So those are the very simple functions, that we need to integrate over the limits minus 1 and 1. Also observe that in here c is an odd, c leads to an odd integral, right? Over the limits minus 1 and 1. So this is actually, this also works out to a nice and simple form, which is we have fA he, divided by 2 here, and that integral from minus 1 to 1 picks up only the constant term, okay? The linear term being odd vanishes, as you probably know from evaluating these kind of simple integrals, all right? So, that's e, effectively an integral over half integral of half over minus 1 and 1. All right, so we get, in both cases we get one. In these two cases we get 1 and 1, okay? And this you recall came from our original integral in the weak form, which had this form. Okay? So we will write that this is our final form for the force, and function too. All right this is a good place to end this segment. When we come back, we will talk about what these matrix vector forms give us further."
7PSLJQYi4rs,"All right. We'll continue with our establishment of the matrix vector weak form. Before we, sort of plunge onward, I should just clarify some notation. So I know I used the following symbol. I've been using this symbol I probably said what it means but let me make completely clear, this symbol is since or because. All right, I realize it may be slightly archaic notation that has fallen by the wayside, but I tend to use it still. Okay, so that's where we are. Let's continue then with the matrix vector form. What we did in the previous segment was  write out the important contributions, so, that arise from the weak form for, for it generally, right? And we consider the two main integrals that arise in the weak form from the, the integral that remains on the left hand side. As well as the one  that the one on the left hand side which comes from the stress. And the integral that, that's on the right hand side, that arises from the force and function. We wrote these out for general elements. What I'd like to do now is, take the step of just, sort of particularizing then for the first element, right, because of the fact that it has that tertiary boundary condition. 'Kay, so note that for e equals one. Okay? We have integral over omega e w h comma x sigma h A d x. This is simply c2 e, multiplying EA over he, right? Times let me see, how do we, how does it work out now? Oh right, correct. This times minus 1 1 d1e d2e. Okay, right then and, and you can actually see this in a very simple manner by just taking the general expression for a general element e and recognizing that c1e is not present in the expansion for element one. All right? Likewise, we get integral over omega e Wh f A dx equals, in this case it's just a scalar, right. In fact, well, actually all the integrals do turn out to be scalars when we com, when we complete the matrix vector products, okay. But in this case, we don't even need to use matrices and vectors. So the forcing term is just c2e. We have f A he divided by 2 times 1, okay? So those are the contributions from element e equals 1. All right. What we're aiming to do now is pull everything together. We are going to use the same approach as we used in going from summations over the degrees of freedom for each element integral, except that we're going to now apply that idea to the sum over elements. Okay and in doing so we are talking of the following of the following complete weak form, okay? So yet again recall the finite dimension of the weak form. It's this. Sum over e integral over omega e wh comma x sigma h A dx equals sum over e integral over omega e whfAdx plus, let's not forget this term that we have not had to worry about for quite a while. Right, the term coming from the Neumann boundary condition. So, what we spent our time in doing over the last segment was expressing these integrals, right, in a more compact form using matrices and vectors. So, what we now have is the following. We have the term above implies a sum over e, sum over all the elements. Now, for the general case, we have a form which is, which involves a matrix vector product. So actually let me do one thing. Let me first write out the contribution for element 1. Okay, the special contribution for element 1. For element 1, we have c2e, but e is 1, for element 1. Okay? E A he, but that's h1, okay? Times minus 1, 1, multiplying the vector d1 1, d2 1. Okay. And recall that we write d11 and d21. The super script there refers to the degree of freedom for that element and the subscript refers to the element number. Right? So this is the contribution from the first element to this integral. Right to this integral. Okay, for the the other elements now we have the sum e equals 2 to nel, 'Kay, of c1e c2e EA over he. Times this, very simple matrix. 1 minus 1, minus 1 1. Multiplying d1e, d2e. Okay? So those two contributions, the special contribution from element one and the contributions from that sum e going from 2 to nel, give us that integral that shows up on the left hand side, involving distress. Okay the stress and the radiant of the radiant function. Now, this is equal to all the right hand side terms. And the right hand side terms, also I can write as as follows. Right? Again, as we did for the, for the integral on the left hand side, I will first write the contribution from element one. That contribution is c2 for element 1, times fAh1 over 2, where the h1 reminds us that, that is element one. Plus, a summation E equals 2, 2 number of elements, 'kay. And that summation then is okay that summation gives us c1e c2e fAhe over 2. Okay? All right? We get this contribution. And let me see. Do we oh, sorry. We have fAhe over 2, and we have fAhe over 2 again. All right? Okay. There we go. That's complete. Now the terms we've written on the righthand side are the ones that arise from that integral. Okay, and the term that we could safely not have to worry about when we were writing these matrix vector contributions, is that one. Because that appears only at a single point. It just appears at the point on which we have a Neumann boundary condition, okay? Now, we recognize that that term alone, wh at l, has a very specific, very simple representation. Can you think of what it is? It is c2 for the very last element. E equals nel. Okay? So, the term that we add on here is c2 for the very last element times the data t, which is the rhyme and condition on the traction, times the area, okay? This is it really. This is a nice compact representation of our finite dimensional weak form. This is everything. One thing that may be useful for me to point out here is related to why we get the simple representation. Okay? It's a fairly straightforward thing. And it's something that you may all already have noticed but it's useful to point it out. Okay? So, Remark. And this remark goes all the way back to our representation of in an element as being the sum of NA, let me write NA as a function, expressed as a function parametrized by c, times dAe. And also to the corresponding representation for the waiting function. Sum over b nb c c b b. All right of course it's implied here that x is I've written this on the left hand side. X it's really x parametrized by c, alright and the same thing here. The same thing here as well. Okay. So now, observe that so, so this is of course for a general element, which is written as which can be depicted as this we would have I'll make an e. Here we would have x e you would have xe plus 1. Those would be the global node numbers, right? And we recall that this comes from a bi unit domain. Which is minus 1 to 1, and that is zero. This is omega z. Right? In the one dimensional space parameterized by z. Okay. Now. Okay. So we have this picture. And what I'm going to use it to do is to ask ourselves what ha, what form these, expansions take for and wh. If we choose to evaluate these, quantities, these fields at the nodes themselves."
o6GfoRWRPfQ,"So, let's suppose that, observe that u h at x equals x e. All right? Is basically u h at x evaluated with C equals minus 1, right? It's the left-hand of the element, right? It's the left-hand side node, which in the omega of C space is represented by C equals minus 1, right? So, this then, is Sum over A. N A evaluated at C uh,1. Right? Because remember, C 1 in the omega of C space is what we identified as being equal to minus 1. Right? So, it's any evaluated at C 1, d A e, okay? But then, we recall that this node, that these basic functions reconstructed had this Kronecker delta property, right? So, this is essentially, the Kronecker delta, delta A 1. Okay? Okay. And therefore, applied to d A e, this becomes equal to simply d 1 E, okay? All right? Essentially, what we've done here is to observe that if we want to evaluate the trial solution at the left node, right? At, it's effectively the first node of this element what we get, is that the trial solution is indeed, equal to the degree of freedom, corresponding to that node. And the same thing happens, if you want to evaluate u h at x e plus 1. This is equal to u h at x with C equal to 1, okay? This is equal to Sum over A, and A C 2, recognizing that C2 is the point C equals 1 times d A e, okay? But then, this again becomes delta, A 2, by the Kronecker delta property of the basis functions.  And therefore, this can apply to d A e, gives us d 2 e. Okay? All right. Essentially, the Kronecker delta property ensures, for us that the trial solution evaluated at the corresponding node is indeed, the degree of freedom value at the node. Okay? Stated differently, the node of degrees of freedom are indeed, the trial solutions themselves. Using these particular basis functions, okay? So and and, the same thing, by the way, happens, of course, with W h, as well. I won't go through the details with W h evaluated at x e is equal to C 1 e, and W h evaluated at x e plus1 equals C 2 E. Okay? So, so the remark here is that, the Kronecker delta property. Then the Kronecker delta property of The basis functions. Right? And by this, we mean the specific basis functions that we're working with, right? The Kronecker delta property of the basis functions ensures That The node of Degrees of freedom, right? Which are these ones, right? Or these for the function, right? This ensures that the node of degrees of freedom of the solution field. Are indeed its values. At the nodes. Okay? This thing, this is sometimes called this, this is sometimes referred to as the interpolatory property of these spaces functions. It is not a universal property of, of arbitrary basis functions. We specifically chosen basis functions, that give us this property, all right? Okay, why did I need to say all of this? I needed to say all of this because it is what we are using here. Okay? We're using it in particular in this to make this identification of W h at L being equal to C 2, for element N here. Okay? All right. So, we have this, okay? And this is the reason, we are able to do that. Okay? Fine. What we want to do here now is go back to what we had when we put together the finite dimensional weak form. When we made all of these observations about, how it can be written out in terms of the matrix-vector products. Okay? You have it on your notes and thanks to the technology here, I have it in front of me, and you can't quite see it. But I can see, that I have it here. Okay. So, what I want you to do is go back to your notes to the point, just before we made the slight digression. To talk about this this Kronecker delta property of the basis functions, okay? Go back to, how we'd written out the weak form, okay? And we are going to work off there. Okay? All right. So, if you look at the way I've written out the, the finite dimensional weak form. Just preceding this this remark, you will see that we are now, able to pull things together from there. Okay? And in order to pull things together, what we need to do, is to observe that there is a mapping between the the degrees of freedom as expressed using local node numbers, and global node numbers. Okay? So, note That The following map Holds Between Local, and global, global node numbers, or, or local, and global degrees of freedom. Okay? I will express this map first in terms of the, degrees of freedom that are used to interpolate the trial solution. Okay? So If we have this particular local degree of freedom, and recall that this refers to local degree of freedom A, in element e. Okay? This can be written as global degree freedom d e plus 1, okay? Let me see. So, d e plus 1. Sorry, it's not d e plus 1 mm, it should be d e plus A minus 1. Okay? So, this let, let me write out in words, what this means. On the left-hand side, I have local degree of freedom A in element Number e, okay? That's what this refers to. On the right-hand side, I have global, Global degree of freedom. Number e, e plus A, minus 1. Okay? Let's check that it works. Always a useful thing to do. All right. So, let's just check, right here. Okay? So, let's suppose we are in element 1, and we are talking of the first degree of freedom. Okay? That would be d 1 1 that is the same on the right-hand side for the global degrees of freedom. d is equal to 1, A equals 1, so we get d1, okay? Likewise, d 2 1 is equal to d e equals 1, A equals 2 minus 1 d 2. Okay? And this goes on. Let's check what happens in the very last element, right? So, when we have d 1 N e L. This is equal to, now, e equals N e L, A equals 1, so we get d N e L, and d 2 N e L is equal to global degree of freedom equals N e L equals 2. We get d N e L plus 1, okay? This works ou,t because if we look at our domain, all right? And this is the very first element. Right? This is omega 1. Right? I will write up here, the local degrees of freedom Right? The global degrees of freedom for this are d 1, and d 2. Okay? For the very last element, which is this 1. Right? This is omega N e L. Local degrees of freedom here are d 1 N e L, and d 2 N e L. And here we have d N e L, d N e L plus 1, right? Makes sense. We have one node more. Than the numbers of elements. Right? The number of the total number of nodes is one more than the number of elements. And that is born out by the way things work out here. Okay? All right. And of course, the same thing holds for the other ones, also. All right? Similarly, we have C e, e equals C e plus A minus 1. All right? So, we've made this the, made the, made the observation about this mapping between local, and global degrees of freedom.  we will use this to go back to that weak form that we had written out. In terms of matrix and vector products over each element. And carry out an important process of finite element assembly. We should probably, do this in the next segment. So, we'll come back to this in just a little bit."
umrFCEiCf88,"All right, welcome back. We'll continue with the delicate task of assembling our global matrix vector equations. From the specific version that we have of the weak form. And that version the weak form comes from about the code to last slide of the previous segment. You probably all have it before you in your notes and I have it here too, so we're going to start working on that. Okay, so, so we're going to write out the matrix vector weak form. In terms of. Global matrices and vectors. Okay. Okay, for completeness and, because there's just, just one more thing I wanted to say about it, let me just, rewrite the weak form that we have, okay? So, we have the following. We have C21 EA over h1 minus 1, 1. D11. D2,1 plus summation, E going from two to number of elements c1e, c2e, 'kay, here we have EA over he. 1, minus 1, minus 1, 1. d1e d2e equals c21fah1 over 2 Plus the sum, e going from 2 to number of elements. C1e, c2e. In the vector, so you're multiplying, the vector, forming a product, actually, with the vector. Eh, with, with components fahe over 2. Fahe over 2. And finally, we have this contribution from the boundary. C2. Nel times t times A. Okay. This is what we have. Now for again notational purposes, I want to state that in, from the conventional finite element literature, from the traditional finite element literature that matrix, including the factor in front of the actual matrix, is traditionally called the stiffness, the element stiffness matrix. Okay? We will denote it as K sub-e, or with K being a general Symbol use for stiffness. Okay, likewise we can call this sort of a force, this sort of vector the the force for vector for element e, okay, all right, so let me just state what these are called. This thing is called the element stiffness matrix. All right? And, that. It's called the element force factor. Now, of course, when we use terms like stiffness and force, this is clearly a a residual from the times when the finite element method was taught off chiefly as a technique for structural mechanics. Okay? So we will, but, but it's convention to continue to sometimes say element stiffness matrix even if one is doing a problem that has nothing to do with elasticity. They probably, it's, it's in the context of heat conduction it would be more appropriate in to call case sub E, the element conductivity matrix and FE as being the element of heating vector or something like that, okay? But, but this is just terminology that you may see around you in, in various spaces. Okay, so, so this is what we have to do. Now we're going to assemble then. And the process of assembly is the following. Okay? All right, this is called finite element assembly, right? So it's often referred to not just as assembly, but as finite element assembly. All right in order to do this, we recognize that because of the sort of mapping, we. Observed on the last slide of the previous segment. We can write these, degrees of freedom that are used to interpret the weight in function, okay? Into a single global vector. We're going write that single global vector as follows, right? And we're going to write it now using global numbering. Because this is what the mapping that we last looked at in the previous segment, this is what that mapping does for us, okay? So the relevant, global degrees of freedom for the waiting function are C2, C3, right, no commas here. C2, C3 so on up to C and E L, and C and E L plus 1. Okay here, I am going to write, way over here, the corresponding vector of global Solution values at the nodes. Right? Global nodal solution values. This would be the vector consisting of d1, d2, and these are global, okay? D3, all the way down to d n e L, and d n e l plus 1. What I've done here, with this vector on the left-hand side consisting of the Cs is collect the terms coming, like C 2 e, all the terms coming from C 1, and C 2 for each element. What I'm doing with this global vector is, collect the contributions coming from here, and there. Recognizing, however, that when we are writing global vectors, we use the global nodal numbering. Okay? Now, the contributions that then come from stiffness terms such as this, or the stiffness matrix K e can be obtained, or can be written, in a big matrix. Okay? Which I will sketch out, and then fill up. The way we do this now, is to observe that there are contributions coming, for instance. C 2 multiplies d 1, and d 2, because of the contributions coming from element 1, okay? All right? So, those would be the contributions E A over h 1, with a minus sign here, and E A over h 2. Sorry, over h 1, again. Okay? So that's, what we get from element 1. Let's go to element two, right? So, element 2 would have would have degrees of freedom, global degrees of freedom, C 2, and C 3, which would come from, C 1 with e equals 1, and C 2, sorry, C 1 with e equals 2, and C 2 with e equals 2, 'kay? Then did the local to global numbering, the global, local to global mapping of the greater freedom numbers would confirm for us. That the contributions from element 2, as far as the degrees of freedom of the waiting function are concerned, would be C 2, and C 3, okay? Now, those contributions come from the entries to this matrix, okay? Right? So, when we put those together, what we observe is that, we get here a, a, an, an additional contribution. And I realize, that I'm already running out of room here. So, let me just move this over a little. Just move this over to E A over h1 here. Okay? All right. The contributions then coming from element number 2, would be the following. We get minus E A by h 1 from element 1, plus E A over h 2, okay? Here we would get minus, E A over h 2. All right? Down here we would get E A over h 2, the minus sign, and here we would get, E A over h 2. Okay? All right? And then, we go onto element 3. Element 3 would have C degrees of freedom, C 3, and C 4. Okay? It would be this one, and another, and the other entry which comes just next to it. Okay? Those would map onto the matrix, to rows two, and three. Okay, as far as degrees of freedom, from the displacement are concerned. We would have contributions here coming from, hang on. Am I doing this right? Not quite. Okay. We're going to have to back up, a little. Okay? So, I'm just going to Okay. I'm going to erase this stuff. I'm going to go back to some point, and when Okay. All right. I'm, I'm, I'm going to go back even more. Sorry. Sorry guys. Okay. So we would, we, we would have to continue again from the point, where I wrote out assembly, okay? And then, went ahead. Okay. In order to do assembly, we need to assemble the local nodal degrees of freedom, into global degrees of freedom. And the way we do this is with recognizing, that we get here C 2, C 3, so on, up to C N e L, C N e L plus 1. We have for the degrees of freedom corresponding to the solution, we have contributions from the global numbering which are, d 1, d 2, d 3, and so on, up to d N e L, d N e L plus 1. So and here, we will write out a matrix in which we will con, collect the contributions coming from the stiffness matrix, right, from each element. Okay, let's look at what happens with element 1. Okay? Because element 1 has a single contribution as far as the C 2 degrees of freedom are concerned. Okay? It will fill out only the first row, of this matrix, right? As far as the solution degrees of freedom are concerned, it has contributions from d 1, and d 2. Okay? So, if we now, look at the line above us, what we see is that the contributions that come in, are going to be are going to be the following. And, and just for I'm going to pull E A out of this matrix. Okay? All right. So, E A is common for, for all the elements, right? And we, we are assuming that. Okay. So now, what that let's us do, is write out the contributions from element 1, as being minus 1 over h 1. And, one over h 1. Okay? So, minus 1 over h1 is is will, will multiply d1, 1 over h1 will multiply d2, okay? All right. Now, then we go into the contributions that come from that sum, okay? And remember, the very first element that makes a contribution from that sum is element 2, okay? Now, recalling our global, our local to global map, we recognized that contributions that come to that come from the c vector, right, in element 2 are from c2 and c3. This means, that we are going to fill out as far as c2 and c3 are concerned, we are going to be filling out the we're going to be, again, using rows 1 and 2 from this matrix, okay? But as far as, as the contributions from the d vector are concerned, the contributions of element 2 are in d2 and d3. So, this means that we are going to be getting contributions from columns 2 and 3. Okay, or co, contributions to columns 2 and 3 in this matrix, okay? So what we get here then, are we have 1 over h1 here, we get a contribution of the form 1 over h2. Over in the next column, we get minus 1 over h2, okay? In row 2 column 2, we get 1 over h2, with the minus sign, and row 2, column 3, we get 1 over h2. Okay? We go on then to element 3, element 3 has contributions coming from c3 and c4, which is just next to it. Okay? As far as the d degrees of freedom of concern, it has contributions coming from d3 and d4, which are next, okay? So, it is going to occupy it's going to provide contributions to our matrix in rows 2 and 3, and columns 3 and 4. Okay? In rows 2 and 3 the contributions are going to be of the form this, plus 1 over h3, right? So that's row 2, column 3. Row 2, column 4 is going to be minus 1 over h3, okay? We will have row 3, column 3, right? Which will be minus 1 over h3, and row 3 column 4 is going to be 1 over h3, okay? This process continues, all right? Continues all the way down, okay? Until we come to the very last element, the very last element will have a contribution coming from, sorry. The, the very last element, and, and actually, the last but one element are the ones who will contribute to the last two rows, and the last two columns of this matrix, okay? The last but one element is going to contribute a term on the diagonal, here, which will be 1 over h n e l minus 1, okay? All right, the very last element will contribute a term here, which will be this plus 1 over h n e l. I need a little more room, and I think I can get just a little more room by moving this over a little. Okay, so, here we will get a contribution minus 1 over h n e l. Here, we get a contribution, also from the element will be, which will be minus 1 over h n e l, and here, we get a contribution, which is 1 over h n e l, okay? So, these are all the contributions that come from the stiffness matrices, from the yeah from the individual element's stiffness matrices, okay? What I'm going to do is so write then, what I'm going to do is to write this out in sort of, operator notation. Now, what is often done here is, in, in the finite element literature is to say, that this is the result of assembling over all elements, e equals 1 to, n e l, okay? It's the process of assembling over all elements contributions of the form c1 e, c2 e times the stiffness matrix times d1 e, d2 e, okay? This is just an abstract way of writing it, and what is involved in this operator eh, in, in, this sort of an operation, is the detailed sort of book keeping that we followed in, by using the nodal and global degree of freedom numbering, right? In order to fill out this matrix."
hULDYR9f5So,"With that, we then go on to writing the same sort of a contribution, from the forcing terms. Okay? So the, for the contributions from the forcing terms, we have again, c 2, c 3, up to c N e L, c N e L plus 1. Okay? Now, looking back again at that expression for the finite dimensional weak form, where we've exquisitely, written out the contributions from each element. What we see, is the following. So once again, let's assume that f A is, is f, and A are are, are the same for every element, right? They're uniform over every element, and in fact, they're the same for every element. That allows us to pull f A over 2 out, as a common factor. Leaving us then, with the task of just filling out  this vector, okay? Now, the contribution, from the very first element, okay? If you go back, and look at the, at your notes, would have been h 1. Okay? For element 2, right? We recognize that, element 2 has contributions coming from The c 2, and c 3 degrees of freedom. Okay? All right? And then, if you go back, and look at, at your, at the expression we have, for the forcing function on the right-hand side. You will see, that the contributions here, are of the following form. We have h 1 plus h 2, okay? H 2 coming from, element 2, okay? Element 2 however, also has a contribution corresponding to c 3. So that, goes here as h 2. And then, we go on to element 3. Element 3 has contributions from c 3, and c 4. Which is the degree of freedom, sitting next to c 3, okay? All right. Tthe contribution from c 3, since, we're now talking of element 3, would be h 3, okay? The contribution corresponding to c 4, since, we're still talking of element 3, would be h 3, okay? The process continues, until we reach the last two entries, in this forcing function. Okay? The last, but one entry, will have a contribution from, correspondent to c N e L, but that would come, from the last but one element, okay? And so, that contribution would be h N e L minus 1. Okay? That entry would also have a contribution, from the very last element, h N e L, okay? And the very last entry of the forcing function of, of this vector, that we are trying to develop, would have a contribution only, from the very last element. And that would be corresponding to, that degree of freedom, okay? That could also be h N e L. Okay? This, once again, using this notion of the assembly operation of finite elements, is, the assembly over e equals 1 to N e L, 'kay? Of c 1 e, c 2 e, f A, h e over 2. F A, h e over 2, right? This is the this is the contribution, that we've assembled. And again that, that's just an abstract way, of writing this operation. The detail of doing that operation involves the sorts of the sort of mappings, that we've just used here. To write out these global vectors, from the local ones, and the sum over the elements. All right? Now, there's one more contribution. If you go back, and look at our at our finite element at, at our finite dimensional weak form. You will observe that we have a term that comes, from the traction condition. Okay? It's the one, that we may have actually had on the previous slide as well. Right. On the previous slide, it is this contribution. Right? That comes from the weighting function degree of freedom, c 2 N e L, okay? Right? And we know, how that maps on to the global numbering system, for weighting function degrees of freedom. Right? This contribution then, will appear in, this position. Right? The very last position here. Okay? It will be corresponding to c N e L plus 1. Right? Because c N e L plus 1 is the same as c 2 for element N e L. Okay? So that contribution here, is well we can't quite write it directly into this vector. So, let us then, also write it as, as follows, right? So, we include here, all the c contributions Right? The reason we couldn't write in the vector above, is because we have this constant factor, multiplying that vector. Okay? So instead, we just observe that this thing can be written with a bunch of 0s. All the way, up to the very last contri the very last entry. And for that very last entry, we just have t times A. T being the traction, A being the, the area. And this is just our very simple minded way of writing, c 2 N e L times t times A. Okay? All right. What I'm going to do now, is pull all these terms together, right? And write them up, write them out, in one single expression. C 2, c 3, c N e L, c N e L plus 1, okay? This is multiplying E A. And again for okay. I was going to try, and make some things even easier for us using by assuming all the elements have the same length. But let's, let's not do that, right now. Okay. So, the expressions we have here are minus 1 over h 1. So, shou, so what I am doing here is putting together all the stiffness matrix terms,'kay? Here I have 1 over h 1 plus 1 over h 2. I have minus 1 over h 2 here. Here I have right, I have minus 1 over h 2. Here I have, 1 over h 2 plus 1 over h 3. I have minus 1 over h 3. Minus 1 over h 3. 1 over h 3 plus there is another term, that'll come here from element 4. It will be, 1 over h 4. Okay? That process continues, and when we finally close out this matrix, we will have here. We will have here, 1 over h N e L minus 1, 1 over h N e L. We will have here, minus 1 over h N e L. We will have here minus 1 over h N e L. And here, we'll, we will have 1 over h N e L. I recognize that what I'd written on the previous  when we actually developed this, a couple of slides ago, was a little cramped. Hopefully, this is a little easier to read. Okay? All of these, sorry, this matrix multiplying here, a global degrees of, of freedom. D 1, d 2 all the way down To d N e L, d N e L plus 1. These are all the contributions coming to us, from the left-hand side of our finite dimensional weak form. And they are equal to, terms that came, from the right-hand side, from the forcing function, as well as the traction. All right? And, in order to get those together, let me just go back to this slide, our previous slide. And make sure I save it, so I can work off it, and then, I can come back to where we were. Okay. So, the right-hand side then, involves the same vector that we had up here, okay? The same row vector, c 2, c 3, c N e L, c N e L plus 1, multiplying f A over 2. It's the vector h 1 plus h 2. H 2 plus h 3. H 3 plus, and the dots there will be h 4, but at some point, I do have to just stop writing those, and use dots for extension. And down here, I get h N e L minus 1 plus h N e L. And here, I finally get h N e L. Okay? We have this, and then, we have the final contribution coming from the traction. Again, V f, c 2, c 3, c N e L, c N e L plus 1. Multiplying the vector with 0s, everywhere, all the way down to t A. All right. We are done, with finite element assembly. But we are not done with the, with our final sort of tweaking of this of the equations. There is more work we need to do here. But the big task of assembly is done. We'll stop the segment here. When we return, we will come back to these matrices to, to this general matrix factor expression. Make some observations about it, and turn it round to the final form, in which we will actually solve, the problem."
_VweAq9waOY,"Hi. In this segment I will introduce you to the Deal.II website, and then we'll look at a way of running your code on your own machine, using a virtual machine through Oracle Virtualbox. All right, so let's go to the screen now. We'll actually go first to the Coursera website. And if you scroll down, on the left there's a resources tab with links to different websites and other resources. So at the very top is the deal II website. It's actually just a dealII.org with the two as a double i. There are three things I wanted to show you about the deal.II website. The first, under 8.2.1, which is the current deal.II release. We'll go to Tutorials. The tutorials are really just example programs, and if we scroll down you see a graph or a schematic of how the different tutorials relate to each other. They sort of build off each other and become more sophisticated the higher the numbers go. We'll go to number eight, which is elasticity, which is something that you'll be coding yourself at some point. Although it will be a little bit different format. Now, I really like the way these tutorials are set up. The first part of the tutorial, as you can see, goes through the underlying physics and the governing equations involved. Can see it gives a pretty in depth explanation of what's going on and what you will be modeling, or what they are modeling in this tutorial. It also gives relations to other tutorials and where you might want to take it further. The next section is what's called the commented program, and as you can see it has comments, paragraphs of explanation interspersed with the code itself. All right, explains the header files, the templates that are going on, the different function names and so on, as well as the DOT data types, okay. So that section is quite long. So I'll scroll down. It also goes over the results, but then at the very end is the plain program. So if you just want to see the code itself, here it is at the very bottom. So you can look through it and see what's going on. There are several of these programs, these example programs, these tutorials. They're really helpful especially if you want to move on, beyond this course and use deal.II in your own research, or for your own work. This is really helpful for learning deal.II on both a simple and an advanced level, all right. Go back to the deal.II website really quickly here. The other thing I wanted to show you, or one of the other things, is the download page. We actually won't be expecting you, or you won't need to download deal.II and install it on your own machine, but if you do want to, this is how you do it. You can install it on Linux and that would be this first archive file, this tar.gz file, you would download it to your Linux computer and here are the instructions for installing that. It's actually really simple to install it on a MAC. Here are the binary packages, and the instructions there. Another option that deal.II has on their website, is a virtual machine image, and I'll go into that a little bit more. We actually won't be using deal.II as a virtual machine image, because it doesn't have the HDF5 library installed, which is necessary for creating the hallmark submission file. So actually, if you're going to be installing this on your own computer, whether it's a Linux or a Mac machine, you'll need to make sure the HD5 library is installed. If you want to run it on Windows, there may be ways to do it by creating a Linux environment, such as Seguin, using Seguin. But it's probably going to be easier to use the virtual machine that I'll be talking about in this segment, or AWS, which I'll be talking about in a future segment, all right? The third part of the deal.II website that is extremely helpful, is their documentation on the deal.II classes and functions, such as the deal.II vector, the deal.II full matrix and so on. I actually usually don't access that through the deal.II website itself, initially. I access it by going to a search engine like google, and searching deal.II vector for example, and it's usually at the very top of the results. Okay, so here is vector class template reference, and at the top of the page shows how the inspector class relates to other classes. That can be pretty extensive with things like full matrix. With a vector it's a little more simple, and then it goes through the public and private member types and data types associated with the vector. So it goes through the different constructors, the functions like reinit explains the different operators involved. All right, so there are quite a lot of functions here, and if you want to know how to use one more in depth, or a more in depth explanation, just click on the link. It scrolls down to the right spot on the web page, okay. And deal.II's documentation is really good, that's one of the reasons we decided to use deal.II with this course, okay. So those are the three mains things about the deal.II website that I wanted to show. Of course there's more to the website than that and feel free to explore. All right, but now I want to shift over and look at the virtual machine. Okay so to do that, we'll go back to the Coursera website under resources again. We'll be using this virtual machine through Oracle's VirtualBox. So here's the link to download that. It's an application that you can use on Linux or Windows or a Mac. Whatever type of operating system you're using, you should be able to use VirtualBox, okay. It's a pretty, It's pretty straightforward to download and install. It's not a huge file. It doesn't take up a lot of space on your computer, by itself. Okay, but once you've installed VirtualBox, you want to go back to Coursera, and you'll download the virtual machine appliance. Now this is what has all of the software that you'll be using, okay? And it's a Linux appliance, it's an Ubuntu virtual machine, okay. It's a .OVA file, so you would just click on this. It would download it. As you can see, it's 2 GB, so it's a pretty big file. I'm not going to download it again to my computer. It can take quite a while, depending on your internet connection, okay. But once you've downloaded your .ova file, then you would go to VirtualBox, which you already have installed, and you'll open that up. I've already imported the appliance to my machine, but I'll show you how you would do that on yours. Go up to file, import appliance, and then you would browse to where that file is downloaded. So here it's in my downloads folder, press Open, and next would take me to the different settings. Okay, you shouldn't have to change any of these settings, probably, and then you would press Import. Now it would take about 20 or 30 minutes to import, so I won't press Import again here. I'll just press Cancel, but again, of course you would press Import. Okay, now once it's imported, your screen will look like this, okay? So you press Start, and it can take a little while to bring up the screen. Okay, and the first time it's running here for you, it may come up with this error, like it does for me, it says implementation of the USB 2.0 controller not found. I just press OK, and I go to settings making sure of course that virtual machine is selected still. Go down to USB and I'll deselect enable USB 2.0 controller. Press OK, and now I'll go back to start. So again, this is an Ubuntu virtual machine image, and so, as it comes up, you may not need to use it but the username and password will be Ubuntu. So, if you ever need to use it as a super user, using as you do. It does take a little bit for the screen to come up. Okay, so we're loading xubuntu now, and even though it's a small screen now we can make it full screen so that it's, as if you're actually working on a new Ubuntu machine. It is a virtual desktop here and everything is actually located right on your own laptop. So you don't need an Internet connection once everything's installed, and again this works with Linux, Mac or Windows. Now after it's installed, the virtual machine appliance does take up a fair amount of space on your hard drive, something like 7 or 8GB. So you will need about that much space on your hard drive in order to use this option. So you can see it is its own desktop, I'll switch to full screen mode. It has its own web browser here, so Chromium. You go up to the top left, is where you'll access the different applications. So actually the web browser here doesn't work. You go over here to the right to Internet, and then you can go into Chromium, go to development, and you have Emacs for a text editor, for editing your code. Let's see, Accessories. Leafpad is also a text editor, but it's not necessarily meant for editing code. It is there though, it's quite a bit simpler than Emacs. Another thing is you have an archive manager, because your co-template will be downloaded as a .zip file. So you can use Archive Manager to unzip your files, and then remember when you're submitting your homework files you will be creating a zip file of all the submission files. So again, you can use Archive Manager to do that. So everything you need to do for your homework you can do right here within this virtual machine. Because you have the text editor, you have the webpage or the web browser, you have the archive manager. You also have VisIt to visualize your code. You can double click that, and it opens up the GUI. Okay of course, so now the main thing that you need to know how to do on here is how to run your code right? So up here as well, you would go to Terminal Emulator, and here you have your terminal two actually run your code. Okay so do ls. So we'd want to, for an example, I'll go into the deal.II examples, so it's cd deal.II examples, and here it has step one and so on. These correspond to the tutorials that we saw on the deal.II website. So if go into the step-8, you can see it has CMakeLists.txt file and a step-8.cc. So the first step to run your code is to do cmake and then CMakeLists.txt and press enter, so this will create a make file, that's specific to our .cc file to our source code okay? So now if I press ls, you can see that it's created several different files including this make file, and the make file will compile and run our code so I can do make run, and it will run step-8 for us. It does take a little bit of time, just to compile your code, even if it doesn't take a long time to run it on the virtual machine. It does take a little while to compile it and link everything. It's finally linking and running it for us. Okay, so now if we list the files. You can see it created several VTK files which you could open up and visit and look at. All right, and when you run your code you'll be doing something similar, you'll have it a schemiclist.txt file as well as some source code. Of course you'll be editing the source code, and you'll be running it using Cmake. Okay, so that should be enough to get you going with the virtual machine and in the future segments we'll be talking about how to run your code using AWS."
QnSxLO_f1-c,"In this segment, we'll talk about how to use Amazon Web Services, AWS, to run your deal.II code, okay? So let's go over to the computer. And first we'll talk about getting set up with AWS and loading the AWS, the AMI. Okay, so first you would sign in, you go to aws.amazon.com, and then sign in. You can use your existing Amazon account, or there's an option to create a new account there. All right. You can just choose the free account. So now once your to the Amazon Web Services, your main page here. You'll go up, at the top left there's a link called EC2 virtual service in the cloud. And we're going to launch an instance. Okay? And to choose the one that has deal_II installed, we'll go to Community AMIs. And you can just search for deal_II, double I. Okay, and you can see that this is the only machine image that has deal.II installed. If, for some reason, there was more than one, you would choose the one that's ami-93217fa3, okay? That's the one that's been created for this class, at this point, at least. All right, so, we'll select that. And I'm just choosing the free tier, the T2 micro, and I'll launch that. Okay, now here you have to create or choose a key pair, so that just links this instance to the key pair that you will download for your own computer. So, it's just a security measure. So I could create a new one. You can choose that here. You could create a new key pair. I've already created one. So I just called it deal_II_key_pair. You can name it whatever you want, and I acknowledge I have access to it. Okay so I'll launch the instance now. And it's associated with that key pair. It's a file on my computer. Okay. So I'll go and view the instances. So now, it's been created or it's creating. You can see right now it's still pending, it's initializing, all right. But that's all you need to do to get set up with AWS. The next step is to access it. Now, it doesn't create a virtual desktop like the virtual machine did that we talked about in the previous segment. So, in order to access AWS, we'll be going through SSH or something like SSH, okay. All right, so you can see here that the instance is running. I'm running Windows right now, and there are a couple ways to access AWS through Windows. I'm going to show the easier of the two first. It's easier if it works. Okay?  So depending on your situation it may not actually work. So the one I'll show you first is the second option. A java SSH client directly from my browser. So, as it specifies, Java is required. So, when I tried to run this on Ubuntu with Chromium, it doesn't work, because they're not supporting Java anymore. I have to use Firefox. However, right now in Windows, Chrome is still supporting Java. Okay so, for username we actually change from root it becomes ec2-user and that this will be the same for you no matter how your log in to AWS, okay. Notice it automatically has the keep hair that I specified before. And it saved the file path right where I have saved it before. I had to put that in the first time, okay? So, you'll, when you do this for the first time that won't be there. Okay, you'll have to type it in. Okay, and remember to include the name of the keeper, the file name. Okay, and you can store it in browser cache if you want to. And then it will save the file pathway. Okay then press Launch SSH Client. And most likely it will come up with this warning. Application blocked by Java security, it's because they haven't, AWS hasn't updated one of their certificates. And so if you want to use this in order to use it on your computer you'll have to add this website to the whitelist in Java. So you notice I went to the start menu. I have configured Java here already. But, if you want to you can type it in down here in search, configure Java, and select it there, okay. And it brings up the Control Panel. So what you do is go to Security. We're on High security. And then you do Edit Site List, and you can add the the location that it has listed there. So, I'll type that in. Now, hopefully, they will update their certificate soon. So we don't have to worry about putting it on Java's whitelist, but, okay, and that matches. We can press OK. And that's set up. So, after we change the setting in Java, after we add this site to the whitelist, then you have to close out the browser and then reopen it, okay? All right, so, after we've added this website to the whitelist on Java, we'll close out the browser and open it back up. And go to aws.amazon.com, sign back in and go to EC2. Now, we want to see our instances. We've already created the instance in the Machine Image. So, we'll go to instances  and click Connect. Now we should be able to choose this Java client. Then we need to change the username, again, to ec2-user, and then my, I actually changed where I put this key path. It should be in Example \deal_II_key_pair. Let me just go there really quick, under C, Example, and right there, deal_II_key_pair. Delete that file, because we'll be talking about it later. Okay so now, and I will store that in my browser's cache. Launch the SSH client.  And it comes up with this security warning. I accept the risk, and I do want to run the application. And it will bring up in just a second. There it is, okay. And now it's brought up this terminal that accesses AWS. All right, and so it's all ready to go. We can do print work in directory. And you can see we're in the home ec2. Dash user, directory, do ls, and there's a desktop folder, and an x example folder. All right, so now we're able to run it. We can, for example, go into the example folder, and it has these same examples that we saw on the virtual machine. Okay, again this correspond to the tutorials that's going to step eight. And I did already run this, but just to show you again the process we do C make, CMakelista txt to create a make file. And once you have the make file, you can do make run, and it will run. Again, this is a elasticity code, and it will create several VTK files. All right. Recreate them, in our case, since I already had ran it before. All right. So, there it's pretty straightforward on now it's using this is just like having a terminal open in the virtual machine, or on your own computer. If you had the installed on your own computer, so just like using a terminal there. All right? But again, since this is just a terminal, you can't. It doesn't have a way to visualize, it doesn't have a way to edit your files, so in order to do that you'll have to copy, or transfer your files to and from AWS to your own computer. With this mind term, which is the Java application here, it's really straight forward. You go up to the top here, Plugins. And then there's SCP File Transfer, click on that and it's very easy. You can choose for example, a file on my computer so I'll go to C and example. And let's see over here on ECT user. I'll make a new directory called example. So now, it's there, I'm going in to that, and let's say I want to copy main.cc from my own computer to AWS. Just press the Celero and it copies it over. Done, okay. And I can go the other direction too, with the other arrow. All right. If I wanted to. All right, so it's very straight forward very easy to use. Once you have Java set up to accept this application, all right. And again hopefully Amazon will change that certificate since so that we don't need to put it on the white list to make it work. All right. So that's one option. However if changing the settings on Java perhaps isn't an option for you on your computer, or you're a little concerned about the security risk there, there is another way to connect. And that's using the, this first option choose a standalone SSH client. Now for Windows, we can use what's called PuTTY, and here's a link here that explains how we would connect using PuTTY. All right? So, I'm going to minimize this screen a little bit. So that we can keep these directions on the screen. Okay so the first thing is to install PuTTY. So we have the download. It gives us a link to the download page here. I'll pull this out into another window for us. All right, and again there are two options here. I'll go to the download page. You can download it itself, actually install it on your Windows computer. And that would be down here, putty-0.63-installer.exe. Executable. So, you could download that and actually install it on your machine. However, if maybe you're not, not your own computer, and you don't have installation privileges on the computer, then we can do it another way. There are three other executables that we want to download. There's PuTTY,exe. There's PSCP. You remember when we were using we used the SCP to transfer files. So, this is PSCP. So, it's the PuTTY's, we have using SCP, and then PuTTYgen. PuTTYgen will convert our key pair from the .pem file to a file type that PuTTY recognizes, okay? So I've got these three files, these three executables, in my Downloads folder. Okay, so now you can exit out of that. And we'll look back at the directions. Now these other directions are information about our instance. So I'll X out here. So for example we will need the main thing that we'll need here is this public DNS address. Okay? So let's go back to the instructions. We also need to know where the private key is. That will come up. Okay? All right, so first let's just connect to AWS using PuTTY. And then we'll talk later about transferring files. Okay, so the first thing is to convert the private key type. Okay, so here I'm in this example folder, which is where I have my deal_II_key_pair.pem. I'm also going to open up my downloads folder And you can see here, puttygen. Okay, this is an executable, so I can just run it, I don't have to install. Okay, so I need to go to Load. Again, these instructions are on the web page, website here. So, it says, Start, PuTTY(gen). If you'd installed it, then you could just go to your start Start menu and type in PuTTY(gen) and run it from there. Okay, we downloaded the executable, and so that's why we just double clicked on the file in our downloads folder. Choose SSH-2 RSA, and then we go to load. And, again, it's here in example. It went there automatically. Choose all files, and then we choose the .pem. Okay, so it successfully imported it. We'll need to save private key. You can see that's the next step here. Save private key, and I'm not going to do a pass phrase. And I am going to just rename it, or name it the same as my .pem file. However, it is a .ppk file, a putty private key file, now. All right, so I'll save it there, and that's ready to go. I can exit out. Now to actually start a PuTTY session, We'll go to, again I'll go to my Downloads folder. If you did install PuTTY, then you would go to the Start Menu and find PuTTY there. I'll just double click on PuTTY and run it. Now let's look at the directions here. So in the Host Name we entered user_name@public_dns_name. Okay? So our user name is ec2-user, that's going to be the same for all of you. Then @, now we need to go to the instance. Okay, and you'll notice that this public DNS has the public IP. It's the same address there. So I'll copy that into the Host Name. Go back to the instructions,you'll see it says to make sure that the Connection type is SSH and the Port is Port 22. All right, now, over here in the Category pane, we'll go down to Connection, and then we'll expand. Press the little plus sign on SSH, then select A-U-T-H. All right, authentication. Now we need to find our key pair, and now it's this .ppk file that we created. So we press open and then that's the last thing, now we just need to press open again to start our PuTTY session. Okay you, again, you can save the host key or not. I'm not going to in this case. It doesn't matter too much. And now we have, again, this terminal open so that we can access AWS. Exactly the same now as it was in my intern, just use the command line. Commands, you can see we have already, we still have that Example folder that we created from line term because AWS. Saves the files when you exit out. Okay, so you can see we even still have main.cc. I'm going to delete that for now though. Okay. All right, so, that's empty. Okay, so now we need to look at how we would transfer files to and from AWS. Okay, since we won't be editing them trom this terminal, okay? So you'd be editing your files on your own computer and visualizing your output files on your own computer. All right, so let's scroll down a little further on the instructions. And it talks about transferring files. There are actually two ways. I'm just going to talk about using the PuTTY SCP that we downloaded. If you're more comfortable with using a GUI, Then you can download this WinSCP, and the instructions are here, but I'm not going to go through them with you, okay? But that is an option. All right so, now to use this PSCP, we're going to open up a Windows Command Prompt. So, you can type that in here at the Search, type in command and then choose Command Prompt. And for those of you who've used SSH on Linux, or. On a Mac, this will be very similar to using SCP on a Windows, or a Mac. Or, on Linux, or a Mac, excuse me. Okay, so the first thing is we need to specify that we're using PSCP. Okay, now, again, if you installed it using the installer, you would just type in pscp, which is what it says here in the instructions. However, we didn't install it. We just downloaded the executable, so what we need to do is actually type in the file path. So, for me it's in C:\Users\Greg\Downloads. You can press Tab, and it completes the file name, okay? And then the name of the executable is pscp.exe, okay? And then next, we just type in the minus i, -i. Now, first I'm going to. Well, whether we're transferring from the computer to AWS, or from AWS to the computer, the next step is the same. Now we need to specify where our key pair is. So then I put that in my example folder and it's called deal_II_key_pair and again we used the .ppk since this is PuTTY. Now for this first example, I will transfer a file from My Computer to AWS. So, first I'll specify where that file is and I'll use the same example of passing main.cc, okay? All right, and now I need to specify where I'm transferring it to, specifically, I need to type in my username, ec2-user@. Again, I need to do, I'll just right-click and Paste. Okay, again, that's the DNS that we used before to access PuTTY. Okay, and I'll do a colon, and then I will put in the file, or the path to where I want the file to go. Okay, so that's home/ec2-user, and that would be the same for everyone, and I want to put it in that Example folder, okay? So now if I press Enter, it hasn't seen this key before. So I'm going to press yes, store the key in the cache. And now it transferred it. So if you go up here to our PuTTY terminal, and we do ls, you can see that now Example has this main.cc file in it, okay? Now, to transfer the other way, it's going to be very similar. We're going to press up to get back to that previous screen. Okay, very similar. So, again, the first part is just to type in where pscp.exe is, if you didn't install it, -i. Then, again, the path to the key pair. Now, I want to transfer. I'm going to transfer that main.cc back from AWS to my own computer, all right? So I will do the same. It's essentially the same command, only switching the last two. So first, I'll do this ec2-user@ a DNS. Paste that. And I want to copy the file home/ec2-user/example/main.cc, okay? So that's the file of origin, and I want to copy it to. My example file here, and I can actually rename it if I want to. I'll name it main2.cc, just to distinguish between them, okay. I press Enter and okay it says there's a problem here, cannot identify. It's because I was using a backslash instead of a forward slash. So that's something to watch out for. Windows directories use back slashes, but Linux uses forward slashes, so I need to use forward slashes here in my file path, okay? All right, so now that should have transferred over, we can go to our folder here and there it is main2.cc, okay? And so once you've know how to transfer your homework files, your .h, ,cc, and .txt over to AWS and then you can run it on AWS. Once you got your output files, you can again transfer those back to your computer to visualize them. All right, so that should have you set to use DO2 on AWSC3 using mine term for Java or using PuTTY. On the next segment, I will talk about using SSH to access AWS from Linux, and it's also the same on a Mac."
AjTCdB-51MA,"Hi. We made a small change to the AMI, that we use on AWS, that we made available to you on AWS. And so I wanted to show you three changes that you'll need to be aware of as you're accessing the AMI as you're using AWS. All right so the first thing to notice or to be aware of is that AWS is split up into regions. All right, so here I'm the top right of your page here on the EC2. I can even go back here to the EC2 dashboard. You'll notice there are several different regions, 9 different regions, spread out throughout the world. So you'll want to choose a region that's closest to you probably. That will work the best for you. So I'm choosing North Virginia, for example okay, and now we want to launch an instance. Okay, and we go down to community AMI's, this is just as I showed you before, but because we changed the AMI a little bit, it's going to have a new number, and also, each AMI is specific to the region that it's in. So the AMI for north Virginia will have a different number than the AMI for Oregon or Ireland, for example, okay? So we'll have a list of the regions and their corresponding AMI number on the Coursera website for you, all right. So you'll have to look there to figure out what AMI you'll be looking at. You'll still probably be able to just search for DL2 within that region and select that. But again, if there's more than one DL2 image, or if for some reason in the future we rename the AMIs, to something that's not DL2. You can always refer to the Coursera website, look up that AMI number for your specific region, and search for that number, okay. Now the other thing that changed, so those are the first two things to be aware of, the region that you're in and the AMI number. The third thing is that we switched the Linux type for the AMI, whereas before it was an Amazon Linux. AMI now it is in Ubuntu AMI just like the virtual machine, and so whereas before you were always using EC2 user for the user name, whenever you were connecting to AWS through SSH. Now the user name will always be Ubuntu, and we'll have more reminders of this on the Coursera website. But again remember, whenever you're using SSH to go into AWS for these new AMIs, the username is Ubuntu, and I'll show you an example of where that might come up okay. So I will go back, to my instances. For example here, I already have one running and I'll click Connect, and one of the options that I'll be showing you more within this segment is the standalone SSH client, or this Java SSH client, and here the default user name is always root. But again, you want to use Ubuntu, and that's the same, whether you're using SSH on Linux or Mac, or if you're using Putty on Windows to SSH to use the AMI on AWS.  Excuse me. Okay. So those are the three things that I wanted to make you aware of. The region. The AMI number and the user name is now Ubuntu, all right."
KW_w3Hu_3uU,"Hi, in this segment we'll talk about how to access AWS using Linux or Mac through SSH, and how to transfer files as well. So let's go to the screen. Going to aws.amazon.com. I'm assuming you've already set up the instance like we talked about in a previous segment. I'll sign in here and go to EC2. The top left of our screen, And that will pull up our instance. So, on the left we choose Instances, And this Instance is selected. Going to minimize the screen for a second. Now we press connect, all right? So this gives us some really plain instructions for how to connect, Through SSH, okay? So first I'm going to go to the folder that has my key pair in it, which is my Documents folder. All right, and now the first, or I guess step number three, it says to do the chmod 400. This is to make sure it's a private key and not public, or that it's not publicly viewable. Type that in, just as it has written. And nothing shows up, which means it worked. All right, and so now the next step is to type in the command that it shows under example. And we'll just make one small change, ssh -i deal_II_key_pair.pm, but our username is ec2-user, not root. So that will be the same for all of you. And then I'll type in the IP address, okay? Press Enter. And now we're logged in. So pretty straightforward on how to connect to AWS though SSH. Okay, you can see all of our files are there. Still have Example. Oops, cd Example. And we have main.cc in there, I'm going to delete that. And then we'll transfer that over again, Using SCP. Okay, so now we're all set up through SSH, but we do need to be able to transfer files back and forth, all right? So I'll click down here on connection documentation and it'll pull up another page. And click on Linux using SSH, and scroll down to transferring files. Okay, so first you need to have an SEP client, which you probably already have. Okay, so I am going to open up another terminal and we'll use these directions. Okay? This step one is optional. So we'll go to step two. Step two says to change directories to go where your key pair is located. So we could do that. I'm going to do it another way. I'm just going to stay in my current folder and then just specify the file to my key pair. That's just another way to do it. So we type in S-C-P dash I, and since I'm in Linux, since I'm using Linux, I can use this ~/ to specify my home directory, Documents/deal_II_key_pair.pem. Okay, so that specifies the location of my key pair. Next, we specify the location of the file that we want to copy. So first, I'm going to copy the file from my computer to AWS. So, that's in Desktop/Example/main.cc. Okay, now I need to specify where I want it to copy to. And so that will be on AWS, so I'll use my username ec2-user, at now I need my public DNS. So I can close this out and scroll over so I can see public DNS. I can copy this address. Okay, Ctrl+C, Ctrl+Shift+V. And then colon, and I'll specify the file path on AWS. So again, I can do tilde to specify the home directory and I want to put it in example. Press Enter, and there, it's copied over. We can look at our SSH terminal and you can see again we have main.cc on our AWS instance. All right? So now let's copy that back just to show you how to copy, for example if you've run your code on AWS, you have some .VTK files you wanted to visualize. So, this is how you would copy them back to your own machine to visualize them. So we'll still do the same scp -i and give the location of the key pair, documents/deal_II_key_pair.pem. Again, that's the name I'm using. Whatever name you've given your key pair, of course you'll put that in there. Now, I want to copy a file from AWS. So I'll type in, I'm essentially just switching the order from the previous command. So ec2-user at my public DNS. The file is in Example/main.cc, and I want to copy it to my Example folder on my Desktop. And just to distinguish between the two files, I'll call that main2.cc. Hit Enter and it's copied over. We can open up that file, and you can see, there it is, main2.cc. All right, so now you are able to copy your files back and forth on Linux, or it'd be the same on a Mac, and also run your code on AWS. So now we have a couple different ways to run DL2 either through AWS on Mac and Linux using SSH or on Windows using PuTTY or Mindterm. Or we could use a virtual machine on Linux, Mac, or Windows. All right, so you should be all set up now to run the example code or to run your own homework files using DL2."
Fy0zdJQvTbI,"Welcome back, we continue with our development of the finite element equations. At the end of the last segment we got as far as looking at the individual integrals in the finite dimensional weak form, the weak form and writing them out as matrix vector products, okay? This is where we're going to pick up today. The topic of this segment at least is going to be the finite element equations in matrix vector form. Okay, now, what I'd like to do here is, perhaps, just recall the forms the integrals. Okay, so, we saw that when we put everything together, we had, for this integral, integral over omega wh, x sigma h Adx. We had the following, we had the rho vector of degrees of freedom corresponding to the weight in function. And this began with this particular entry, right, C2. C3, in terms of global degree of freedom numbers, right? This went all the way until we came to Cnel + 1, okay? This was multiplied by EA. And then, we had this matrix, which I'm going to try to write out really neatly today. So if I recall correctly, in this position we have -1 over h1. In the next we have 1 over h1 + 1 over h2. And here we ought to have had -1 over h2. In this position, we have -1 over h2, and here, we have 1 over h2 +1 over h3. Here we have -1 over h3, -1 over h3. Here we have 1 over h3 + other terms and I'll stop because if not I will, I won't have a suitable point to stop, okay. This continues all the way down until we have in the very last position here, the very last little sub matrix, sub two by two matrix. We have 1 over hnel -1, 1 + 1 over hnel- 1 over nel and here finally we have 1 over hnel- 1 over hnel, okay. And this matrix is closed here, and okay. I'm going to multiply this by a row vector, okay, because I've run out of room there. I'm going to write to row vector down here. Sorry, the column vector down here. This column vector has global degrees of 1, 2, all the way down to dnel + 1, okay? And let's just recall that this goes up here, okay. This is what we had for our integral on the left-hand side, all right? For the integral on the right-hand side, which as you recall comes from the force, we had the following. The integral itself was integral over omega whfAdx. And this, when all was said and done, also was multiplied by the same c vector. Up to cnel + one. And multiplying it here was fA, and let me see. I think I add an fA, perhaps, over 2. And the contributions here were the following. We had in the very first position, we had h1 + h2. Next position we had h2 + h3. And so on. Up to, in the very last two positions, we had hnel- 1 + hnel, and finally here we have hnel. Okay, all right, we had this and then we have the very last term, which was an easy one to write. The last term was Wh(L) tA and this following the same convention that we've adopted. For the other terms was multiplied by the vector of C degrees of freedom. Which is probably the waiting function degrees of freedom going all the way up to Cnel + 1. And the vector that it was that that rho vector was multiplying, the column vector here was filled with zeros all the way up to the very last entry. And at the very last entry we had tA. Okay and I realize now that for all the h's here, for all the element lengths, I used super scripts. Up to now, but the first two slides of this segment have been using subscripts. We'll go back to superscripts, but let me just put in a remark here, so that if you review these lecture you will note that as well. We're using h sub e here instead of h sup e and we're now going to go back to h sup e, okay. More photos on the next slide. Okay, so we have these terms here and what we want to do now is to begin assembling them. As a first step what I'm going to do is just to get a little more insight and make it a little less unwieldy, I am going to say that we are going to consider the case where all the element lengths are the same. The development so far and especially the form we have on the previous slide on this one and what we've done in the previous segments should convince you that method is completely applicable to formulations where or the measures where the elements do not have the same lengths. Just for some sort of gravity in writing and convenience and some on my defense side as well, I'm going to say all the HEs are the same, okay. So let me just make this remark also here. Consider he constant for all e. So there's a single h e we don't need to distinguish h1, h2 and so on, all right? It just makes things a little easier to write. So now if we go back and look at the matrices we've put together, what we should observe is something about the sizes of these matrices, okay? So we'll do this, but now we're going to use common element lengths. Okay, note that the finite dimensional weak form that we have with all these matrix vector expressions is essentially this. C2, C3, Cbel, plus one, times EA. Now, because we have a common element link, just for convenience I'm going to put it out. Again, this is what I wanted to achieve as well. Okay, this now multiplying the matrix which has the form minus 1, 2, minus 1, minus 1, 2, minus 1 and so on. Okay, until we come all the way down here to 2 minus 1, minus 1, minus 1, 1. Okay, all of this now multiplying the displacement degrees of freedom. If we're doing an elasticity problem or more generally the solution degrees of freedom, right. The trial solution degrees of freedom, okay. This is our entire left hand side of the weak form equal to on the right hand side, we have C2, C3, Cnel+1, fAH over 2. And now again because all the H's are the same we have here 2, 2, all the way down until we have a one in the final entry, okay? Plus C2, C3 Cnel plus one multiplying zeroes everywhere in this column vector until we come down to ta, okay? So for the special case of a mesh where all the elements have the same length HE, we have this nice simple representation, right? And that's an HE there as well. Okay, what I'd like to do here now is just pay some attention to the. Sizes of the relevant vectors and matrixes should be completely clear of course it's pretty much written here the way we've written out the scroll vector. That this scroll vector has dimensions in year, so of length and year. Right, simply because it's going from two to nel plus one, right? This column vector here is NEL plus 1, which leads us to conclude that this matrix is of a dimension NEL cross NeL + 1, okay? Essentially, the same thing on the right hand side tells us that this column vector, as well as that one, are off length NeL. And like and of course this is the same, the c vector is the same one, right. Right so both of these are off length Nel. Right, obviously just for consistency of the matrix vector products. Now. So we have this form and let's see what else we know about it. Can you recognize what we know about one of the degrees of freedom in the vector of nodal trial solution values. So look at that vector, the D vector carefully and think of what we know about at least one of those degrees of freedom and in fact one of those degrees of freedom. Because of the chronicle delta property of our basis functions, we've demonstrated that this particular entry, right Is actually use zero, not right, and that comes about because you remember what our mesh looks like. Right so this is element one and this position here is X. This node is X 1 equals zero. On that node the degree of freedom is D 1 which is U zero. Okay, which is a node Right? Because it's R, the traditionally boundary condition. Right? What this tells us is that when we look at the matrix vector product on the left hand side of this matrix vector equation, we can actually take this entire column, right, and I'm going to do something which is a little not strictly legal. But I'm going to draw that column vector, right. And because of the nature of this matrix, okay? Sorry. The other entries in this column vector are 0. Right, all the way down. Okay? All right. Essentially what we can do now is because we know d1 equals u0, right? And we recognize that this particular column vector that I've marked out in the matrix, is the one containing matrix elements that multiply D1, which is equal to U0, which is known, we are at liberty to actually move it to the right-hand side. Okay. All right, so what we will do here is, okay, is the following. We are now going to write this as C2, C3, CNEL plus 1, EA over H E times the matrix now that is considerably simplified, well not considerably, but at least somehow simplified. Two minus one, minus one, two minus one, going all the way down to minus one two minus one. Minus one one. Okay and the diagonal is this one. And the upper and lower diagonals of those ones. Okay. This thing multiplying the vector now d2, d3, all the way down to dnel+1 right? = C2 C3 up to CNEL + 1. Now multiplying, I'm going to put all the column vectors together here, okay? The first column vector is fAhe over 2. Multiplying twos all the way down except for the very last element which is one. Plus another column vector filled with zeroes all the way down to the very last entry which is T times A. And now there's a newcomer to the party here, right? The column vector that we looked at here, I'll draw yet another arrow to it, can be moved to the right hand side. Recognizing that we have a minus one in this entry, okay, we have on the right hand side, EA over HE multiplying D1, but D1 is known right D1 is known to be equal to U not because of two things. Because of the tertiary boundary conditions, as well as though the chronicle delta property of all basic functions. Okay. Now when we look at the size of our matrices, we recognize that really it's just this is the only one that we need to correct because it's lost a column. Early it had nel rose and nel+1 columns, so it has now lost a column, okay? So and we are going to recognize therefore that it is a matrix of size nel cross nel. Okay. And we're going to do one more thing. We are going to call, so nel across nel is the dimension, or rather the dimensions, of this matrix. And we are going to write this as the matrix key. Likewise for obvious reasons we're going to regard this vector of degrees of freedom for the waiting function as the vector c. Since all our vectors are we like to think of vectors as being column vectors and recognizing that we've written this a a role vector, I'm going to call it c transpose. A vector is terminology that we reserve for column vectors. Likewise, this vector, again for obvious reasons, is going to be my D vector. Okay, here again we have our C vector and now I'm going to look at all of this, right? All of these contributions together and call them my F vector. Okay? I just recognized this c vector again is a c transpose, again using this idea of row and column vectors. Okay?"
01LvqvKwHrU,"Let me pull all this together and write it in a very simplified manner. Okay, so what we have here is c transpose Kd equals c transpose F. Okay. This is close to the final form of our finite element equations in matrix-vector form. Okay? But there's one other important step we need to take here. Which is, to recognize that this vector of weighting function, degrees of freedom, actually does interpolate the weighting function. Right. So recall that in each element, we had wh of e equals sum over A, NA cAe going back just briefly momentarily actually to our local element numbering system for the c degrees of freedom. Okay. Also recall that our weak form, our statement of the weak form is the following, right, in much abbreviated, in a much abbreviated version it was defined belonging to S such that, St is short for such that. For all wh belonging to V, and I'm sorry, these are both, that's Sh and Vh, right, right? For all wh belonging to Vh the following integral holds, the following equation holds, right? So this is our weak form again, yet again. Okay. What we've done now is essentially reduced that integral weak form to the equation at the top of the slide. Okay? In matrix-vector form. All right? So, and let me, yet again, state that here. All these statements, while, while a little superfluous, are repetitive definitely, are useful to drive home points. So we, we make them repeatedly. All right. So, that is our matrix-vector weak form, and the important thing that I want to point out is that this holds for all wh belong to Vh. My question to you now is that, what is it in the matrix-vector weak form that represents wh? Remember, using basis functions, finite dimension basis functions, we've gone from this integral form at the bottom of the slide, the last equation on the slide which involved fillings. To matrix-vector form which involves just degrees of freedom for matrices for vectors, and we have entries for matrices. So in that setting, what is it that represents wh? Okay? Essentially, what represents wh is the following. This equation of ours is completely equivalent to our statement at the top of the slides. C transpose Kd equals c transpose F. When we realize that this has to hold for all c, right? Belonging to this R nel space. After all is just a vector with nel entries, okay? It is when we specify that our matrix-vector weak form must hold for all c, right? Because all c belonging to this nel degree of sorry, this nel dimensional space that we are essentially imposing the same requirement as we do here. Why is this? It is because the c of degrees of freedom which interpolate the weighting functions. If our weak form has to go for all weighting functions belonging to Vh, we've already fixed the functional form of Vh by choosing particular polynomial basis functions. All right? Or by choosing certain basis functions. That degree of arbitrariness which must still hold within the space Vh is insured if we require, that the matrix-vector weak form holds for all c belonging to this nel dimensional space. Okay, well, but if this is, if this is the case, it is clear therefore that this can only hold if Kd equals F. There's a formal way of demonstrating that, and this just involves moving everything to the left-hand side, and then considering what forms c can possibly have, all the forms that c can possibly have and discovering that this must be the case. Okay. We can make that, that rigorous argument if you need to, if we need to, but I think it's pretty clear why, why this works. Okay, so, this is the final form of our final, for, for our finite element equations, okay? FE short for finite element. Right. Remarks. Okay. The first remark I want to make is that the matrix K that you have in front of you or probably in your notes is symmetric. It's positive definite. Okay. Furthermore, it has abandoned, so-called tridiagonal structure. With  banded, tridiagonal structure. Okay. The symmetry comes from the fact. Symmetry from the fact that the term of the weak form which gave rise to our matrix K has this sort of structure. Right? Where this is our stress. Okay. So if you look at this integral as a as an object called a functional in w and u. It is bilinear, okay? Furthermore, it remains unchanged if we just swap the places of w and u, okay? Right? So the reason this thing is symmetric is because we have this term in the weak form and we could very well interchange those two positions, right? Wh and. Okay? This is what makes our final K matrix symmetric as well. If we were working with a different set of equations which, for some reason, did not give us this form of an integral where wh and could be interchanged, and still have the same meaning, right? And, and still have the same integral, then we would not have a symmetric matrix K. Okay? Oh, of course, I should also state that now K, again for obvious reasons, is called a stiffness matrix. Okay, and as you might imagine this is related to the fact that when we look at it in, in the form Kd equals F which you have before you it is reminiscent of a of a spring. Right, K being the spring stiffness, d being the displacement of the spring and f being the force on the spring. And indeed, in the early days of finite elements when there was a lot of structural mechanics work done with finite elements, this was the, well this was the right interpretation, okay? But now if you, if you imagine that you're doing a heat conduction problem, you would probably would, you would probably, properly call it something else maybe the conductivity matrix, right. But traditionally, it's called a stiffness matrix. So let me just put that in quotes. Okay now, the other bit is the positive definiteness. The positive definiteness comes from the fact that our in this particular case, our so-called material constant, E, is greater than 0. We didn't actually state that but if E and D is greater than 0, then we have a positive definite system. All right the particular banded tridiagonal structure comes from the fact that there are two first order derivatives, and that we have a linear set of basis functions. Okay, so. The bandedness is actually is, is included when I say tridiagonal. So tridiagonal from. Single derivative on wh and. All right, in this form that, that's before us in the integral, okay there's a single derivative on each of them. And from the fact that we have a linear basis functions. Okay. The next remark I want to make, and really it's the final remark of this segment, it is the fact that our force vector in this case, I'm just looking back to my notes here to make sure that I have, oh, there we go. That I have here before me, so that I don't make any errors in putting in back up here, okay. Our force vector in this case is the following. It's fAhe over 2, times 2 being all the way down to 1, plus 0 all the way down to tA, in the last entry. Plus EA over he, multiplying our Dirichlet boundary conditions u knot, and zeros all the way down. All right, there are a few things I want to point out about this. If we look at this contribution to the force, to the force term, it comes from the distributed forcing function that was specified at every point along the bar. Right, our original function, F. Right, which we assume is a constant in order to get this final form. If F were not a constant, it would be inside this it would be within this the column vector here. Okay so, so one thing to note is that if we consider the mesh and consider a stretch of the mesh with elements of that type. Okay? What we're seeing is that the contribution on a node, which has elements on either side of it, is of the form fAhe. Okay? It's only the very last element. Right? This is the point L. Okay? It's only this very last element which has a contribution, fA. So at this very last node which has a contribution fAhe over 2. Okay? And this is really because an interior node has a contribution fAhe. It sort of has the entire force on it. Because it has elements on both sides. So both those elements contribute force to it. However, this last node has a contribution only from the element to its left. Okay? This is pretty obvious. This contribution is from the traction and it's no surprise that it appears only on the very last node, because that's where the traction is indeed applied. This contribution is also very important one. You recall that this came about from the fact that Dirichlet boundary condition is known and therefore that degree of freedom in the d vector in our trial solution vector could be moved over to the right-hand side. Dirichlet b.c for boundary condition. 'Kay? So what this is giving us is the loading, right, on the problem, obtained by specifying a Dirichlet boundary condition, okay? So this is what you may call within quotes, a Dirichlet driven load, right? You, we, we all very well know that partial differential equations can be driven by either the Dirichlet or Nuemann boundary conditions. This is how the Dirichlet boundary condition itself drives the problem. Of course, in the kinds of problems, we are considering, the sort of boundary value problem we're considering, u knot was typically zero. Okay? So that actually drops out. But in the context of elasticity, what it does for us is give us the effect of the, the, the, the, the load that gets transmitted to the structure. Because of the fact that on the left end, at the left end of the structure, we've fixed the displacement to be equal to zero. Okay? So let me say just one more thing here. This Dirichlet driven load is equal to 0 in the considered case. Okay. So there we have it. The various contributions to the, to the effective forcing on the problem, coming from the distributed forcing function from the traction, the, the Neumann boundary condition, right. Since I'm calling it traction, I also ought to say that this is the Neumann boundary condition, and finally, the effect of the Dirichlet boundary condition as well. Okay, we will stop this segment here, but essentially at this point we have completed our simplest finite element problem in 1d for linear elliptic equations. Okay. Well, I should, I should also state one more thing, right. I've just said it's formulated the problem. What is the solution? Since we have Kd equals F, the solution is d equals K inverse F. Okay, that is the formal solution. Of course, you may or may not actually invoke K depending on the size of your problem, that is an entirely separate question which we don't get into here. This is just the formulation. Once we have d, of course we can then go back and reconstruct in any element as being simply the interpolation over A using the bases functions, which we know very well, times dAe. Okay? So we've recovered the actual field from the solution. Okay, here's where we definitely do stop the segment."
Nrwrd8DLPG0,"So, there was a question on, the definition of symmetric and positive definite matrices. I realized that I had neglected to define them so, here we go. So. A matrix K is symmetric. Right? Matrix K is symmetric if K equals K transposed. Okay. What this does for us is that in the context of the kind of, views we were putting this matrix to, it means that if we consider our vector C, okay, so if N we can compute C transposed K d. This turns out to be exactly equal to d transpose KC, right where d and C are vectors as defined in the previous segment, okay. So this is what we mean by seeing a matrix as symmetrical. Okay. Now, a matrix is positive definite. Positive definite. If, right, and once again let's use the same matrix key. Okay, the matrix key is positive definite if, let's do the following, if, first of all, let's, we, we need to pay attention to the dimensions of K. Okay, so let's do what we had here. Let's suppose K is an n plus n matrix, all right, and the problem we were looking at we got an NEL cross NEL matrix, okay? All right, so if K is an n cross n matrix then it is positive definite. If, and for all d belonging to Rn, right, for our n dimensional vectors. We have d K d d transpose K d is greater than or equal to 0, okay? In particular, okay. In particular. D transpose k d Is greater than 0 if the vector d is not itself the 0 vector, okay? The 0 vector is simply a vector with 0 in every single entry. Okay? And then what that means, of course, is that therefore if K is positive definite, then d transpose K d equals 0. If and only if, so IFF means if and only if, okay? D equals 0. Okay, so essentially, the, the notion of a positive definite matrix is in, is a generalization to matrices of the notion of a number being non negative. Okay, so if a number or a, or a variable can be either only positive or equal to 0, but never negative, then the equivalent definition of a matrix is, positive definite. Well, actually, that's, that's not strictly right. It, it, it is the generalization of of a variable being positive. Okay? That's what makes, that, that is equivalent to a matrix being a positive definite, okay, but anyhow, that, that is just an interpretation, the important bit is this these, the, this definition. All right, we'll stop here."
hDDEtp70YcI,"Hi. In this segment we will begin looking at the coding template for the first coding assignment which is homework two. All right. So, before we look at the code itself, think to yourself about what are some of the functions and data objects that you'd want to include in your C plus plus program. All right, think about that on your own for a minute. Okay? Now, let's write down some of those. First, we would have our basis functions themselves. Right, so those would be the functions that are interpolated for you to create your finite element solution. And along with the Basis Functions would also be the gradients of the Basis Functions. If you look at your first assignment, you will also see that another function that we need to create is a function that will calculate the L2 Norm of the error between the actual solution and your finite element solution. So that's another function that you will need to create. Now how about the steps of the finite element method? What are the different steps that we go through? Well, we have to generate a mesh. Or import a mesh of some sort. Right? We need to create F local and K local At some point. And once those are created, then we have to assemble F local and K local into the global stiffness matrix and the global forcing vector. All right, and then we also have to define and apply our boundary conditions. Now those can be Dirichlet boundary conditions or Neumann boundary conditions. Okay. And after that's created have K and F, then we have to solve for D. So we have to have some function that solves, for D. So D = K inverse F. And then you'll probably want to output your result somehow into some format. Alright, so these are some of the functions that we would have. Some of the data objects themselves, what we've already mentioned here, would need our stiffness vector, or our stiffness matrix, K. We need F and D as well. Our foreseen vector, and our solution vector. We would need some sort of object that would relate our local degrees of freedom to our global degrees of freedom. Sometimes we call that a connectivity matrix in general. Although it may not actually be a matrix so I'll just call that a connectivity object. We, of course, would need some sort of object that actually stores the information in the mesh, which would be related to this connectivity object. So, I'll include that here in mesh. All right, there may be others, too. We'll look more at that now as we look at the code, okay? So let's move over and look at the code itself. As we told you in the homework file in the homework description, we're giving you two homework files. There's the .cc which is the source code and a .h file which is the header file. The header file is what includes your finite element class. All right. So, that contains the bulk of the programming. But to begin with, let's look at the main .cc which is the main program that's running. All right. So, here you can see I've included some standard C++ header files, but I've also included this FEM homework to template.h. That's your header file that includes the information about the class. And here on line 12, you can see that I said this using main space deal.II. Now the reason for doing that is because we have a lot of deal.II objects and normally, we would have to type in deal.II, double colon, and then the name of the data object. Just like with standard. For example, standard vectors we type in SDD::vector. If we instead wrote using name space standard, then we wouldn't have to do the standard colon colon every time we wrote vector or c out, for example. In this case, we're using namespace deal.II. All right. And then our main program. Okay? This first part, it just has to do with some of the mechanics going on. Our program really starts here. First we define the order of our basis function. That will be an input to the class constructor. Here I have order equals one but again for the first homework assignment, it can be linear, quadratic or cubic. Okay? So either order one, two, or three. Also there are two parts to the homework problem, part one and part two. And so again that's also an input. The only difference between part one and part two is in the boundary conditions. Problem one has two Dirichlet boundary conditions and problem two has a Dirichlet and a Neumann condition. Alright here on line 25 is where we actually declare our object. The name of the class is FEM. It's a templated class and all I'm going to explain about that is that there is a template parameter that goes into the class that the functions and data objects are based off of. In this case it's the dimension. Okay, so I put in a one here between the angle brackets. Just specify that it's a one dimensional problem that we're working on for this homework assignment. I'm naming it problem object but again that's just a name I'm giving it, it's not the class name, it's just the name of this particular instantiation of the class. And again the inputs are order, the lagrounging basis function order and the problem number. All right, so now we get into some of the actual functions. Remember, in order to call a function for a class, we first include the name of the object and then a decimal and then the name of the function. Alright, so the first function that I'm calling here is generate mesh, and as an example here I am inputting 15. That just tells, or this is where we specify how many elements we want in our mesh. So in this case I am saying a 15 element mesh, of course you can change that. In this case, with the way I've set it up, this will make 15 equal elements. Okay, all the elements are of equal length. Then I call this function, called setup system. Of course this doesn't really come in from the finite element method, but it's mostly for initializing our data objects. Resizing the force vector, the solution vector, the global stiffness matrix. That's also where we will be setting up, or defining our quadrature rule. We'll decide how many quadrature points we need per element, and then we define what those points are and what the corresponding weights are. After that is assemble system, and it's within assemble system that we actually create K local, F local and then assemble those into the global stiffness matrix and the global force vector. All right. We've previously defined the Dirichlet boundary conditions within setup system, the Neumann boundary conditions, you'll see we define as we're creating F local. And then we actually apply the Dirichlet boundary conditions within assembly as well. Solve is a very short function that simply solves for D equals cambers F. Here, I am outputting to the screen, the L2 norm of the error. For this, for the given basis functions, and for the given number of elements. And finally, we output results. We output results as a .vtk file. And for the linear problem, there's actually not much to see, since it's just a straight line. But when we move on to the 2D and 3D problems, then I will show you how to open up one of those files, in a visualization tool, called ParaView. Okay? So let's move on to the header file. Now scroll to the top here.  Now in the comments you see at the top. I've specified that anywhere you actually have to make a change or modify the template I've included the word edit in capital letters, okay. So, anywhere you see edit you'll have to edit something, otherwise you can let it be. Also, as you're writing your code don't change any of the function names or inputs that I've already set up for you. You are, however, free to go ahead and create new functions, or additional functions, if you find that helpful. So there are a lot of header files here that you can see in the code. And almost all of them are DO2 header files. We actually won't necessarily be using all of them in these first homework assignments, such as quadrature point. We'll be creating our own quadrature points, we'll be creating our own basis functions. However as the class goes on, in the last two homework assignments you will be using more and more of DO2's actual functions. Okay. Now let's scroll down to the declaration of objects. So I've created everything as public, so that means we can access all of these objects and functions from main.cc. All right, now to begin with here's our constructor, and we've already seen this constructor in main.cc. We have as an input the order, the order the Lagrangian basis functions, again, that's one, two or three. For this homework assignment, of course it could go higher if you chose. And then the problem, again that's part one or part two. Here we also have a class destructor. No inputs to it, but you will see that there is something going on in the destructor. Now, here we get to the functions that I've set up for you. The first function is a function called xi_at_node. And I'll explain a little bit more what that means when we get down into the actual definitions of the functions, here we're just declaring them. But it has to do with, the differences in the local node numbering between DL2 and what we've seen in the lectures already. So this function will help you get the value of xi using the DL2 node numbering. Here we have our basis function and basis gradient. And now down to the solution steps. We have generate mesh, which we solved before already. Define boundary conditions. That's actually called from within setup systems, so that's why we didn't see that in main.cc. Then again setup system, assemble system, solve, and output results. And again, after that, we have the l2norm_of_error functions. So those, the rest of those are functions we saw already in main.cc. That concludes the functions. Now, we have some class objects. These first three are objects, are DO2 objects that they've, that they've defined and we're declaring them here. Triangulation basically stores the mesh. FE and DOFhandler have to do with the connectivity, keeping track of the degrees of freedom, as well as general basis function ideas going on. Okay? Now we get into our own data objects. And these next three have to do with the Gaussian quadrature, that we'll be using to perform the integrals, that occur in K local and F local. All right. So the first is an integer called quadro. It's just the number of quadrature points. In your element. Now, you'll have to decide how many quadrature points, and we'll look at that a little bit more later. And then we define what those quadrature points are, again in 1D, and the corresponding quadrature rates. All right. So, it's an integer and then the next two are standard vectors of doubles that we've looked at before, in our review of C++. Okay. More data structures. The first one is a sparsity pattern. And we use a sparsity pattern with our global stiffness matrix, K. Because as you remember from the lectures, most of K are zeroes. And so, when we're dealing with a large stiffness matrix it doesn't make sense to store a lot of zeroes and so we use what's called a sparse matrix and it only, it essentially stores just the non zero components of K global okay. Sparsity pattern is what keeps track of where those locations actually are in the real, in the full matrix itself, okay. We have a vector of doubles, D and F, that's our solution, solution vector and our global force vector. You'll notice this isn't a standard vector. It's a DO2 Vector. There's a capital V. And so the functions related to vector, the DO2 Vector are a little bit different than the standard vector. And mostly we use that in our solve function. When we're solving for the solution vector D. Okay. Now, I have this standard vector of doubles called node location. This is a vector that just gives you the x coordinate, okay, and the real domain of each global node. And then, a map. A standard map called boundary values. Let me give a little more information on the standard map. This is a C++ data object, the standard map. And as you can infer from the input there, you have to define two data types. In my case, in this case we've defined two number types. This first is an unsigned int, and the second is a double. And so this basically just, as you might expect from the name, it maps an unsigned int to a double in our case. One big difference though from a vector, is that a vector, in a vector the indices start at zero and they just go sequentially up from there. So zero, one, two, three, and so on. With a map, what you might call the index, doesn't have to be sequential. So in a standard map, the standard map of boundary values. So we have a standard map of unsigned int, and that should be. Let me erase. Should actually be an angle bracket. Okay. Unsigned int and double. Called boundary_values. Okay. So in boundary_values, we will be relating the Dirichlet boundary node number, the global node number, and the actual value. So, if we have, if this is our domain, and we're breaking it up into elements. Let's say at the left we're applying a Dirichlet boundary node of g1 = 0 and at the right we're playing a boundary node value of, a Dirichlet boundary value of g2 = 1. All right? So all we need to, and I'll let these be our global node numberings so I'll just go sequentially, zero, one, two, three, four, and five. So in boundary values we want to store the node number, with the Dirichlet value. So the node value is an unsigned int. The Dirichlet value is a double. Okay? And so all we need to do here is we do boundary_values. The index, this first term, you can think of as an index. And so at node zero we have a boundary_value of again zero. Alright so there we've defined that, we've set that into this boundary values map. On the right, at node 5, we have an applied boundary condition of one. Okay, and so now, whereas in a vector we would have had to have a vector of length 6, in order to have an index of 0 and an index of 5. With a map, a standard map, this only, if you were to do a boundary_values.size, it would return a size of 2, because we're only storing two values in this map, okay? So let's go back to the code. After the boundary values map, I've defined several doubles. There's the basis function order, problem, or prob. Those again will store the Lagrangian order that we're using and the problem, part one or two. Then I have L which is the length of the domain. G1 and g2, which again are the Dirichlet boundary values. G1 at the left and g2 at the right. Okay, now under here, solution name array, these are just vectors that have to do with the output, so you actually won't be dealing with these at all. Okay. Let's stop this segment here and in the next segment, we'll start looking at some of these functions themselves."
FmCG-MbeGOw,"Okay. We will continue with other aspects of our finite element method. We're sticking still, with our linear one dimensional elliptic PDE. But what I want to do now, is take the opportunity to go to the case, where we could have a problem, a boundary value problem. where all the boundary data are specified as Dirichlet data, 'kay? So, what if there is no Neumann data? Okay. So, we will call this segment pure Dirichlet problems. Okay? Or just, Dirichlet problems. So, in the context of what we are doing here, the statement is the following. Find U h belonging to S h, which is a subset of S, where S consists of, all functions u such that u, and 0 equals u not, and u at L equals u given, okay? Now, since, S h is a subset of S, what this implies, of course, for us, for our finite dimensional weak form is that S h consists of functions u h. Now, they could belong to H 1, right? Like we have considered for our finite dimensional weak form. When we consider Dirichlet, and Neumann data, okay? So, u h belongs to H 1, and omega, such that, u h, and 0 equals u 0, and u h, at L equals u g. Right? That's the kind of u h, we want to find. Okay. So, find u h of, of this form such that For all w h belonging to V h, and just for consistency, with whatever the problem, we do this properly. V h is a subset of V which consists of all functions w, such that, w at 0 equals 0. And now, w at L, also equals 0, okay? So wherever, we have Dirichlet data, the weighting function satisfies the homogenous Dirichlet condition, okay? And then, this implies for us that V h consist of all functions w h, also belonging to the space H 1. On omega such that w h of 0 equals 0, and w h at L, also equals 0. Okay? Okay. So, essentially, we're saying find u h belonging to S h, where S h consists of functions that have that are H1, and have Dirichlet data on both ends. Okay? Such that, where all w h belonging to V h, which now, consists of functions satisfying homogenous Dirichlet data. Right? At x equals 0, and x equals L, 'kay? Okay. What needs to hold? Well, the same old weak form. Integral over omega w h comma x sigma h A d x equals integral over omega w h f A d x, and we are done. There is no Neumann data, right? There are, there are no Neumann boundary conditions in, in, in this problem. So, we don't have the contribution from the traction, right? This is it. Okay. Essentially, the formulation is the same, as we've studied for the Dirichlet-Neumann problem. The only detail is what happens with our homogenous Dirichlet conditions, on the weighting function, okay? So here is the picture actually. So the physical picture, if you care for it, is this one. Right? At this end, we know, that we have u not. Which we know, we are thinking of, as being equal to 0. At this end, we have not a traction condition. But we have that displacement, if this is an elasticity problem, is equal to some given value. Right? Alternately, if this were a, heat conduction or a mass diffusion problem. It would say, that we would be saying, that we have to the temperature specified, at both ends, right? Or maybe the concentration specified, at both ends. Right? And we have of course, our forcing. Right? This is f. Okay?  Sorry. All right. This is our forcing, in this direction. Okay. Right? So, so this is the physical picture. Now, we proceed just as before, right? Let's assume that we have a decomposition of the domain, a partition of the domain, into our elements. Right? And the subdomains which are elements. That would be omega 1, that would be omega N e L. Okay? Now, observe that what we are saying is that because of the form of our Dirichlet data, at both boundary nodes. We are going to do something special about the weighting function, basis function, the basis functions for the weighting function, in elements omega 1, and omega N e L, okay? So, what this implies is that As far as basis functions go. Here again, let's suppose that we are working with linear polynomials Over You know, these are polynomials that compact support, right? Just as before, all of that is the same, right? Over each element omega e, okay? So then, what we're saying is that w h in element e equals 1, is equal to only, basis function 2. Okay? And, and our basis functions 1, and basis function 2 are exactly, the same as before. Okay? So, we take only basis function 2, function of c through x times c 2 e equals 1. Okay? And the same sort, well, the similar sort of thing, holds for element e equals N e L. Okay? This one is, which one, which basis function do we use, for element N e L? Remember, that this element is this one. Element 1 is this. Right? And so, what we're saying is that for omega h, we use only that basis function, right? In element 2, sorry, in in element 1. Element N e L on the other hand, is going to be this one, right? I jumped ahead of myself, by also drawing the basis function, right? But you get the idea. That is a basis function, the only one that we will use in element N e L. Okay? So, this one is just N1, c 1 e equals N e L. Okay? So really, this is the only thing. This is really, the only difference. Now, when we go back, and look at our the integrals contributing to our weak form, okay? So, we get the following, okay? Integral over omega w h comma x sigma h A d x equals, now, when we work things through, what we will see is that, we get in element 1, we get c 2 1, okay? Multiplying e A over h e Minus 1, 1. And here the co, the column vector is d 1, for the element 1, d 2, for element 1. Okay? We get the usual summation over all the L, over the remaining elements, e equals 2 to N e L mi, minus 1. Okay? I'm sorry, I should Got too close to that one. Okay. Right? Note that the sum goes from element 2 to element N e L minus 1. Okay? So, that means, that we are looking at all the elements in between here, right? As contributing, the same sort of term. Okay? As I'm about to write out in this summation. Okay. So here, we get properly c 1 element e, c 2 element e. EA over h e. Our little matrix here, which gives us our element stiffness matrix, would be 1 minus 1 minus 1 1 And multiplying it here, are the degrees of freedoms d 1 e, d 2 e. And then, added on to this, is the very last, the very contribution, from the very last element, from element N e L, which we have above us here, in the figure. Right?  You know in the figure.And up here we see, that the contribution comes only, from That degree field. Okay? And in our global numbering system, right? Well, let's look at it, in our local numbering system, that is c 1 N e L. Right? Element N e L. Okay? Just as this, as far as c degrees of freedom were concern, was c 2, element 1. So, here we get c 1 N e L. Multiplying. E A Over h e. Okay? E A over h e, but now, it, this contribution from the So called stiffness contribution, from this element, it takes 1 to form 1 minus 1. And here, we get  it is multiplied by d 1 for element N e L, and d 2 for element N e L. Okay? And in a related manner, right. In a related manner, what we will see is that, what we obtain is that, as far as our force contributions are concerned, right? Which come only, from our forcing function, there being no traction condition here, w h f A d x equals. Okay. So just as before, we, we get a contribution c 2 1 f A h e over 2, okay? This would be the only contribution, right? From element 1. From the other elements, right? We will get a contribution, which has the same form as our main sum over all the elements, when we were doing the Dirichlet-Neumann problem. Right? This is the sum of e equals 2 to N e L minus 1. And multi, and here in this sum the, the contributions from the, from the degrees of freedom for weighting function c 1 e, c 2 e F A h e over 2. Okay? This thing now, will be multiplied by 1, and 1. And finally, from the very last element, we would get once again c 1 N e L. We would have f A h e over 2. Okay? These are the contributions, we would get. Right? And really this you know, if you go back, and look at our general Dirichlet-Neumann problem. What you will see is that, we are just accounting for, if you look at this term, right? If you look at how, we obtained This term. Right? You could go back, to the Dirichlet-Neumann problem, right? And look at what we had, for the very last element. And in there, simply set c 2 N e L, right? Equal to 0. Which should be like, setting the weighting function, corresponding to the second node of that element equal to 0. Okay? And this is what, you would be left with. And the same would apply, for this term here. Okay. If you were to do the same thing, in the force contribution. Right? So this is all, you would be left with."
C5QsF96kCg8,"So. So we have this and now let's look at what the contributions are to our stiffness matrix and our force vector. What I'm going to do now, to do that, in order to get there is to simply carry out the process of assembly, right? Of these two equations, the last two equations on this slide of that and that. Okay? Recognizing of course that we have this map, right? So this map between local and global degrees of freedom. Okay? So what we do is we carry out now the process of finite element assembly. Okay? Recognizing that our vectors such as c1e, c2e, right, is equal to, is, is actually identically equal to our entry ce, ce plus 1. Right? And it's, and, and basically the same thing for the d vec, for the d degrees of freedom as well. Okay? So. In element e, d1e d2e, or d sub e and d sub e plus 1. Okay? These are local degrees of freedom. And these are global degrees of freedom. Okay. So, when we put all of this together, what will we get is is the following. Right. We get here, the matrix c, sorry. The row back to c2, c3, all the way up to cnel, multiplying EA over he. And I didn't state it explicitly, but you note that I have already fallen to using our our special case where all the element lengths are the same. Okay? All right. So we have that and now the matrix that we will have here will have the following form. We'll have minus 1 here, 2, minus 1, minus 1, 2, minus 1. Okay.  So this cnel corresponds to c1nel, right? In terms of local node numbering. And what we see is that this process essentially ends with a 1 here and a what, sorry. It's, there's a 1 on the diagonal here.  There is a 1 on the diagonal here, and a minus 1 here. Okay. This is multiplied by a column vector which is full. Right, there's nothing lost in this column vector for d degrees of freedom. Comes all the way down to dnel plus 1. Okay? This is equal to, we're going to write it down here just because I just need more room. We have, for the forcing vector, we have c2, c3, all the way up to cnel. Okay? Again using the, the special case of all the element lengths be the same, we have fAhe over 2. Okay, and now the contribution that we get here is of, of a sort where all the entries are 2. Okay. As far as the sizes are concerned, we note that this now has size nel, minus 1, right. It's clearly, if something is going from 2 up to nel, right? This now has length nel minus 1, okay? Whereas this vector has length nel plus 1. Accordingly, the dimensions of this matrix, right, are nel minus 1 rows, and nel plus 1 columns. All right. This is also nel minus 1 and therefore so is this nel minus 1.  Okay. All right, and here now that we're we are getting used to thinking about these kinds of ideas, we make the observation that d1 is now equal to u naught, right, using the kronecker delta property, right? Whereas dnel plus 1, which is the trial solution degree of freedom corresponding to the last node, which is the second Dirichlet boundary condition at x equals L. This is u given. And both these quantities are known. Okay? And also note that now that we have this form, recall that the other elements in this column vector which I am going to mark out in a step that is recommended only for clarity and not something that you should normally do in a matrix. All right we have right over here two. We have zeroes, right.  All the way up here. Well, it's a good thing I turned back to look at this matrix, because I realized that I had forgotten that in this very last element here, the one, I made an error, right. There is a contribution coming to that element or, or to that entry from the nel minus1 element. So that entry there, properly, is 2. Okay. If you just go, again, if you would go back and look at our contribution to the, to the eventual stiffness matrix from our Dirichlet, Nuemann problem, you would see that there is a two there. Right, I was simply forgetting the fact that there was an, an element before it. Right? Okay, so those are traps for the unwary. And for some reason I seemed to have introduced a 1 here. That's a 0. Okay. All right, so essentially the first column and the last column have a minus 1 in one position, right there, and zeroes. And the very last column has a minus 1 in the very last position and zeroes above it, okay? We recognize the fact that in the multiplication on the left-hand side of this matrix vector product that column vector multiplies u0, whereas this column vector multiplies ug. Okay? And since u naught and ug are known, we can move the to the right-hand side. Okay? We do that, and so what we're left with then is c2 up to cnel minus 1, multiplying EA over he. There's a matrix here which has the form 2, minus 1, minus 1, 2, minus 1, going all the way down to 2 here, and. Now we would have a minus 1 here and a minus 1 here. And just for completeness, let me put a 2 there, as well. This multiplies now. Now what we're left with here is a vector starting with d2 and coming all the way down as far as dnel. Okay? The first and last entries from the trial solution degrees of freedom vector have been lost, okay? But they haven't been lost forever. They've just moved to the right-hand side. And on the right-hand side, we have our c vector consists of starting with c2, going as far as cnel minus 1, okay. This multiplies, first fA over he, divided by 2. It multiplies the force vector now with entries 2 in every position. Okay? Additionally, it has EA over he, which is the contribution from the left-hand side right? Correct? The, the contribution that arises from the fact that our first and last nodes have Dirichlet boundary condition specified on them. Okay. So you get EA over he. And I realize I've given myself very little room there. So let me just move it to the next line.  Plus EA over he. Multiplying first, a column vector with the u naught in the first position, and zeroes everywhere else. 'Kay, and this is the contribution from the Dirichlet boundary condition at the left end, at x equals l, where we have the trial solution values specified to be equal to u naught. And then we also have EA over he multiplying a column vector which is zeroes everywhere, except in the very last position where it is ug. Okay. We close it here. Okay. Note that this parenthesis, this right parenthesis closes that left parenthesis. Okay?  Well now is before we act, you know, everything is the same as before. We have c transpose, right. This is our stiffness matrix K, which has dimensions nel minus 1 cross nel. Sorry, nel minus 1 cross nel minus 1. Okay? And this is our reduced d vector for this problem. Okay. These are the unknown nodal trial solution values. Okay? Once again here, we have our c transpose vector, and everything that multiplies it from that left parentheses, parentheses to the right parentheses forms our f vector. Okay. So we have once again, we have c transpose K d equals c transpose F. For all vector c, now belonging to a smaller dimension space Rnel, minus 1, okay? And then what this implies as before, K d equals F. Okay? Our final finite element equations to be solved. Okay? What I'm going to do now is just write out our final forcing vector in the finite element equations and note its contributions. All right, F is the force vector in this case. All right? Where it is equal to fAhe over 2 with entries 2 everywhere. Right. And this is the contribution from the forcing function. Right. Our function F. All right. Plus, EA over he multiplying u0 a column vector with u0 in the first entry and zeroes elsewhere, right? This one is the contribution from the Dirichlet forcing. Right, at x equals 0. Okay, of course, if u naught equal to 0, this thing would just drop out. Okay, plus EA over he multiplying a common vector with zeroes everywhere except in the very last position which is ug, which now is a Dirichlet data at  at x equals L. Okay? So here you have a situation where clearly having only Dirichlet boundary conditions is actually driving the problem, right? There are no Neumann data in this problem, purely Dirichlet. Okay, so this would be a good place to stop this segment."
mBLzFl3DMJs,"There was an error in board work that I made on this slide and what I'm going to do right now is just fix it directly on the slide. The equation in question is this matrix-vector weak form. And it appears at the top of the slide right here. That's the equation we're going to fix. And the correction is a very small one fortunately. Here I denoted the, the index of this last entry as nel minus 1. That is incorrect. It should be just nel, so we excise the minus 1. Okay? And now when we go on to the right-hand side of the equation, it's the same thing. We get rid of that minus 1. Okay? So that index of the very last component is nel. Once we have that, everything else is correct and the equations on this slide are now fixed."
DTW616pbd8A,"In this segment we are going to extend our development of basis functions in one dimension to higher order polynomial basis functions. Okay. So the topic of this segment is going to be. Higher order polynomial Basis functions. Having already developed the linear basis functions, we will do this in a sort of deductive manner by going first to the quadratic functions, developing them explicitly and then laying down a general formula for arbitrary order polynomial. Okay. All right. So, let me write that down. We'll start with quadratic basis functions. Okay so when one is talking about quadratic basis functions in for a one dimensional problem here is the setting. This is our domain of interest. And, this is 0 that point is L. Okay? And, we have our nodes, right? So let's suppose that these are our nodes. Okay? So that would be x1, x2, x3, so on. And now let's suppose that this is element omega e. Okay? It turns out, that just as linear 1d elements have two nodes, because we need essentially two degrees of freedom in order to interpret linears, in order to write out linear equation. And so it follows that quadratic elements in 1d have how many nodes? Three. Right? So if this is omega e, if you just follow the numbering system, what you will see is that the node numbers, the global node numbers here are x2e minus 1, x2e x2e plus 1. It continues, and, essentially ends with x twice nel for number of, elements. Plus one. Okay? That's the, the global number of the last node. All right. So this is the setting we have. We are going to take this element out here and develop basis functions for it, okay. So we have here our element sort of blown up. Well let's read, renumber the nodes here. Okay, so this is the the quadratic element in it's Physical domain. As we did for the case of the linear elements and linear basis functions, we will suppose that this physical element is created for us or is obtained through a map from a bi-unit domain which is the parent domain, right. In which, on which our coordinate is labeled c. And in terms of, of c, we have this middle node with coordinate c equals 0. We have c equals minus 1, and c equals 1. Okay? And you will notice that this is very much like our the case of linear basis function, except that we have this extra node in here, right located at c equals 0. We suppose, as we did in the case of linear elements and linear basis functions, that our physical element is obtained from map, from this domain. Okay. And essentially what we are going to do here is write out our basis functions and also you know, work out what they are, what they're going to be. Now, when, when one is working with quadratic basis functions, our trial solution over element e, is going to be written as a, as essentially it's an, an expansion over the basis functions for that element, right. In this case we have e which represents the local node number or the local degree of freedom number. Running over from one to number of nodes in the element right, and in this case, equals 3, okay. So we have NA, right. We know that it's a function of x, but really it is a function of x through c. Just as we did for the case of linear basis functions. Okay. We have this and it multiplies da sub e. Okay, da sub e being the, the actual, degree of freedom value, right, a scalar, which is being, interpolated, so to speak, by our basis function, NA. All right? Also, much as we did, in the case of, linear basis functions, we will write out wh, our, weighting function, as the same sort of representation right, a equals 1 to number of nodes in the element n a x also parametrized by c through our mapping multiplying cAe okay. And the question that we are going to address over the next few minutes is just how do we construct these Na's for the case of quadratic basis functions, all right. And here's how we do it. I'm going to write out the basis functions to begin with and then we'll talk a little more about them. We have N1 and I'm going to write it now directly, parameterized by c, though we know we get to c Through x and through the mapping and so forth. Okay. N1 is c times 1 minus c divided by 2. N2 is 1 minus c squared, N3 is c times 1 plus c divided by 2. All right? You observe, of course, that these are clearly quadratic in c. It's useful to plot them up, just as we've done for the linear basis functions. And, the way I'll do it is the following. I will suppose that this is the point, c equals minus 1. C equals 0 and roughly there should be c equals 1. All right, this is of course c. All right, if here I give you the value one. All right. The very first one of them is, the following, okay? I'm drawing N1. Right? It's quadratic, okay? That's N1. To draw N2 let me try to use a different color. Just for visibility. Okay. N2 takes on the shape, Of a quadratic function. Looking like that, okay? It's meant to be symmetric. I didn't quite get it symmetric, but it is meant to be symmetric, right? And this is N2. All right, and finally we have N3, which is the following. Okay, that's N3. All right. So those are our three basis functions in the case of quadratics. Something to note about them is the following, right? If we denote c1 equals minus 1, c2 equals 0 and c3 equals 1 to be the actual values of c at the 3 nodes in the sparing domain. What you observe is that the chronicle delta property holds, right? NA cB, Na valuated at cB equals delta AB. Okay? The same chronicle delta property that our linear basis functions possess. All right. And then there is this other property, which is that sum over A, NA at some, any, at an arbitrary value of c, the sum equals 1. Okay? And this is the property that is important in order to be able to represent constant functions, okay? All right. So we have these two properties for our basis functions, just as we saw these properties in the case of linears. Right. What we will do from here is make the observation that this sort of expression, right, the expressions that we've used here to write out the quadratics, actually, are routine from a more general expression that holds for this family of Lagrange polynomials, all right? And we'll approach it by writing out the general formula for Lagrange polynomials and recovering the quadratics as a special case. Right. Okay, so higher order basis functions. To generate higher order basis functions, we fall back upon the Lagrange polynomial formula, okay? So higher order basis functions are generated by the formula for Lagrange polynomials, Lagrange polynomials. Okay? Let's suppose that we're looking at basis functions of some order, and let's denote that order is Nne. Okay. Let's suppose that we have Nne order polynomials. And you see how this works, right? Why does it work to always say that we have Nne order polynomials? Because of course, that is the number of nodes in a single element. All right, Nne. Okay. Here's how it works. So, so what we're looking at now is a single element, okay? This is all omega e, and it has some arbitrary number of nodes, okay? Right, so the local node number in here would be, local node one. And that would be local node number Nne, okay. This, this is the sort of situation we are looking at. Okay. In this setting again, we're seeing that this element comes to us from a parent domain. Where likewise we have the same number of nodes in this parent domain as well. Okay. That's node one. That's node Nne. We will denote the coordinate in that parent domain as c, right? It is a bi-unit domain again, and that is seen by the fact that c here equals minus 1. And here we have, c equals One, right. Therefore, by unit because the total length is two. All right. In this setting, here is how we write out the polynomials. NA in terms of C is the following. It is the product B=1 to number of nodes in the element. However, B can not be equal to A. Okay, where a is what we have on the left hand side. Okay, we have here c minus cB, right? And this thing is divided by another product, the limits of which are the same. But here, the expression we have is cA. Minus cB. All right, that's it. Okay. And this holds now for A=1 up to the number of nodes in the element. All right. Let's check this out in fact let's check it out not for quadratics, but actually for an even simpler class of basic functions for the linears. Okay? So let's do that now. So. Okay. In the case of Linears, let's just recall one thing before we go onto the case of linears. If you look at that element that we had. And we look at it in the, parent domain, Omega of C. Right, we have, this is equal to, this is -1, right? C=-1, C=1. All right? What we're seeing here is that when we label the c's as ca, cb, and so on, we did that right here, I just pulled up the previous slide, you see that ca and cb and so on, that we're use in our formula at the bottom of the slide, okay? Here's what they are. This is the point C one That is the point C N and E. And inside here, we may have cA, cB, right? And those would take on the actual values of C at those points. Okay, so with this setting, all right for linear's. We have just two of them, right. Just two of those node points. So in this domain, we have c1 equals minus one, c2 equals one. Right? So let's construct our polynomials from this formula. N1 is going to be the product B equals 1 to which is 2, right? Because for linears, we know that number of nodes in the element Is 2, okay? However, b cannot be equal to a. On the left-hand side, we have a=1. Right here, right? So we're saying b goes from 1 to 2, but b cannot be equal to 1, okay? And here, we have c minus cB divided by the produce over the same limits. C1 with that which was cA equals one of the left-hand side, of course, minus cB. Let's work this out in greater detail. So the numerator is, is has a single term in the product. It is c minus cB. We look at the limits here. B goes from 1 to 2 but B cannot be equal to 1 so B has to be equal to 2. And in the denominator again, we have c1-c2. Well this is c-c2? What is c2? it's right here, correct? It is one, divided by c1-c2. C1 is minus one, c2 is one. We get back one minus c over 2. Kay? Which we recognize to be N1 for the linears. Taking the same sort of approach, what you will find is that n2, c equals the product probably don't have quite enough room here, so let me go the next slide. N2, c equals product B equals 1 to 2. Now B-Not equal to 2. Here we get c minus cB. Divided by the product over the same limits. CA, which in this case is 2, minus cB. Okay? And in those limits, we recognize the only value b is allowed to take is 1. So when we work this out, we get c Minus, C-1. C-1 however is minus one. Divided by c2 which is one minus cB which is minus one. Okay. And we get one plus c over two which we also recognize to be N2. For linears. Right? So here we see that our formula works out very well for linears, it works out for quadratics, which you can check out, and it also does work for high order polynomials. Right? This is a good point to end this segment."
zzpiN8vWqQo,"I had made an error in board work on this slide. It is, right where I started talking about the order of the one monomial basis functions in each element. On the slide, I indicated that the order of the polynomials is N any, well that it is N any, it is actually N any, minus 1. So the place where the error appeared is right here, and that should appear as N any, minus 1. Okay. That is the order of the polynomials, and the idea is that if we are working with linear elements we'd have, two nodes in there. Right? So number of nodes would be two. But the order of the polynomial would be one. The order of the polynomial basis function would be one. So that would be 2 minus 1, which is 1. Okay? That fixes this slide and the rest of it works fine."
Vi-EwmYfWk0,"Welcome back. In the previous segment we looked at how to construct quadratic basis functions for quadratic elements, and also pushed ahead to high order basis functions, right? For high order elements. The formula for higher order basis functions is obtained from the Lagrange polynomial formula. All right and we saw all of that. Just a couple of points I want to make about the Lagrange polynomial formula, and here are those points. So first of all the Lagrange polynomial formula or the Lagrange polynomials themselves. Right, they satisfy the Kronecker delta property which our basis functions have possessed. Okay, so the general formula also satisfies this Kronecker delta property and I just want to demonstrate that. So the Lagrange polynomials satisfy. This Kronecker delta property. Okay? Let's check it. So, you recall that if we have, an element with, Nne nodes we have this formula for the eight of those, basis functions, right? NE of xi we found is, the product B going from 1 to number of nodes in the element, B not equal to A. xi minus xi B divided by a product with the same limits. But, the, the, term that we're multiplying here is xi A minus xi B. Okay, so let's check this out, right. And this holds, of course, for A equals 1 to number of nodes in the element. Right. Let's check this out. So we know that from the Kronecker delta property, if we have NA, xi A, right, just substituting xi equals xi A in the formula above, we see very clearly that NA xiA, equals pi. B equals 1 to number of nodes in the element. B not equal to A. xi A minus xi B Divided by pi, B equals 1 to number of nodes in the element. B not equal to A. Again xi A minus xi B. Right, so therefore, this is, indeed, 1. Right? Which is what one would expect from the the Kronecker delta property, right? NA of xi A should be equal to 1. All right? Let's see what happens if we take a, well if we evaluate xi, at some other value of, at some other position in the, in the element in the parent domain. Okay, so, in particular let's suppose N, we evaluate NA, not at xi A, but at sum xi C, all right? Where, of course, we are saying here that C is not equal to A. Okay? So, we have the product, B equals 1 to number of nodes. Not equal to A. Here we have xi C minus xi B. Right, remember that's a product, right, of all those sorts of terms. Divided by another product, same limits. B not equal to A here. We get xi C, minus, sorry, here we get xi A minus xi B, okay. And now you see what happens right? In the numerator the product runs over B equals 1 to Nnee but B not equal to A. However, for C not equal to A what we observe is that for one of these terms does take on the value xi C itself, right, because xi B is equal to xi C. And that's what sets this u, expression to 0. Okay? So we get here 0 when C is not equal to A. All right? So the Kronecker delta property does indeed check out. It's a simple enough exercise for you to check that the other property holds. All right? So, check that, now sum over A, NA, at any value of xi, is equal to 1, okay? Using the Lagrange polynomial formula. Okay. Check this out as an exercise. All right. So, having looked at those two properties, let's move on. But now, let's try to develop our, equations, our finite element equations, our finite element formulation, for the case of quadratic basis functions. Okay? So, what we will do now is develop the finite element formulation. With quadratic. Basis functions. All right? And we will take this up directly in the finite dimensional weak form. All right, and specifically we're going to take it up in the finite dimensional weak form where we've already Introduced the notion of the partition into elements. Okay, so we have sum e going from 1 to number of elements, integral over omega e. W h comma x sigma h A dx, and we recall that sigma h is what? Sigma h is given to us through the constitutive relation which is E times u h comma x, right? This is equal to sum e equals 1 to Nel, integral over omega e, Wh f A dx plus. Let's suppose we're dealing here, and in fact let us deal with the, Dirichlet-Neumann problem. So, we get our traction term, or our, Neumann boundary term, Wh, t bar A. Okay? And I should mention that here we have the Dirichlet-Neumann problem problem. Two questions, what other kind of problem could we consider? Correct, we could consider the Dirichlet-Dirichlet problem. Another question, why do I say that this is a Dirichlet-Neumann problem? Where does that show up? Right, it shows up in the fact that this is the Neumann boundary condition. Okay? The Dirichlet condition is, of course, already embedded in our spaces. All right. So this is the problem we are looking at, and, let us now focus on this term. Okay, this left hand side integral over the element. Right? Okay. So, let's consider integral over omega e. Wh comma x we have sigma h, but let us directly use our constitutive relation here. We have E u h comma x, and I've left a little space here, because I want to introduce the area in there. Right? dx. Now, how are we going to write this out? Okay. In order to write this out we need to express our gradients. Okay? Right. And in order to express our gradients what we will do is we will recall, as we've done before, that u h comma x. It isn't element E, because we are talking of integrating only over element, over the domain omega e. But I'm going to save ourselves the writing of that extra subscript here. Okay, so I'm just going to write it as u h comma x. This is, sum A going from 1 to number of elements, but we're working with quadratics here. Right, so we have A going from 1 to 3. NA, and you'll recall that just as we did before, we are going to calculate this gradient by taking the gradient of the basis function, okay? And a similar sort of expression for the waiting function. Okay? Where we may further ask well how do we compute these gradients? The change rule is the answer. Right? So the way we would right NA comma x is NA comma xi, which is easy to do because our basis functions are indeed parametrized by xi. All right, this times xi comma x. Okay? So, having written out or having recalled how we are going to write out our gradients, right? The only difference here between with respect to what we did for the linear problem is that we need to sum over three basis functions now because we're doing quadratics. Okay, so the way we will use this is by writing this as an integral over omega e. Now for Wh comma x I'm going to write sum A going from 1 to 3, NA comma x, but we already know NA comma x is NA comma xi. xi comma x. CAe. All of this represents Wh comma x. Multiplied by EA, multiplied again by sum over B going from 1 to 3. NA comma x, but again we are going to use our change rule. It's not A, it's B here, because we are summing over B. And B comma xi, xi comma x, dBe, okay? dx. That is how we're going to compute that element integral, right? Okay then, let's see what else we can do about this. Let's, let's start out by computing each of these quantities, 'kay? The computation of xi comma x takes a little more preparation, as we know from before. And we'll postpone that for just a few minutes. All right, so let's start out here. N1 comma xi is d d xi of N1, but N1, we remember, is one-half xi times 1 minus xi. Okay? It's that derivative, all right? Which is dd xi of one-half, xi minus xi squared, okay? Which we see is one-half times 1 minus 2xi. N2 comma xi is dd xi of 1 minus xi squared, which is d, which is, well this is easy to do, I don't need another step here. This is just minus 2xi. And finally N3 comma xi equals dd xi, of one-half xi times 1 plus xi. And when we carry this out, we get one-half 1 Plus 2xi. All right? Straightforward enough. Now, what about xi comma x? What about computing this term?"
EkIH8eorBjg,"Can you recall how we did in the case of linear basis functions? Right, in order to do that, we take the following approach. Right? We use the invertibility of our mapping. Okay, and what is the mapping we're talking about? It is the fact that the geometry is mapped as follows, right? x over element e is also written, parameterized by the coordinate in the parent bi-unit domain, right? And the way we do this is to actually use the same basis functions as we use for our trial solution and waiting function, right? So we have NA parameterized by xi times xAe, and you record what these are. What are they? Right, they are our nodal coordinates. In the physical domain, or physical space. Right, and this is the fact of the isoparametic mapping. 'Kay,isoparametric, because we are using the same parameterization for the trial solution and waiting function, as well as for the geometry, right? 'Kay, what that lets us do is the following, right, what this implies for us is that over element e, we have x,xi is now sum A going from 1 to 3, NA,xi times xi, sorry, xAe, right? But, just on the previous slide, we have computed NA,xi for A equals 1 to 3. Right, there it is at the bottom. Right, the last three equations. So, let's use those. We get here. Im going to write out the sum explicitly, 'kay? We have N1,xi, xi 1e plus N2,xi, xi 2e plus N3,xi, xi, sorry, x3e. We have N1, N2 and N3 derivatives with respect with xi. So that turns out to be one-half 1 minus 2 xi times x1e minus 2 xi times x2e plus one-half, 1 plus 2 xi times x3e. All right, now what I'm going to do is note that there are just two constant terms here that is terms independent of xi, and those are the ones. I'm going to pull those constant terms aside first. x3e minus x1e divided by 2. The other terms depend upon xi, okay? And those terms are the following. I have, from here multiplying x1e, I have let me see. I'm going to write all of this as a, as multiplying xi. Right, because the only other term is linear since we take the derivative of a quadratics. All right, for x1e, if we just look at that second to last line, we get minus x1e. For x2e, we get minus 2x2e, and for x3e we get plus x3e, okay? I'd see that I may have made a algebraic error on the previous slide so let me just go back here, right. If I look at our quadratic basis functions, I observe that I have xi times 1 minus xi. I really need to have a negative sign inside of this. Okay, so I need to have a negative sign here. Okay, that works out now, and then what happens here is that I pick up a negative sign here as well, and when this all works out, these, the sign changes as follows, okay, all right? That is right now, and then when I come back here, the signs here change, okay? All right, now everything works out fine. So this becomes plus, okay. That matters to get things right because we need to examine what happens with this term here, okay? And, in order to examine what happens with it, let us look at our element more closely. We have omega e, a quadratic element, right? Or, alternately, a three node element with quadratic basis functions, right? The, the midside node, right, the, the node between x, let's say x1e, x3e and x2e, right, that node, remember is obtained from a mapping, that node or indeed the entire element is obtained from a mapping from a bi-unit parent domain, in which the coordinates of xi equals minus 1, xi equals zero, and xi equals 1, right? What that means is that x2e is the midside node, implying that the position of x2e is x, the position of x3e plus the position of x1e, divided by 2. If it is a midside node, that must hold, right? But then, this implies that in our expression for x,xi equals x3e minus x1e divided by 2 plus xi times x1e minus 2x2e plus x3e. We see that this is equal to 0, okay, because of that result. On the other hand, this is what we can now once again denote as he over 2, right, because he is the length of the element, right, which is just the distance between the last node on the element and the first node on the element, which is x3e minus x1e. If one just looks at that figure again, okay? What this tells us then is that because of the invertibility of the map, 'kay, the fact that this map is invertible. 'Kay, implies that xi,x equals 1 over x,xi, 'kay? That's what invertibility tells us, 'kay, which is just 2 over he. So we have a very straightforward way to compute our gradients, all right? What we're seeing is that even though we have quadratic elements, the essentially the element length, right is still given of course by the distance between the last node and the first node, okay? And indeed even our so-called tangent of the map, which is x,xi, right, it remains equal to he over 2, which is a constant, right? So what we're saying is that the tangent of the geometric map. Is constant. Right? This follows because the mapping that we have is also what is called an affine map. 'Kay? Right, so with all of that in place, what we observe is that now, we have a very straightforward approach to computing this integral, 'kay? So integral over omega e wh,x EAuh,xdx now is integral over omega e. Sum over A, NA,xi, xi,x, but xi,x we've just concluded is 2 over he times cAe, and this is how we write wh,x, we get EA, and for uh,x, we have sum over B, NB,xi, xi,x once again is written as 2 over he, dBe dx, okay? We're essentially able as we did in the case of the linear based functions, to reduce our gradients to gradients in terms of xi. And therefore we're able to express this as entire integrant in terms of our coordinate and the parent bi-unit domain which is xi. All right this is a good place to end this segment. When we return, we will proceed with computation of the other parts of our finite dimensional weak form."
6aq8dkuJrz0,"All right, in the previous segment we figured out how to go around computing how to go about computing the the gradients, right, which go into our integrals. So let's pick up there and complete the evaluation of this integral. And you will recall that the integral we were working with is the following. Integral over omega e, w h, x. EA, u h, x dx. And using what we developed in the previous segment we know that we can write this as sum over for integral over omega e Let me see, we can write it out as sum over A. N A, C, C, X, which is two over h e, times C A e. All of that is w h, x multiplying E A. Times sum over b and b, c. C, X for which we again use two over h e. D E, D B E sorry. D X Okay. But note that for d x we could write it as d x. D ksi, d ksi. All right, and we know that d x d ksi is just x, ksi, which is what we computed in the previous segment to be h e over two. Even for quadratic basis functions. And this comes about from the from the fact that we have an affine map. It also has to do with the fact that Lagrange polynomials are complete. Okay? All right. So, we have this representation and now we can write out this integral in an even simpler form. We are going to note that because of the fact that we have h e over 2 here, this cancels out with that, okay? And we can now write out this integral. Using what we call matrix vector notation. Okay? And in order to use matrix vector notation. Let's, let's actually set it up a little more carefully, okay? We will use matrix net vector notation but let's do, do just one step before it which is a step we've taken when we were working with linear basis functions. And that is to recognize that c a e d b e's are independent of c. And so too is this product E A of constants and this remaining two over h e. Since your independent of position they, can be pulled out of the integral. Okay, right so we're getting up to using major vector notation but before we get before we actually do it lets write that integral as a sum over A and B of the following quantity we can actually pull even E A out of that summation right, so we can have 2 E A over h e sitting outside the summation, okay? And then we have the sum over A and B, c A e, right? Integral over. Well, we'd, we'd written it up here as an integral over Omega e, but using this chain rule right there, we observe that we can actually write that integral as an integral over. Over what? Over a different domain right, right its all omega C. Okay. Now the integrant is a simple one it is N A comma C and B comma C okay, d ksi. All right. And, I'm going to put parenthesis on this integral just to make it clear that it's that's where the integral ends. And then, here we have dBe. Okay? The use of matrix vector notation as you will recall from our development of the method for linear basis functions the use of matrix vector notations comes in to relieve us of the need to have this explicit sum. Okay? So, now using matrix vector notation here is what we get. The, that sum over A can be replaced by the sum over A can be replaced by C 1 E, C 2 E, C 3 E. With these being the degrees of freedom that interpret the weighting function in element e. We get 2 E A over he. Right. We get our integral, right. However, each, we, we, we get a bunch of integrals. Each of which, is one component, of a matrix. Right here we get integral, over Omega C of N 1 comma c and 1 comma c. However, the integral over on the N 1 comma c, I'm going to write explicitly as an integral from minus one to one over c, all right, because that is the extent of our bi-unit domain. So here I get n1 comma c and 1 comma c, d ksi. The second position I get integral minus one to one and one comma z and two comma z. And the third position the One 3 component of this matrix. I get integral of N 1 comma C. N 3 comma xi. All right? Here I get integral minus 1 to 1 N2,xi N2,xi d xi and here I get integral minus 1 to 1, N2,xi, N3,xi d xi, and here I get integral minus 1 to 1 N3,xi N3,xi d xi. What about the rest of the matrix? Right, it's symmetric. All right? So, multiplying all of this, I have here Instead of the dBe, I have those represented as d1e, d2e, d3e. Okay, so this is our matrix vector nota, or representation of our left-hand side integral over an element for quadratic basis functions. What we're going to do next is actually explicitly calculate those integrals. All right, so. So integral minus 1 to 1, N1,xi N1,xi d xi equals integral minus 1 to 1, let's see. It is let me just check what that integral eh, what that derivative is. That derivative is 2 xi minus 1 time the whole square d xi, right, d xi minus 1 is N1,xi, okay? All right so we get here for this integral, we get, this is integral, the integral minus 1 to 1, 4 xi squared minus 4 xi ma, plus 1 d xi, and observing that this is an integral from minus 1 to 1, we know that the odd terms don't survive the integral. All right, it's only the even terms that survive. When we do this we get 4 xi cubed over 3 plus xi limits minus 1 to 1, okay? This is 4 over 3 plus 1 minus minus 4 over 3 minus 1, right, which is 2 times. 7 over 3. Let me just check this out. Hm,  N1,xi, all right there is a one-quarter here, okay, that's because N1,xi is one-half of 2 xi minus 1, so we get a one-quarter here. We also get a one-quarter here, one-quarter here. Right, and we get a fourth here. So, when we carry this out, we get 7 over 6, right, which is what I was expecting. All right, now integ, the next term over, right, my integral, minus 1 to 1, N1,xi, N2,xi d xi. This is integral minus 1 to 1, N1,xi, we know's one-half times 2 xi minus 1. And 2 xi as we saw from before is minus 2 xi, okay? Integral over d xi versus then one-half integral minus 1 to 1, okay? What do we get here? We get here 2 xi minus 4 xi square, okay, d xi, which is one-half. Once again we don't need to compute the odd term, right? We compute only the even term, we get one-half let's pull the minus sign out, we get minus one-half times 4 xi cubed over 3, limits minus 1 to 1, all right? And this turns out to be. Minus two-thirds times 2, which is minus 4 over 3, okay? All right, moving on, integral minus 1 to 1, N1,xi, N3,xi d xi is integral minus 1 to 1 one-half 2 xi minus 1 times one-half 2 xi plus 1, d xi, 'kay? This is one-fourth integral minus 1 to 1, 4 xi squared minus 1, d xi, which is one-fourth times 4 xi cubed over 3 minus xi. The limits on this are minus 1 to 1, okay? Let me actually use square brackets here as I did previously. Which is now one-fourth. Times 8 over 3 minus 2, which is 1 over 6, if I'm not mistaken. Yep, right. Okay carrying on, let's just move onto the next slide here. So, the next term we need to calculate is integral 1 to 1, N2,xi, N2,xi d xi, which is integral minus 1 to 1, N2,xi is minus 2 xi, and we get the whole square here, right? And, this is an easy integral to carry out. It is just 4 xi cubed over 3, limits minus 1 to 1, which gives us 8 over 3. All right, integral minus 1 to 1, N2,xi, N3,xi d xi. All right, that's the 2,3 term of the, 2,3 component of the matrix. This is integral minus 1 to 1, minus 2 xi times 2 xi plus 1 d xi, which when we carry it out is integral minus 1 to 1. Let me see what do we get here, we get a minus 4 xi squared minus 2 xi, d xi, which is oh, as I've been doing often in this calculation, I'm missing a half here. So I get a half here. Okay, so that is one-half again using the fact that it's only the even terms that survive, we get minus 4 xi cubed over 3 minus 1 to 1, and when we go ahead and compute this we get minus 4 over 3. The very last term to be computed here is the integral minus 1 to 1, the very last component is integral minus 1 to 1, N3,xi N3,xi d xi, which is the integral minus 1 to 1. We pick up 1 over 4 and here we get 2 xi plus 1, the whole square, right, and that is because N3,xi is just one-half 2 xi plus 1, all right, so here we get 1 over 4, integral minus 1 to 1 4 xi squared plus 4 xi plus 1, d xi, right, one-fourth 4 xi cubed over 3 plus xi. And of limits of the integration are minus 1 to 1. And when we compute this we get one-fourth of eight-thirds plus 2. Which is right, 7 over 6. All right, excellent. Now, collecting results, what we get is the following. We set out to compute integral over omega e wh,x, sigma hAdx. Upon first substituting the constitutive relation for sigma h, then using our basis functions to write out the fields and their gradients and then going ahead and doing the computation, right, do, doing an explicit integral over xi, integration over xi, we get the following. We get, c1e, c2e, c3e, right, which is our vector representation of the nodal degrees of freedom of the weighting function, okay? 2EA over he, and here we get our matrix which we've just computed, and we've just computed it to be 7 over 6, minus 4 over 3, 1 over 6, 8 over 3, minus 4 over 3, 7 over 6. It's symmetric, but now let's just explicitly fill it in just this time. And for our matri, our vector representation of the nodal degrees of freedom of the trial solution, we have d1e, d2e, d3e. Okay, now as we observed before for the case of linear basis functions and linear elements, right, two-noded linear elements, this is what we will denote as Ke, right, our element stiffness matrix. All right, this would be a great place to end this segment."
u8TtqJHfhbw,"In this segment we'll move on to looking at the actual declaration, or the definition rather, of the functions that we looked at previously. All right, so let's move over to the code, and the first function that we're looking at is the constructor. Remember the 2 inputs are an unsigned int called order and an unsigned int called problem. Now here, I actually have something going on here between this declaration of the function name and its inputs, and the {. And this is so that we can call the constructor of some of these data objects that were declared earlier as class objects. So if I scroll up here, you can see that here's fe, which is an FESystem and dof_handler. Now when we're declaring objects in the class declaration, we can't call their constructor. But normally, we actually call the constructor when we declare an object, right? So in order to get around that difficulty, we can call the constructor here with, the constructor for these class objects we can call within the constructor for the class itself, okay? So in the constructor for fe, we input this FE_Q, which again is a deal.II object, but notice the input is order. So FE_Q will keep track of what Lagrangian basis function order we're using. And the reason it needs to know that is so that it can keep track of how many nodes there are, right? Because obviously if we're using a higher order of basis function, then there are more nodes. We have midside nodes. All right, so we tell it the order, and we also give it the dim, which again is the dimension. Now, again, this is that template parameter I was talking about in main.cc, and we input a 1, all right? So anytime we see dim in this template, it will actually be 1 because that's what we've declared it as in main.cc. All right, dof_handler now will take in triangulation. Triangulation again was the mesh, or holds the information about the mesh. And dof_handler holds information about the degrees of freedom, and obviously they're related, okay? Now we'll get into the constructor function itself, and here I'm just passing variables. So order is the input, and now I'm storing it in the global integer basisFunctionOrder. And the same thing for problem. Problem is an input to the constructor, and I'm storing it in the global integer of prob. Now you'll notice here I have an if statement, if(problem == 1 | | problem == 2){prob = problem;}. Otherwise, I output an Error saying that the problem number should be 1 or 2. So if you try to input problem part 3, it'll give you an error. Why, because obviously there is no part 3. You could have a similar sort of if statement and check on the basisFunctionOrder. However, some of you may want to use the generalized formula for the Lagrangian basis functions. So, orders higher than 3 may be valid, so that's why he didn't put that check in there. But, if you'd like to, you're free to go ahead and do that. All right, the second part of the constructor, again, has to do with the solution names for outputting results. So I won't look too much at that. The destructor is short, but there is something in it, dof_handler.clear(). Again, that's something going on with the deal.II data object, so I won't go into that too much either, all right? So that's the constructor and destructor. Let's move down now to xi node. Now here, I want to explain to you a little bit about how deal.II actually does its node numbering. And we'll look at it for linear, quadratic, and cubic basis functions, okay? So for linear basis functions, it's exactly the same as in the class. So here's an element, sorry, it's not exactly the same. In the class, or in the lectures, I should say, The node numbers started at 1 and went to 2. For deal.II, The node numbers start at 0, okay? Now globally, so this would be for an element. And for the system, globally It would be very similar. So, here I'll just do a 3 element mesh, and in deal.II the numbering just goes sequentially up, 0,1,2,3. Whereas in the lectures, it was started at 1, of course, but that's the only difference there. For quadratic, it starts to get a little bit more different here. Okay, so now we have our element. We have 1 midside node now, all right? In the lectures we again just started 1, 2, 3, right? In deal.II, there's a significant difference here in that the left node is always 0, and the right node is always 1, and then we go to the midside nodes. Okay, we actually follow that same pattern, or a similar pattern, here on the global scale. Here each element has 1 midside node. Again, in the lectures we just went straight across. In deal II, globally, we sort of go element by element following the same pattern. So, we do 0, 1 then our midside node, 2. Then the right node of the next element, so 3, 4,5,6, okay? And then cubic Follows that same pattern. All right, so we have our 2 midside nodes now, again in the lectures. We went sequentially 1, 2, 3, 4 in deal.II. Again the left node is 1, the right node is, or sorry, the left node is 0, the right node is 1, and then, 2, 3. All right, and again, we'll do it globally as well. With 2 midside nodes, Or 2 internal nodes per element. And I'm not going to bother with the lecture in method. It's just sequential as before, but for deal.II at 0, 1, and then 2,3 on the midside, 4,5,6,7,8,9, okay? This will come into play when we're defining our basis functions. Now, getting back to the code, why do I have this xi node? This will take your deal.II node number, and it will output the value of xi. So for example here, xi of 0 is = -1. Xi of 1 = 1, xi of 2 = -1/3, and xi of 3 = 1/3, okay? And so what this function does if you input a 0, and you're working with cubic bases functions, if you input a 0, go get -1. If you input a 1, you'll also get 1. So this is your input. And your output. All right? Actually, for any basis function if you put in a 0 you will get -1 and put in a 1, you'll get a 1 because of the way they're numbered. However, if it's a cubic basis function and you input a 2, Then you'll output a -3, or a -1/3, excuse me. Or if you input a 3, you'll receive a 1/3 as your output. All right, obviously that would be different than for quadratic, if you input a 2, you would get 0 here, right? See 2 = 0 in this case, all right? So that will come into play as you're defining your basis functions as you'll see in a minute. All right, so you actually don't need 2 of them. There's nothing you need to define within this object's xi node, it's just there in case you want to use it. You actually don't need to use it if you don't want to. All right, let's move on to the next functions, and these are our basis_function and the basis_gradient. All right, so, for the basis_function, We have input the node number, and you're inputting the value xi, xi again is in the bi-unit domain so it's from, anywhere from -1 to 1. Generally, you'll be using the basis_function in your quadrature when you're numerically integrating. And so usually the value of xi will be add the value of the quadrature point that you're at. Okay, and again, basisFunctionOrder is a global value, so you can access that here. They're a couple of ways you can define your basis functions, you can just use if statements. For example, you would say, if basisFunctionOrder = 1 and if node = 0, then you would define your linear basis_function for node 0 evaluated at xi, or you could use your generalized basis_function. Now that's in the lectures, but I'll write it again here just to point out some things, okay? So here's our basis_function at node A evaluated at xi, so you can see it's the same inputs. We have input of xi. In the function, and we have the input here. Of the node. Okay? Now, I'm going to change this formula just slightly from the lectures. Because our basis functions or our node numbering, rather, in deal.II start at 0 instead of 1. So, remember this, big pi is a product. It's similar to a summation with sigma, only now, we multiply each term instead of adding each term. All right, so we start at B=0, and we'll skip it. We'll skip B=A, and as long as < # nodes el. Okay, so of course that upper bound will change depending on the basisFunctionOrder. Okay, that's actually what this is. So the number of nodes in the element is basisFunctionOrder + 1. Okay, and then within this product we take xi, which is the same as this xi here,- xiB/xiA- xiB. And again to get this value, you could use the function xi_at_node, and you would input, for example in this case, (B). Okay, so that's an example of where you might use that function. Of course, you don't have to, you can use if statements as well if you like, okay? Now you'll store whatever that value is, you'll store that value within this double value, and you see that's what we return. And it is a double, okay? Now again, in the code we move on to basis_gradient. In general, a gradient would be a vector. All right, but this is 1 d, so there's just 1 component, so I'm only returning a double. But we have the same inputs, node and xi. All right, and remember this is just the derivative with respect to xi, it's not the derivative with respect to x in the real domain. Again, we're dealing strictly with a bi-unit domain here. And again, you'll store that value within this variable value which is then returned. All right, so those are the basis functions and basis gradients. You need to be sure to use deal.II's node numbering here, all right? So that's very important for you to remember that. So if you're doing your if statements, remember to do 0 is always at the left, 1 is always at the right. And then you count up on your midside nodes, as we showed on the previous slide, okay? If you use the function xi node in this generalized formula, it's already taken into account the deal.II node numbering, okay? So let's wrap up this segment here, and in the next segment we'll move on to generating the mesh and the boundary conditions."
iCkgh-vzvv4,"Welcome back. What we worked out in the previous segment was the form of the stiffness matrix for a general element. We proceed now with getting together the other parts of our matrix vector version of the finite dimensional weak form. And the first thing we have to do is carry out a similar equation for the forcing function. That is, the, the, the distributed body force. All right? So let's start with that. So, we will next. Consider. The following term. Integral over omega e W,h,f,A,d,x, all right. No gradients in here so this actually becomes somewhat simpler integral to calculate. So integral over omical e. For wh we have sum over a. Na, Cae. And we'll put parenthesis around here to remind ourselves. This is what wh is represented as. We have f, A dx now. Because we know that N A ultimately is represented in terms of C, that is the coordinate in the by unit pairing domain. And because we know that we have availability, we have available to us this map. Right? For the geometry, isoparametric map. What this tells us is we can very well write this as an integral now by changing variables as an integral omega X C. Sum over A, C A E. The reason we can do that is, what? Reason we can pull C A E out of the integral because of course it is just a degree of a freedom that is used to build our Representation of the waiting function, it does not depend upon C, right, or does not depend upon position, so we are able to pull it out, just as like, just as we've been doing all along. All right and remember that this sum runs A going from one to three, right, for three nodes in the element. All right. We have C A e. N, A, f, A. Now, the integral over dx can now be written as dx, dz, dz. And of course, we remember, from before, something we calculated a couple of units ago. dx, dc is he over 2. All right? We're going to go ahead and build this integral. And in order to build it, let us just, just in order to be able to fix ideas, and get something that we can compute. Let us consider the situation where f and A are uniform, right? They're uniform over Omega E right? Over the element of interest. What this allows us to do is to pull f and A as well out of the integral. And h e is of course the element width, which is independent of. Right? So what that tells us is that what we're trying to compute from up here, right? I've represented it in here in dots with ellipses. This is sum A equals one to three CA e Times FAHE over 2, integral minus 1 to 1, NA, DZ. Right. Now, just as we did before, in the case of linear basis functions, and just as we did in the previous segment in order to compute the left-hand side integral. Right? We go now to matrix factor notation. All right. So, what we're trying to compute on the left hand side, right? The integral that comes from the top of the slide is now represented as c1e, c2e, c3e, using vector notation for representing our degrees of freedom of our waiting function over the element. Times F A H E over 2. Now, we get a vector here. Right? And this is integral. The first component here is integral minus 1 to 1. N 1, right? And, 1 however, we already know, is one half C times C minus 1 d C. And, 2, the, the integral from N 2 is integral minus 1 to 1. Or the integral from, ga. The integral for component 2 is the integral minus 1 to 1. Of n 2. Which is one minus xi squared d xi. All right? And the third component is integral minus 1 to 1 one-half C times C plus 1 dC. All right? Okay. We go ahead now and compute those integrals. They are relatively straight forward to compute. Let's do that. We get our C vector for the element. We have F A h, e over two because f and a, we are assuming are uniformed and for our integrals we get here first, we get one half of c cubed over 3 limits minus 1 to 1. For the second one we get c Minus Z cubed, over 3, minus 1, 2, 1. And for the third, we get one-half, Z cubed, over 3, also minus 1 to 1. Right. When we compute result, we see that we are left with c1e, c2e, c3e, fAhe over 2 and here, we get 1 over 3, 4 over 3, 1 over 3. Right. Okay. So this is our general representation of the contribution from the right-hand side forcing function term, for a general element, okay? What we're going to do now is go to assembly, right? So, assembly. Of global matrix vector equations All right. Now, it's worthwhile just in order to position ourselves to recall that what we're attempting to do here is actually carry out an integral over the entire domain. Right, over our entire 1D domain. And, we've used this partition into elements to write that as a sum over element integrals. Of that term, that element integral. Equal to the sum over elements of that element integral. Plus a term which arises from the fact that we have a, what kind of a boundary value problem do we have? We have a, there  boundary value problem. Right? So we are WHL times T which is the attraction times A. Okay? So, this is our, finite dimensional weak form expressed as a sum over element integrals. And we've just, over the previous segment and over the first few minutes of this segment learned how to compute this integral, in the last segment, and that integral, in this segment. For general elements. Okay? Okay, now, in order to carry out the assembly, we just need to recall something. Okay? We need to recall. That for the Dirichlet Neumann problem. Right. Because we know that u h add 0 equals some given value which actually is equal to 0 in this case. Right? What that implies for us is that w h at 0 is also equal to 0. All right, because we have a tertiary boundary condition at x equals 0. We also have a homogenous tertiary condition on the waiting function. Of course, it turns out that our tertiary condition on the left hand side, on x equals 0 is itself homogenous, meaning we are saying that the trial solution there has to be 0. Okay? Even if it were something other than 0 not, if it were non-zero, we would still have a homogeneous Dirichlet condition on the waiting function, right? And that's just the nature in which we built our waiting function, okay, the function's base, in fact. All right, so what does this mean? What this implies is that for element 1, okay? Because w h at 0 is equal to 0, we construct w h e equals 1 as sum A, not starting with 1, but starting with 2, to number of nodes in the element, which is 3. Okay, so we have this. N A C A e. Okay. And in fact, going from local to global numbering of degrees of freedom, we will get here here is what we get. Okay? Here is what we get, because of the fact that we can go from local to global numbering of nodes, here's what we get. When we right out our matrices, right, matrices and vectors coming from our weak form, here's what we will get. We will use the fact that this thing can be written explicitly as N2 times c, 2e plus N3, c 2e plus 1 for e equals 1. Okay, we're going to use this. Where do we use it? Here's what we get. Right, our global equations for the matrix vector weak form. Okay? They take on the following form. For element e equals 1, we have something a little special, right? We have here c 2 e, okay? C 2 e, plus 1. Okay? This is for e equals 1. Okay. In fact, let me do this. Let me explicitly use the fact that e equals 1 here. When I do that, c 2 e becomes c 2, and c 2 e plus 1 becomes c 3. Okay, we have 2EA over h1. Okay? We're assuming here, of course, that E and A are uniform or not only over each element, but also between elements, right? Effectively over the entire domain. There is nothing preventing us from developing the more complicated formulation where E and A are allowed to vary with position. Okay? All right. Now, what we get for the matrix contribution here is a matrix that is not square but has only two rows. Okay, and in fact, that matrix has the form minus 4 over 3, 8 over 3, minus 4 over 3. 1 over 6, minus 4 over 3, 7 over 6, okay? Multiplying it here, however, is the full d vector for that element. Okay? That would be d1 in terms of global, no numbers, d2 and d3. Okay. That's all for element one. Okay, and note that I called that h, h1 or e equals 1. Thus, plus now, sum E equals 2 to number of elements. Here things look the same. 2e minus 1, the same as for general element, c2e minus 1, c2e, c2e plus 1. 2EA over he. Our matrix, which we developed in the previous segment which is 7 over 6 minus 4 over 3, 1 over 6, 8 over 3 minus 4 over 3, 7 over 6. Completing bisymmetry. All right, all of this multiplying here, d2e minus 1, d2e, d2e plus 1, right? Global numbering of degrees of freedom and nodes. Okay, so those terms together, that's this one, for the first element and this for a generic other element give us our left-hand side. This is now equal to, okay? The right-hand side. Again, the same thing happens with the forcing function contribution, okay? So we get for the c vector, for the forcing function contribution, again, we get c2, c3. Okay? The contribution here is fAh1 over 2. And it multiplies a vector which is just 4 over 3 and 1 over 3, 'kay? The first element, the first entry, the first component from the general form of the factor that arises from the forcing function is absent. Because in element one, we start with that global degree of freedom. Because the weighting function satisfies the homogenous Dirichlet boundary condition. All right, we have this plus, sum over e equals 2 to number of elements. C2e minus 1, c2e, c2e plus 1, fAhe over 2. Now we get the full form of this vector 1 over 3, 4 over 3, 1 over 3. In addition, let's never forget the Neumann boundary, too. Right. Which is wh at l. But that is simply c2, nel plus 1, times tA, okay? That's it. That's really our weak form written out fully in matrix-vector notation."
9N2qXvjmNO4,"All right. Let's now write this out entirely in terms of matrices, getting rid of this summation over elements. All right? Okay. In order to that, what we get is our big vector c2, c3, c4, c5, c6. So on, until we come to the last element, c2nel minus 1, c2nel, c2nel plus 1. Okay? Now in order to write this, allow me to assume that every element. Or allow me to consider the case, where every element has the same length, okay? So, if he equals h for all e, okay? That's a, that's the case I'm considering. It just makes it a little more makes, makes what I have to write a little more, a little less cumbersome, right? In particular, what it does is that it allows us right here 2EA over he, okay? Sorry, over h, if you just agreed,  right? Now, we are going to write out our big matrix. Giving myself plenty of room. Okay. For the very first element, we have contributions only from c2, c3. All right? And if you go back and look at what we have on the previous slide, let me just pull it up for a second, all right. Okay, look at the contributions of c2 and c3 on the left-hand side here, okay? We get here, minus 4 over 3, 8 over 3, minus 4 over 3. And here we get 1 over 6, minus 4 over 3, 7 over 6, okay? We move on to the next element. Element two has contributions from c3, c4, c5, right? Or alternatively from global nodes 3, 4, 5. Global degrees of freedom 3, 4, 5. Okay. It is going to get slotted in to these positions here. Okay. Right, that's where the contribution from c, from c3, c4, c5 will come. All right, now the contributions on the side of columns will come in here, right? Which will come all the way down here. Right. It come all the way up to this, this column and to the next two. Okay. So, what that means is that when we come here, we add on 7 over 6, right? From the one, one component of the stiffness matrix of element two. And because I need a little more room, here let me move these arrows across a little. Okay? So. We get 7 over 6, and then the rest of that, of this stiffness matrix of element two just fills itself here. Minus four-thirds, 1 over 6. Right? We get here minus 4 over 3, 8 over 3, minus 4 over 3. And here we get 1 over 6, we get minus 4 over 3. And here we get 7 over 6, okay? The important thing is that, that is where the, the, the contribution from the common node or the common degree of freedom of elements one and elements two comes about. And that is from that degree of freedom, okay? The process continues of course until we come, we arrive at our very last element. Okay? And for it, we observe that it's going, it's going to form this little block at the very bottom of the bottom right of our matrix. Also observe that as we go onto element three and so on, we are going to add some more terms here, right. That's where terms are going to get added, right. That's where components are going to get added into this stiffness matrix. When we come down here, at our very last element, nel, we see that it will give us a contribution, 7 over 6. But that contribution itself will be added on to a contribution that was already put in there from element nel minus one, okay? So, we get seven-sixths here, minus 4 over 3. 1 over 6 minus 4 over 3, 8 over 3 minus 4 over 3, and here we get 1 over 6 minus 4 over 3, and 7 over 6. 'Kay? The d vector here is full, right? It starts at a d1 d2, d3, d4, d5. And d5 is the last trial solution degree of freedom for element two. 'Kay. We carry on until we come here to d 2neL minus 1, d2neL, and d2neL plus 1. And all of this, then, gets closed. Okay? So that is our stiffness matrix. Right? 'Kay, that or, or those are the contributions from the left-hand side of the weak, of the weak form. All of these now equal to, on the right-hand side, again our c vector just as before. C2, c3, c4, c5. We go on until we come to c2nel minus 1, c2nel, and c2nel plus 1. Right. Now there are two types of terms which need, which need to go in here, right? The contributions from the forcing function as well as the contribution from the traction. Okay? So, I will open a parenthesis here, since I've allowed us to assume that we have a fixed element length, we don't have to use he, we use h instead, okay? Right. Which is uniform for all the elements, the same element length for each element. So, we get a contribution fAh over 2, and now we get our vector from the forcing function. For element one, because those are the only degrees of freedom from the weighting function we go back and look at the way we wrote out our factor from the forcing function. We get four codes, okay? We get 1 over 3 from element one. We come to element two and those are the contributions. Okay, from the weighting function degrees of freedom. All right, and if we go back and look at how we wrote the sum over elements of the contributions from the forcing function, we see that we do get indeed a one, and a one-third being added here. We get four-thirds from the c4 degree of freedom and we get a 1 over 3. But this 1 over 3 will then be joined by another one-third from element three. Right? And this process continues until we come all the way down to the log, to the to the contributions from the last element. Okay, the contributions from the last element will have the first node from the last element will give us a contribution of one-third where the first degree of freedom will. But it will be added on to an existing contribution from n element, nel minus 1. The second degree of freedom gives us four-third and the very last degree of freedom gives us one-third. Okay? This vector then contains the contributions from the forcing function. For the traction, remember, if you go back and look at the way we wrote out the traction vector contribution down here, it's the very last term on this slide. The only contribution comes from c2nel plus 1, okay? Since we have the entire c vector sitting here, what that tells us is that in order to write the traction contribution as a vector also, we fill it with zeros all the way down except for the very last component, which is tA. All right. So that when, and oh, we need to close the parenthesis. Okay, so this is you know, now if we go back and look at the matrix that we have on the left-hand side here, we sum over, you know, we, we add up the terms that needed to be added. We likewise, we added, add up terms that need, need to be added here. We end up with a matrix vector problem, okay? One thing I want you to note, and this is what we're going to pick up when we enter the, when we start the next segment, is that the length of this vector is, what, how many components on that vector? 2 times nel. Whereas that vector has 2 times nel plus 1. Consequently, this matrix that we have here is 2nel times 2nel, plus 1. Right? This vector again here is 2nel. Each of those vectors here, right. Each of those vectors is however, 'kay, of the same length, right. Because they're forming a sort of dock right up with the, with the c vector. All right? So we're going to use this when we return to the next segment."
oEulZlPMkpk,"Welcome back. At the end of the last segment, we had essentially carried out an assembly of our matrix vector equations for the finite element weak form. Let's pick up there and see and just make some more observations about the form that we've arrived at for quadratic basis functions. Right. So where we are is the following, right. So we've got the global matrix vector equations. All right? And the form that we had was the following. c2, c3, all the way up to c2nel plus 1. Multiplying 2EA over h times a matrix which is minus 4 over 3, 8 over 3, minus 4 over 3, one sixth, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6 minus four thirds, eight thirds minus four thirds, 1 over 6 minus 4 over 3. We'd get, again, 7 over 3 here, and then the process would go on. Right. And at the very bottom of this matrix, the bottom right of this matrix, say somewhere down here, we'd get 7 over 3 minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3, 1 over 6, minus 4 over 3, 7 over 6. And because this matrix has grown a little bigger than I had anticipated, let me make these bounding brackets bigger. All right. The d vector that we get here is d1, d2, d3, all the way down to d2nel plus 1. Right. Now, this is equal to a vector which here is c2, c3, up to c2nel plus 1. 2, sorry. Right now, here we get the two contributions to the forcing, fAh over 2. Now, the way this adds up is the following. We get 4 thirds, 2 thirds, 4 over 3, and so on until for the, in the very last element, the contributions are 2 third, 4 over 3, and 1 over 3. Right? And here we get a vector which is the contribution from the traction. It's filled with zeros all the way down to the very last component, which is t times A. All right? I'd like to point out a few things which is observe that here I've explicitly included the addition of components coming from the same global degree of freedom, but on different elements, right? Likewise over here. I've also done the same thing with the vector rising from the forcing function, right? Here and there. Okay? Also, at the end of the last segment, I made the point that the dimension of this vector is 2 nel, whereas this vector is 2 nel plus 1. Consequently, the matrix that we have here is 2 nel times 2 nel plus 1. It is not a square matrix, okay. This thing being 2 nel, so are these vectors, right, the vectors that are involved here are also 2 nel. Okay? So, at this point, we are essentially, we have everything in place. The only thing we have to do yet is account for the Dirichlet boundary conditions. And the way that shows up is, when we look at our D vector, right, our vector of degrees of freedom for the trial solution, we note that D1 here is our given displacement at x equals 0. I think at one point, I refer to it as u g, but we're actually calling it u0, right? This is the known Dirichlet boundary condition. All right. BC is short for boundary condition. Since it's known, what we can afford to move it to the right hand side, right, and really have it derive the problem. And the way we go about doing that is to observe that that degree of freedom, d1, which is equal to u0, is the one that multiplies out this column of our matrix. Okay? And also note that because of the sparseness of this global matrix that's formed, we have zeros here, okay? Zeros all the way down. Okay? So, we can considerably simplify our, problem here, by essentially moving this column that I've just marked out in the big matrix to the right-hand side. All right? And when we do that, we get c2, c3, up to c2nel plus 1 multiplying 2EA over h. Big matrix here. I'm going to write it yet again. It's 8 thirds. Sorry. 7 over 3 minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3. And here I get 1 over 6, minus 4 over 3, 7 over 3. Continuing on, I'll get terms of this type. Okay. I'm not going to write the rest of this matrix below here, but essentially indicate that it continues until we end up here at 7 over 3 minus 4 over 3, 1 over 6 minus 4 over 3, 8 over 3 minus 4 over 3, 1 over 6 minus 4 over 3, 7 over 6. Okay? This is the big matrix that we have, right? Now, because we've gotten this by moving our column over we're going to note that this is a square matrix. We'll come back to that in just a little bit, okay? The, the degree of freedom vector for d is now starting with d2, d2, d3, going all the way down, up to 2dnel plus 1. Okay? This is equal to, on the right hand side, again, we have our c vector, c2, c3, c 2nel plus 1. Okay? Multiplying. Now, fAh over 2. Here we have 4 over 3, 2 over 3, 4 over 3, going all the way down until the very last entry here is 1 over 3. We have the contribution from traction, which is full of zeros, except for the last component, which is t times A. Sorry, sort of messed that up. t times A. Okay, now, that column that we moved over from the left-hand side shows up here. It is minus 2EA over h times d1, which we know to be u0. All right? That's our specified Dirichlet value before the displacement, right? And then the column that, that we obtained from the left-hand side is this, minus 4 thirds, 1 sixth, and zeros. Okay. Let's look at the size of our vectors. This remains a 2 nel. This has now become 2 nel from 2 nel plus 1 on the previous slide. That's because we've lost the first component by moving it over to the right-hand side. Consequently, we now have a matrix here, which is 2nel times 2 nel, it's a square matrix. Okay? Here again, we have 2 nel, it's just the c vector, and all the vectors here are all 2 nel. Okay? All right. So, what we're going to do is when we go to the next slide, we're going to call this the global c vector, this will be the global d vector, so I'm going to call this c transpose. This will be denoted as d, okay? This is, of course, c transpose again. And the sum of the three vectors that I have in the parentheses on the right-hand side is going to be our f vector. Okay? Our matrix here, of course, is going to be our k matrix. Right, our stiffness matrix. Okay? All right. We have all of this, let's write it out. Okay, we have this, and I'd like to make a few remarks about it first, all right. And in order to make those remarks, I am actually going to go back to this slide and annotate this further. And because I've already annotated it a bit, I'm going to annotate it now using color. Okay, the first thing I want to point out is that if you look at the entries in the stiffness matrix, you will see that they're really quite different from the entries that we had in the case of the stiffness matrix for the linear basis functions, right? They're not just twos and plus or minus one, right, they're a little more complicated. All right? Okay, so let's make that observation first. Remarks. One. K matrix components. Are different from linear case, okay? Remark two is the following. If you observe the, bandwidth of our matrix, of our K matrix, observe that the bandwidth is larger. Okay? The bandwidth is 5, okay? So that is a second remark to make. Okay? The bandwidth of K is 5. Okay, so what we're getting here is that because of the use of quadratic basis functions, right, we are observing that the degrees of freedom have much more interaction between them due to use of quadratic basis functions. All right? The third remark to make, again, moving back here look at the part of the forcing vector on the right-hand side that arises from the force, from the forcing function terms. Okay? And in particular, observe that the midside nodes have a larger contribution than the end nodes, right, of an element. Okay? So, midside nodes have a larger contribution to F vector, okay? Also due to the use of quadratic basis functions. Okay, those three remarks are important to understand and appreciate some of the differences that arise from the use of different basis functions. Okay?"
ajelt6TEsHs,"In order to proceed now, what I'd like to do is fairly quickly go over how things change if instead of looking at a Dirichlet-Neumann problem, we were to look at another type of boundary value problem that I have sometimes spoken of, for this prob, for, for this case right. Let's look at the Dirichlet Dirichlet problem. Okay, and, and fairly quickly set that one, set that up. All right, so now what we're going to do is consider. The Dirichlet problem. Okay. And we distinguish this from our Dirichlet-Neumann problem by noting that when I say it's just a Dirichlet problem, without any mention of Neumann, what I mean is that all the boundary conditions are Dirichlet boundary conditions. Okay? So, what we would be attempting to do for this sort of problem is the following. We'd say find belonging to Sh, subset of S, where Sh consist of all belonging to, H1 functions on such that at 0 equals u0, as we've been saying. And let's suppose that at L, equals u sub L, okay? So we have Dirichlet boundary conditions at both ends. We do not have a Neumann boundary condition at the right end, right, there are no Neumann conditions in this problem, okay? All right. Find of this sort such that. For all wh belonging to vh, sorry. For all wh belonging to vh. Subset of 3 where vh equals the collection of waiting functions belonging to h1 on omega, such that wh and 0 equals 0, and wh at L also equals 0. Okay? All right. Find belonging to Sh such that for wh of this form, right, where you note that Sh and vh are different looking spaces because they have Dirichlet boundary conditions at both ends. Right? Okay. What do we want to solve here? Okay? What has to hold is the following: integral over Omega e Wh comma x, sigma h, A dx equals integral over omega. Sorry, I've been saying integral over omega e, it's actually integral over the entire domain omega. Integral over omega. Wh fA dx plus, plus nothing. All right? Because we don't have a Neumann condition. That's it. Okay? That is our weak form. Now because of the nature of our waiting functions okay, here's what we have. If this is our domain omega. Okay, and let's suppose that were working with quadratic basis functions. So those are our nodes for element one, and those are our nodes for element NEL, right? So this is element omega one, and that is element omega NEL. Okay? Right? We know that the sort of basis functions we'd be considering inside here are that and. That. On that, on element omega one. Here, we will have that basis function for the mid-side node. And, that basis function, right? For the first node of that element. Observe that in both cases for element omega one there is no use of our basis function from the very first node right, from node A equals 1, global node equals 1. That node does not contribute a basis function to our waiting function. Likewise for the waiting function here, our node A equals 2nel. Plus 1 has no contribution. Righ, okay? However, node a equals 1 and node 2nel plus 1 do have contributions for it's only for wh that they don't have contribution, okay. All right. When we work things through, here's what we will get. The matrix. Vector. Weak form. For this problem? Global. Here it is. We get here, c2 c3. Everything carries on until we come to the last element. We get c2nel minus 1, c2nel. 'Kay, that's it, we don't get c2nel plus 1, because we're simply not using that basis function there. Okay. Here we pick up 2E. We, we are now coming up to the, to work with eventually give rise to our stiffness matrix right? So here we have 2EA over h. Right? We have our matrix here. Okay. Now this matrix is going to have contributions when we add up all the contributions coming from the different elements and accounting for the common degrees of freedom across elements. Right? We are going to have a, contribution that, takes on the following form. It will have a form, 8 over 3 minus 4 over 3, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6. Minus 4 over 3, 8 over 3 minus 4 over 3, 1 over 6 minus 4 over 3 7 over 3. And so on. Of course here we know that we get some more terms. Right? And so on, right? Except, that when we come all the way down here for the last element we get 7 over 3 minus 4 over 3. 1 over 6, okay? And here, we get minus 4 over 3. 8 over 3. Okay, and 1 over 6. Sorry not 1 over 6 it's, minus 4 over 3. Right. We get this, that's the last row we get, okay? In particular, what I want to point out is that if compare this with the corresponding metrics at this stage, for the Durtally Noin problem, there is an extra row, for the Durtally Noin problem, which is missing here. Okay? This would now multiply the following matrix, right? It will multiply D 1, D 2, so on, all the way down to d 2 n e l plus 1. Okay, actually I realize that I do need another. Column here. Sorry, let me just put it in here. I do need another column here which is Minus 4 over 3. 1 over 6. That's what I did. Okay. Right. We have this equal to, on the right hand side, we have our c2, c3 vector. We're going all the way now to c 2 n e l c, 2 n e l. Minus 1, and 2 c n e l. Okay? This multiplying. Alright, vector here which is filled with our multiplied by f a h, over 2. Okay. And here we have 4 over 3. 2 over 3, 4 over 3. So on, until the last entry here is one third. All right, actually no. We don't get the one third entry here. Because we have only up to c2 n e l degrees of freedom. The C vector. Right? As a result, the last entry here would also be 4 over 3, okay? 4 over 3 would be the entry from the mid-side node of the last element, okay? The contribution from the last node of the last element, which is the one that I've marked out here Is missing also in the vector that arises in the forcing function. Okay? We have here also then the contribution from traction. Well we don't have a contribution from traction, because there is no traction in this problem. Right? That's it then. Okay, there's actually, I don't even need this parenthesis. Okay, that's it. That's what our right hand side looks like, okay. I can point out something about dimensions. This is matrix is, this vector is 2nel minus 1. This vector as before is 2NEL plus 1. This vector is 2NEL minus 1 just like the C vector on the left hand side because it is the same. The forcing vector now rises only from the uniformity, from the distributed forcing. Which we are resuming to be uniformly distributed and therefore has a constant F. Okay so this is also 2NEL minus 1 in dimension. Okay. This matrix that will eventually give rise to our stiffness matrix, we know has therefore, dimensions 2nel minus 1 times 2nel plus 1 in the d vector here. D1 is known. It is U0. Right. It is this condition. This is also known. It is U sub L. That known condition. We will do what we did in the case of the Dirichlet Riemann Problem. We will note that the d1 degree of freedom multiplies out that column. Whereas, a d. 2 N E L plus 1. Degree of freedom multiplies out, back column. Since D1 and D2NEL plus 1 are known, because U0 and Ul are given to us, we are going to move close to columns that are marked over to the right-hand side, just as we did in the case of the problem, except  problem, we did that only to this column, all right? Okay, let's go ahead and do this now. Right? Redo this. We get C2 C3 up to C2nel-1 C2nel multiplying 2EA over h times this matrix now which is 8 over 3 minus 4 over 3, minus 4 over 3, 7 over 3, minus 4 over 3, 1 over 6, minus 4 over 3, 8 over 3, minus 4 over 3. 1 over 6 minus 4 over 3, 7 over 3, and so on. Now, we come down here and it ends in 7 over 3 minus 4 over 3 in the last element, minus 4 over 3, 8 over 3. Okay? All right? Multiplying In the d vector, we don't have d2 because, sorry, we don't have d1 because we moved it to the to the right-hand side. We come all the way down and we end here at d2nl. Okay? Equal to here, c2, c3, up to c2nel, okay. Multiplying fAh over 2. The vector here from the distributed forcing is four-third, two-third, all the way down until it ends in 4 over 3, at a mid side node of, at the mid side node of the last element. The last two contributions are 2EA over h times d1, right. Multiplying the first column, right, of our matrix on the previous slide, which is minus four-third, one-sixth, zero. All right, zero's all the way down And minus 2EA over h d2nel plus 1, multiplying 0, 0, zeroes all the way down except 'til we come to the last two entries, right, and here we get one-sixth and minus 4 over 3. Okay? We close parentheses, here. And note that d1 is known, because from a Dirichlet condition, that is U not. And d2nel plus 1 is known because from our Dirichlet condition, that is UL, okay? We have here C transpose. That matrix is K. This vector is D. Okay? C transpose has dimensions 2nel minus 1. D also has dimensions 2nel minus 1. K therefore is 2nel minus 1, squared. It is a square matrix. C again is 2nel minus 1, the entire vector here has dimensions 2nel minus 1. Okay? This is our f vector. Everything sitting inside these parentheses is our f vector. This, of course, is C transpose. Okay? Putting it all together. Again, for this Dirichlet-Dirichlet problem we end up with C transpose, K d equals C transpose F, just as we did for the Dirichlet-Neumann problem. Okay? So, for the Dirichlet problem also we have this. Okay? So whether we have the Dirichlet-Neumann problem or the Dirichlet problem we see that C transposed K d minus F equals 0. Now, remember that when we write out our weak form, right, such as in this case at the top of this slide I've written out our weak form fairly briefly and I made a statement on how it has to hold for all WH belonging to the space VH. My question is when we come down to this final version of the weak form, whether for the Dirichlet problem or the Dirichlet-Neumann problem. Where can we see a term that represents the weak the form? Right, it is c transports right? Or, or the c vector. Because that contains the degrees of freedom which are going to be interpolated to form the actual reading function field. All right? They have been interpreted in some basis functions. So if our weak form has to hold for OWH what can we say about this final equation, with regard to C? All right. We can say that this equation must hold for all C belonging to R2nel. This would be for the Dirichlet-Neumann problem. Or, for all C belonging to R2nel minus 1. This would be for the Dirichlet problem. Right. Well, if that is the case, one can show from stand, standard arguments that this implies therefore that K d minus F has to be equal to 0. Okay? Or Kd equals F is our standards form of the finite element equations for linear problems. Okay. And as we saw before, as we did for the case of the linear problem, we say from here d is now equal to K inverse F. How we actually solve this is a different matter which we won't get into in this series of lectures, but nevertheless, this is the definition of our nodal solution. Right? Once we have that, we can go back and regain our fields by using the basis functions. All right, this is a great place to end this segment."
7EauMTn5Zgg,"What I'd like to do in this segment is talk about how we actually carry out integration of the different terms that show up in our weak forms. And I'm referring specifically to the fact that when we carried out the formulation for linear and quadratic basis functions. We explicitly carried out certain integrals which we could do by the way because we assumed that our coefficients of the problem, our, our coefficient e if we were talking about elasticity, or it would be like, it would be ended something like a conductivity, if we were talking about some other problem. That coefficient was uniform over the domain. We also did the same thing for the forcing function, right? The distributed forcing function f we say was uniform over the domain. It made our integrations fairly easy, okay. However, that's not always the case. So we will need to develop the ability to carry out numerical integration. Okay. It's needed if the coefficient in our problem, such as E our forcing function f. And other quantities, maybe the area a, and so on are complec or let, let me see, complicated functions, right, we're not really talking about getting into the comple, into a complex space here, but, but, but we need them, however, of e and f and so on, are complicated functions. All right, functions of position x. Okay? All right we could also need them if the sort of basis functions we need are very high order. Right. And sometimes integrating them or, or if all of those basis functions are something more complicated than simpler, simple sort of polynomials we're using here. Okay, all right also a complicated Basis functions are used, okay? Now, so develop numerical integration for this. In particular, the approach that we will take is what is called Gaussian quadrature. Okay? We will take, we will consider Gaussian quadrature, which can be shown to be optimal for polynomials. Okay by optimal we mean that it's possible to integrate certain types of polynomials. Polynomials exert an order exactly, okay?  There's a systematic rule about how we go about doing this. All right, so let's see what we need. If you look at to any of our integrals either, either the integrals going to the stiffness matrix or into the for, into the distributed forcing function, you will see that we always are faced with the task of integrating over minus 1 and 1, a function of the form g of z dz. Okay. The whole business with any sort of numerical integration, any sort of quadrature is the following. We replace it with a sum, okay? Over L. Going from 1 to mint, where n int simply means number of integration points, okay? We replace it with the sum where every of term in that sum, in that series if you like is in that finite series, is g evaluated at a certain value of c indexed by L. Multiply it by what we call a weight for L. Okay? This is the general form of a quadrature rule. Alternately and numerical integration rule. Let me say a few things here. N sub int is the number of integration. Points. Okay? See L is an integration point. Okay. Wl is the corresponding weight. Weight ascribed. To the integration point. Okay here of course L equals 1 to an int. And here, too. Okay? Now, essentially the way to understand this perhaps is to say that, well, if this is what we're trying to do, if this is c, and this is our function g, and maybe g looks like that and we are here between minus 1 and 1, right? What we're trying to do here is pick certain points z, that is zL, one of those zL's, we look at the value of g, right? And that value of z, right? So this is g at z sub L. Okay? And we are basically forming that sum, like I wrote on the previous slide. Okay. We're giving some certain, certain weight to the value of G, X, E, L. All right? Okay. And then there are just rules, right? Before we get to those rules let's just see one more thing. The weight wL are such that some over L going from 1 to n int, wL has to be equal to 2. Can you think of why this might be the case? Why do we have this requirement? It's because If g of z is a constant, we know that integral minus 1 to 1 gz, dz, is equal to twice of that constant, right? Right? Well, this is satisfied automatically by saying that what, when we sum over L g of zL, wL, if that is a constant, and this rule for wL is satisfied, right? For the wL to satisfied we get back twice of the constant. Okay, so we ensure that at the very least we can integrate constants exactly. All right, so Okay. All right, here are the integration rules, okay. So n int equals 1, okay? For this rule, z1 equals 0, right? And w1 equals 2. For n int equals 2. We have z1 equals minus 1 over root 3. W1 equals 1. Z2 equals 1 divided by root 3. W2 equals 1. Going on, n int equals 3 as z1 equals, minus 3 over 5 square root. W1 equals 5 over 9, z2 equals 0. W2 equals 8 over 9. Z3 equals 3 over 5, square root w3 equals 5 over 9. You can check that for all three rules I put down the sum of the w's is always equal to 2. Okay, these are all rules from Gaussian quadrature. Right, and it goes on, right. We can right out rules for any order of n int. Okay, I mentioned that Gaussian quadrature is optimal. It is optimal in the sense that a Gaussian quadrature rule. With n int points, right, and n integration points. Exactly integrates. All polynomials of order. Order lesser than or equal to twice n int minus 1. Okay. So, if we go back and look at n int equals 1 integrates linears exactly. N int equals 2 integrates octo cubics exactly. N int equals three integrates octo pentics. All right. And so on. All right. That's what we need to save for numerical integration and we'll end this segment here."
__5tvLVBTtE,"Hi. In this segment, we'll start looking at the function for generating the mesh, and for defining the boundary conditions. All right, so let's look at the code. Generating a mesh, there's actually not a lot for you to do. I have this global variable L, you remember I declared above, it's a class object, a class variable. So you simply have to define the length of the domain. I'm assuming that the value of x at the left of the domain is zero. And then of course, the maximum value of x would be L. From there, I take the x_min and x_max and I put them I create these DL2 points. A point in DL2 is, it's essentially a location vector. A position vector. The dimension or the number of components in the point is defined by dim in this case, and it holds doubles. I have actually, I don't use point a lot in this template, you won't actually need to use it at all yourself. But that is the input that DL2 requires to create the mesh. All right, so I take the limits of the domain, which you define here with L. And remember the input is an unsigned int called numberOfElements. In main.cc, remember, I had put in 15 as an example. But that just gives the number of elements in our mesh. All right. And then DL2 will use that number to create a mesh. It stores that again in triangulation, which again is a class object. All right. So not a lot for you to do there, but an important step. A lot of it is, again, DL2's functions. All right. Now, in define_boundary_conds, there actually isn't anything that you need to do in this function for this assignment. However, for the future assignments, you will be writing the defined_boundary_conds function yourself. It will be very similar to this function, but again, you have to change anything here, but I want to step through with you now because again, you will be using in future assignments. All right, so here we're using that boundary values map that we talked about, and we already talked a little bit about how the map itself works. However because of the way DL2 does their node numbering, instead of simply saying okay, we know how many nodes there are, so I'll just do boundary values at zero, is equal to the fixed boundary condition of zero. And if I have a Dirichlet boundary condition on the right, then I'll just take the last node. But again, since they aren't numbered sequentially, we can't just take the highest node number. Because that's not necessarily the last node in the domain. Right? So again, as an example, let me take Let's create a domain here. And I will, for simplicity, I'll create four elements. And on the top, I'll do the x coordinate. So, this isn't the real domain. We're going zero. And I'll go from zero to one. That would make these 0.25, 0.5, and 0.75. And just to make my point about the node numbering, I'll make this quadratic, quadratic basis functions. And so we have one mid side node. Down here, I'll put the node number. Of course in DL2 node numbering. Okay, so it's 0, 1, 2, 3, 4, 5, 6, 7, 8. Again so, this node, assuming we're doing two Dirichlet conditions, we want to fix this node. And this, let's say we want to apply a displacement of 0.1, that's a pretty big displacement here actually. Since it's linear elasticity, I'll do 0.01. So let's say, g2 is equal to 0.01. All right, so what we want to do is boundary_values of node zero is equal to zero. And boundary values of the last node, which in this case is actually node 7, not node 8, is equal to 0.01. These are semicolons, and so since we don't necessarily, okay, for the one d case, we can keep track of the way DL2's numbering their nodes. However, once we get to 2D and 3D, it gets more complicated from there. So instead what we're doing and what I'm doing in the code, is I just have a for loop. I'm looping over all of the nodes, so you can see here I have unsigned int globalNode=zero, and I'm looping up through nodeLocation.size. Remember, nodeLocation.size is a vector of the locations of the nodes. All right, so let me write out what nodeLocation, what this vector would look like, in our case, the actual data that it's storing. Okay, so we have nodeLocation. And this is what it's holding. Okay, so its hold for node zero, again looking at the indices are these node numbers down here. So for node zero, we're holding the value at zero. For node 1, the x coordinate is 0.25. For node 2, it would be 0.125. All right, and that goes on, until node 7, which has a value of 1. And then node 8, which would have a value of 0.875. Right. Okay. So those are the entries in nodeLocation, okay? And so the length, you can see it has a size of 9, which is of course equal to the number of nodes in this system, okay? So, here in the function define_boundary_conds, we're simply looping over all of the nodes using this nodeLocation vector. And then we have an if statement. If the value stored in the node location at the current node that we're on is equal to zero, in other words, if we're at the left end of our domain, then we set boundary values of whatever the node index is equal to g1. And actually, I can do here an else if, if I wanted to, else if, or just if nodelocation at our current node, wherever we happen to be, is equal to L. In other words, if we're at the right side of our domain. And if problem equals 1. So problem 1 is where we have two Dirichlet boundary conditions. So if we have a Dirichlet boundary condition in our problem, and we are at the right hand side of our domain, then we will store the Dirichlet value of g2 in our domain. So that's this one right here. Okay, so again, there's nothing you have to edit here. But it's good for you to understand what's happening so that you can use it on the next homework assignments. All right. Let's move on, and in this segment, let's still look at set up system. Okay. So in set up system, first we have to define. Again, we were using g1 and g2 in the previous function, then here's where we actually define what those values are. And you'll have to look in your homework assignment to see what those values will be. Of course in problem 1, we use both g1 and g2, in problem 2, g2 won't actually be used, because its a Dirichlet Neumann problem. But you can still use the same value of g2, right. Okay. This next line is just DL2 distributing the degrees of freedom. It's just keeping track of degrees of freedom. Now here, there are several lines going on here, but essentially all I'm doing is creating this vector node location which holds the x coordinates for each global node, again by its global index. Now that those structures are defined, now we will call this function define_boundary_conds. Okay. Notice that since we are still within another class function itself, we don't have to create like, problemObject. We don't have a class name, since we're not dealing with an actual class object yet. So we don't have this problemObject.define_boundary_conds(). We can simply use the function name itself, because it is also an FEM class function, okay? All right. So we call that function, which is the one we were just talking about. From there on, this next section just has to deal with resizing the matrices and vectors. You can see rather than using the function.resize, which is what standard vectors use, the DL2 vectors and matrices use .reinit, short for reinitialization or reinitialize. And we use this dof_handler.n_dofs. That gives us the total number of degrees of freedom in our domain. Since we have one degree of freedom per node, it's the same as the total number of nodes in this problem. Now, here we define the quadrature rule. I set up a quadrature rule of two quadrature points per element. However, that's not going to be enough. Remember that for the quadrature rule, let's say you have a quadrule, whoops. Let's say you have a quadrule of 2. This is actually good for up to, for an integrand of up to third order. So, remember that Good for and integrand of third order or third order polynomial, right? So remember that this Gaussian quadrature is exact for polynomials, Of order equal to 2 times the quadrature rule, minus 1. So in this case if you have a quadrature rule of 2, it's good for a third order Because 2 times 2 is 4 minus 1. So if you have an integrand of order 3, with polynomials of order 3, then the Gaussian quadrature will integrate exactly for you. Okay. But remember, you'll be using not only linear, but also quadratic and cubic basis functions. And you'll be doing an L2 norm. Okay? So you have to, in each of these cases, don't look actually just at the order of your basis function. Look at the whole order of the integrand that you're integrating. And once you know that polynomial order, then you can decide what quadrature rule order you need. Now having said that, you don't have to necessarily change your quadrule for each problem. You can decide what's the maximum quadrature rule that you need. And that actually will come from the L2 norm using cubic basis functions. You can decide what quadrature rule that is. Is it four, is it five, is it six? Decide what that number is, and then you can use that quadrature rule for all of your problems on homework one. Okay, for whatever basis function ordering. All right. So again, you will define that here. So you would change quadRule to whatever value you want. And then quad_points and quad_weights are, again, just vectors. You put in the, so quad_points stores the values of c for each quadrature point. For two point, it's minus squared one third and positive squared one third. And the corresponding weights are one. Okay. If you go to a higher quadrature rule then you will obviously create a longer vector. You can see that here I do quad_points.resize, and I resize it according to the quadrature rule that you define up here. Okay, so it will automatically go to the correct length. You have to define the correct values. And those can be found online, or in a book on finite elements or on numerical integration, all right. Okay, so let's stop this segment here. And in the next segment, we'll move on to the assemble system function, the assembly function, which involves creating f local and k local."
W_vbVPBeNK0,"In this segment, we'll start looking at the main function in the template file and that's assemble system. So, let's move over and look at the code for a second. The assemble system is only called once, of course. So here we zero out K and F. I've defined several constants and objects up here. So I don't think I've used this before. I have const unsigned int. So what that constant does is, it allows me to define this variable only once, so it's actually not even a variable. It can't vary, it's a constant. So this dofs_per_elem is degrees of freedom per element. And of course that changes with your basis function order. Same as number of notes per element in our case. Then I declare a full matrix. Again, this is another DL2 object, and it stores doubles and as I'm initializing it here, Klocal is dofs_per_elem by dofs_per_elem. Okay, it just defines the size of the matrix again. Flocal is also a DL2 vector. With length dofs_per_elem or nodes per element. Now here I'm creating this standard vector of unsigned ints called local_dof_indices. It also has a length of dofs_per_elem. And this vector will actually update for each element. It's different in the values,and will be different for each element. But what it does is it relates our our local node numbering to the global node numbering. So let's look at that for a minute. Again, I'll create another sample domain. I'll do let's, I guess in this case three elements. For quadratic. So we have one mid side node. And let me do the global node numbering on top. And then on bottom I'll do the local or the element node numbering. Again, both of these are in DL2's syntax. All right, so we have 0, 1, 2, 3, 4, 5, and 6. And now I'll break this up into element 1, it's called it e1, e2, And e3. Okay, so those are our three elements. So locally, these nodes would be labeled 0, 1, 2. Same for all of them, 0, 1, 2, and 0, 1, 2. So local_dof_indices will look different for each element. So local_dof_indices. For element 1, this is what it will look like, local node 0 corresponds to global node 0. Local node 1 corresponds to global node 1. Local node 2 corresponds to global node 2. Okay, so they all match up there, and let me specify here that these are the global nodes or local nodes. All right let me change that. So on the top this is the index, is the local node, and the actual value stored is the global. So the index is local. And the value is the global node. So for e2, again we have 0, 1, 2. But globally now we look at element 2. Local node 0 corresponds to global node 1. Local node 1 corresponds to global node 3. Local node 2 corresponds to global node 4. Now, of course, DO2 is handling all this for us. It will populate this local_ dof_indices vector. But I just wanted you to understand exactly what's what's going on here. So, for element 3. Now we look at local node 0. Here corresponds to global node 3. Local node 1 is global node 5. Local node 2 is global node 6. So if we, let's say we're working with element 2. We've initialized local_dof_indices for element 2 and so now using the C++ syntax, if I put in 1 as my index it would give me 3. If I put in a local node of 0, it would return 1, a global node number of 1. So this vector local_dof_indices will relate our local node number to the global node number. The index to the vector is the local node number. The value stored is the global node number. And we'll need that especially when we're going to assembly. So now let's go and let's see I also have these variables stored here they're all doubles, h_e is the element length, x we'll use to find the value of x at quadrant points. Which we'll need to define our forcing function. And then f is the value of that forcing and function. Now we have a loop, a loop over elements. This is where, remember in our review of C++, in our introduction to C++, we talked about iterators and they're similar to pointers in some ways. And that we can use these iterators and four loops. In this loop over elements, we're actually using an iterator. You can see here, it's an active cell iterator. By the way, in the DL2 name, nomenclature, they call elements cells. So when you're seeing a name that DL2s set up, it usually says cell for the element. So, here, we're actually looping over elements, they're called cells. And now all of this I could have put, this line elem = dof_handler.begin_active() I could have put here in this declaration portion of the for loop. But since it's so long I can declare it ahead of time and that's okay to have an empty spot there in the for loop, declaration. So I've declared two objects here, I have elem, which is my iterator and it's And I'm setting it to the beginning element. I also have this endc, which is the last element. Or really it's a marker after the last element, okay. And so my for loop, I'm already starting at, element equals the first iterator. For the iterator is equal to the first element. And my condition is, as long as I'm not at that marker past the last element, then I'll go through the for loop. And again, this ++elem, it just increments to the next element. Again, even though elem is not an integer, it's an iterator. The ++ still iterates forward, okay? So now that we're within a particular element, we have to update the local_dof_indices vector, okay? So we do it this way, elem->get_dof_indices. Remember this arrow is the way we, for either a pointer or in this case an iterator, this is how we access a function. Okay, so since elem is an iterator, instead of doing elem.get_dof_indices, we have to use this arrow essentially, okay. And so that will populate the correct local_dof_indices with the correct, global, node numbers, okay? All right, now, I'm going to find for you the element length. As I told you before, the way we've defined our mesh, each element will have the same element length. So, you could actually have just taken the length of the whole domain and divided by the number of elements to get element length. This is just a little bit more general in case you ever set up your mesh to have elements of different lengths. What we're doing here is I'm saying h_e = nodeLocation at local node i minus the nodeLocation at local node 0. Okay. Remember, local node, or sorry, local node 1. So local node 1 again is always on the right. You can look back here on our schematic here. The local node 1 is always on the right of the node. Local node 0 is always on the left. If we take the x coordinates of those two and subtract them then that gives us the element length for whatever element we happen to be in. Again I use local_dof_indices though Because I may not be in element 1, I may be in element 2. And I really want the, and remember, nodeLocation, the vector nodeLocation, is indexed according to the global index. And so I would want, in this case, if I wanted the x values at local node 1 and 0, I would need the global node numbers 3 and 1 in order to access the correct x values within my nodeLocation vector. So that local_dof_indices will come up again as well as nodeLocation. But now we have h_e which is our element length. Now, first let's create our local force vector, Flocal. Now there are really two parts to this. There is the portion due to the forcing function, or the body force, and then if you have a Neumann, condition, a non-zero Neumann condition like we do on problem two, then you'll have to add in the effect of the traction at that boundary, okay? So first we will do the portion of Flocal due to the body force. Okay, so here I've set Flocal = 0. Again, we zero it out for each new element. And now in looping over A, so each component of Flocal, so I'm at Flocal. And now I'm looping over my quadrature rule. Okay, I'm looping over each quadrature point, because we will need, remember in Flocal and in Klocal we're performing integrals. All right, so let me actually write out this general form for this portion for Flocal, and again this is in the lectures. Let me write this out. So Flocal is equal to the area the cross-sectional area times the element length over two. Remember, we use that element length over two to convert from bi unit domain to the real domain. We're integrating from negative one to one. Basis function a corresponds to our index here evaluated at node xi, times our forcing function or our body force, f. f we're given as a function of x, but since we're integrating over xi, we need to evaluate everything in terms of xi. Okay, and then we integrate it over our d xi domain. Okay. Now since we're using quadrature, let me write this out using quadrature points. Ahe/2 is still on the outside. However, now, we are going to loop over our quadrature points. And so, I'll just do q equals 0 as long as we're less than quad rule. Okay, and now we're going to evaluate the integrand at each quadrature point. And remember you get the quadrature point from that quadPoints vector. I'm going to distinguish it here just by saying it's xi q. So xi q is the quadrature point. Okay? And f is a function of x, but again we evaluate, you can get x, a value of x for that particular quadrature point. Now it's no longer an integral. It's a summation. And so we also multiply it by the weight. So that's short for the quadrature weight at that particular quadrature point. And again, we also have a vector of quadWeights. Now, this term should be easy enough to evaluate because we already have our basic function that has as an input the node and xi. Now here, you're given a forcing function or a body force. How do we do this term here? We have a value of xi. We need a value of x. So this is also something from the lecture, but just as a reminder, we can interpolate. Over the values of x at our local nodes to find the values of x at xi. Let me specify here that this is the value of x at xi q for example. Okay, so what will we do? We'll start at A = 0, and as long as A is less than the number of nodes, the element, we will take x, A times our basis function evaluated at xi q. And what do I mean by x a? This is the value of x at node A. In other words, which we can get from nodeLocations, okay? NodeLocations, but remember, A is a local element, or a local node number, nodeLocations requires a global node number. And so, to convert we use this local_dof_indices of node A. I've actually written all of this up for you in the code, but I wanted to explain it to you in this one case. Because you will be performing other integrals in this homework, and in other homework assignments, so I want you to understand what's going on here, okay? So, going back to the code you can see, again, we're looping over B=0, B < dofs_per_elem, in other words, less than number of nodes in the element. And x +=, so we're doing this summation, nodeLocation(local_dof_indices) * basis_function(B, quad_points(q). Okay, again I'm just using the next B instead of A but it's the same thing right? Okay, so now you have the value of x here, this is where you have to start doing it yourself. So now that you have x, you'll need to evaluate f, evaluate N A, and essentially just create this integrand, well what was an integrand and is now this inner portion of this summation. And you will use that to define Flocal at A, okay? So you'll need to do that. Now that takes care of the body force. However, in problem two, or part two of the this homework, you also have a Neumann condition. Of course that only applies at the right hand node. And so what I've done here, is if were at problem two, then it's checking if your node location, if the node that your at is on the right hand side, then that's where you'll have to add in the traction times the area which is H, okay? So you're really just adding an h to the appropriate component of Flocal. And again you can review your lectures if that process is a little bit fuzzy. So that will take care of your Flocal. So we've taken care of the body force and when applicable we've taken care of the Neumann boundary condition. Now the next part is to define Klocal. And let me write out the general form for Klocal as well. Now Klocal is of course a matrix so let's look at component A, B of Klocal. We have 2EA, E is the Young's modulus, A is the cross-sectional area over h e where h e is the element length again. And here we've already converted to an integral over the bi-unit domain. That's why you have the h e in there. Now we're dealing with gradients. So this is the partial, I guess the derivative of basis function A with respect to the xi, times the derivative of basis function B with respect to the xi, okay? And so again, these two terms you'll get from your basis gradient functions that we've defined previously, all right? And again, here you'll convert this integral into a summation over quadrature points, multiplied by the appropriate quadrature weight. Okay, so I've set up the for loops for you. We have a loop over A and a loop over B, and a loop over your quadrature points. Now you simply have to write out what is Klocal A B equal to? And of course, since it's a summation, you'll be doing a += just like you did with Flocal and like we did when evaluating the value of x at the quadrature point, right? Okay, so once you've filled in those lines, you'll have your Flocal and you'll have your Klocal, okay? Flocal will already have any Neumann boundary conditions that are involved in the problem. Now you need to go to assemble system, you have to actually assemble them into the global matrix in the global force vector. Now again this was done in the lectures. You can go back and review that there. The real trick of it here is to remember to use local_dof_indices and it makes everything very simple. So for example if for F We have the global nodenumber. So essentially you'll, for each element, you'll be looping over each node in each element, okay? And you'll put that value of Flocal into the corresponding value, that should be a +=, because we don't want to delete any values that were already in F, right? We just want to add on to them. And so you add on the Flocal for the local node number. Okay and the local node number will of course be going from zero up to the number of nodes in the element. And then how do you go from the local node number to the global node number?. Again its just local_dof_indices, okay? All right so, it's actually pretty straight forward. You will do that with F and you'll also do that with K. Of course, K has two indices, so we have two loops here. We have a loop over A and a loop over B. Now, the one thing that you need to be aware of, really, is that since K is a sparse matrix, we can't just do K  just as a short hand and  You can't just do += like this. And of course A and B here are the local. All right, you can't do it this way. We instead do K.add. And it's because it's a sparse matrix and we're not dealing with all of the, not all of the indices are actually available to us, okay. So we do this K.add, you have your A global, B global. So these are the indices, these correspond here. And then this term is the actual value that you're adding. So it would be this here. Okay? So those are equivalent as far as the thinking goes behind it. This is, of course, the actual writing that you'll do. Okay, now this assembly process is really just two for loops, just a few lines. It's good for you to understand what is going on here because you'll be doing this exact same process in all of the remaining homework assignments. It's actually the exact same lines of code. Okay. And so it's good for you to understand what's going on here in this first homework assignment. Now the last function that's called in the symbol system is this apply boundary values. Now we're not going to go through all the mechanics that we learned in the lectures of applying the Dirichlet boundary conditions. You remember in the class we had K, D equals F, right? If we happen to know that boundary conditions, if we had Dirichlet boundary conditions, and say these, excuse me. If we had Dirichlet boundary conditions in these two, on these two nodes, then what would we do? We would actually remove the corresponding rows and columns. Okay, we'd also remove the corresponding entries in F, and that would give us a smaller matrix vector system, right? I think we set these as bar, and then these were actually K, D, and F, or something like that, right? The way deal two does it a little bit differently. Instead of actually deleting these rows, it just modifies the entries. And KM modifies the entries in F to give you the correct values of D, or the correct displacement values, okay? So you can actually look into that a little bit more if you'd like to, but the main point is that the deal two will be taking care of the application of Dirichlet boundary conditions, and it actually does it without modifying the size of K, D, or F. Okay, so that is what's happening here. You can see we're inputting boundary values, which is the map that we created earlier that holds the values for the Dirichlet boundary values, K, D, and F. This false has to do with whether or not deal two is modifying entries in the row only, or if it also modifies entries in the column as well. False means it's only modifying entries in the row. Again, that's not something you'd need to modify, but it's helpful to see what's happening there. Okay? All right, let's stop that segment here. And in the next segments, we'll look at solve and output results, which are really short functions, but then, we'll look mostly at calculating the LT norm of the error."
l86hK6L-4BY,"Welcome back. At this point, what we've managed to do with our formulation of the finite element method for these 1d linear elliptic PDEs is actually state the problem in completeness, right? We've derived the matrix vector equations for linear and quadratic basis functions explicitly. And we've also laid out the parts to be followed when we go to higher autobasis functions. And, while, in the formulations we develop, we carried out analytic integrations of all the terms that were needed. We've also looked at how we would go about carrying out numerical right? Numerical integration. And numerical integration is what would actually be done in a code, which before too long you will also be writing. What I'd like to do now is well actually observe that we are ready to move on to other problems, right? We are now ready to move on to multidimensional problems. Before we do that however, it's important to understand a little more, just a little more about the mathematical basis of the finite element method. And there are at least two things we need to do. What we will start doing with this segment, and it'll take three or four segments, or maybe a little longer. What we will do is to understand why the finite element method works, what its special properties are. And also, get a a fairly high-level view of the way finite element conversions works. Right? And we'd see the mathematical basis just to see a few of simple proofs for why the method works. Okay? All right. So that's the plan. We are going to start by talking of norms. Okay. And that, and by this, we mean mathematical norms, right? How do we estimate the, so to, so to speak, the magnitude of certain functions. 'Kay. In a manner, that makes sense for what we have to do here. Okay. Let me give you a little background before we really plunge into the subject and let us start by considering right. Consider the the finite dimensional trial solution that we are working with, okay. So consider the finite dimensional  trial solution. Okay? One thing I'm going to start doing from now on is perhaps stop referring to this as the finite dimensional trial solution, though it is that but actually start calling it the finite element solution. Okay. And so, this is the field that we would get by carrying out the finite element solution, and maybe going back and re-interpolating from the trial solution degrees of freedom. Okay. So let me also state that. Also called The finite element solution. Okay? That's probably how we, are we refer to it from as, as we move on. All right we consider it in the context of the problem we're solving. So we have something like that, and let's suppose we have some number of elements here. Okay, what I'm going to do is consider to show that there is some generality, complete generality with what I want to state. Let's consider that we are using here quadratic basis functions. Okay? So, that would mean what are intended to be elements here, each of them has midside norms, okay? So this now is omega 1, right? That represents some general element omega e, right. I'm going to now sketch out the quadratic basis functions. All right. So we have that. We have N2. And I'm sorry, sort of straight over a little. Let me try to do better.  We have N2. And we have N3. All right, for this element. Okay, I'm not going, well let me label them just for this one, maybe. N1, N2, and N3. Okay. For element two, let me perhaps use a different color so we can highlight a certain point. Okay. So for element two, my shape func, my basis functions are going to be, that. Okay, can, can I do better than that. Sorry. Okay, that's N1 for element two. That is N2 for element two, and that is N3. Okay? So, again, I will say N1, N2, N3. Okay? Now, of course, we also know that associated with each of these nodal points, we get trial solution, degree of freedom values, right? So let me write those. And here, I'm going to write them using global numbering, okay? So here I have d1. I have d2 here. I've realized I switched from sub to superscript in the first one. Okay. D1, d2, I'm going to write d3 here. D4, d5, and so on, okay? Now, the point I'm trying to make here is that the way you would, we would construct our feed now, right, within each element e, would be obtained by that process. Right? DAe, and we now how to go between local and global no degree of freedom numbers. Okay, observe the nature of our basis functions here. I'm going to ask you a question. What I've sketched here in red and green are the basis functions, respectively, for elements one and two. Is the solution continuous? So, the solution, the finite element that one would generate by re-interpolating from the trial solution degrees of freedom. Would that be continuous? 'Kay. Think about it. The answer is yes, it is continuous. Okay. Mathematical notation for a continuous function is C0 on omega. Okay? When we see a function of C0 on omega we're seeing, C0 simply means the function itself is continuous, right? Over the domain omega. Okay what about its derivatives? Are the function, or its derivatives continuous over omega? Think about that. Well let me ask you in step stone comma x. It turns out that it is continuous within each element. Right? So if I look at e comma x, right? Restricted to a single element, right? Within that element, e, the derivative is also continuous. Okay? So we would say that this function is C0 on omega e. Right? So the derivative is continuous within each element. Right? What about over the entire domain? Is the derivative continuous across the domain? In general, well not in general. The, the derivative indeed is not continuous across the domain. In particular, there are points of discontinuity on the, in the derivatives, right? At element edges like there. And there, right. What we will see is that comma x is discontinuous. Okay, all right. So, so comma x is discontinuous. In all of omega. Right? So now here what I am specifying is that we are not considering restricted to a single level. Right? It is across elements. Okay? So it's discontinuous in omega, and so what we say, however, is that comma x, the, the derivative is not in C0. When we take the entire domain omega into consideration, okay? So this is an important thing to notice, right? And where does this come from? Why is it that the solution right, the final element solution as itself continues, but its derivatives when considered over the entire domain omega, are discontinues. Where does that, where does that come from? Right, it comes from the basis functions, right? So what's important to notice here is that the Lagrange polynomial basis functions have been constructed. To be only C0 on omega, right. We have basis functions, which are only continuous, and that's immediately observable if you just look at this sketch that I made of the basis functions, right. Clearly the basis function that continues, but of course if you look at the inter-element nodes, it's clear that, that the derivatives are not going to be continuous. And of course, once you have that, any function that you then construct using that basis to represent the function is also going to be only continuous but not be continuous in its derivatives. Okay? So, we say that it's only C0. We say that it is not the function itself now, the function eh, or referring to basic functions themselves are not Cn on omega for n greater than 1. Sorry, n greater than 0. Okay? All right, the basis functions themselves have been constructed to only be continuous. There you see 0, okay? Their derivatives are not continuous, okay? Right, if the derivatives were continuous, we would say the functions would be Cn, right? Where n would be the number of derivatives that were continuous. Okay? So we're saying that the basis functions are not Cn, they're only C0. Okay? All right. So let me just give you an additional statement here to give you the more general case. In general, a function is in Cn omega if its derivatives Up to order n are continuous on omega. All right, our basis functions themselves are continuous. None of their derivatives are continuous. Therefore, they're only C0. 'Kay, the zero indicates no derivative continuity. 'Kay? However, remember when we talked about spaces. We introduce the spaces L2 and H1. Okay. So however, does right, the finite element solution belong to H1 on omega? Okay. All right. Remember, so what we're asking is, what this means is, if you recall, is the following. If we integrate over omega squared plus 1 over measure of omega to the power 1 over number of spatial dimensions. Okay. Sorry, this should be 2 over number of spatial dimensions.  Okay? Times comma x squared, dx. Right. If we do this, okay, what we are asking is, is this quantity bounded? Think about it. Is not continuous, okay? So what we will find is that this term, right? The derivative term is discontinuous. However, if we integrate it, right? Discontinuities are not unintegrable, all right? They can't be integrated. Okay so when we integrate it, we will see that this quantity that we put down here on the left-hand side is indeed bounded. Okay? So the answer is yes. Okay? All right, so even these continuities themselves don't pose a problem. Okay? All right. Now, okay, so does belong in H1, okay. So I'm going to use this background to say that and, and of course you know we, of course belongs in H1, because that's how we constructed right. Remember when we defined the Sh, right. Okay. So recall, belongs to Sh, which consists of, we, we said that it consists of all functions to each belonging to H1, right? On omega. And then we put in boundary conditions and so on, right? So, so by construction, it does belong to H1 and I'm, I'm just pointing out that yes, it does indeed belong to H1. Okay, with this as background, I'm going to start defining some norms."
NzqUwHWhooQ,"There remained an error on board work on the very last slide of this segment and the error came in here. The term that I've circled there, which is a factor multiplying the gradient of the function square, should be replaced with just, the following, all right? So we, the term in the denominator is really all we need. So we need measure of omega to the power 2 over n s d. That's it. We do not need that term which I have just written in green to be in the denominator. It is in the numerator and that's all we need to have as the factor. All right that fixes that slide and the rest of it or what came before it of course is correct."
KahsBm_Ve_Q,"HI, in this segment, we'll be finishing up the template for the first coding assignment. We'll look at the solve and output results functions, and especially we'll be looking at the L2 norm of error function. Okay, so let's move over here to the code. Solve you can see is a very short function. We first create this SparseDirectUMFPACK object A. Again it's also like a sparse matrix, but it's set up for a particular solver which is the UMF pack solver, okay? Now this A we initialize with our global matrix K. Again we've already applied the under conditions, we've already populated it from K local. And now we're going to do A.vmult D and F. What that is doing is D is = to K inverse times F, okay. I just want to point out here in the future when we are doing some matrix spectrum multiplications we will use dot vmult with DL2's full matrix data type. Just want to point out that here with SparseDirectUMFPACK.vmult does a matrix inverse times a vector. But in the future one we're dealing with just the DL2 full matrix and DL2 vector dot vmult simply does a matrix vector multiplication without doing any inversion. So again, in this case, with SparseDirectUMFPACK, it does a matrix inversion with DL2 full matrix it does no matrix inversion, okay. But again there's nothing here in solve that you need to change. Help results again is nothing you need to change. It creates a .vtk file, which is actually a text file, but that can be read through visualization software. Okay, in 1D there's actually not a lot to see because it's just a straight line, but in 2D and 3D it's really helpful to see what's happening with your temperature plots or your displacement plots as the case may be. All right, so we'll move on from there down to the L2 norm of the error. All right, Now here I'll write it out on the screen what this is. So we have out L2 norm of the error. Again, this is written on your homework assignment, and we talk about L2 norms in the lectures. Right here it's the square root of the integral over the entire domain of your exact solution, minus your finite element solution. Put dV just for generalization of course, in this case it's just across the x component. Alright, so this U is the exact solution. And is your approximate, or finite element solution. Okay, so let's look at how we'll do that. All right, so first I define this double L2 norm is equal to zero and eventually That's the value we'll be returning really the square root. So L2 norm will be that integral and then we square root it as we return it, okay? We have again a constant unsigned int of dofs_per_elem, which is again, just a number of notes per element. Our local_dof_indices vector and I've created these variables u_exact or just which is our approximate solution, our finite element solution. X, which we will use to find the value of X at each point. Again, X is the value in the real domain, the coordinate in the real domain. And HE, the element length. All right, we have the same loop over elements. Let's go back to the board for a little bit. We're going to be integrating this through Gaussian quadrature. The reason why we have this loop over elements is because we're breaking up this integral. First we break it up into loop over elements. And so now instead of performing the integral of sigma over full domain, now we're adding up individual integrals over each element's domain. Still have u minus dV. All right, now, this integral. Remember, we're using Gaussian quadrature. So, inside this element loop, we will now have another loop. Actually, first, let me, let's convert to our bi-unit domain. So we're going from minus one to one. U. Again, now these are being evaluated like c. And to convert, we multiply by he over two. Remember, we had that same he over two factor in our f local from doing that conversion as well, right? And now it's in the d c domain, okay. And now from there it's easy to go to our quadrature. Then I'll keep our square root we still have our loop over at the elements. So that would be element equal to zero less than the total number of elements. Although actually we will be using our iterator again to loop over elements. And now we're going to loop over quadreger points. I'm just going to put a q here but understand that's quadreger points. We'd be going from q equal to zero, less than quadral. Now we will be evaluating u at that quadrature point, and at the quadrature point. And then we will. Oh, I forgot to put my square there. C minus squared from the beginning, right? So that quantity is squared times he over 2. And now we have to remember to multiply by our quadrature weight. And all this is on the inside of the square root still because this is all involved with the integral. So this is the weight for cq, okay? Now again, u is actually a function of x, similar to our forcing function or our body force. It's a function of x and x, itself, is a function of cq. And so you would evaluate c the same way we did with f. Our solution vector D has the values of At the nodes, right. But here we want the values of At the quadrager points, so you will also do similar to finding the value of x, you will interpolate to find the values. So Of CQ is equal to the sum over A, with the same bounds as before. It will be DA, so that's your solution vector, at node A., times NA CQ. And again this D will come from D, we use local d ref indices again to convert from our local node number of A. To the global node number because d, of course, is a global vector. And so we need a global index. All right, and so now with this, you should be able to create that integrand, okay. Let's go back to the code I've actually again I found for you, just as before. I found x for you again. Just the same way as before. No location, again, is how we find xa, or in this case xb times basis function b at the particular quad point. Now remember, when we're doing a summation we always need to zero out x equals zero, equals zero before we start adding to it. Otherwise the numbers will be, if we don't zero out X and Beforehand, then they'll be including previous values of X and And they'll be getting much larger than they actually should be. Okay, so we zero those out before you perform the summation. And Again, here is D, local dof indices of B times our basis function. All right, so now we get to the line that you have to do. And essentially, this is what your doing. Adding it up with an L2 norm okay. And then we gout out of loop and it returns the square root. All right, so if you remember, why do we even do this L2 norm error? It's so that you can verify that your finite ultimate solution is converging to the correct solution as you increase either the basis function order or as you increase the number of elements. Now something to watch out for there's something to think about is what happens when your basis function order is same as the polynomial order of your exact solution. What does that do to your L2 norm? Think about that. Also, what happens as you increase, once you have a really high number of elements in your mesh. And look at what happens the L2 norm. Is it still decreasing? Is it still converging at an optimal rate as you increase element sizes? If not, think about why that might be the case, okay. And think about what relation the computer, the machine error has to do with that, all right? Okay, so that concludes the first coding assignment and our discussion of the template."
rB9e0NweTVA,"Hi, in this segment, we'll be looking at two visualization tools that you can use to look at the results from your finite element program. All right, so the two that we commonly used, although there are others are Paraview and VisIt. Okay, so let's look at their websites Paraview's just at paraview.org, and VisIt is at visit.llnl, that's Lawrence Livermore National Labs, .gov. And that's where you can download them, although if you're using the deal to virtual machine ParaView will already be installed there, and I think VisIt too. Okay? So if we go over to the website, here at paraview.org. It's really simple, they have a download tab here. And for the ParaView installer, you just choose your operating system, Windows, Linux, or Mac. And it's pretty straightforward from there to download that and install it. If you are using, or using Linux, you can open up the file, and actually within this bin folder, you can open up the executable. You can run the executable here. Okay? So you actually don't need to install anything when you're running it in Linux. You will install it with Windows, and with Mac it's also pretty easy to just download and run. Okay? So for an example here, I'll open up file. And let's go to our FEM templates. Here's an example of an elasticity file. So I open the .dtk file and pressed apply. Now it has this blank or this grayed out box. Okay, this box is a domain. But I don't want this solid color, I can come up here and change it to U, which is our displacement vectors. Okay, now it's giving me the legend with the magnitude of displacement as the color. Okay, I can rotate that if I want to. Since this is an elasticity problem, you'd probably be interested in seeing the displacements. Not just as a color but the actual displacements themselves. And so we go here, press on WarpByVector. And I'm going to scale it by 50, because I know that the displacements on this problem are actually pretty small. So I'll press Apply. And now you can see the displacements there and the magnitudes. You can also come up here and change for Magnitude of displacement to the X displacement, Y and Z and so on. Okay? All right. So that takes care of it pretty well. You could also. Look at the outlying surface with edges, so you can see your mesh. Wire frame just shows the mesh itself. Okay? So you can play around with some of the other options if you want, but this should cover you for the problems we'll be working with. All right, I want to show you one other thing here, I'll open up some solutions to the transient heat conduction problem. So here I can actually open up a group of vtk files that have the same, it's solution, dash, and then a number. So it groups them together, automatically. Okay? I can just press Apply here, and it automatically has the temperature profile, the temperature distribution. But since there are more than one, I can come up to the top of the screen, and I can select next frame, and watch the evolution. You can actually press play, but since there are only four files here, it goes through them pretty quickly. But if you're outputting more files, you could watch the evolution of your model as time steps on. Okay? So, this is a pretty powerful tool in looking at your finite element results and kind of getting a feel for what's happening in your solution, all right. We'll go quickly over to the VisIt website too. Again, that was at visit.llnl.gov. And it's similar. You go to the downloads tab. Here, the Executables. And then you scroll down and you can download for Windows, Mac, or Linux. All right, let me open up VisIt for you, I run mine from the terminal on this computer. It can do essentially the same things but it has a little different format. Now we can open up the elasticity problem. FEM templates. Homework 4. Solution .vtk. All right, so, now the file is open, I want to add a few different things. I'll add the mesh and press draw, so you can see the mesh there. I can see the mesh. Scroll out or in. Then I'll add sudocolor, a view magnitude. Again the displacement vector as hue, magnitude just again, gives me the magnitude. In VisIt, it's not quite as easy to just get the X, Y, or Z components of displacement. All right, here I can rotate it around if I want to. And to look at to deform it according to the displacement, I have to come up here to Operator, Attributes, down toTransforms, and Displace. All right? I need to select which variable I want to be my displacement variable to vector. And it's the vector u. And again, I'm going to change the multiplier. That to scale the displacement by 50, okay? Apply and draw, can dismiss that file, or that box. Okay, so again, you have the displaced domain, all right? And again, it's very similar if you want to look at your temperature distribution on the heat connection problem. Okay, so that should be enough to get you started using VisIt or Paraview in order to visualize your finite element results."
XIvPxWjti-w,"What we are going to do, is to use this as background to define to define norms. Okay? We are going to restrict ourselves to defining a, the sort, only the sort of norm we, we, we want, okay? So what we will do is define the H1 norm, okay to be the following, okay? So supposing we're taking a func, we have a function, v, okay. We're going to define the H1 norm of v, and we will write it as that. Okay? The double lines are, are a kind of bracket. They're they're usually used to designate a norm, and that's subscript 1 indicates that we're talking to H1 norm. This is defined as the following 1 over measure of omega to the power of 1 over number of spatial dimensions. Integral over omega, v squared plus measure of omega to the power 2 over number of spatial dimensions, 'kay? Of v comma x squared dx. All of this raised to 1 over 2. Okay. Just a couple of things to notice here. We have divided through by measure of omega raised to 1 over number of spatial dimensions to get rid of the effect of having integrated, the function and its derivative squared over the domain. Okay? Defined as this, we have what is called the H1 Hilbert norm of our function v. Okay? Hilbert norms are, are a special case of a more general type of norm that's called a Sobolev norm. Okay. This is a this is an example of a, a, this is an example of more general norms called Sobolev norms. Okay? The idea of Hilbert and Sobolev norms come from functional analysis in Hilbert and Sobolev spaces. The fact that we are raising the function to the 2nd power as well as its derivative to the 2nd power, and then taking the one-half power of the whole thing is what makes it from a Sobolev norm in general to a Hilbert norm. Okay. 'Kay, for Sobolev norms, you would be raising to other powers, and also sort of taking that, the root of that corresponding power, right? So that, that, that leads to Sobolev spaces. Okay, so, but, but this is what we need to work with here, and so we work just with just H1 Hilbert norms, okay? Or we will just recorded them as H1 norms. Now note that you get a, a special case. Not, this indeed is actually a special, special case of a more general way of defining even a Hilbert norm as one can extend this to, to the Hn Hilbert norm. Okay? By, by just taking up to n derivatives instead of a single derivative as, as I have done here. Okay? So we can extend this to define the Hn norm, 'kay? By including the first n derivatives. Okay, instead of taking just one derivative here. Okay, now, the fact that we can do this also allows us to define what is called the H, what we may choose to call the H0 norm. Okay, by just saying well, I don't, I don't consider any derivatives, I just consider the function itself. Okay? So the H0 norm would then be denoted by either 0 here or it's more common not to put anything. Okay, so when I just write this without any, without, without the subscript 1 as I have in the first equation of the slide, we have what is called the H0 norm. Okay, which is just now 1 over measure of omega 1 over nsd integral over omega v squared dx to the power 1 over 2. Okay. This is called the H0 norm. Now, you note that the way we first talked about H1 functions. At that time, we also talked about another class of functions called L2 functions, which were simply squared in the. Right? Okay? And, and in that case, we just simply multiplied the function, we squared the function, and integrated over the domain. Right? And, we said that if that, this quantity in fact is integral that we've written on the right-hand side, we said that if it were bounded, the function is an L2 function. Okay so this is entirely equivalent to also the L2 norm. Okay. So in fact what we have here on the, at the bottom of the slide is more commonly referred to as the L2 norm rather than the H1 norm. Okay, that is more common terminology. But, but you see that the L2 norm is basically the H0 norm. Okay, so this is all background. What do we care? Where does any of this help us? Here is a result I'm going to put down that we will use today. Okay. The result is the following. We can also sort of leading up from this idea define what we will call the energy norm. Okay. You can also define the energy norm of v, okay, which is the following. We will denote the, the energy norm as I'll tell you what it is first. It is integral over omega, okay, of v comma x, E v comma x, dx. Okay? All of this to the power one-half. Okay? This is the energy norm of v. Why is it called the energy norm? This is a throwback to the time when structural mechanics was considered problem that finite element methods were applied to. In that setting, if v were your, your, if v were your displacement field, then the quantity I've written out here will be related to the strain energy, okay. So this comes from the notion of the strain energy of v. Okay, you would tend to think of v as a modulus, and therefore, you would conclude that this is a strain energy. Okay? So this is the energy norm. Okay? All right. We have a result which is a result on equivalence, equivalence of norms. Okay? And that result is the following. Okay? It says that if we take v, we form it H1 norm, and multiplied by some constant, say c1. Okay? We're seeing that this H1 norm of v, bounds from below, the energy norm of v. Okay? Okay? It can be a lower bound of the energy norm of v, which itself can be an upper bound of the H1 norm, sorry, which, which itself can, can be another load bound also of, of the H1 norm. Okay? Alternatively, what the statement says is that essentially what this thing is saying is that we have the H1 norm of v and all we have in these two terms that I put arrows on. The only difference between them is the fact that they have different constants. Okay? What we're saying is that just by multiplying it by different constants, it is possible to bound the energy norm of v from below and above. Okay? All right. So, the only way this is possible is if, fundamentally the H1 norm and the energy norm behave in the same manner. When one goes up, the other goes up too. When one goes down, the other goes down too. Okay, if that is the case, then we can always find these constants c1 and c2, such that you can bound the energy norm from below, and below like here, or from above like there, using the H1 norm. Okay. So essentially this, what this thing is saying is that one can bound the energy norm from below and above by the H1 norm. Okay, that really the only difference is a constant of multiplication, okay? So it's in this sense that we, that we say that the energy norm and, and this H1 norm are, are fundamentally, you know, they're, they're fundamentally equivalent. Okay. To end the segment, I just want to pre, present a little more notation, okay. It is the following. We will define what we will call inner product notation. Okay? So. We will define w comma f as written like that to be the integral over omega of w times f dx. Okay? Right. We will define as well the following. We will see that, so this is set to be the inner product of w and f. Okay? In particular, this is what is called the L2 inner product. Okay, and you just take two functions, multiply them together, and integrate them over the domain, over which both functions are defined. We get the L2 in our product. Okay? We also have what we will call a bilinear form notation. Okay? And for our focuses, we will define the bilinear form of w and u. As this to be the integral over omega of w comma x, E u comma x dx, okay? All right. You observed where this is coming from. Where does the integral on the right come from? Why are we defining it in this form? What is motivating it? Right, what is motivating it, is that is the fact that this integral on the right is, in fact, the integral that shows up in our weak form, right, this is what leads to the stiffness matrix. Okay, and in fact the inner product that I defined up here is exactly the sort of inner product that shows up on the right-hand side, right, the forcing function. Okay? All right, so this is motivation for defining them. Why do you think this form is said to be bilinear? Okay? Right. The reason it's bilinear is that it is linear in w, in the function w. As well as in the function, u. Okay? So it is bilinear in w and u. Right? Or, or alternatively, there's linear in each of w and u. Okay. Therefore, we say this case is bilinear. Okay, so that is perhaps the bi here is superfluous but I use it just to re-emphasize what's going on. It's individually linear in w and u. 'Kay? Last question for this segment. Do you see a relation between what I've just written between this bilinear form and the energy norm, that we defined in the previous slide? Right. Note that a of u comma u, right? If we use the same function in both slots of the bilinear form, we essentially recover our energy norm. Okay. And what comes ahead of us, the analysis, we will tend to use this notation, a of u comma u, for our energy norm of u. Okay? Okay, we'll end this segment here."
CkUdAi3ehbI,so there was a question on the equivalence of the H1 and energy Norms uh essentially asking whether one can always establish this equivalence the short answer is yes and the more detailed answer is that yes one can and the reason one can do this is um draws from from from from two facts okay um let me go over those facts right now so um does the equivalence of the energy Norm sorry the H1 norm and the energy norm and now let me use our new notation for the energy Norm right let me use this right does this equivalence um always hold okay and the short answer is yes now to understand why I'm not going to give you the entire proof but uh here's the basis of the proof the basis for why this works first of all observe that in order to speak of either of these Norms they have to exist okay okay right um note that uh there exists that's the symbol for there exists right there exists the H1 Norm of v and uh the energy Norm right both of these exist uh if and only if iff is for if and only if which means it's a necessary and sufficient condition okay so if and only if uh this Norm the the H1 Norm is less than infinity and so is the energy Norm okay to even speak of them for these Norms to be worked with they have to exist and what the sorry that's than infinity what this means is and the reason we can say this is that um Infinity is not a real number right the Norms both have to be real numbers for us to work with them which means they have to be finite numbers okay those are the the real numbers only include finite numbers okay um first of all so this works and then what what we can also use is the fact that Omega okay the measure of Omega uh is also um less than infinity what this means is that that our domain Omega is finite okay what this means is that Omega is finite okay then it's a little technical but then one can actually use the fact that since Omega is finite uh one can always represent the effect of the function itself over the domain right the function squared and integrated over the domain right one can always represent the effect of of from that contribution uh by just scaling the energy Norm with uh by by a constant okay so what this means is that if you now look at the H1 Norm right and I'll write it just in this form so if you look at it as b plus uh measure of Omega to the^ 2/ NSD uh V comma X the whole squar DX right and yes of course there is also the one over measure of Omega 1 / NSD here and this whole thing raised to the half power right this is the H1 Norm right now if you are comparing this on the side with the energy Norm which is um which is of this form right um the energy Norm is integral over Omega V comma x e v comma X DX uh to the half power right you're comparing these two what is missing on the right hand side is this term right using the fact that both the Norms have to be bounded for them to be defined and the fact that the domain Omega is finite right one can essentially demonstrate that the difference between these two Norms brought about from the fact that the H1 Norm does have the effect of V squ while the energy Norm does not have it that difference can essentially be compensated for through constants right through finite constants so this is why when we multiply this by C1 we can indeed show that it's lesser than or equal to the energy norm and with just using a different constant one can demonstrate that the energy Norm itself is lesser than or equal to to some other constant multiplying the uh the H1 Norm okay so this equivalence holds in general it it's based upon upon this fact and that one okay
Pz7YMpWeXLc,"Okay, so we move on with our analysis of the more mathematical properties of the finite element method. What we did in the last segment was establish some facts about norms. In this segment, we are going to look at two properties of the finite element method, okay? Those properties are consistency and the best approximation property. All right. In order to do that, there is, just one more thing I want to do here. Okay? So, let's start by recalling. Recall, the finite dimension, the, let's just recall the weak form. Okay? Right? Recall the infinite dimensional weak form if you'd like, okay? Very quickly let me right this out. Integral over omega w, x and then now rather than write sigma, I'm going to write E u, x. We add an A here. Okay? This is equal to we said integral over omega w f A d x plus w(L)tA, okay? Now, note that, this, okay? Is essentially like our, this in fact our definition of a w,u. When we define a w,u, we didn't include the area in that definition which could very well include in the definition. That's a w,u. Likewise, this is w, f, all right? The way we defined it in the last segment. And, in this case, you know, I defined it earlier in the last segment for general mathematical functions. Here, we have an Adx. But then we can just say that A is sort of a part of dx and you know, sort of lump it into that into the definition. What we will do now is also define this quantity to be w, t, okay? All right? At L, okay? Note that this last quantity we've just defined W,t on L is also an inner product. Okay? All right. It is also it's linear in w and it is linear in t. Okay? All right. So essentially what we're saying is that using this notation we can rewrite the weak form in what is called abstract notation. Okay? So in abstract notation we have A w, u equals w, f plus w, t at L. Okay, this is just abstract notation for our weak form. Now. In exactly the same sort of manner note that we can rewrite we can also write our finite dimension in weak form. Okay? All right. The finite dimensional weak form let me go to the next slide just so that I have enough room for everything there. integral over omega w h, x E, u h, x A dx, okay? Again everything's sigma a just E u h, x. All right? Integral over omega, w h f A d x plus w h at L t A, using our notation, using our abstract notation, note that this is a w h, u h, equals w h, f, plus w, h, t at m, okay? Abstract notation for our finite dimensional weak form, okay? So we have these two, weak forms in abstract notation. Okay? Now, let's do the following. Okay? So let's look at the abstract weak form in the infinite dimensional case. Okay? So, consider a w, u equals w, f plus w, t at L. Okay? So this is the, abstract form of the infinite dimensional weak form, okay? Now, remember what we say here for, the waiting functions, okay? We say that this weak form holds for all w belongs to v. Okay? All right. But then, also recall that wh, okay? Which is our finite dimensional weighting function, all right, belongs to Vh, which is a subset of V. Right? Right? What this says is that when we say here that the infinite dimensional weak form must hold for all W belong to V, it also holds for finite dimensional weighting functions, right? because finite dimensional weighting functions also belong to the same space, okay? So, what this means, is that so, so this relation, let me call this relation A, 'kay? Right? That equation the infinite dimensional weak form, 'kay? So, A also holds for w h because w h itself, belongs to V, okay? What this implies is that we are able to write a w h, u equals w h, f plus w h, t at L, okay? I'm going to call this relation B, okay? All right, and this just comes from the fact, that our finite dimensional weighting function, also belongs to the same space V, as our infinite dimensional weighting function. All right? Okay. Now, let me go back, and label the proper finite dimensional weak form that we worked with, as C, okay? All right, and now, what I'm doing to say, is subtract. B from C, okay? We're going to go to the next slide, and actually do this, all right? So, just start at B and C, and let's subtract them from each other. Let's subtract, sorry, let's subtract B from C. So here is C first. A, w h, u h, going to leave, leave myself enough room here, okay? Equals w h, f plus w h, t at L. This is just equation C, which is nothing but our finite dimensional weak form. I'm going to subtract from this B, which we caught by observing that the finite dimensional weighting function, also satisfies the infinite dimensional weak form, okay? All right? So, what do you get here? Okay? What you see on the right-hand side is 0, 'kay? Because these are, these, these forcing terms exactly cancel out. Now, recall that the functional, that, that, that A is a bilinear form, right? Which means it is linear in each of its arguments. It's linear in the first argument, as well as the second argument, right? Likewise for this. What this lets us do, is just simply write out, what we have on the left-hand side as a w h, u h minus u equals 0, okay? All right. So, we've got as far as here, 'kay? Now, look carefully at that term, u h minus u. It gives us the difference between the finite element solution and u, which is the solution to the infinite dimensional weak form. The solution to the infinite dimensional weak form would be the exact solution, okay? So, u h minus u is a very special, very important measure of our numerical method. What would you call it? It is the error, e, 'kay? It's the error in the finite element solution, and why is this? Because u, which is the solution to the infinite dimensional weak form, has to be the exact solution, right? The infinite dimen, dimensional weak form is equivalent to the strong form. The solution to the infinite dimensional weak form has to be the solution of the strong form, which has to be the exact solution, all right? Right, u is also. The exact solution, okay? All right, so, what we've just demonstrated here is a consequence of the condition that we had on the previous slide. This condition, right, the condition that we've marked as B, okay? So, what we've seen here is a consequence of this condition, okay. So, let me state this here. This results. From B, okay? Which is called the consistency condition, okay? All right. Let's look at what we have here first, and then try to understand the consistency condition better, okay? What we have here is a statement saying, that a of w h, e equals 0, okay? Now, this bilinear form that we have, a, can be viewed as a mathematical generalization of the idea of taking a function, w h in this case, and projecting it along a different function, e, in this case, okay? So, effectively, what this is saying, is that suitably defined, the projection of the error on our space of functions w h, right, or space of functions V h, that projection is 0, okay? What this is saying, is that the, that the projection. Of The error on the space v h is zero. Right? Viewed differently, the saying that any error that exists in the finite element solution, actually lies outside the space that we're even considering for our finite dimensional solution. Okay? So, already the saying that in some way we are actually doing very well in having chosen a space v h in order to get our solutions, in order to look for solutions. We're doing very well, because any error that we get is not even in that space. So within that space we're doing as best as we can. Okay? All right, so it, it's, it's saying that in some sense it is saying the error is orthogonal. To v h. Okay? Right? One can define, formally and rigorously, a notion of orthogonality for functions, using our, bi-linear form. And what it is saying is that with that def, definition of orthogonality, our error really lies outside of our space v h, because it has no projection left in v h, and that's why we're seeing this result that this projection is effectively zero. So in a sense, it's saying that we're doing very well with, with having chosen the space v h. Okay? That's one interpretation. Now I said that it comes from this condition b. Right? So let me rewrite the condition b. Okay. The condition b states this, right? It says that a of w h comma u equals w h comma f plus w h comma t at l. Okay? This is what we have. Compare this with, compare this with the equation that we actually solve to get our finite element solution. So, and that equation which you recall is the finite dimensional weak form. In this abstract notation, the finite dimensional weak form is this. Okay? Right? This is the, this is the equation we're actually solving to get u h. If you look at condition b, we can regard condition b as being obtained, if we simply went to our finite dimension week form and replaced u h with u. Okay? If we simply went to our finite dimensional weak form and replaced u h with u, we would get condition b. Right? So what is this saying? It is saying that if somehow we were to come upon the exact solution, okay? U, that exact solution also would satisfy the equation we are trying to solve. Right? So, what this is saying is the exact solution u satisfies the finite dimensional weak form. Why is this important? The equation we are actually solving in our numerical is the finite dimensional weak form. What we just demonstrated is that if our method were such, or, or, you know, by chance, were were to well not by chance, if we would actually pick the exact solution, right? Maybe our space was such to include the exact solution. Then, if we were to take that exact solution and plug it into our solution, into our, into the equation we are working with, we would find, we would find satisfaction of the equation. Right? So, we would, our method admits the exact solution as one of the solutions it can find, that it will accept. This is not the case with certain other numerical methods, right? Most prominently, the Finite Difference method. With the Finite Difference method equation, the underlying equation itself is altered so that, if you take the finite difference equations and substitute the exact solution, you are not guaranteed satisfaction of the finite difference equations. So in that case, you are actually working with a set of equations which does not any longer admit even the exact solution as an acceptable solution. The finite element method allows the exact solution, a. Okay? And this is why this condition, b is referred to as the consistency condition. Okay. The exact solution is consistent with the finite dimensional equation we are solving. Okay, the, the exact solution is consistent with the finite element method. Okay. So this is the consistency of condition. Okay. And I should also extend this node to saying, well we've all ready said the exact solution u satisfies the finite dimension of weak form. Right? So this means two things. What it means is the finite element method can recover the exact solution. Exactly! Right? Right? And then we also to mention here, this is not the case for other numerical methods. Right? Of course, other numerical methods unless they inherit this particular property wouldn't do it. Most prominently, finite difference methods do not have this property, okay? So this is, this is important to recognize. Okay? And a great deal of the strength of the finite element method comes from this result. Okay? And this result, consistency, is what leads to the conclusion that we observed in the previous slide that the error in the solution that we obtained, the error in our finite element solution is, orthogonal. In, in, in a suitable mathematical, in a rigorous mathematical sense, it is orthogonal to the space in which we are looking for solutions, right, in, in the space v h. Okay? All right. Let's stop the segment for here."
_ouSo_1TcIw,"Welcome back. In this segment we are going to look at a at, at another key property of the finite element method. And this is called the best approximation property. It draws in a very important way from the main result that we had in the segment preceding this one. And that result is the consistency of the finite element method. Okay? So, let's get started here. We're going to look today, look at in this segment, the best approximation property. Okay. In order to build up to this result, let's consider the following. All right? Let us denote, as we've been doing so far. Let u h, which we will recall, belongs to s h, be the finite element solution. Okay? All right. And let w h as we've been doing also. It belongs to b h, be a weighting function Okay? And note that I'm saying it is a weighting function, and the idea is that we are going to consider any waiting function living in v h. Okay? Now I'm going to introduce a code field that we have not previously considered. Okay? I'm going to denote this as capital U, h. It too belongs to s h, okay. And however it is not the finite element solution, okay. So what does this mean? This means that when we say that U h belongs to s h. We are recalling that s h finally consists of functions, right? Since capital U h is meant to denote a general member of s h which is typically different from the finite element solution, right? What that means is that u h belongs to h one on omega. And u h satisfies the Dirichlet condition. Right? The Dirichlet boundary condition. Okay. And let us suppose that we're working with the Dirichlet problem. And you know how this would work if we were doing the pure Dirichlet problem. We would have the other boundary condition also in there, okay? So this is u h. All right, if we have this, let us note that, okay. Note the following, okay? Note that any u h, okay, can be written as the finite element solution, okay, plus w h, okay? Why is this? Note that as I made a point about, w h is any waiting function, u h is a very specific member of s h. It is in fact the finite element solution. All right. Also note that u h belongs to h one and so does w h. Okay. Therefore, their sum also belongs to h one. We haven't quite proven it, but you can probably. See why that works out. It does indeed work out rigorously. Therefore, when we add u h and w h, we recover a, we recover functions which live in h one, which re, which remember, or which we recall from just out here, is also a property of u h. Okay? So take. So adding little u h and w h maintains that property of capital U h. What about satisfaction of the Dirichlet boundary condition, which capital U h has? Well, remember that since little u h is the finite element solution, it does satisfy the Dirichlet boundary condition. What about w h? Does w h not mess up that result? It doesn't, because w h is zero on the Dirichlet boundary. Okay. Okay, so note that when we write u h in this form capital U h in this form, it does indeed satisfy all the requirements we have for this new set of functions capital U h. Okay. So let me just state that. U h equals little u h. Plus w h, right, where little u h is a finite element solution and w h is any waiting function, okay? This does indeed belong to h one, right? And I would just write and satisfies. The Dirichlet boundary condition. Okay? In what we are going to do today, the main result of this segment, we will use this result in a in an important manner. Okay, so with that in place let me state the main result of this segment. It is a, it's a theorem really. Right? And than theorem is the following. If we consider the energy norm of the error. And here I'm writing the energy norm using abstract notation. In one of the segments preceding this one, we demonstrated that with abstract notation for the bilinear form as we had defined it. We do indeed get the, the energy norm of the error, if we write it in this form. All right, so. The theorem is that the energy norm of the error is bounded from above. By the energy norm of what you may also consider the error, except it is capital U h, this new field that we've introduced, minus u. Okay this is the result that we are going to prove. Why is it of interest? Note that e, the error is the finite element solution that's little minus u, okay? So what we're saying here in this theorem statement is that the energy norm of the error, 'kay, is such that it, that, it is such that the finite element method has picked a member of S, H, okay, to be the finite element solution, which gives us the lowest energy of any possible other member of S, H, 'kay? Right, so let me state the, sort of give you a plain English statement for the relevance of this theorem, 'kay? So what we are saying here is that the finite element solution minimizes, minimizes the energy known. Okay? Of U h minus u over all members. Belonging to Sh, 'kay? The space Sh consists of a number of functions, okay? One of them is the finite element solution. That finite element solution is that member of Sh that minimizes the energy norm of the difference between that function and the exact solution, 'kay? In this sense, with the energy norm as a measure, the finite element method picks the best possible approximation among all members of Sh, okay? And this is why we call this segment the best approximation property, and in fact this theorem is called the best of the, this is, this, this statement that we have here represents the best approximation property, 'kay? And this holds for all Belonging to Sh, okay? All right, we're going to set about proving it. To prove it, let us first consider the energy norm of a slightly different function. I want to consider the energy norm of e plus wh. All right? Now we know how to expand this out, right? We get a result, which works just like the expansion of a perfect square, 'kay? It is that a, this is equal to a, e, e plus 2 times a. Well let me write it out more explicitly this, this is a subsequent step, plus a(e,wh) plus a(wh,e) plus a(wh,wh). All right, and this comes about just from the expansion. If you were to go back and look at what this abstract notation actually represents for the energy norm, that using the fact that it is bilinear in its two arguments, right, with these being the two arguments. You will get this result, right? It's just like, just like the expansion of a perfect square. And then, one recognizes that because the bilinear form also is symmetric, right, we note what we get for the result in the bilinear form is symmetric. Right, we see that these two terms, right, are equal, okay? So that lets us then write it out as energy norm of e plus the bilinear form a, let me write it as (wh,e). 'Kay, recognizing that the second and third terms in the line above are equal. Plus a of (wh,wh), all right? Okay, stare at this now and try to recall whether we can say something special about this term. Right, indeed we can. In the previous segment we demonstrated that this term is equal to 0. And this term is equal to 0 is a result that we've termed consistency of the finite element method. Right, it is the result that it, it, it, it, it, it comes from consistency of the finite element method, and consistency of the finite element method itself, you recall, is the fact that the finite dimensional weak form admits the exact solution as a solution, right, it recognize the exact, recognizes the exact solution. Okay, so what this says then, is that what we started out with at the top of the slide is a of e,e plus a of wh, wh. Now, each of the two terms on the right-hand side is a, is an energy norm, right? The first is the energy norm of the error, the second is the energy norm of the waiting function. There is a property of norms, right, a fundamental property of norm. Right, almost the definition of a norm. It is that each of these has to be greater than or equal to 0, and you can go back and check from our definition of what the energy norm is, that indeed this holds, okay? All right but if this is the case, what this implies for us is that let me now just pull down the term from the left-hand side here, a of e plus wh,e plus wh equals this. Okay, what this implies for us is that a of e,e, I'm just writing the energy norm of the error, right, this term. Okay? What it implies for us is that. Well, let me ask you. How is this term related to a of e plus w h? E plus w h? What's the relation here? Right, it is lesser than or equal to it, okay? All right, simply because the sum of two nonzero terms in the right-hand side is equal to the term in the left-hand side so we get rid of one of those nonzero terms. Well, what remains must be less than or equal to what we had in the left-hand side and then just flipped around their positions. Okay, and now we're getting somewhere. In order to proceed, let me go to the next slide. I'm going to rewrite this term differently, okay, the term on the right-hand side. Note that. A, e plus w h comma e, plus w h is equal to a. Now, the error. I'm going to rewrite the error in terms of the finite element solution and the exact solution. The way we define the error was simply the finite element solution, little u h, minus the exact solution, u. And we have w h there, the same in the second slot, u h minus u, plus w h. Okay? But now I can recombine this, right? I note that when I get u and w,h, u h and w h, this is from the result we proved at the beginning of the segment, just capital U h, right? The function that we introduced which we identified as being any arbitrary function in S h. Okay? That minus u, comma, same thing on the left-hand, on, for the second slot. All right? Now, we put this together with what we had at the bottom of the previous slide, right, which is this result, in the bottom of the slide. So, combining. With the inequality. Above, right. The inequality that we came upon just a few minutes ago, we see that that inequality which is a of e comma e is lesser than or equal to this term. 'Kay? Well, that means it's also, it's, it's lesser than or equal to U h minus u comma U h minus u. This is what we set to prove out, set out to prove, okay? We're done. What we've done here is proved the best approximation property, right. And I'll restate the significance of this result. It says that the other way to write this is instead of the, you expand out the error right, which is u h minus u. Okay, so the energy norm of the error where the error is defined as a difference between the finite element solution and the exact solution is bounded from above, right, by the energy norm of any other function living in S h, right, any other function, all right, for all U h belonging to S h. But little u h, the finite element solution also belongs to S h. So what the finite element method does is that it picks for us the solution, as the solution, as the finite element solution, that member of S h, that minimizes the energy norm. All right, of the difference between the function and, and the exact solution. So it's in this sense that we have this best approximation property. Okay, so re-emphasize that, let me just state that this is what we mean by the best approximation. Property. Okay? And just remember in all of this, that what we have is little u h, is indeed the finite element solution, all right? So, the finite element method does a very efficient job once we pick our space S h, does a very efficient job of choosing that member of S h that minimizes the norm of this of this difference. 'Kay, and says that well, that is your finite element solution. Okay. All right, good. We'll end the segment here."
THsKroJbkVc,"All right. So, we've just seen this best approximation property of the finite element method. And hopefully, it gives you a little more, little insight into how the method works in picking solutions, from the space that we have chosen for it.  In this segment, I am going to derive another result that is sometimes called the, the equivalent of the, of the Pythagoras' theorem. For this for this particular problem that we are working with. And it has a nice consequence that is useful to note when one is actually looking at numerical solutions, and comparing with exact solutions. Okay? So so right, so, so the title of this segment is The Pythagorean, The Pythagorean Theorem. Right? And it's really in quotes because it's, it's similar to that, but it's not the original Pythagoras' theorem. Okay. Right, here is theorem first of all. This theorem is really a corollary of the fact of consistency, but but, okay, so let me state this as a corollary. Okay? And the corollary is this. It is, that right. If we consider the energy norm, of the exact solution, okay? This is equal to the energy norm of u h comma u h plus the energy norm of the error, okay? For, for a specific type of problem, okay? This holds for a problem where S h equals V h. Now, if you're wondering what that means, it simply means that our Dirichlet boundary conditions, for this particular problem, are homogenous ones. Okay? So, any place that will specify there is a Dirichlet boundary condition, we are saying that, that Dirichlet boundary condition is 0, okay? All right. So what this means is Dirichlet boundary conditions are Homogeneous. Okay? All right. And you note that this actually holds for the Dirichlet-Neumann problem. In fact, something that we often remarked upon during our development of the finite element method, for that particular problem was that, well, yes, if, if the, if we have the left-end of the bar fixed, then S e h and W e h are, are, the same, right? We just  S h, and, and V h aren't the same, right? Or alternately, little u h and w h satisfy the same conditions, right? The same  conditions. Okay, it is for this case, that we, that, that it holds. So, proof. Okay. So, let's in order to start looking at this, let us consider here right. Let us consider the following. For this setting we have u h Minus u equals e. Right? That's our definition of the error. Right? This is the error. The error in the finite element solution. And from this, we can therefore, write that U equals u h minus e. Therefore, when we consider on the left hand side, the energy norm of the exact solution. We see. Of course, we see this. But now, just as we did in the previous segment, we can expand out the term on the right hand side. By just using the fact of bi-linearity. All right. And this gives us a u h comma u h Minus a U h comma e minus a e comma u h Minus, sorry plus a e comma e. All right? Okay. And then as before, we recognize that the second, and third terms are the same because of the fact of fact of what? Fact of symmetry, right? The symmetry of the bi-linear form, right? So, that let's us write it as a. U h comma u h Minus twice a. U h comma e. Plus a e comma e. Okay? But now, if you look at the condition in the statement of the corollary, you should be able to see something about the second term there. It should be able to say something about the second term. What can you say? This is equal to twice a of w h comma e because the spaces S h, and V h are the same. Right? So, u h is the same as w h, right? U h is a w h, it is indeed a weighting function. Okay? And this therefore, is equal to 0, by consistency. Okay? And there we have it, right? We, what we are left with is, is the result we set out to proof. Okay? This is a, the energy norm of the finite element solution plus the energy norm of the error. Okay? This holds of course, for the problems with the Dirichlet boundary conditions are homogeneous, right? So, it's a useful to think to note in some sense this looks just like the Pythagoras' Theorem. Recognizing that the energy norm involves a square. So, it's like saying that w, w, if we use the this bi-linear form to define a more generalized version of an inner product, right? It is, it is indeed an inner product, but we can use it to use to define a more generalized notion of a square, right, of two functions. Right? Then this generalized notion of the square of functions tells us that in as much as we admit, this to represent a square. We are saying something to the affect of u square equals u h square plus e square, okay? All right. So, that is why, we call it the Pythagorean theorem. What good is this? Okay so a corollary of back Okay? So, let's call this corollary Maybe, I should call this corollary 2, and I call that corollary 1. So, let me do that. Let me go back now, and call this one that's corollary 1, okay? Okay, corollary 2. It says that, I mean, corollary 2 is actually let me state it. The corollary 2 says the finite elements solution. Okay? Underestimates The energy norm Of the solution. Well, let me  the solution, let me see the energy norm of the problem. Proof is a single step from corollary 1. We see that a U comma u equals a U h comma u h plus a e comma e. Now, because of the nature of this bi-linear form and the fact, that it leads to an energy norm, when we use the same function, both slots. We know, that this is greater than, or equal to 0. And so, is this, right? This is a fact that we've remarked, upon the previous segment. All right. What this implies for us then, is that if we drop the last stone, what we get for the right-hand side is that a of u h comma u h, the energy norm of the finite elements solution. Is lesser than, or equal to, the energy norm of the exact solution. Okay? And this what we mea,n by saying that the finite element solution, u h, underestimates the energy norm, all right? Of the problem. The exact energy norm of the problem, is what we have on the right-hand side here. Okay? When we get the finite element solution, and we go back and compute the energy norm, which is what we have on the left-hand side. We will be computing an energy norm, which is less than that of the exact solution. Why does this matter? Well, it is, as all norms do the, this is a particular way in which to get some sense of another sense of control over the solution. What is the solution doling, how big is it? Well, you can compute it's energy norm, it turns out that the energy norm that we get from finite element solution is not as big as the energy norm of the exact solution. Why might, where else, when might it matter in a more physical sense? If you are solving a problem in linear elasticity, for instance. Where the ene, the energy norm as, as written out here actually, has physical meaning in it. And in fact, corresponds to, what corresponds to the strain energy, right? What it says is that, if you had the exact solution available to you, and you were to go ahead, and compute the strain energy of the exact solution. And compare the strain energy of your finite element solution, what you, what you ought to, to see is that the finite element solution actually, underestimates that strain energy, right? It gives you a smaller strain energy. Okay? So, that's a particular result of this, of this, of this method. Okay. We are actually going to end this segment here. It's a shorter segment, but we are, we are getting prepared for the main results of, of our, analysis of the finite element method."
JPmvH5xkGas,"That was a question on my use of the of the term homogeneous when I described Dirichlet boundary conditions so let me explain that. So the question would be something like what are. Homogeneous. Dirichlet boundary conditions? Here the term, the views of the term homogeneous simply means 0. Okay, so, when I say that a problem has homogeneous Dirichlet boundary conditions, what we mean is. Homogeneous, sorry, Dirichlet. Boundary conditions implies that u h at 0 for instance. This is the setting in which I used it. Sorry, that's a little too close. U h at 0 = u 0. So that's the form in which we write our Dirichlet boundary condition in the Dirichlet-Neumann problem, for instance. This is equal to 0, right, this is what makes it homogeneous. That's what we mean by homogenous that's all. So once we have that, the point that I was making in the previous segment is that once that we have that condition. We also have always that since we have a Dirichlet boundary condition at 0, we always know that w h(0) = 0 okay? But now with this fact that both u h and w h vanish on the Dirichlet boundary, right? This, plus the fact that u h and w h both belong to, H1 on omega essentially implies that the spaces are identical, Sh = Vh okay? That was the condition under which we derived our last two correlates, okay? All right. I should probably state here that the use of the term homogenous comes from at least in my usage of it, comes from basic theory of differential equations. Where differential equations where the, with the forcing functions that is 0 are referred to as homogeneous differential equations. And in fact, the solution to those differential equations are called homogeneous solutions, right? Okay, we'll stop here."
huxL37pFyRg,"Okay. So we're now shaping up to get to the main result related to finite element analysis, right? An, an analysis of the conversions of the method. To get there, to, to actually get to that result, there is one more class of results that we need. So the title of this segment is going to be Sobolev estimates. Sobolev of course, refers to the type of space that we are working with and that I'll tell you what we're trying to estimate here. In order to say what we are trying to estimate let us sort of reuse our symbol U, capital U of h that we introduced before. It continues to represent a general member of the space Sh, which means it satisfies the, the rationally boundary condition and it lives in H1. Okay? For, for our particular problem, it, it lives in H1. But in the context of Sobolev estimates, which are applicable to spaces with which, which are applicable in general to Hn spaces. Let me just redefine this for now. Okay. Let me say that Belongs to Sh, which now consists of all functions. That for our, for our purposes now, which actually hold in more general settings. Okay? Consist of Hn functions. And additionally, of course, they need to satisfy the Dirichlet boundary condition. Right? And so, if we were doing the, the Dirichlet-Neumann problem, we would have At 0 equals 0. Okay. So we're re-invoking this, this same function capital. Remember, U, capital Does not need to be the, the finite element solution. It could be, right? But it does not need to be, right? So let me state that. Does not necessarily represent little Right? Which is the finite element solution. It is any other solution in Sh, right? Where Sh now has been defined to include all functions that belong, that live in Hn. Okay? All right. I want now to consider a very special member a member of this set of functions. Okay? Consider Such that. At any nodal point let me say, this as X let me just say, x sub A. Okay? So At x sub A equals little at x sub A. Right? Okay. But if A here just denotes our global numbering of degrees of freedom, then this remember, using our finite element vector notation is just the degree of freedom, d sub A. Okay? All right. All right. So what this, what this says is that A is a global degree of freedom, right? And this, this then implies that xA is a globally numbered node and dA is a globally numbered Trial solution degree of freedom. Right. Or finite element solution degree of freedom. Okay? All of that should be clear. So what we're saying is that we want to consider a special member of this class, where Is equal to the finite element solution at the node, the degrees of freedom. Right? At the nodes, okay? But you know that if we do this, we would have capital Actually equals to little everywhere, right? Because of the fact of the finite element representation. Okay? Right? So this is not, not exactly the solution we want. Not exactly the the function we want. Consider another function, U tilde h which is such, that U tilde h. Okay? At xA, now equals the exact solution evaluated at that point. Okay? The idea is that if you had the exact solution and you could evaluated it at a nodal point, right? Take that value and let's suppose that u tilde h is such that it hits the exact solution right there on the nodes. So what we're seeing here is that u tilde h, right? Expressed now as a function of parametrize by little x, right? By position. U tilde h is what is said to be nodally exact, okay? I'm going to sketch it out for us, suppose this is our domain. And for simplicity, let's just suppose we have linear basis functions. Okay? And suppose our exact solution is the following. Let me go to a different color here. Our exact solution I'm going to plot in green, okay? Let's suppose this is our exact solution, okay? All right. This is our exact solution and what we're talking of doing here is following. We are talking of we're talking of looking at these nodal points. And identifying the value of the exact solution at the nodes. Okay? And then we're talking of so, so this, so this is u, the exact solution, okay? All right, and what we are, what we want to do now is identify the u tilde h. Okay? U tilde h, remember, is a member of S h, right, which means it inherits the representation that we get from our basis functions. If we are working with linears here, u tilde h can at, can at best be linear for each element. However ,we want it to be such that actually hits the exact solution at the nodes. Okay? For many of the other elements it's, it's not going to be very different from the exact solution because perhaps the exact solution is not too different a linear, sorry, it's not too different from a linear solution elsewhere. Okay. You see that there is a difference, right? See, this is our u tilde h. And this is what we mean by saying that u tilde h is nodally exact, right, it's exactly the nodes. It hits the exact solution in the nodes, but it is a member of S h, so over an element in this particular case it is as, at, at, it is, it is, it is linear. Okay? And let me go back to my red here, okay. So this, this is just plotting up a solution up here. All right? Okay, so this is what we mean by saying u tilde h is nodally exact. Such a function, u tilde h, is what is often called an interpolate of the exact solution. Because what we are doing is taking the exact solution at the nodes, and then using the basis functions to essentially interpolate between those exact nodal values to get u tilde h, okay? U tilde h is nodally exact and is also called the interpolate. Okay? All right. Okay, and just remember that u tilde h belongs to S h, right, and that's re, reflected in the fact that it is linear over elements. In this case we are considering linears, okay? So we're considering, in this particular example only linear basis functions. All right, okay. So this is what we have. Now, here is a result. The theory of Sobolev spaces, right, or, or mathematics of Sobolev spaces, functional analysis on Sobolev spaces, gives us the following so called interpolation Error estimate. In Sobolev spaces. Okay, the interpolation error estimate is for the following quantity. It holds for the m norm, the h m norm of the difference between this interpolate and the exact solution. Okay? It's important to note why this is called the interpolation error, right? So the term on the left is my interpolation error. Interpolation error is this, okay? U tilde h minus u. The reason it's called interpolation error should be pretty obvious, right? And what is that? Think about it. What it is saying is that, suppose you had the exact solution, right, and at the nodes you were to hit the exact solution, which is what U tilde h does, right? But then because of your choice of a finite dimensional basis over the element, you are now interpolating from that exact solution. But since you are interpolating with a finite dimensional basis, you fail to hit the exact solution over an element, right, most prominently here. Therefore, this difference that we are looking at is indeed the interpolation error, okay? All right. So the Sobolev, estimate here, and, and I'm, I'm just going to state it, we're not going to prove it, is the following. It is that this interpolation error estimate is bounded from above by c as a constant. h e is our element size. Okay, and in these error estimates the we assume, or the error estimates have been derived for the case in which we have a uniform element size, okay? So the c h e to the power of a number alpha, which I will define in a little bit, okay, c h e to the power alpha times the h r norm of u, okay? So what do we have here? U tilde h minus u is the interpolation error. It's the error incurred by the fact that we're using finite dimensional basis functions even if we had the exact solution at the nodes, okay. H e, as before, is the element size. c is a constant. r is what is called the regularity of the exact solution. Okay? So note that when we're talking about u r the r norm of u, we're seeing that yes, if we were to square integrate the exact solution and also square integrate its derivatives up to r, right, up to its rth derivative, we would get a quantity which is bounded, right, and that is the r norm. Okay? All right, so let's just recall that this quantity, the r notm of our u Is a measure of smoothness, of regularity, really. r comes from regularity. Right, which is really a measure of smoothness Of u, all right? If you have the r norm of u, it means that you can take up to r derivatives of u and still have that quantity bounded, which gives you some sense of how small the u is, okay? All right, the next thing I need to tell you is about what alpha is. Right, alpha is the exponent in to which h e is raised, right, the power of h e. Alpha satisfies it, it's an exponent, obviously. And it satisfies. The following condition. Alpha is equal to the minimum of right, k plus 1 minus m, and r minus m. Well, what is k now? k, k is the order of the polynomial order of our finite dimensional basis. k is the polynomial order of the finite element, sorry, finite dimensional basis. Okay? All right. Let me do just well, let, let, let me rewrite the result here, okay. So that is h minus u interpolation error. We are computing the m norm of it, right? And this is lesser than or equal to c h e to the power alpha. Okay, so what this is telling us is that supposing in most cases, let us suppose that we have a solution just to fix ideas. Let's suppose that our exact solution is such that we can actually take derivatives of it up to a very high order and you know, we're, we're able to take derivatives of it to a very high order, which means it's very smooth, okay? What it means is that in this definition of alpha, r is going to be very big, okay? So even though we want to take, so, so, we, we are going to take m derivatives of the solution and we are interested in knowing what the m norm of it is. If r is very big, what it says is that the order of our of our exponent here, the size of our exponent is controlled by the polynomial order of the basis functions that we have. Okay? Now why does this matter? The question is, what happens with our interpolation error as we refine the mesh? Okay, if r is big enough, then alpha essentially reduces to k plus 1 minus m, right? So if r is large, okay, which means the exact solution is very smooth. Right, we have a nice, well-behaved problem. Okay? In this setting what we will see is that alpha being the minimum of those two quantities, is indeed going to come down to k plus 1 minus m, okay? And therefore, what this thing is saying, then, is that our result is that the, the interpolation error. The m norm of the interpolation error is lesser than or equal to c h e to the power k plus 1 minus m times this r norm of u. But then if u is very smooth, we expect that the r norm is not very big, right? We won't get a very large number when we integrate the functions, to integrate the function and its r derivatives up to r, okay? But that tells us then that now as as h e tends to 0, right, if our quantity k plus 1 minus m is greater than 0, what happens with the interpolation error? What happens with the norm of the interpolation error? It vanishes, right? Okay? It vanishes at the rate k plus 1 minus m, right? So it tells us that any nor, that, that this h, sorry, this m norm that we want to compute of the interpolation error also tends to 0 at the rate k plus 1 minus m. Okay? So as we refine the mesh, as we make h e smaller, right, remember h e going to 0 means we are looking at mesh refinement. We are going to smaller and smaller elements. Okay? So what the Sobolev estimate tells us is that yeah, as you refine the mesh eh, if you were to look at this interpolation error it will vanish, provided, provided what? Provided that number is greater than 0. How can we make that number greater than 0? We can make the number greater than 0 by taking a higher polynomial order, right, or making sure that we are not taking too many derivatives in computing the norm. So we don't, if we are not looking for very high m norms, right, we're not looking for taking many derivatives when we compute the m norm, right, then this holds. Right, and this is an important thing to know. This, this property, note we should note that this property comes to us directly from the Sobolev space. Right, we're not saying anything yet about the finite element solution. This result is a property of our Sobolev space S h. Okay? How is it a property of our Sobolev space S h? Well, that's what determines the polynomial order k. Right? That is determined by, by the space we have picked for S h, right? So it says as long as you pick that to be high enough, and as long as you're not looking for too many derivatives in this interpolation error it does converge. Okay? No talk yet about the finite element solution. Okay? That will come next. And that will come in the next segment."
wBZ1sUdG7fM,"Okay, we are here now and at the cusp of what I hope will be our final result of this analysis of the finite element method. So this is, best titled, Convergence of the Finite Element Solution. Okay. Before I state the result and prove it there's one result that I need to recall from the, from a few segments ago and generalize it. Okay? So recall that when we actually started talking about the analysis of the method. We used this or I stated the fact that the H 1 norm is equivalent to the energy norm, okay? So recall equivalence of H1 and energy normals, Right? And the way we stated that result was the following. We said that If we look at c1 times the one norm of v, right? Some function v. This is lesser than or equal to the energy norm of v. Well, this is actually the energy norm squared, so we do that. Okay? And it's also less, it can be bounded from above by the energy norm. And the energy norm itself can be bounded from above by some other constant multiplying the v 1 norm. All right, and I believe there was also a question about what, what exactly we meant by this equivalence and how it held why it might hold. And I said that well, it really, it really rests crucially upon the fact that when we speak of these norms, we are talking of quantities that are, real and therefore bounded, and not infinite. And then using the fact that our domains are, finite, we can actually prove this result. Okay? So so we have this. Now it, it, it emerges that this equivalence between the h norm and the energy norm extends in fact to the general Hn norm also, okay? So this extends to the H n norm. Right? So this says that if we just look at c1 times the n norm of v, it is bounded from above by the energy norm. And that itself can be bounded from above by some other constant multiplying the n norm of v, okay? And the re, and the reasons why this holds are pretty much the same with the H1 normal. Why, where the result holds for the H1 norm. Okay? So this is what we're going to use. All right, so here is our statement. Again, I'll state it as, as, a theorem. The theorem is 1 on the, um,n norm of the finite element error. Okay? And it states that it can be bounded from above by some constant, whatever constant, some constant. Right? So let's denote this constant as c bar. h e, again, raised to the power of alpha, where alpha is the same exponent that we came up on in our sublib interpolation error estimate, okay? This times, the r norm of u. Okay? So, this looks very similar to what we saw for the sublib error estimate except that now we are indeed talking about the finite element error estimate. Okay? We are indeed talk of what happens with the finite element error, right? And we just remember that this is u h minus u. Okay? All right, proof. And let me just start, all right, I can start the proof here. And to start the proof, let us actually start with this s, result I just stated at the beginning of the segment. The result just above theorem. Right? And let us start by writing c1 times energy norm, sorry, n norm of, finite element error. Using the result we just stated above, it is clear that that can be written as being bounded by the energy norm of the error. Okay? All right now, let's move on. Okay. I'm actually going to reproduce this, this last statement on the now slide here. c 1 n norm of the finite element error is lesser than or equal to energy norm of the error. Okay, now the energy norm of the error itself you recall from a couple of segments ago, is bounded by a quantity. Which is also the energy norm of u h minus u. Right? We call this the best approximation property. Right? This is the best approximation property. Okay? Now I want to state something about notation here when one is working with inequalities. When one is working with inequalities and I write inequalities running in this fashion. The way to read it is that, what we have here, the energy norm of e is greater than or equal to C1 times the N norm of E. When we come down the second line what we're seeing is that what we wrote on the first line is itself less than what we wrote upon, on the second line. So it's very similar to actually taking what we have on the second line here and, moving it back here, okay? That's how we understand these, these, that's, that's the notation that's used when no, one is writing on multiple lines, okay? So what, really what this is stating is in fact, the best approximation property, right? Because there we did indeed prove that the energy norm of the error is lesser than or equal to the energy norm of this particular quantity, okay? But now note that when we choose those functions Okay, included in those functions was this interpolate U tilde because After all was just functions living in Sh, right. And satisfying that there is a boundary condition. Since U tilde h also lives in s h and is in fact equal to the exact solution and the nodes it also does satisfy the conditions, right. So it is in, in fact one of these members, right. So, the, the reason that we get this result is the best approximation property and the fact that U tilda h also belongs to Sh, okay? All right, but now, one can again invoke the result as stated at the, before we started this theory, right, on the equivalence of thee energy norm and the end norm. So we can see that that energy norm that we have on the right itself is lesser than or equal to, let me see this. Right, I could directly write this as some other constant C2, right? Times the N norm of U tilde h minus U. Okay? Just from the fact of our statement on equivalence of norms, okay? All right, so, we have this, and, what we can do now is invoke, so, this, this, this result follows from equivalence of Hn and energy norms. Okay, now continuing to work. We recall our sublet interpolation error estimate. Okay, so That interpolation error estimate is one that is applied to U to the H minus U, right? because that's interpolation error, okay? And that was the U to the H minus U is, lesser than or equal to, some constant CHE to the power alpha times the R norm of our exact solution. Okay so this is the interrelation error Estimate. Okay? All right, but now look at where we've got. We started out with C1 times the N norm of our finite element error and through this process, right, of repeatedly invoke, of invoking various results that we've accumulated. We have arrived at the fact that it is bounded from above by the term on the right hand side. Let me now just collect these results to say that, in the N norm of the finite element error is lesser than or equal to C2, which is a constant, times C, which is a constant, and divided by C1 which is yet another constant, times He to the power alpha Okay, now, this is our new constant C bar. Okay, so, our final result is that the, the finite element error in the N norm is lesser than or equal to c bar, it's a constant He to the power alpha times the r norm of u. Okay, that's our result. Now, let's let's just spend a couple of minutes examining this result, okay? What it tells us is that again, you know if we look at cases where the solution the exact solution is very smooth for U sufficiently smooth.  What you're seeing is that alpha that is defined as the minimum of K plus one minus N and O. Okay, in this case, we're working with the n norm, right? So we have k plus one minus n. I'm sorry. Okay? And, r minus n. Okay? Alpha is a minimum of those two quantities, right? So for u being sufficiently smooth what we see is that alpha is indeed equal to K plus one minus n. right? Where k is the, polynomial order of our baseless functions. Right? And n Is really the number of derivatives we are taking in calculating the norm of the error. Okay? Alright, to get a little more insight, lets consider. Let's consider what happens with let's consider special cases, right. Consider n equals 1. So we're considering the, the 1 norm, right, the h 1 norm, and Well let's just consider n equals one. Okay? In this case, our result becomes that the h 1 norm of the error, what is, what do we mean by the h 1 norm of the error? What we're seeing is we're looking at the error in the finite element solution. Square integrating it and also square integrating the derivative of the error. So we're trying to gain control of not only over the, over the error but also it's derivative. The demanding that the error is somewhat smooth, because we want to say that even its derivatives should not get very big. Okay? So we're not saying that the error shouldn't be too much, but it also should be smooth. All right? Okay. So this thing now is lesser than or equal to our constant c, r, he, okay. To the power k plus 1 minus 1 times the r norm of U. Okay. So, it is saying that if we are looking at the H1 norm of the error, basically we are trying to the error itself and its derivative. Right? It converges at the rate of k, right? Our polynomial order, right? So the linear baseless functions, right? For k equals 1, okay? We get the error in the, in the one norm is essentially proportional. Right? To, well, I may not say proportional. The error in the, the the one normal error is bounded by C bar he. So, indeed as we refine our mesh, he goes to zero, right? Because he tends to zero. The one norm of the error will also tend to zero. Okay? But note now, that if we were to take, take, if we were to go to higher polynomials. Right? The same one norm of the error would now converge at a higher rate. Okay? All right. Okay. Now so this is the main sort of result that we want to establish. And of course, as we go to higher and higher order polynomials, our, our error converges more rapidly. Now, if we want to compute the error in the L2 norm, right? Which is equivalent to the H0 norm. There is an additional technique that needs to be invoked. It's called the Aubin-Nitsche. Aubin-Nitsche method, which we wont do, because that takes a lot more work. Okay? But essentially in this setting, the result is the following. If we consider the L2 error, right? And we'll make it explicit that we're talking about the L2 error. Now we're just talking about square integrating the error, right? No derivatives. Then it, the form is essentially the same. Okay? In this case, we get h to the power of k plus 1. No derivatives, right? So we don't lose any powers here. We don't have k plus 1 minus n. Okay? Times the r norm of the exact solution. Note that of course, we are assuming that the exact solution's sufficiently smooth, so that exponent alpha always reduces to k plus 1. Right? So this is the condition for the L2 error. So what this means now is that if k equals 1, the L2 error of our finite element solution converges quadratically. All right? It converges at he to the power of two. Right? And so on, right? So, if you had k equals 2, we would converge at the rate he to the power 3. And so on. Okay? So, essentially this summarizes the the key result that is used in conventional finite element error analysis. It says that as we refine the mesh, our solution converges. Right? And depending upon the measure of error we want to use either just the L2 norm of the error, which would be just square integrating the error and making sure that that quantity is decreasing. Okay? Or whether we wanted to include higher order derivatives of the error. We can, we can, we can address all those problems. Okay. We can address all those questions. In all of this at least in the, in, in the way I try to, to, to bring out to, to give you greater insight into what is happening we're assuming that the exact solution itself is smooth enough. And note that this is a requirement. If you start out with a, with trying to solve a problem whose exact solution itself is very badly is, it's very irregular, you can't hope to converge to, to solutions easily. Right? It just becomes much more difficult. There are methods that, that, that, that can be used to address such problems and also to carry out the error analysis of these, of these methods. The other thing I should note is that the interest in, in compute and always looking at errors, which involve an integral over the domain is precisely to make sure that we are actually controlling the error all over the domain. Okay? And this is why we always take into the square integral of the, sorry, the, the, the error itself squared and integrated or the or we compute it a further derivative on it and square and integrate that. Right? The whole reason for integrating it is to make sure we have control over the error over, over the entire thing. Okay. This is a good place to end the segment."
oKn3AFD2wx0,"Welcome back. At this point we've completed our derivation of the finite element method for one d linear elliptic p d e's. And we've also spent a little time in understanding the basic mathematical properties of the finite element method. And especially with application to this problem. As well as how that pans out into studies of the consistency and conversions of the method and therefore to error analysis really, right. We looked at how the, the error of the finite element solution in the, the error in the finite element solution converges with element size tending to zero.  What I'd like to do in this segment and maybe the next one or two, is look at an alternate way in which the weak form can be obtained. And this approach is a a subset of a type of calculus that may be sometimes called variational calculus or more broadly. Various known methods. Okay? So the topic of this segment is going to be we're, we're working towards variation of methods so uh,so let me start out with talking about. Variational methods, okay? Before we can get there, however, we, of course, have to motivate it, right? And in order to motivate let us consider the following, all right. Consider the following integral. Okay. Consider, let me call this, i func, i depending on u. Okay? Written in this form. And I'm going to write it as, I'm going to define it here. It is the integral, over, the domain of the following quantity. One half E A, u comma x. The whole squared. D x. All right? Now, when you stare at this integral, if you, remind yourself, of, one dimensional linearized elasticity, you will recognize E to be the modulus. Right? And you will recognize u comma x. Right? That quantity to be the strain. Okay? And then, when you stare at this integral, you would recognize it to be the, the strain energy, right? The strain energy in. Linearized elasticity. The strain energy in linearized elasticity of course in one d. Right. So, so this is all well and good. Now what one can do is, is the following. So we, we, we, we make this recognition first and then. The, you know, once, once we recognize that there is this notion of a strain energy, we can ask ourselves, well, what does it do for elasticity problems, right? Of course it gives us a notion of, of the, of the energy stored in the elasticity problem. But it also allows us to construct a different kind of quantity, okay? It allows us. To construct. The following integral. Okay. Now, now I'm going to start putting down the kind of notation that we need for the development ahead of us. All right? It allows us to construct the following in a, integral. Or to construct the following integral which I'm going to denote as pi. It also depends upon you. Right? And in the context of elasticity, this is the, well, this is just a displacement field. Right? U is the displacement field. And pi of u. I'm now going to define by a few terms, by using a few terms, the first is going to be the same, term, that we wrote above, the integral that we wrote above, the, strain energy. Okay? Now, recalling the other data of the strong form of the problem, all right? I am going to add on a few more integrals, I'm going to add on integral over omega f, which you recall is our distributed forcing. Right? Times u, multiplied by A for the area, d x minus I am also going to consider the traction that we apply in the context of a Dirichlet Neumann problem. Okay? So t A would be the force, right? The, that, that makes up the traction. That, multiplied by u at the position l. Okay? So I want us to consider this. And consider this integral where we're seeing as we did for the strong form of the problem that u belongs to the space s, right? Our usual space of functions for the for the exact solution, right? The, the space's function, functions from, from which we draw the exact solution, right? So u belongs to s, which now consists of, all functions, u, such that, we're thinking, we're thinking of this in the context of our Dirichlet problem. Right? So we're thinking of this in the context of the Dirichlet condition. U at zero equals u nought. Okay. All right. And f t and the constitutive relation that we're familiar with. Sigma equals E u comma x. Right, all of these are given. All right. Consider the following, consider the setting. Now what I want you to do is con is focus your attention upon this integral that I've defined that I've written as pi. And ask yourself what it is, right? What does it tell us physically at least for this problem if we're thinking of an elasticity problem. Right? Take a few seconds to look at it. Note that the first term was the strain energy. This term, I'll point out to you, the gives you the total work done by the force f on the displacement. And this recognizing that t A is also force. Is the work done by the traction specified for the Neumann boundary condition under displacement at that end. All right? So, in the context of elasticity, if you've studied that problem, this is something that you probably recognize as a potential energy. All right? More broadly, pi is pi of u. Is something we actually can call the Gibbs free energy. Okay? When we restrict our attention to mechanics only. Right? The Gibbs free ener, energy for purely mechanical systems. Or, purely mechanical problems. All right. Also called, like I said, earlier, the potential energy in the context of mechanics. It turns out, as many of you may know, that the Gibbs free energy is actually applicable to general processes and physics. It's typical to have a contribution from chemistry. Maybe from, from, from temperature and so on. Also in the Gibbs free energy. However if we restrict ourselves to only mechanical problems as we're are trying, as we're doing in this case, then pi of u would be recognized to be the Gibbs free energy restricted to that. Okay. All right, so that, that, that's something to note. Now I want you to also observe that as a matter of notation. I've written u in rectangular brackets there, right? What I'm trying to point out here is that there is the, the, the reason I'm writing u in in rectangular brackets is that I want you to think of this quantity pi. As not just a function. Okay? So I'm going to state that here. Pi of u is not a function. Okay? The reason it's not a function is because a function, properly defined, mathematically defined, takes on a, point value of it's argument. And returns to us another point value. Okay. Or more typically, a function is a mapping from, typically, real numbers into real numbers. Okay. Let me just, just, just state this. So, supposing we do indeed have a function. G of x, okay? This may typically be a mapping from the space of real numbers, right? From which you may take x. On to real numbers, right. And graphically this can be represented as follows. So on this axis I have x, I have g. And in order to get a particular value of g, you would go on to the horizontal axis, choose a particular value of x, and say, all right, what is the value of g? Maybe it is that one. You take another value of x, you pick your value of g, that's that. All right, maybe this is this, and. The process goes on. Right? Now the function that we have in mind is what we get by connecting all of these dots. Right? And maybe it does something else out here. Okay? Nevertheless, the important idea is that when we take a particular value of x. Right? We get back a, well, actually, what I've drawn is yeah, it's, it's okay. We get back a particular value of g. All right? Okay? So, in this sense, a function takes a point value of its argument and returns a point value. Or it takes, in this case, a real number and, and returns a real number. Pi, however, is a little different. Okay? And I want you to think for a few seconds, or maybe a little longer, in what manner pi, as defined in the previous slide, is different. Is it, in what manner is pi different from this function g that I've specified here. Right? So, how is pi of u different? You'd know the answer immediately if you've gone through this sort of exercise and you may still know the answer. You may have figured it out. But the response is that in order to evaluate pi, we need more than just a point value of u. Because pi is an integral, we actually need the entire field u in order to evaluate pi. And then, in fact, when we supply the entire field u defined on our domain omega, we get back a real number for pi. Okay? So, let me state that here. Pi of u is a mapping of a field, right? The field u, which is a, u is properly a function of x, right? Because you, you pick a particular val, value on your domain x, and you get a particular value for the displacement, right? You get back a point value for the displacement, right? You get back a real number for the displacement, right? So that's the nature u, u is a function. However pi is a mapping of that entire field u, okay? To, the real numbers, All right? And, why to real numbers? Because we have recognized pi to be an energy of sorts. It's a free energy or potential energy. Which after all is a scalar. Right? It's a number. Right? It gives us a real number. Mathematically stated, we have the following. Pi of u is a mapping from something that tells us where we draw u from. Where do we draw u from. Right? We draw it from our function space s. Okay? So prize and mapping s, to the real numbers. Okay?"
AhazUtXxjes,"All right. So, what this means is that In order to use this notion of pi, we need to think of things a little differently. Okay? We need to think of We can think of a function, we can start out by thinking of a function, and indeed we do need to think of a function, because a function is involved in here. Right? We do have u, which is drawn from a certain function space, right? So, this is x, right? And our domain is 0 to L, right? This is our domain omega. Right? This is our domain omega. On this domain, we do indeed have a displacement field, right? It's a function. Right? And recognizing the sort of boundary condition we have at x equals 0. Let us set u equal to 0 there, and then, we may have Something like that. Right? Okay. So, that could be our displacement field, right? In one dimension. Now, when we have a displacement field of this type, okay? We get a particular value for pi. Okay? So from here, you may think of now constructing this mapping to a pi of u, right? Note, however, that in order to calculate pi of u with this value, with, with this particular displacement field. You need to know the displacement field over the entire domain, because we need to integrate, right? Now, if instead of this displacement field we were to have a different field, which I will use green, all right? So, you may have some other field, right? Also, satisfying the same boundary condition. But, perhaps, our field is this one, Right? And, from here we would get a different pi. Or field, maybe I should call this u2, and the original field u1. Okay? That is u2 maybe, and this original field is perhaps u1. Okay? So, it's critical to understand, that in order to calculate each value of pi, right? A single scalar value of pi, you need the entire field. All right? So, in this sense, this is fundamentally different, from a function which you can evaluate with just a point value of the argument, okay? All right. So, what we see is that pi is in this way, it is fundamentally different, from a function. It is what gets called a functional. Okay? And this because anything that takes as its argument an entire field, is a functional. And clearly, it, a, an, an integral is, is a functional. Right? There's another object, a very familiar object, that is also properly defined a functional. Can you think of what that might be? It is closely related to integral, actually. A derivative, right? Strictly speaking, a derivative is also functional, because we cannot evaluate a derivative, with just a point value. Right? Properly defined, a derivative needs a field description, all right, you may have a formula for a derivative, right? But in order to obtain that formula, right? In or, in order to even define a derivative, you actually need a field, okay? The pi so, all right, so pi is a functional, all right? And this is the function that we're going to work with. Okay, so, so this is the setting, and let's, let's proceed from here. So, so pi, pi is functional, in particular it is a, it can be called, and this is the term I will use for it. It is in fact, our free energy functional. Right? The Gibbs free energy functional. Okay? All right. Okay. So, what can we do with a free energy? Right? Or, or an energy, an ener, an energy in general, but particularly, a free energy? There is a certain commonly used technique, or principle, if you want to call it that that. That is often invoked, when we have a free energy. Can you think of what that is? Yes, it's the ideal of minimizing, or extremizing free energies. Right? So, extrema of free energies. 'Kay? Characterize, and why do we bother with extremizing free energies? It's because extrema of free energies are characterize states of equilibrium of systems. Okay? And therefore, there is this interesting calculating extrema of free energies, okay? And I'm going to draw the very common picture, which I am sure you've all seen, and either in high school, or early on in your college careers. It's the idea that well, if you have a If you have a free energy. Right? With respect to some with respect to some variable and, and just not to confuse things with anything else. I'm going to denote the variable as y, let's suppose our free energy here is f. Right? Where our energy is f we know these kinds of notions, right? That at extrema, right? There are our system is at equilibrium. Right? So, we may have that. We may have a different extremum there. And then, we may have that type of extremum, as well. Okay, so at each of them, we know that, a, f prime is equal to 0. All right, as well as here. All right? And we know that when f prime is equal to 0, we are at an extremum. Right? Each of these are extrema they are also  these points of equilibrium. All right? So if so, these are, so f prime equals 0, are  Okay? And then, of course, we can go on to the notion of second order extremum, conditions. And make statements about which of these extrema are which of these extrema have equilibria that are stable, or unstable, or metastable, or, or neutrally sta, stable and so on, right? We, we, we can go on, and have that discussion, right? So, so, let me just put that down, right? So, typically, if if a double prime let me see. So, if f double prime is greater than 0, we have a stable equilibrium. Right? If double prime is less than 0, we have unstable equilibrium. And if it's equal to 0, we have a metastable, or probably more widely called a neutrally stable equilibrium. Right, and they are characterized by these three.   This is all standard, fairly straight forward stuff. So well, this is all standard straightforward stuff but the question obviously rises can we do the same thing for our free energy functional pi? Right? So, can we find extrema Of, I'm sorry I have a slightly recalcitrant pen, today. Can we find extrema of pi? Right, and clearly once we, if we can find extrema of pi, we can recall these statements about the system being at equilibrium, stable, unstable, and so on. All right? Ask us can we find extrema of pi? The answer is yes, yeah, we can of course. But is it straightforward? Do you know, directly how to do it? Well, sort of, yes, if you think about it. We know if, that if, if, if a function is sufficiently smooth, we can define it's derivative. And it's derivative is what gives us notions of extrema, right? Where, where the derivative vanishes is what gives us an extremum, and that's what we saw in the previous slide. The same sort of thing would apply to pi as well, if, if pi could be sufficiently smooth as a function out of u we can, we can indeed find extrema. Right? Assume that it is smooth, right? The question then remains, what do we mean by a derivative, right? And, and the challenge here is the following. What is, a, an, oh, wait let me see, an appropriate notion. An appropriate Notion of, I'm going to write it out in words, right? Of a derivative Of pi with respect to w r t with respect to u. Okay? Question mark. What is the appropriate notion for this? Why is this not obvious? Why is this not ob, Why is this not something that we can directly inherit, from our conventional notion of derivatives of functions? Well, because we really need to say what it means to differentiate a functional, which is a set of real numbers, right? It, it maps into the space of real numbers. We need to talk of what it means to differentiate it, with respect to an entire field, okay? And this is not that straightforward, if you think about it, right? In fact, in fact, unless you are already know how to do it, it, it, it should not obvious to you, how to do it. Okay? So so, we are really talking here of differentiation. With respect to a field. Right? A field being u, in this case. Right? We need to appropriately definite this notion. Okay? If we can define this notion we can actually make progress towards talking about  states of equilibria and so on. Okay, we are going to do that but this is a good place to end this segment. We'll come back and define this notion of a functional derivative."
Ka7UcnAKdz4,"Okay, so in, in the last segment we observed the the existence of a free energy functional that has relevance to our problem. At least if, if viewed as an, an elasticity problem, all right. And then we motivated, used that as motivation to talk of begin talking of states of equilibrium of, of, of the extremer of this free energy functional. And made the observation that, well, we really need to see what it means to take a derivative of a functional, right? Of the free energy functional with respect to an entire field, right? Which is the displacement field. What we will do in the second is actually work through that work through that question, okay? The method we are going to use here is call a a variational method. Okay? And the reason it's called a variational method is actually very straightforward. It's simply because we're actually going to consider variations, and the kinds of variations we are going to consider are the following. I'm going to once again draw our field u. That's x u, 0, l, all right. And let's suppose that this is the field that we have. Okay? This is u. Now, in order to carry out the kind of mathematical operations that we require, we are going to vary this field. And in particular, the way we are going to vary it is that's right, I am going to say that the, after the variation, the field u, will look something like this. Okay? This is the varied field u and I will also now use yet another color to mark out what we sometimes call the variation, okay? So the variation that we're talking about here is this, right? We took the field u, varied it as I'm showing you with these blue arrows to get the varied or sometimes called a perturbed field, right? Which shows up in green, okay? So right. Now, I am going to label the, the green field, here. The green field is what I am going to label as u sub epsilon, okay? Where that subscript epsilon is, it comes for a parameter that I'm going to introduce that we used to sort of control the amount of variation we have here, okay? Okay, so here are the definitions, right?  So what we are doing here is consider. A variation. Okay? That I am going to label w, okay? Such that. The quote unquote varied. Or perturbed field Is the following. U sub epsilon which is now defined as the original field u plus epsilon times w, okay? Now, I will introduce two new quantities here, right? I'll introduce w as well as epsilon, all right? I need to see something about that. Epsilon is just a number, okay? So epsilon belongs to the real numbers, it's just a number. So, if I state this and as you can imagine now the, the w's of variation I'm calling u sub epsilon the varied field. It implies that w is also a field, right? Right, because u is a field. I've given u epsilon is, u sub epsilon is also a field and clear and, and epsilon is just a real number. Obviously, w has to be a field. That's right and if you stare at this figure I've drawn here, the graph, what you should be able to see is that let's switch colors, here. The blue arrows essentially represent the field epsilon w, right? So let me try and draw epsilon w here. It varies something like that. Gets a little smaller here. Bigger again. Okay? Something like that, right? So this is epsilon times w, okay? Now, what do you. Now remember, epsilon is a real number, right? What is that telling you about w? Okay. Note, definitely the way I've drawn it, at least. W of 0 equals 0, okay? With all these leading hints, you should know and, and given the fact that w itself is a field. It's a field which has, which satisfies w at 0 equals 0. It should tell you something about the space that w belongs to, right? Observe that as drawn w belongs to the space that we've been denoting as v, right? Where h consists of all w, such that w at 0 equals 0. Right? Indeed, that's the case. Now there is a reason, a very strong physical and mathematical reason, why w is always chosen to be to vanish at at 0. In fact, more importantly w always vanishes at the Dirichlet boundary, okay? So, w vanishes. At. Okay? And this would be principle and variational methods. The, in order to see why w vanishes at the Dirichlet boundary it is useful to recognize that really the field that we care about is u. W and in fact Epsilon as well are things that we have introduced, objects that we have introduced into the formulation. In order to serve our purpose and in order to take us where we want to go, okay? In particular as you can given the fact that I've called u, epsilon the, the perturbed field, the very perturbed field. You may imagine that now we're, and the fact is we're going to use it to it to construct our notion of a derivative. In computing derivatives of our free energy functional you will appreciate that it is if critical importance that we do not violate the Dirichlet boundary condition, right? Because if we do, we are actually changing the problem. And so it is that w vanishes at the Dirichlet boundary in order that the use of excellent does not violate the Dirichlet boundary condition, okay? This is, so that u sub epsilon obeys or satisfies. The Dirichlet boundary condition. If we were to take a variation of, if we were to construct a perturbed field that violated the Dirichlet bound condition, we would no longer even be talking about the same problem, okay? The nature of the problem would have been changed, all right? It is for this reason that w must vanish where ever we have a Dirichlet boundary condition, okay? Of course by now you are recognizing w as our waiting function, right? What we call our waiting function in our formulation or at least in the weak form leading up to our finite formulation. Indeed that's correct and it's no, it's no accident that I labeled this w as well, okay? All right, how does all of this help? Here's how it helps. What we're going to do is consider. Consider the perturbed functional. Okay? Consider the perturbed functional by, but now evaluated with u sub epsilon, okay? Right? It is essentially the, what we would compute for our free energy functional by using the green curve from the previous slide. Okay, the use of epsilon. All right, now is where our epsilon comes into the play, comes into the picture, all right? All right, consider we consider the functional pi sub epsilon and in fact consider the following, right? Supposing we take pi sub epsilon. Sorry, pi of u sub epsilon, all right? Now, the role of epsilon here, if you look at how we used it in constructing use of epsilon. The role of epsilon is to sort of control the amount of variation we're applying to u, okay? Think about it, and actually for that, let me go just back to this slide. Think about it, look at this slide. We've defined what w is and having defined w, it already gives us a, variation. In fact, given that epsilon is just a real number it should be clear to you that if I change epsilon, if we change epsilon, that blue curve on this graph will simply get scaled, okay? But not fundamentally change its character, all right? It will simply get scaled, so really the role of epsilon is to control the amount of variation. W controls the nature of the variation, epsilon controls the amount of the variation, okay? So what we want to do is having looked at how the free energy functional changes and gets perturbed with our variation. We've also, we've already decided about the shape of the variation by choosing w, right? Or by taking various w's. In order to control the amount of that variation, we are going to look at how the perturbed functional varies with respect to epsilon, right? As we look at its derivative with respect to epsilon, we get a sense of well, okay, I'm going to add a variation of a certain shape. As I now control the amount of variation I know how much my functional varies, right? So you want to consider this, right? So this gives us the amount. Of. I'm sorry. Variation. In pi. Having sorry. The amount of variation in pi for variation. In u, having chosen, shall we say, the form of the variation? And the form of the variation comes from w. Okay, so already if you'll think about it, we're now getting close to this notion of well, how much is pi changing with, with variations in our field, u? Okay, now however, what we want is a variation in pi right about u. Okay? All right? So if we further consider now, d d epsilon of pi u sub epsilon. We construct this derivative with respect to a scale of epsilon, which is easy to do, right? Because one thing you will note is that with respect to epsilon, pi will be a function. Okay, and we'll see this clearly. All right, maybe I should just state his here. Pi is a simple function of epsilon. Well, it's a functional. All right, so let's get back to it. We want to know how pi varies with u, but right around the actual value of u. Well, that can be delivered for us if after computing this derivative of pi with respect to epsilon, we simply set epsilon equals zero. Because after all, epsilon equals zero makes use of epsilon same as u, so here we have it, right? So, what this tells us is the derivative well, let's simply call this a derivative. This is the variation in pi with respect to u at u itself, right? For that, for that field, okay? And this indeed, if you think about it now, this indeed is our notion of the functional derivative. Okay, so now we've, we've constructed a clear notion of what it means to take a functional and look at how it varies with respect to a function like field u. Okay? All right, this is a good point to end this segment as well. When we come back, we will apply this machinery to the particular functional that we have created."
X_8Af756_H4,"Okay, so over the last two segments, we've done the hard work of, first of all, defining what functionals are. Observing that we can define a free energy functional in the context of, of elasticity problems. And then we've also gone ahead and constructed the notion of the derivative of a functional with respect to a field. And this we said we wanted to do in order to be able to talk about extremization of three energy functional and thereby talk about equilibrium. Okay? So now we'll in the segment, let's just apply it to our function all right. So, extremization of pi, right? Let's actually compute it. We know how to do it, right? So here's what we want to do. What we want to do is find u belonging to s, such that, now, remember that that variation that we constructed was based upon our field w. Right? But w was really meant to just give us a certain type of variation. Right? Therefore what we want to say is that when we try to find the extremization of pi we want to find extreme point. Right? We want to find this field u that extrimizes pi but for all possible variations. All right? In a certain class. So what we say is that we want to find u belong to s such that for all w belonging to b, right? And recall that s satisfies the remember, s consists of u, such that u at zero equals u not. And then v consists of w/w of zero equals zero. Okay? All right. Find u belonging to s such that for all w belonging to v the following holds, right? d d epsilon of, pi u sub epsilon. Evaluate it that epsilon equals zero, right? As to the extremization, this is the derivative that we've written out. What is the extreme condition? The derivative vanishes. Okay? Let's do this now. What this implies then is that d d epsilon of pi all right, and I'm going to write out pi. And let me give myself big enough brackets here. Okay, d d epsilon of integral over omega, okay? Here we have one-half EA. Now it's not just u, but u plus epsilon w, the whole thing, x squared d x minus integral over omega f times u plus epsilon w A, dx minus tA times u plus epsilon w, okay? All of this, we take those derivatives, set epsilon equal zero, right? And then say that all of this should be equal to zero. Okay? That's what we need to do. Now, observe that in computing this derivative with respect to epsilon, the integral really does not pose us any problem. And this is because epsilon is not a field, right? It's just a number. If you would actually go ahead and compute those integrals you would get some value depending upon epsilon being a real number. And you just differentiate with respect to epsilon all right? So we're actually free to carry to take a derivative with respect to epsilon into those integral signs all right? What this implies then is the following all right? Okay. We have here right? So now let's take this derivative inside, right? So we have here integral over omega one-half E a, now because of that square. We get 2 u plus epsilon w, x right? Times w, x, right? Dx minus integral over omega, F Times w A dx minus tA times w, okay? All we've done here is recognize that epsilon belonging to real numbers is just a it simply multiplies out each of these integrals. So we're free to take the derivative with respect to epsilon and into the integral sign and this is what we get, all right? So now after computing all of this, we have to set epsilon equal zero. All right, and then the extremization condition is that this result is equal to zero. And just to make it clearer, let me draw a partition here between the previous line and this one. Okay? Now of course, the sum simplification that one can get. All right now, using epsilon equals zero in what we had on the last slide which is right here. Okay? We observed that epsilon shows up in only one place here, right? So setting epsilon equals zero. Reduce this integral to the following form. Integral over omega. Now, we have EA u comma x times w comma x, dx minus integral over omega, fw A dx minus t A w. Okay, this is what we get by setting epsilon equal 0. The extremization condition is that this is all equal to 0, okay? The final step, I'm going to reorganize this, just one of these integrals, into the following form. W comma x, EA, u comma x, dx minus integral over omega w f A dx minus. Oh, I just realized that here we had, since we had u evaluated at L, what we get here is w also evaluated at L, okay? And so it is on the previous slide as well. Okay, so here we get w at L dot t A, right? All of this equals 0. All right, now you may have already noticed it, but then, what we have here is thanks to our constitutive relation, it's just sigma, okay? So finally we get integral over omega, w comma x, sigma, A dx minus integral over omega, w f dx. But there is an A here, minus w at L, t A equals 0, okay? And you recall that this is the condition we said must hold for extremization for all w, okay? And you recognize that what we have here is simply the weak form. Okay? So it is that when we have working for us, a so-called variational principle, right? And in this case, the variational principle is that the free energy functional is extremized, right? When we have that variational principle working for us, we can indeed derive the weak form from a using these so called variational arguments by starting out with the energy functional, the free energy functional. So remarks. When an extremization principle. Is available. Right, and in this case, what is the extremization principle? It is the ex, extremum, or extrema, of the free energy. Okay? So, when an extremization principle is available, the weak form. Can be obtained Using variational calculus. Right? And often this is combined into saying that a variational principle exists, right? One says, therefore, that a variational principle exists. Okay? Let me see. All right. So this is a useful thing to know. Now, what we see in this case is that, of course, we get back the weak form when we invoke elasticity. However, we also know that the same weak form works for other problems as well, right? It works for heat conduction. It works for mass diffusion, right? So one can ask the question, what about those problems? The fact of the matter is that there does not exist a physical variational principle for problems of heat conduction and mass diffusion, right, at least not one that talks directly of extremization of certain free energies. All right? Those, some of these arguments are applicable to mass balance or, or to mass transport, but in a more restricted sense, okay? A separate series of lectures addresses that question. But anyway, so, what about those problems? Well, one can use the same weak form, but one cannot demonstrate that it, that it is avail, that can, that it can be obtained from the sort of variational principle. Therefore, we tend not to pose those weak forms as being derived from the variational principle. The issue with those sorts of problems is that the quasi-static or the steady state problem is only a very restricted case of heat conduction or mass diffusion. The full time-dependent problem does not subscribe to an equilibrium state and therefore, these kinds of principles are not applicable. All right? So pertaining to that, I will state that this. Derivation is not appropriate. For the physics of our heat conduction. Or mass diffusion. However the mathematics works, right? You may, you may simply ignore the fact that a physical principle does not exist and just say, well I don't really care. I'm going to define something that I call an energy or an energy like quantity for heat conduction or mass diffusion. And go ahead with this process, right? So the mathematics works, but the physical principle does not exist, and therefore we tend not to present it as such, okay? So let me just state that the mathematics, however, works. The fact that this principal of this variation principal exists for problems with elasticity is a very powerful one. And it is used to construct much more general and, and indeed more powerful finite element methods, variationally based methods, for problems involving elasticity, okay? And that is the topic of a more advanced class in finite element methods. I should probably also state that problems for which variational principles exist are. The following, or at least some problems for which variational principles exist. For elasticity, right? In all its forms. Okay. Property actually that exists for elasticity at steady state. Okay, variational principles exist for elasticity at steady state. But once you talk mode elasticity where there is nonlinear elasticity, or linearized elasticity, or elasticity. With other constitutive conditions added on to it like velocity or damage or physical elasticity or other things. It works. Okay? Another problem in which it also works is the Schrodinger equation at steady state. Okay? Schrdinger equation, right? Also at steady state. Okay? And therefore, for these kinds of problems and, and some others, as well. One can construct variatonal principles sorry, one can construct basic as well as more advanced finite element methods using this approach. Okay? It does not exist, for instance, for problems of heat conduction mass diffusion, like I said. Also, the problem of fluid proof, right? For the  equations there does not exist a variation of principle. All right. That's about what I wanted to say in this segment, and indeed for this unit. At this point we're done with everything we wanted to work on for 1D Linear Elliptic Problems. In the following unit, we will embark on multidimensional particles"
URy5pgetGHI,"With this segment we begin our treatment of problems in multiple dimensions, and we're going to go straight away to three dimensions here. The problem we'll start out with is in the context of mathematical description of PD's. Also a linear, elliptic pde in three dimensions with the further specification that the unknown we are solving for is scalar. Okay, so that's what I'm going to write out, and we'll start with it. So we are now looking at linear, elliptic, pdes in three dimensions, Okay? With scalar unknowns. Or scalar unknown. Right? Because it's a single unknown we're solving for. So linear, elliptic, pdes, three dimensions, scalar unknown. Right? The canonical problems that are, canonical physical problems that are des, that are described by this sort of pde include heat conduction. Right? Steady state heat conduction. And also at steady state. The mass diffusion problem. All right? Okay we'll plunge right into it. We'll start out with the strong form. Okay? So, the strong form of the problem we are looking at is the following. So, let me deal with this. Before I start writing out the strong form, let me just right, sketch out the domain that were trying to solve things on. Just as for the one deal in your elliptic problem, we sketched out this this idea of a bar sort of embedded in a wall, right? Fixed in a wall, So let me do that. Now, because we are doing things in 3D we are going to make use of vector notation. All right? So we have here. What I would refer to as our basis vectors. Right? And these will be denoted by e1, e2, e3. Okay? Note that the notation I use for vectors is to put an under bar. Right? On them. That's, that's just the convention I follow. When we get to needing tensors, I will do the same thing, and we will simply distinguish between vectors and tensors by context. Okay. So the domain we're interested in is a some arbitrary domain, and I will draw what, in the context of continuum physics, is often referred to as a continuum potato. All right? So we have that here. The domain of interest here is labeled omega, as before, except that omega now lives in 3D. Okay? So let me write out things here. Ei where i equals 1, 2, and 3, right, the set ei why, where i equals 1, 2, 3 constitutes. An orthonormal. Cartesian basis. Okay? Do you recall what orthonormality means? The ortho refers to perpendicular, and normal refers to unit magnitude, okay? So what that means is that if we look at ei, dotted with eg, this is equal to delta ij, where delta ij is the chronicle delta. Okay? And you remember what the particular properties of the Kronecker delta are, right? Where delta ij equals 1 for i equals j, is equal to 0, for i not equal to j. Okay? And we note that this directly covers the orthonormality property, right? Because if i is equal to j, it says that each of these e's, e 1, e 2, e 3, is of unit magnitude, right? And if i not equal to j, tell us that they are perpendicular to each other. Okay, so that's what is implied. And here I will make it more obvious by doing this. Okay, so these are perpendicular to each other. All right Cartesian, for our purposes, simply means that they are fixed. Right? The basis vectors do not change. Right? They are fixed in space. All right let me also, just for, the purpose of making this completely obvious, state that we are doing all of this in three dimensional ambient space. Okay? So this is the setting for the problem we want to consider. I have some props to help us through the rest of these lectures. That represents our basis. Right? You can think of this as E1, E2, E3. Each of these are, is of unit of magnitude, and they're, of course, perpendicular to each other. The domain of interest to us is this one. Okay? This is our representation of the continuum potato. It happens to a, to be a University of Michigan Nerf or not Nerf, but a, but a University of Michigan foam football. All right? But this is the domain over which we will be describing everything happening. And this is omega, for our purposes, all right? The, the other thing that we will need about omega is we will, we will repeatedly use the, refer to its boundary. Right? So, omega is as before with our domains, omega is an open set, right? So when I talk about omega, I don't include its boundary. Right? We will use certain notation for the boundary of omega. So let's put down that bit of notation. The boundary is going to be denoted, partial of omega. That does not imply that we're taking anything like a derivative, it's for our purposes, it's just notation. All right? Okay. So that's pretty much what we need to begin with. Okay, so yeah, maybe I should just say one more thing here. Omega is open in R3. And if this is the first time you're encountering it, I will also say that partial of omega is the boundary of omega. Okay, so this is the setting we have, all right? In this setting, what we are trying to do, is the following. Let's state the strong form. We are interested in finding some function, u. Okay? Find u, given some other quantities. Okay? So we are going to be given f again. F, as before, is a function of position, except that now position is a position vector. Okay? It's going to be denoted as X. All right? And 'kay, at the risk of going back and forth, let me just add one more thing to that figure I had on the previous slide. I am going to say that some point, x here, has a position vector. Right? So, that's the position vector of point x. In the context of our props, okay? So we're talking of the position vector from the origin of this Cartesian basis, to some arbitrary point in this domain. Right? In our continuum potato and omega. Okay? So, so, okay, so we remember that this is where the basis is, we're talking of the vector from here to there. Okay? That, that is what we, we, we have in mind. I could use this as a prop, right? So if, if, if, if the tail of the spin, the top of the spin is where the Cartesian basis was, and we want to talk about this point, we have that. Okay? So that's the cartesian vector, sorry, that's a position vector for x. Okay. Right. So we need that because we want to talk about the dependence of this forcing function. Right? So the forcing function is a function of x, it's, it's parameterized by x. It can be defined at, at at any point over the terrain. So we're given f f of x. As before, we are given u sub g, all right? And we are given, j sub, n. Okay? We're also given the constitutive relation. Okay? We're given the constitutive relation that I will initially write using what is called issue, or coordinate notation. Okay? We are given a constitutive relation j sub i equals minus kappa ij u,j. Okay. Alright. Where again, since this is sort of the first time I'm writing it out, I'm going to tell you that i, j equals 1, 2, 3. All right? We're given all of this. All right? Right. So we want to find u given all this, this information, such that, such that, again, sticking with coordinate notation minus ji,i equals f in omega. All right? And we also have boundary conditions. Right? B.C. for boundary conditions. U equals u sub of g on partial of omega sub u, and minus j dot n equals j sub n on partial of omega sub g. Well that's our strong form. Now obviously, I have a lot of explanation to do here, right? I've introduced all kinds of terms, and I haven't really, and I need to make, make them clearer. Let's begin let's begin with something that we already know. Let me say, let me tell you more about what, what I mean by decorating the boundary with these subscripts u and j. All right? So we have our basis again, this is something that I'm going to end up drawing repeatedly, at least over the next couple of segments. Okay? We have e1, 2 and 3, we have our domain omega, right? And now the boundary is partial of omega, and rather than mark it as partial of omega, I'm going to tell you what partial of omega u and partial of omega g could be. Let us look at that actually let me make that an open interval, sorry. Let me look at that interval. Right? So that would mean all the points that lie between those two braces, if you would, just walk around the, walk around the boundary. Rght? That is what I may choose to call partial of omega u, and the compliment, right, of that set, is what I'm calling partial of omega j. All right? What this simply means is that we've taken our boundary and partitioned it into disjoint subsets such that they're disjoint, just like I said. So partial of omega u intersection, partial of omega g, is the empty set. Okay? Phi here denotes the empty set. This symbol denotes intersection, okay? So the intersection of these two, subsets is empty. And, partial of omega is always written as partial of omega u, union partial omega j. All right? So really, if you think about the way we've marked out these boundary subsets, I've chosen to make partial of omega u open. But that means the partial of omega j is closed, right? In order to make sure that we don't lose the boundary points between partial omega u and partial omega j, right? Right? And, and of course their union gives us the total boundary, partial omega. Okay? Just a way of, you know, splitting the, the boundary. And then what we're saying is, when we go back to these boundary conditions ,what we're saying is that we have we've specified u equals u g on one part of the boundary, okay? And actually let me take off this brace bracket. If you're specifying u equals ug on some part of the boundary, what kind of a boundary condition is that? Recall, from our treatment of the problem in one dimension? That's right, that is the Dirichlet boundary condition. Right? And the other term, the other boundary condition, is our Neumann boundary condition. All right? Okay. So this is what we mean by the boundaries. Right? By the boundary subsets and the boundary conditions. Let's go back now and talk about the other quantity we introduced here without much without much fanfare, j. Right?"
IHCOKP_j7ZE,"J i is what we will call a flux vector. Right, in. Coordinate notation. 'Kay, and for coordinate notation i equals 1, 2, 3. For our purposes, this is just a way to specify explicitly, the components of a vector in three dimensions. Okay if, if we were doing a class in continuum physics. If we, we were doing lectures in continuum physics, this would be set up much more carefully than that, right? But for our purposes, it just denotes the components of a vector. Right? And when we explicitly refer to the components of vector that sort of notation is called coordinate notation. All right, so then what they're saying is that j if we wanted to write the vector and so called direct notation, okay? J is just the collection J1, J2, J3, okay? And this is what we call Direct notation. If properly the left-hand side of this last equation J equals J1, J2, J3 is direct notation for the same vector. Okay, so we will often switch back and forth between the, between these two type, between these two notations, though for what we need to develop we are going to use coordinate notation a little more than direct notation. Okay? But they are essentially just different ways of describing the same thing since J is a vector there, there is also a way of describing the J as a vector. To simply saying that J, being a three dimensional vector, j belongs to R3. Okay. Just this notation, all that means is that j is a 3D vector, okay? You know that exactly the same thing can be said for x as well, right? So, likewise, we have x equals x1, x2, x3, arranged together as a vector. Right, and x belongs to R3, and this thing has more meaning because not only is is x a vector, a three dimensional vector, but x also is a point in three dimensional space right, so this has even extra, even more meaning here really, more physical meaning. Okay. That's what we have. Now let me see. What else do we need to state here? Okay. In order to say more about this it's actually useful to go to a particular physical problem that this PDE could represent, okay? So in order to say more about this let's consider heat conduction. Heat conduction at steady state. Right? In 3D right. Let's suppose this is the problem we are talking about. One thing you recall of course that steady state simply means there is no time dependant right? The time dependent has been dropped from this description of heat conduction. Okay, so this is the keys what is u? What does u represent in the, in the problem of heat conduction? Can you recall from you recall from your study of heat conduction previously? Right, it's the temperature. Okay, J then is right the heat flux vector. Right, which is essentially the amount of heat. Crossing perpendicular to a unit area. Per unit time. Okay. That is J, right? Now, when we have the constitutive relation. Ji equals minus kappa ij u comma j, okay? This represents for us now, it essentially tells us that the the heat flux vector is driven by the temperature gradient. Right, so u comma j u comma j you recall cause this just partial of u with respect to xj. Right, so that's the temperature gradient. This constitutive law goes by the name of the Fourier law of heat conduction. Okay? Now, here well, let me just state this here, this is the temperature gradient. Temp as short for temperature, Okay. Do you remember what kappa ij represents here? What is it called? Right, in general, it's called the conductivity tensor right, or the heat conductivity conductivity tensor. Okay? Right. This a denser can be thought about again in our setting for the purposes we need here it can be taught off as say generalization of a vector and with the provisional basis that we have, what we find is kappa ij can be well that's not really right. With use of a base is that we have here, we can also write the heat conductivity tensor using direct notation, it would be kappa, right? Also with an under-bar, and like I said by context, we will understand whether something a vector or a tensor. Here it's a tensor, why do we know it's a tensor, because of the way it acts in this equation. Okay. So capital is direct notation for the heat conductivity tensor, and just as we could represent, the direct notation for a vector in terms of its components or related to coordinate notation, kappa can be written with the use of a basis, which we have, as a matrix. All right, and that matrix consists of the components kappa 11, kappa 12, kappa 13, kappa 21, kappa 22, kappa 23, kappa 31, kappa 32, kappa 33, okay? Now, it turns out that this tensor kappa is symmetric and their reasons for it to be symmetric so we would consider Kappa to be symmetric, which me, which we write as kappa equals kappa transpose, right? So this means that in general that the 1 2 component is the 2 1, 1 3 is equal to 3 1, and 2 3 equals 3 2. Right? Which is also written in coordinate notation as kappa ij equals kappa ji. All right, kappa has another property which is important for the physics of heat conduction, 'kay. So not only is it symmetric, right? It is also what we call, positive semi definite. Okay, and what this means is that if C belonging to R3 is vector. Right? Then. If we construct the following quadratic product, right, we allow kappa to act on C. Right? And the product of kappa acting on c gives us back the vector, right? Just like if you know, you've all, you've probably experienced linear algebra, a matrix acting on a vector gives you back a vector. All right, so that's what's happening. So capital T is a vector, if we dot that vector with c 'kay, c dot capital C, is greater than or equal to 0 right, for all c, right. For any c. Right? Well, actually, I, I, I realize I don't really need to say for all c. All right. So if c is a vector, c.kappa c is greater than or equal to 0. Okay? What this means? That there are actually some directions right, for which we are allowing the possibility that there is no conduction of heat in certain directions, right. So we are allowing the possibility that for some c, right, some vector, vector c this thing could be 0, all right, okay. So and this, this the fact that we're allowing the possibility for this quadratic product to equal 0 is what makes it semi definite, all right. In terms of Neumann condition, all right? Do you recall what we are allowing the possibility for by including that product being equal to 0? What physical possibility are we allowing? It allows the possibility that for some direction c this material actually acts as an insulator, all right, that there is no conduction in some direction. All right, so this basically corresponds to. If c.kappa c equals 0. Importantly, this has to be equal to 0 for c itself not equal to the 0 vector. Right of course if c's a 0 vector then that's a trivial result. Right? But if there exist some direction c not the 0 vector for which this quadratic product is itself equal to 0 then, there is no conduction, no heat conduction. Along c, right, in that direction. Okay. The very last, well, actually there's mo, not the very last thing, but there's one more thing I wanted to say, here. When we go back and look at this law, here. This, of course, the Fourier Law of heat conduction. What it tells us is that the heat flux vector is directed along the negative temperature reading, right. So tre, heat tends to flow from high temperature to low temperature, provided we also have this property of c being positive semi definite, okay. So let me state that as well. Heat flows down a temperature gradient. All right, that's what the formula of heat conduction tells us with the additional condition that c is positive semi dense. All right, the last thing we need to state here to wrap up our our introduction to this strong form of the problem, is the boundary conditions, right. So when we, when we return to the boundary conditions, the first one is fairly straightforward. When we have u equals ug on the Dirichlet boundary this is simply a temporary boundary condition. Right, so in the context of our continuum potato. Right, we have our basis here, the continuum potato here, the, the region of interest. Let's suppose that the maze part of the boundary is the Dirichlet boundary or the temperature boundary. What you're seeing is that on this part of the boundary we have the, we're, we are controlling the temperature, right. We're setting the temperature to be ug, ug of course could be a field, right. That's important here. So what we're seeing here is that this could be a nonuniform field, right, so that's possible. Okay, that's allowed. And then for the second part of the boundary this condition can be written as we're writing it as let me see how I wrote it actually, probably sorry, and let's go back here. Okay, I did it right. Okay, this part of the boundary, when we wrote it in direct notation, I think we wrote it as minus j.n equals j sub n. Okay. What that dot? What the n there is doing? Is that it is the unit outward normal to the boundary. Okay. It's a field. It's a vector field. Right, that's what n is. So, when we look at minus j.n, what we're seeing is that given the fact that n is a unit outward normal, what we're seeing is that we are controlling the heat influx. Boundary condition, okay? All right be, because n is the u, unit outward normal j.n would be the outward heat flux, and the minus sign essentially makes sure that it converts that into a heat influx. We're saying we're controlling the amount of heat flowing over the compliment to the Dirichlet boundary over the Neumann boundary, right, so over the blue part of the continued potato. We are controlling the heat, the heat flux, the heat influx, right? So we're controlling me, that of the heat influx. We're controlling the amount we're getting there, right. Okay. So, so we'll note that we're not controlling the vector itself, right? We cannot control the vector, and that the theory of PDE tells us that, in fact, physics also tells us that we cannot control the entire vector right? What we can control is just the normal component of the, of the heat flux vector here, okay? I should also write this in coordinate notation, in coordinate notation is just the dot product, right. So it is minus ji and i equals jn, j sub n. All right and this is coordinate notation. Okay, I believe that completes our basic introduction to the weak, to the, sorry, to the Strong form. We will end this segment here."
e1xzkNGA8Jk,"So, there was a question on why, when we go from 1D problems to multiple dimensional, or 3D, 3D problems we don't directly go up to, elasticity, whereas we really set up the 1D problem mainly in the context of elasticity, right? E, E, Even though we said it applies to other problems, heat conduction and diffusion as well in 1D, we did repeatedly make use of the, of, of, of the physical analogy of, of the physical, underlying phenomenon being, elasticity, right? The reason we don't directly go to three dimensional elasticity is because for 3D elasticity to be posed our unknown is the displacement, which is a vector. Right? And handling vector unknowns is more complicated from the standpoint of the PDE itself. RPDs turn out to actually be 3PDs for each of the vector components. The constigitive relations get a little more complicated, and actually the boundary conditions need some careful bookkeeping. And, as you may imagine, the same thing translates over, carries over into our finite element implementation. So, we will get to 3D elasticity, of course. We will look at vector problems in 3D but it's probably going to be a little easier having first gone through the scaler problem 3D. Okay? So, to be followed."
4lIS70sa6Xs,"Okay, so let's continue. In the previous segment, we began looking at the strong form of the problem for linear elliptic PDEs in three dimensions with scalar, with a scalar unknown, right? We wrote out the, the strong form, we wrote it out in coordinate notation, learned a little bit about the quantities that show up. And the very last thing we did was make a connection with the, the physical problem of heat conduction at steady state. Let's carry on with this and what I want to do first in this segment is for completeness, also make the connection with mass diffusion in three dimensions, okay? So just as we looked at heat conduction, let us now consider the problem of mass diffusion. Okay? Of course, the, the, the strong form is the same so we're not going to rewrite it. Let's just say what the variables would translate to here, u in this case would be a concentration, right? Concentration of some field. All right, now it could be a concentration either in terms of mass or unit volume or maybe number of particles of a certain kind per unit volume. Al right, so this could be a concentration in terms of mass over unit volume or number. Volume unit for u or it may even be, normalized by some reference concentration to be rewritten into something to call a composition, right? So I will just write this here. Composition is a essentially a re-parametization of the composition that is often used in physics, okay? So that's what you would be right? What about the j? Now that we know direct notation it's safe to use still, to just do that, right? J in this case would be the mass flux, right? The mass flux or the number flux. Right? Which would be, mass flow perpendicular, perpendicular to a unit area or unit time. Okay, or alternatively, if you were doing number flux it would be the same sort of thing, right? We would be talking here of the number of some particle flowing perpendicular to a unit area by unit time, right? So number flow Number of particles Flowing Perpendicular to unit area and so on. Right? You can complete that statement, all right? So that's what j would be. Our constitutive relation j equals minus kappa. Now, now if actually, I realize that this is probably the very time I'm writing the entire constitutive relation in direct notation, okay? So, the way we would write a temperature gradient would be that, right? Being the spatial gradient operator, okay? Could be this or perhaps you are familiar also with writing that gradient as partial of u with respect to the vector x, okay? So if we had this constitutive relation kappa then would be the diffusivity tensor. Okay? And as we did from the case of heat conduction, we would make the observation kappa equals kappa transposed. It's symmetric and it's positive semi-definite, right? So, if c not equal to zero and c being a vector, okay? If, if c is a non zero vector, then c dot kappa over c is greater than or equal to zero. okay? To the same, the same mathematical properties as the heat conductivity tensor. All right, and then the condition that u equals u, g, would simply be a condition of stating that we are controlling the concentration or the composition, right? On Dirichlet boundary, all right? On the specified sub set of the boundary. And finally saying that j, sorry, minus j dot n equals j sub n on omega is sub j would be the mass in flux boundary condition. Okay? So, you know, everything else is well, everything else is really the same between the heat conduction and the mass diffusion problem in 3D, okay? At steady state it turns out, actually, when we, when we take away the steady state assumption also there is this analogy between the two. And of course the equations would be the same, we will study those equations as well and of the finite element methods for them. So, okay. So, at this point we've looked at the strong form and, and coordinate notation. We've also looked at some direct notation just for completeness let me write out the PDE of the strong form in fully indirect notation, okay? Okay, so this is, find u given u sub g, j sub n, f, and it's constitutive relation J equals minus kappa, u, okay? Find u, given all this stuff such that Now, j, I, comma, I, is essentially the diversions of j, 'kay? So we get minus del dot j, right? And del dot is the diversions operator in direct notation. Equals f in omega, 'kay? The boundary conditions, the  boundary condition is straightforward because it's only on the scalar unknown. U equals u sub j on partial omega u and the boundary condition is minus j dot n equals j, j sub n on partial omega sub j, okay? All right, and this actually is it, right? For as far as our strong form is concerned in direct notation, okay? And one, one useful thing to look at here is what happens to this equation when we make the substitution of the constitutive relation, right? So when we make the substitution, we so substituting. J equals minus kappa grad u in the PDE, right? We get minus del dot minus kappa grad u equals f, right? In omega, okay? And then something that's commonly done, is that if if kappa is specially uniform, right? So that means kappa is not a function of position, right? What does gives us then is the following equation, right? It gives us minus del, sorry. It gives us, if kappa is uniform, it tells us that we get kappa contracted with the hessian of u, okay? This equals f, all right? Now it's useful to write this coordinate notation to see exactly what is meant here. That cont, the the double dot there which I've referred to as the contraction essentially is an extension to two tenses of the idea for that product, all right? And what this thing is doing for us is, it is doing kappa, i, j, multiplying u, i, comma j, okay? In this context, what we have here, that is the hessian operator. Okay? We could also take the special case, which is often done. Which is that which is to assume or to consider cases where kappa i j is can be written as a scalar kappa multiplying the chronicle delta, okay? Sometimes called the Kronecker delta tensor. Okay, when we do this, what it implies for us, is that now if we go back to the problem, where that we wrote just about, the form that we wrote just about. And here we're noting that not only is the tensor kappa representable as a scalar multiplying the chronic of delta. And that furthermore here too we're seeing that kappa is the, the scalar kappa is uniform, right? If this thing is uniform then what we arrive at is kappa delta ij, u comma ij. All right? Equals f, right? And here if you observe what the action of the Kronecker delta is it reduces that relation, that PDE to kappa u comma ii equals f, okay? In omega. Okay? All right, so this from of the equation is often called the Poisson equation. Okay, so this is good. Okay, in direct notation the same thing will be. It would be kappa. Now, there's square u but note that I do not have an under bottom on the square, implying that it's not a tensile this is just a Laplace operator, right? It has very a few  Laplace. Okay, all we've done is write the, the Laplacian using that operated , okay? So and of course, in this sort of case if we have indeed that kappa is that the tensor kappa can be represented as a scalar kappa, right?. If we say that we have that representation then for the Neumann boundary condition. Right, so the Neumann boundary condition which is minus j dot n equals j sub n. This is actually reduces to the requirement that minus kappa grad u dotted with n equals j sub n, okay? It reduces in this case to a requirement on the normal radiant of temperature, okay? So this thing is this is the normal, normal gradient Of the temperature u, okay? And these are simplifications that are commonly used. In fact, it is very often assumed that kappa does indeed have this representation by the scale of kappa and the chronicle delta, right? And what this translates to, is that the physical the physical sort of situation that this represents. So we have kappa equals that, sorry. If kappa ij equals just the scalar kappa, multiplying the chronicle delta, what we have is, what is called isotropic heat conduction Okay? We're specifying here that this body is such that when heat is flowing in here, heat flows only in the direction of the temperature gradient, right? So if we in, int, introduce a temperature gradient in one direction, it does not induce heat flow in any other direction, okay? That's, that's what it reduces to and furthermore, it also says that the amount of heat flow we get by introducing a temperature gradient in one direction is exactly the same heat flow that we get. By introducing a temperature preheating in another direction. In both cases, the heat flow back would align with the direction of the temperate preheat itself. And the amount of heat flow would work to be the same, if we specify that the same temperature created in different directions. Okay? So, those are the sort of simplifications that are often used in representing heat conduction, all of those equivalently mass diffusion as well. Okay, we're actually going to end the segment here when we return and pick up the next segment. We are going to find ourselves ready to look at the weak form of the problem."
KVzu-zulHdY,"There was an error in board work on, this particular slide. It's a very simple error. It's one of the most common ones, however. It occurs right about here. And, the error is that our definition of j, itself, is that j is equal to minus Kappa grad u. As a result, the minus signs cancel out, and what we have in the next line is plus Kappa grad u.n equals jn. Okay, so with that little fix, that slide is all correct, and so hopefully is the rest of the segment."
Hig3Gn8Ls6U,"Welcome back. So, we now find ourselves ready to look at the Weak Form of the linear elliptic p d e for a scalar, for scalar unknown in three dimensions, 'kay? So, let's just state the weak form and, and, and move on with the thread because we've written the strong form, and at least a couple of times, and in different ways, so All right. Here is the weak form of the problem, 'kay? I'm going to first state it, and then, obtain it, right? So, find. U belonging to S, right? Where S is equal to space of functions, right? So, equal to some functions u, such that u at u equals u g on The Dirichlet boundary, 'kay? Find u in this space from the space given everything else, right? Obviously we're given g, u g. We're given j sub n, right? We'll be given the data f of, and the constitutive relation. Our constitutive relation now is j sub i equals minus kappa i j u sub j, okay? Given all this, right? Find u such that Such that, for all w belonging to V, where V consists of space V consists of holds functions w. Where w, again, just as we saw before for the waiting function, vanishes on the Dirichlet boundary. Okay, find u belonging to S, given the data such that, for all w belonging to V, the following condition is satisfied. Integral over omega w, i j i d V, okay? Equals Integral over omega w f d V, okay? Minus integral over omega, sorry, integral over omega sub j, w times j n d S, okay? Some things to note here, first of, this is this is our weak form, 'kay? Some things to note here are that we have here the elemental volume d V, and d S here is the elemental surface area, 'kay? That should be clear because on the golf course is a subset of three dimensional space, it's volume in 3D. And therefore, it's boundary partial omega sub j, that's subset of its boundary, is indeed a surface, right? Okay, so, that's the situation we have, right? So,  d V is the element of volume that's, that allows us to integrate over the, over this entire volume. And d S is every little elemental surface area, okay? So, this is the weak form, what I'm going to do now is get us to it, right? I'm going to show, show you how the weak form is obtained, right? As before, we are going to take, we're going to take one of the approaches that we took before which is to start out with the strong form, which you've already put down, and get to the weak form, okay? So, here's how we do it, right? So, we start out by considering Consider the strong form. Okay? We have used this stuff, right? So, we have find u Given all the data And I'm just going to put down the constitutive relation here directly without calling it a constitutive relation Right? So, given all this stuff, find u such that, st is short form for such that. We have minus j i, i equals f in omega. Because I'm a little short of space here, but I want to have it all on this slide, I'm going to put down the boundary conditions here. Dirichlet boundary condition, and our Neumann boundary condition Okay? This is what we have. Now, the approach we are going to take is the one that we took formerly, the, the approach that we took in obtaining a weak form from the strong form for the 1 d problem, okay? All right, so, what we have, is we have the strong form. What we are going to say, is consider W belonging to V, right? Where V, of course, is the, is the has the properties that we stated before. It consists of all functions that satisfied the homogeneous Dirichlet boundary condition. Right? That's what that is. Okay? Now just as we did for the 1 d problem, what do we do here? Do you recall? That's right. We multiply the pde of the strong form by w, and we integrate by parts, right? So multiply. Pde. Right? To the strong form strong form S f being short form, for strong form of pde. Okay? Multiply the pde, the strong form of the pde by w. Right? And then, we integrate by parts. Okay, that's what we do. So, let's do it in steps, right? First let's multiply the pde, and integrate it over the volume, and then we'll invoke integration by parts, okay? So, integrating it, by multiplying it by w, and then, integrating it over the volume, we have this, integral over omega minus w j i comma i d V. Right? We took the minus divergence, from the left-hand side, and multiplied it by w. Okay. And this is equal to integral on the right-hand side, integral over omega w f d V. Okay? Okay, down here is multiply the pde by w, and integrate over the volume. Okay. We're now, going to do integration by parts, and if you recall, the application of integration of parts, and functions in three dimensions, right? Where the integrations happen in three dimensions. We of course do that to the left-hand side too, right? So, this is the one that we are going to integrate. By parts. Now, when we did this in the 1 d problem, I did mention I think, that the integration by parts is really nothing other than an application of the product rule of differentiation. Combined with the, the divergence theory. Okay. That indeed holds in, in multiple dimensions as well. And to bring uh,it's useful to bring that out, because it makes very clear what how, how the integration by parts proceeds. So, we'll do that. Okay? So, recall here that when we say integration by parts, what we are talking about doing, is the product rule And Divergence theory. Okay? Firstly, the product rule tells us the following, right? It tells us that we really need to look at w multiplied by j i comma i, as being just one term, in applying the product rule of differentiation, right? So, it tells us that, that really is integral omega. W j i, the whole thing, comma i, and we should make sure not to forget our signs, right? This plus w comma i j i. Okay? So, if you expand out the first term in the integrant, you will see that it can, that one of the terms it produces because of the product rule cancels out the second term. Right? And we're left with, what we had in the first line. Okay? So, this is equal to integral over omega w f d V, okay? But then, when we stare at our left-hand side integral, we realize that we can actually, view w j i as essentially a vector, which it is. Right? Because j is a vector. Right? Soum, this is a vector. Right? Component i. And that term, is essentially the divergence of w j. And there, we invoke the divergence theorem. Okay? All right? So, this step is really just a product rule. When we do the divergence theorem, we get the following. Right? The divergence theorem tells us that this integral over omega is really an integral over, over the boundary of omega. Okay? Minus integral over the boundary of omega of w j i n i. Right? Where n i, is of course, our unit outward normal, something that we'd observed in one of the previous segments. Right? It's this, integrated over the surface of the body, plus next term remains a volume integral. And the right-hand side stays the same. Okay? And where, how we get this is through the Divergence theorem. Okay? What we are going to do next, is take the surface integral. Sorry. Take this surface integral over to the right-hand side. When we do that we get, integral over omega w comma i j i d V equals integral over omega w f d V plus. Now, that integral over the entire surface partial omega, I'm going to break up two integrals over each of the two subsets, we've identified for the boundary, right? And those subsets, you recall, are the following. One of them is the Dirichlet boundary partial omega u, right? And so, here we have w j i n i d S plus integral over the Neumann boundary w j i n i d S. Okay. Now, let's stare at the last two boundary integrals. What can we say about the first one? What can we say about the first integral? Think about it. It's over the Dirichlet boundary. It's w j i n i d S. Right, we can say that over the Dirichlet boundary because w, we are picking to live in the space V, which satisfies homogeneous Dirichlet boundary conditions. We have that, right? So, that first term, that first integral drops out. What about the second integral? We're starting with a strong form, which includes the boundary conditions, explicitly, right? And in particular, the way we wrote out the boundary conditions we specified that the heat influx vector, which is minus, it's not the heat influx vector. The heat influx minus j i n i on the boundary, right, on the Neumann boundary. Is, heat influx, if you're talking the heat conduction problem, right, and mass influx, if you're talking mass diffusion. Anyway, j i n i, we identified as being minus j sub n, okay? So, when we put these things together, we arrive at the form that I had put down, originally, right, I think at the top of the previous slide. Integral over omega, w, i j i d V equals integral over omega w f d V, okay? Minus integral over the Neumann boundary w j sub n d S, okay? This was the, what I posed originally as the weak form, okay? And so we've got there, right? We've obtained it from the strong form, right? Now, in the case of the one d problem, we also went the other way, right? We demonstrated that the weak form, and the strong form are completely equivalent, and indeed, that holds in, in this case, too, right? It holds for actually every problem, right? The strong form and the weak form of any PDE are completely equivalent, all right? And we can adopt the same approach in proving it, in this case, as well. So, we could start out with this weak form, we would have assumed the space V. We would be given the data. We would essentially do integration by parts in reverse. And then we would go through the tricky business of invoking those variational arguments, right, on how did the, on, on, on the manner in which w can be chosen, right? Or, or, or the very fact that w has to hold for, for, for all functions living in V, allows it to state, state that it also holds for certain spes, special functions, okay? And that is what would bring us to the strong form. We're not going to go through that argument here. It holds, you can try it on your own if you like. But it, it's not crucial for what we want to do, which is, of course, to work with a weak form, okay? So, we're just going to stop here, as far as deriving the weak form is concerned. Let me also, make a remark. Let me just state here, actually, it's more than just a remark, so I am going to state it in sort of the main text, so to speak. Which is to say that the weak form. What we demonstrated is that the weak form is implied by the strong form, right? This is, this is, this is what we just demonstrated in the, in the little derivation that we did. 'Kay, one can prove also that the other direction holds. Okay, it's not difficult. You can just follow the notes there and, and follow the steps we took in the one d case. Okay, we're not going to show this, this, we're not going to show the right-hand side implication. We demonstrated the left-hand side implication. Okay, well, this is it, right? So, so this is, this is our weak form, and this is what we're going to work off to, to develop the finite element method. That will however, be best done in a different segment. Before we end this segment, there's just one remark I want to make, which is actually, applicable to the, to the strong form, okay? Just a remark I want to make here, which is that, if you look at, recall the PDE of the strong form. PDE of strong form, recall the PDE of strong, the strong form, right? Which is minus j i, i equals f, right? Or in in direct notation it is minus del dot j equals f, right, in omega. I just want to recall for us the interpretation here, now,  you know, that if we take this body, and we look at the PDE that we had the PDE really holds point wise, right? So, it holds over every little infinitesimal volume in this over this domain, right? So, if you will go inside, cut it open, and take a little volume, that PDE the, the strong form of the PDE holds over that little infinitesimal volume, 'kay? And what I just want you to do is recall for us here the interpretation of the minus divergence, all right? In this, in this, in this setting in 3D, the minus, the, the divergence itself, vector is the, is the total net outflux over that little volume, okay? So, the negative divergence is the total, is, is the net influx into that little volume. And all this PDE is saying in strong form is that their net influx into every little infinitesimal volume is driven by what we recognize to be a source term, okay? So, let me just do that little make that little argument, too. All right, so, if we have our, this is, this is our body, omega. What we're talking of doing here is considering a little volume element. Okay? And we are, the, the net influx is. Is obtained from, you know, by considering, with, with this little pill box type argument, which is sometimes presented often in the context of classical fluid mechanics. And sometimes, maybe also in the context of heat conduction, right? So, this is the net influx, right? So, so this net influx here is denoted by minus divergence of j, okay? So, what we're seeing is the net influx, right, over a little volume equals the source term, right? Or minus del dot j equals the source term, which is f, right, at every point in omega, right? So, that little pill box there represents a point in omega, right? So, we may choose to say, that okay, that has the position vector x, right? So, the point out there has the position vector x that, that little pill box has been constructed about that point x, okay? All right. I just wanted to recall this argument of this, this interpretation because I omitted to mention it. Okay, so we're done with the segment. What we we've done very quickly actually is get to the weak form. When we return, you know how this is going to proceed. We are going to set up the finite dimensional weak form. We're going to first observe that this is the infinite dimensional weak form. We'll set up the finite dimensional weak form, and develop our finite element methods. All right, we'll stop."
e6jm1iNtblk,"All right, welcome back. We continue with developing our finite element method for linear elliptic PDEs in three dimensions, with scalar variables. Okay? So, let's let's just start by recalling the setting, that we have. So, remember this is where we are. We have our basis vectors, we have our domain of interest, over which we are solving this problem. And, if we are done thinking of heat, heat conduction or mass diffusion again, we call that we have two different parts of the boundary. I think maybe I call the maze the Dirichlet boundary. This could be the Neumann boundary. Over the Dirichlet boundary, we're controlling the field itself. Either the temperature or the concentration field for the diffusion problem. Over the Neumann boundary, we are control, controlling the influx, right? The influx of heat or mass. All right, and we have distributed sources. All right. For, for, for either physical interpretation of the problem. Okay. So in the last few segments, we started out by looking at, we started out with the strong form of the problem. And then derive to weak form. I, I stated that they are completely equivalent. I showed the equivalent in one direction. From strong form to weak form. Okay? And we spent some time understanding all the different terms in there. So this is where we are. Where we were, at least. And what we're going to do today is pick up from that weak form, and take the steps that will eventually get us to our finite element equations for this problem, right. And as you will recall from the 1d problem what that means is that we need to work out the finite dimensional weak form. All right? And, for setting the context, let me go to our usual sketch. We have our bases. And that is our domain of interest, omega. We have some point in there which has position vector x. I'm not going to draw the position vector just not to get this diagram too busy, and we have here our boundary conditions. Bound different boundaries, right. We may have, we have the Dirichlet boundary, and the Neumann boundary. Okay? So this is the setting that we have and the, the infinite dimensional weak form that we have already derived is the following, right? I'm, I'm going to first write the infinite dimensional weak form, and then we get to the finite dimensional weak form, right? So what we've got as far as, is the following. We've said all right, let's what we need to do here is find u belonging to S, which includes the Dirichlet boundary condition. Right given all the usual data in the problem. Given, sorry I called this u not, it's ug. Given ug. Given our mass influx. Given our forcing function, and given our constitutive relation which is j equals sorry, I'm going to, I'm going to stick with writing this in coordinate notation. So I'm going to write this as ji equals minus kappa ij, u comma j, right. Given all this, find u such that, for all w belonging to V, which consists of w that vanishes on the Dirichlet boundary. Okay? For all w belonging to the, to the space V, we have to follow in weak form. All right? We have integral over omega. W comma i. Ji, dv equals integral over omega w, f, dV minus integral over the Neumann boundary. W, jn, dS. Okay. Let me just make sure that this all works out, yeah. All works out, right. So so, so this is our weak form that we derived. And of course, you know, at this point, we haven't said anything special about our spaces S and V, except for the fact that S includes the Dirichlet boundary condition, v includes the homogeneous Dirichlet boundary condition. So at this point, when posed as such, we are really talking of an infinite dimensional weak form. And as we made the observation in the case of the 1d problem that does not make it any easier to solve than the strong form. It bears complete equivalence to the strong form. And so we really haven't made any steps towards making it easier for us to solve or to deve, towards developing approximation. Right? And just as before, we develop approximations by going to a finite dimensional form. Right? So the finite dimensional form is the following. Right? Just the statement of it is going to look very much the same as before for the 1d problem, right? So what we want to do now is find belonging to Sh, right? Which is, is a subset of S. Okay? And as before Sh is a finite dimensional function space. All right, okay let's say a little more about Sh, all right. The way we construct Sh is again going to look like we did like, like everything we did in the 1d problem. Sh of now consists of functions like. Right. And now we specify as we did in the 1d problem that we are interested in functions that live in each one. Right, over the domain of interest over omega. All right, and because Sh is a subset of S, it must inherit the Dirichlet boundary condition as well. All right. So equals ug on the Dirichlet boundary. Okay. All right. And then the rest of it just follows right, given everything else that we have. Of course, we've given ug. We are given the influx condition. We are given the forcing function. And we know that the same constitutive relation applies. Right? Okay. I guess properly when we are stating the constitutive relation, the context of the finite dimensional weak form, we are no longer speaking of u being drawn from the full space S. So, we can already put an h there and there. Okay. And you recall that just as we did in the 1d problem, the h is the soup. H is just to remind us that these are finite dimensional functions and the way we construct them of course, critically uses the, the notion of an element size. Okay? So that's what the h show, indicates. All right. So, we're going to find in this sort of space. such that For all wh in vh subset of v, and where vh consists of functions wh. Also, in each one. On an eh, over omega. But satisfying the homogenous Dirichlet boundary condition. Okay? So, for all such wh again, the finite dimensional version of our weak form is satisfied. Right, and that takes on the form, integral over omega. Wh comma x, sorry. I'm lapsing to my notation for for the 1d problem. Wh comma i, jhi, dV equals integral over omega wh, f, dV minus integral over the boundary, wh, jn, dS. Okay, and once again, we observe that the data are not finite dimensional function. Right? We have exact representation of that data. Right? Okay. And, and you'll note that really in writing out this weak form, the only thing we did was replace all our any function that is drawn or, or obtained from u or w with the corresponding finite dimensional version. And of course, we defined what the finite dimensional spaces are going to be, right? As for the 1d problem, we are drawing then from H1. Okay, so this is our finite dimensional weak form, for the problem, okay? And, and you recall now that as in the 1d problem, what we need to do in order to proceed with the, with the formulation is to define what we mean by these finite dimensional spaces. Okay?"
fPuMMUM5JAs,"So, as, as in the 1D problem, the finite. Dimensional Weak form is the basis of our finite element formulation of the problem. Okay. Now, and, and of course, what we have to do is essentially define or construct the spaces that we need. We need to define S-H and V-H. In order to define these spaces, what we will do is construct a partition of our domain, omega. So we define S-H and V-H by partitioning. Omega into. Subdomains. Omega sub e. Where I think we were working with an omega sup e actually, so let's just stick with that. Omega sup e, okay? Where, e, once again, runs between 1 and, nel. Right, nel as before, stands for number of elements in the partition. Okay? And, furthermore, since each omega e is a subset of omega, which itself is a subset of r 3, we are indeed talking of each element sub-domain omega e as being a three dimensional sub-domain. Okay, so let me just draw the picture that we have in mind and then we'll say more things about it. Okay, so this is omega, and what we are trying to say here is that we have we're thinking of partitioning this cell sub domain. Then, sorry, there's domain omega into sub domain such as this one. And maybe another one, right next to it. All right, and so on. All right, and maybe we'd call this one omega e1 We'd have omega e2, right? And so on. Okay? So mathematically, just as we had before, we see that omega is for each omega E Really is open in, in R 3. Right, so, we consider each of these formula at least to be an open subset, and therefore our total domain omega which is the union of each omega e. e running from 1 to number of elements, right? And so, and, and you recall that as in the 1D problem to make sure that we pick up, all the interface points, right? The points of the interface between two of these elements of domains. We apply closure on the left and right, okay? And that is an, an equality. All right, and of course we also know that omega E 1, intersection, omega, E 2 is the null set. Okay, since each, since each of them is open, their intersection is is definitely a null set, right? Because we we, we don't get any boundary. So, then, but but, again, that's really, a technical thing. We do indeed, imply that they are non-overlapping elements. But, what happens on their boundary is really a technical thing. Okay, so, this is the setting. What we have to do now is, talk about how, how we construct these these sub-domains. And once we do that, we will also be on our way to defining the the underlying finite element basis. Okay, there are, of course, many ways to partition these partition the domain omega into subdomains and three dimension. What we will start off with is actually not even the simplest search partition but it is it is indeed the one that is most easy to get to given what we know about the 1D problem, okay? And what that implies is that we are going to construct a partition. Where each omega E. Is a, is, is what we call a hexahedral element, okay? So, consider hexahedral. Element sub-domains. Omega e, all right? e going from 1 to, and yeah. Okay, so, what we are talking of here is the following: We have this picture We have one of these element subdomains. Right? And I'm going to denote this one omega E. That is our domain omega. Right, so that's omega e. Right? Now in order to proceed with our final element formulation let's pull this element out of there, right? Let's examine it more closely. Okay? So that is our subdomain. Right? This is omega e. Okay, the simplest case that we are going to start out looking at is where on this hexahedral element, we are going to construct basis functions that are trilinear. Okay? So we, we will consider. Trilinear. Basis functions. Okay? Now if we are talking of cons of considering trilinear basis functions, what this implies is that. We need to do this by introducing nodes over these hexahedra elements where we have one, one node at each Vertex point of the hexahedral element and no more, right, no other nodes. So, our nodes are at the hexahedral points, sorry, at the vertex points of the hexahedron. Okay, so we have. There we go, all right? This means we have eight nodes, okay? So this is when we're going to work with trilinear basis functions. We have there, elements of the means are sometimes also called eight noded elements. Right? They're sometimes called bricks, a very colloquial way of describing their, their construction. Okay. Okay, so that's the setting. Now what we will do is as before we will denote coordinates for physical coordinates for each node okay if this so this is element omega E using the idea of local Numbering of degrees of freedom and of nodes. Let us suppose that this is node a for this element. Okay, so, we are going to denote the coordinates of that point as x sub e sub A, okay? And of course, your A equals 1 through 8, okay? All right, so when I have X, A, sub E, i'm going to see that this is local, this uses a local numbering of notes, all right? Okay, and, the numbering that's followed is. Actually we get to the numbering that's followed in just a little, let's, let's, let's just stick with this now. All right, if we have if we know that a runs over one through eight, let us then use that to write out our basis function straight away, okay? So now what we have is Sub E, right? So that's In element E, is the sum, A going from 1 to number of nodes in the element, which we know by the way, is 8, right? Because of the fact that we've chosen to construct trilinear basis functions, right? With this we have our basis functions and we're going to use our, the same notation that we had before right. These basic function are general, written as, being parameterized by physical position, right? X, by position, physical space x. And, just as we had before, we are going to multiply them By decrease of freedom N D A sub E, and here too I'm using a local numbering degrees of freedom Right, and this local numbering of degrees of freedom essentially reflects the local number of nodes. All right, we have this. And then we also have w h in element e is the sum. A going from 1 to number of nodes of the element. N a function of physical position C B E. Okay. And at this point thinks look very much like thinks before except for the fact that we have a much, greater number of nodes, but we know, how these nodes are positioned right there, they're vertices of a, of, of a hexahedron. Okay, so now if you think back to the way we proceeded in 1D, we took our physical element and regarded it as being obtained as through a mapping from a so called parent domain. Right? We're going to do exactly the same here. Okay? So we will think of omega b as being obtained. By a mapping from a parent domain. Again, we're going to denote that as omega sub c. All right, having stated that this is probably even stop this segment. Because once we get into talking about pairing domain omega c it is going just difficult to extract ourselves very quickly to end the segment, so we just pick up that definition in the following segment. Okay."
PpyXWtOm-wI,"Okay. So we're well into our development of the finite dimensional weak form for our linear elliptic problems in three dimensions with scalar variables. We are in fact at the point of having already decided to work with hexahedral elements. And I've stated that these will be constructed from a, by mapping from a parent domain, okay. So let's just pick up right there. So mapping from the parent domain which I've already said we are going to continue to call omega z to omega e, okay. So, here's what we have. We have omega e, and I'm somewhat purposely going to draw this as, an irregular hexahedron. So, maybe something like that. Okay. Let's see. Okay, that's reasonably irregular as a hexahedron. Okay, noted still. Okay? But now, the point is that we say that this is obtained from a mapping from a domain in which things are indeed regular. So we have a nice, cubic domain, a nice cubic, structure in the parent domain here. Okay. So this is where we are constructing this, element omega e from, right? And this is omega sub c. Now, in this domain, we, since this is three dimensional, we have coordinates c, eta, and zeta, okay. And in terms of c, eta, zeta, again this is a bi-unit domain. Okay, so omega c is also a bi-unit domain just as, we had in one d, okay. What that means is that in terms of coordinates of, the nodal points in this domain we have for these nodal points. We have the following coordinates, okay. So this point here is minus 1, minus 1, minus 1. This point is 1, minus 1, minus 1. This point is 1, 1, minus 1. And this point here is minus 1, 1, minus 1, okay? On the top surface we have 1, minus 1, minus 1. Sorry. This is minus 1, minus 1, and 1. This point here is 1, minus 1, 1. This is 1, 1, 1. And this point here is minus 1, 1, 1. All right so, so these are the points and, you note that in general each of these points can be referred to as c A, eta A, and zeta A, right? Where A, once again, runs from 1 through 8, okay? But here it matters, that we get, that we establish a numbering, okay? So, let's what I'm going to do is just so that we can, distinguish things I'm going to use a different color here to, to, mark our, nodes, okay? So I'm going to write the local node number. There are only local node numbers, of course, in the parent domain, but we're going to call them 1, 2, 3, 4, 5, 6, 7, 8. Okay? Those are the values taken on by A in the parent domain, okay. Once we have this what we need to do next is go ahead and construct our basis functions, okay? The basis functions that we have are going to be written again in terms of c, eta, and zeta. And I'm just going to list them right now, okay. So, with things set up in this manne,r in general we have this formula N A as a function of c, eta, zeta, right, is equal to, sorry, I need more room here. Okay, N A is equal to. Something expressed as a function of c, eta, zeta. Okay? And here we're already taking this approach that, though we originally introduced our basis functions in A's as being paramaterized by, physical coordinates. We are thinking of them, as, as, as physical positions in turn being parameterized by positions of this parent domain, okay. So, here is the general formula for them. N A is 1 over 8, 'kay? 1 plus xi times xi A times 1 plus eta times eta A times 1 plus zeta times zeta A. Okay. If you look at this form, what you will notice is if you also recall the way we wrote out our Lagrange basis functions, Lagrange polynomial basis functions in 1d, right in 1d, we had only one of these contributions, 'kay? Here in 3D, we have three of them. They're just multiplied together. And this particular way of constructing our basis functions is called a tens of product approach. Okay, so the NAs are, so, the NA written as a function of xi, eta, and zeta, are what are called tensor product functions, right? The idea is that you take your basic form, established in one dimension and just multiply, extended by multiplication to the other two dimensions, all right? And the factor of one-eighth will, of course account for the fact that we have one-half to the power of 3, okay, for the three dimensions. Okay perhaps for clarity it's, it's actually useful to write out the basis functions explicitly, that's what I'll do now, 'kay? So here we go. N1 function of xi eta zeta is 1 over 8 times 1 minus xi times 1 plus eta times 1 plus zeta. N2 function of xi eta zeta is 1 over 8 times 1 plus xi times 1. Sorry. These are both minus 1 minus eta, 1 minus eta . Okay, so it's 1 plus xi, 1 minus eta, 1 minus zeta. N3 equals 1 over 8, 1 plus xi times 1 plus eta times 1 minus zeta. N4 equals one-eighth 1 minus xi, 1 plus eta, 1 minus zeta. N5, one-eighth, 1 minus xi, 1 minus eta, 1 plus zeta. N6 equals one-eighth, 1 plus xi, 1 minus eta, 1 plus zeta. N7 is one-eighth, 1 plus xi, 1, min, 1 plus eta, 1 plus zeta. And finally, N8 is 1 over 8, 1 minus xi, 1 plus eta, 1 plus zeta. Okay, that's the whole lot of them, all right, all written as functions of xi, eta, and zeta. Now if you look at these, you, you will immediately see two properties that follow, right? From these we directly get the chronicle delta property, okay? So NA evaluated at xi B eta B zeta B, okay, is equal to delta AB, the chronicle delta. All right, and we also have this other property which is that the sum over all shape functions, right, at a point. Right at any point, indeed, is 1, all right? Properties that were identical to what we saw in 1D, 'kay the second property allows us to represent constants. The first property is the one that gives us this interpolation property also of these Lagrange polynomial basis functions. 'Kay, so I should probably state here these are the Lagrange polynomial. Basis functions. In R3, in three dimensions, okay? All right and you, and also you, you know why they're trilinear, okay? Note the trilinearity. Right? It's each of these functions is linear in each quadrant, right? For the three coordinate directions here. Okay. So these are the functions that we use to construct our finite dimensional trial solution as well as finite dimensional waiting function. Okay? And furthermore looking ahead. We also are going to use the same basis functions to interpolate the geometry. Okay? So the actual map, okay? The map. From omega C to omega E, okay? Is obtained by really interpolating. X as the function of C and here, I will use the general notation C as a vector to represent the noted coordinates C zeta, zeta. Okay? So the map is obtained by interpreting C x is a function of C, right? Where I will see, see, I'll state it the very first time and we'll use it in the future. All right? We get this interpolation by simply observing that x be e. Parameterized by C is now simply, sorry. Xe paramaterized by C, which is any point in the physical element sub domain. Right? Is obtained as an interpolation, A equals 1 to number of nodes in the element. NA dependent, depending on the coordinates in the paring subdomain. Okay? Multiplied with the actual nodal coordinates, right? In the physical subdomain, but where I've used the local numbering of nodes. Okay? All right. So these are, we just remember that these are just nodal coordinates in physical domain. All right? And that means we're dropping off. For that element, omega e. We are talking of a typical nodal coordinate as being that one, right? We interpolate over all of those and we get any point inside here xe. Okay? That's how things work and, and, and that is constructed from a mapping, from our nice regular, at least this is intended to be regular, parent subdomain. Okay? So that is the mapping that we have underlying our geometry, as well. And you remember, the term for this kind of a formulation. This is, remember, right? It's called an isoparametric. Formulation. Okay? Isoparametric, referring to the fact that to interpolate our final dimensional waiting function as well as the geometry, we're using the same basis functions. Okay? So we have isoparametric formulation. With this, we know how to go ahead now, we will be able to compute our fields including the radians. Right? And we'll end this segment here. But when we come back, we will look at how to use the isoparametric formulation to go ahead and compute gradients and so on and also do the integration. The approach is going to exactly what we did exactly like the one we followed in one dimension, except that we, we basically need to generalize all of those ideas into three dimensions. All right. Let's stop here."
Bci19ifM8WI,"Welcome back. We'll continue with developing our finite element formulation for our linear elliptic scalar problem in 3D. Okay, as we proceed one thing that may be of use, or one thing that may be advantages to see is how to interpret the basis functions that we are using. Since we are working in three dimensions, that actually gets a little difficult. And, and by interpret, I really mean something as simple as how do you visualize them. All right, visualizing them in 3D is a little difficult because of the proliferation of dimensions. And so what I propose to do in the first few minutes now is give you a the 2D case where it's actually easier to plot up these basis functions and get more insight into what they're doing. Okay, so let's look at that, right? So, so we'll continue with the same material, but we're just doing hm, taking an aside, so to speak, here. To hopefully enhance our insight. Okay? So, the aside here is we're going to look at Lagrange polynomials. Okay in particular we're, we're looking at the linears right? All right. In 2D All right, and so properly, rather than linear, these are really bilinears therefore, okay, because of 2D. All right. So in 2D, the situation would be the following. Instead of the use of my continuum potato represented by the foam football here, I will use a true 2D domain. Okay? So this is omega. We've we have the partition. Right? Into sub-domains, and that is a typical sub-domain. Okay, we're considering now the case where our sub-domains are polyhedral. Right, in particular, we're looking at sorry, we're not looking at polyhedral, we're looking at quadrilaterals now. Okay? Right and that, that's a typical, that's a typical sub-domain. Okay, so what we've done is we've partitioned omega into quadrilateral, lateral element sub-domains. Right and these are omega es, right? As usual e equals 1 to nel. Right, and, and, and everything else holds right, so we have indeed each of these as open. So we have omega, the closure of omega is the union over e of the omega es and the closure of that union. Okay, so we have this. So we partitioned it into quadrilateral element sub-domains. And on these quadrilateral sub-domains, we are going to consider the use of bilinear basis functions. So, consider bilinear basis functions. Okay, the picture by, the picture that we use in order to help us with the construction of these basis functions is is one that continues along this theme of using an of, of using a parent sub-domain from which every element, every actual physical element is drawn. Okay, so this is omega e. And now we, we suppose that we can construct this from a parent sub-domain which is now a square. Okay, it's a square, and it lives in a space where the, where the coordinates are labeled, z and eta. Okay. Since we are working with bilinear basis functions. Or we want to work with bilinear basis functions. And we have four nodes, of course, on the physical element. And so on the pair and sub-domain as well, we have four nodes, right? And this pair and sub-domain, you recall, is omega c, all right? Same notation. The points in the z, eta domain are picked essentially as before. Accounting however for the case that, accounting however for the fact that it is now a square. Okay, so the, this domain is again a bi-unit domain. So that says that these coordinates in terms of z and eta are minus 1, minus 1, 1 minus 1. 1, 1. And 1 minus 1. Okay. All right. Again we have we number these nodes as A equals 1, 2, 3, 4, all right. Okay, and again using notation that we've employed before. We may say that that the coordinates of node A, where we, of course, we're implying this local numbering of nodes, and therefore of degrees of freedom as well, right. Okay, so the coordinates, let's, let's also make it clear here, the coordinates of local node A. Okay? Those coordinates, in general, can be denoted as zA, eta A. Okay, and they take on the values that we've put down here. Right? Minus 1, minus 1, 1 minus 1, and so on. Okay. In this setting then here are our basis functions, right? I'll go to the next slide. So, the basis functions are the following. Now as before though we know we're going to use these basis functions in the physical domain, we take advantage of this barren sub-domain, this bi-unit domain to parametrize our basis functions in terms of z and eta. So, we have here n1, z comma eta equals one-fourth, 1 minus z, 1 minus eta. N2. One-fourth, 1 plus z, 1 minus eta. N3. One-fourth, 1 plus z, 1 plus eta. And, N4. One-fourth of 1 minus z, 1 plus eta. Okay? These functions also can be constructed as you would expect using the general formula for Lagrange basis functions. Okay? And I put down that, that general formula a little later, all right? I'll, I'll give you the general formula for 3D and then you'll, you'll, it should be clear how to simplify it to, for, to the two-dimensional case. Okay. Also you note that a hint to how that general formula is going to be constructed is to notice that here we have again this idea of tensor product basis functions. If you look at any one of these basis functions, for instance this one. All right, N1, it should be clear that it is a product of a basis function in the z direction and in the eta direction. All right? And the factor of half times half gives us one-fourth here. All right? So that, that tensor product idea continues. All right. These basis functions again we observe given our definition of the specific coordinates of the nodal points in the parent domain, right? These basis functions also do satisfy the Kronecker delta property. So we have NA, zB, eta B, equals delta AB. Right, the Kronecker delta property with which we are by now, very familiar. Also, when we sum up over all the nodes in the element. We sum up the basis functions. The result is one at every single point in the, in the element really. Okay? This has been expressed of course in terms of the coordinates in the parent sub-domain. But then, because we know that we're going to construct a mapping by mapping the geometry also we using the same basis functions. Right? Everything will work out. Okay. So, the reason to go into all of this was to be able to sketch out these basis functions and that's what I'm going to do now. Okay? So in order to look at them, I'm going to give you. I'm going to attempt to give you a perspective view here, okay? So, I'm drawing the pattern sub-domain here, but because of the, the attempt to present this as a perspective it doesn't seem quite squished. Okay. So that's our sub-domain. Those are our coordinate directions. Z and eta. Right? Our local node numbers are A equals 1, 2, 3, and 4. Okay? What I'm trying to signify here is that each of these node numbers has been written on a plane that is below the plane of the, of the actual element itself, right? Of the sub-domain. All right. That is our origin. Okay? Z, NA, eta equals 0. I'm going to draw just the sh, the, the basis function corresponding to N4. All right, so. I do this by changing colors here. And let's go with green. Okay, so, if you value at N4, what you see is that at z, N eta are equal to minus 1 and 1, respectively, right? It takes on the value of 1, right, so this value is 1. Okay, so this is N4 equals 1, okay. From there, if you stick to one of the coordinate planes, it comes straight down, right? Okay, linearly, right? So it's, it's linear along eta as we just showed. And along z, okay? Once it reaches this far edge, the function stays zero. All right. And so, as a Kronecker delta property suggests, N4 equals 0 at nodes, other than node 4. Okay? Now, at this particular point, at the origin, it takes on the value of one quarter, right. And this can be checked by just substituting z, N eta equals 0. And then 4 in fact in any one of the basis functions, right, all the basis functions value it to a quarter at the origin. So we'd get, it's just about that big, right? Okay? And what I'm going to try to do is suggest the shape that it has here so it's, it's essentially going to be a. It actually falls pretty rapidly until it gets to that point one. Sorry, one quarter. And from here, it will probably form a little less rapidly. Okay? Okay, I recognize it doesn't look quite quite smooth, but anyway that, that, that's just my poor ability to draw it. Right? The important thing is that at this point right at that point in 4, equals a quarter. Okay? So, so this is what happens with N4. The same sort of thing can be checked with any other basis function, with N1, for instance. And it will look similar, it will be, it will have this sort of tent-like structure. Right? Which obtains a value of 1 at equals 1. Right? And that node ends, and goes down bilinearly to 0 at each of the other norms, okay? So, so this is, this is an attempt to provide a little more insight into what is happening with these basis functions. This tent-like shape is a way to think about them. And as you can imagine, this is, this is more difficult to represent in 3D. So we, so we just stick with the 2D representation to provide a more visual sense of what these functions are doing. All right. I think we just end the segment here, and when we come back, we will launch into we will continue with the derivation of our finite dimensional weak form."
8VgQ_1AIMKM,"All right, we continue with our finite dimensional weak form and recall that we are actually doing things in three dimensions, right? So that side, that you've, probably experienced in the previous segment was only to give us a little more insight into the basis functions. So, return to the, to the final dimensional weak form. All right, and let's recall what that is. It is the following, integral over omega w h comma i, j h i, d v equals Integral over omega, w h f d v plus, or sorry, minus integral over the Neumann boundary. w h j n d s. Okay, so this is our finite dimensional weak form. Of course, we already have over this our partition into elements of domains. All right, so what that allows us to do is recalling that partition which is the omega equals the union of omega is the union over e, omega e, right? What this, what the above integral gives us, right, the, what the above integral equation gives us, is the ability to write it out as a sum over the elements of the the sum over e. Integral over omega e. w h, comma i, j h i, d v, equals sum over e, integral over omega e, w h, f. Dv plus sum over now, let me use a different set here for this sum. And, I'd like you think about why I need to use a different range for this last sum. Kay? Remember that now it has to be over elements that have a surface coinciding with the Neumann boundary. Okay? W, h, j, n and I've gotta be careful with signs because this is not a plus, it's a minus. Okay? D, s. Now, I'm going to say here that it's a sum not over all of the elements but Over e belonging to e sub N. Okay? Sorry, belonging to. Let me make that really a set, so make, let me make that a script e. Okay. What I saying here is that e belongs to the set E sub n if the boundary of element e intersection the Neumann boundary. Okay, is not equal to the empty okay? All right, so all this means is that we're considering elements that, coincide with surfaces coincide with. Okay, so we return to our continuing potato, or maybe we should start calling it our football now, right? And I'm showing you the Neumann boundary, the blue bottom boundary, this is our basis. So, any element that has a surface coinciding with this surface is included in the set E sub n. Okay? All right. That's, that's the sort of se, that's the sort of notation we're using here. So, let me just draw that here. All right? So we have an element, we have our domain, all right? And I think we have been denoting that as the Dirichlet boundary. This is the Neumann boundary. Right? So, we're talking of an element that could have a boundary coinciding with this Neumann boundary. Fedodite does have a boundary coinciding with anointment boundary. Okay, so this is an element say omega e. Right? Where e belongs to the set. Okay? And it is only over that set of elements that this very last sum is, computed. All right? Okay. So, we have the setting. Let's stand still at this finite dimensional weak form. And note that we do need to compute some gradients, as we've had to for the one dimensional problem, right? We need to compute gradients there, as well as for this term. Because we remember that gh of i is just minus kappa ij. U h comma j. All right? So we need gradients as well. Okay, so in computing gradients, just state this, we need function gradients. Right, which is not a surprise. We needed the same things in the 1D problem as well. Now, I'm going to take a step here that will make our notation a little more concise, okay. And what that involves is the following. I am going to take this, the, the vector z, eta, zeta. Right? And write it as, simply, z1, z2 C, 3. Okay, this is exactly the way in which we are writing our physical coordinates as well. All right? This is just as we're seeing X, physical position of a point, is X 1, X, 2 X, 3. In terms of its physical coordinates. Okay, this is going to save us some space and just give us a briefer type of notation as we get further into the development of the waveform. All right. It's going to be immediately clear why we need this sort of thing. Okay, so now we need to consider u h over element e, comma I in general. This is sum over A. A going from one to number of nodes in the element. N A comma i, D a e. Right. And the same thing for w h e comma i is the sum over the nodes in the element. NA comma I, D, sorry. C C A, E, okay? So we need to compute these gradients of basis functions. All right? What I am going to do here is leading up from the notation that we've already established for the coordinates in the parent subdomain, I am going to say that we will use uppercase indices. For components of xi. Okay? For, C in, sorry. Okay, all right, what, what this means is that the components of c, we're going to write as c I, where we're using an upper case I, running between 1, 2, and 3. All right? Okay. So using that and recalling that the way we are developing our basis functions is to parametrize them in terms of the Coordinates in the parent subdomain. We have, quite clearly, N A comma I, which, after all, is just partial of N A with respect to X I. All right? We are going to write this as, partial of Na with respect to CI, capital I, and using the change rule, we have this. Okay, what we have applied here is a certain convention. Do you know what that convention is, we're, we, we we've applied a certain convention to one of the indices here. All right, of course we are implying here that there is a sum on I equals 1, 2, 3. All right. This is the so-called Einstein summation convention. All right. We will use that repeatedly. Okay. All right this is straight forward for us to calculate. Right, and I'm actually not going to go and write it out again. Right because we know each of the NA's in terms of the corresponding C's. Right, C1, C2, C3, which were formally our C89 . Okay, so just remember that we have. All right so we talk about taking this derivative, with respect to a particular See I, right. That I may be 1, 2, or 3, right? We've, we've written out these shape functions. We know how to take those derivatives, right? Straightforward. Now the thing that remains to do is to figure out this quantity, okay? Right. How do we get that?"
Bn4u-HCV69I,"Okay, now you may recall how we did it in the 1D problem and take take that as a cue. All right, can you think of how to do it? What we need to do is recall. The mapping. Okay? So, that mapping was written as follows. Right? We said x. Right? The position vector of any point in the physical sub-domain. Right? It could be in reparameterized in terms of its, position on the parent sub-domain. And the way we caught that parametrization was by using the same basis functions. To expand, to, to, to interpolate, if you want to use that term, to interpolate the physical coordinates of the nodes. Where a now follows the local numbering. Okay? This is our map. Okay? Are now using so called coordinate notation, right? What as well. Right, what we know is x, sorry. Each x little i component, right? Can be parametrized by the full c vector. C1, c2, c3, and this is then just sum a going from 1 to number of nodes of the element, N A c x e, for element e but now component i of the x vector. All right? I've done, I've really written the same thing in both equations except that in the first case it's direct notation, the second case it is using coordinate notation. All right, and this is, this. Okay. From here just as we did in the 1D problem, we can go ahead and compute, partial of x i, with respect to c capital I. All right. Which is just, sum, over the nodes in the element. Now, N A comma I, capital I. X A, e, i, right? You note that there is a proliferation of indices here, right? There are superscripts and subscripts all over the place. The subscript for the element e is just coming along for the ride here, it's really not doing much for us right now. Okay? All right, so this is how we compute this derivative. Now of course this doesn't help us immediately because when, if you recall the form that the chain rule takes as shown here, what we need is actually the inverse of that created, right? Because we need the derivative of CI with respect to xI. Right? The thing marked with a question mark. All right, so how do we go about that? Okay? In order to do that, what we need to observe is that for the mapping that we have here, I'm going to, and I'm going to draw it here. We have our physical element. Our element in the physical domain. Okay, that one, right and this is element omega e. Right. Which we've obtained from this nice bi-unit domain. Right. Now we are calling these c1, c2, c3, all right? And what we've done, essentially, is to observe that for any point here in that domain, we can, given an arbitrary point c in the parent sub-domain, we actually have a map. All right? Okay? And that map is x of c. All right? Now, that is a vector map. Okay? In the context of looking at configurations especially if you have a background in continuum mechanics or some other field where you're looking at configurations and their mappings, this is what we call a point to point map. Okay, so this is a point to point vector map. It's almost superfluous to say vector there because there are, are representation points shown indeed as vectors. Right? We're using position vectors. Okay. So what that tells us is that we can com, compute the, the gradient of that map. Okay? The, the gradient of the map. Right, which is actually properly in the context of mapping configurations, the gradient of the map is also often called the tangent map. Okay? Okay. The tangent map is what I'm going to denote as a tensor, right? Because x is a vector, c is a vector, we're going to compute the gradient of x with respect to c. That gives us a tensor. Okay? So, J is. This derivative. Okay? This is what sometimes gets called The Jacobian of the map. All right? Now, this is direct notation, right, for this tangent map. We can also talk coordinate notation. So in coordinate notation, that tangent map. Is j, little i, capital I, is the derivative of the xi coordinate, x little i, with respect to c, capital I. Okay? Right? Now if you've studied continuum mechanics, you will recognize that to be something. Right, that it is essentially the deformation gradient from continuum mechanics. Okay? From the kinematics of continuum mechanics. Anyhow, we are not going to use that nomenclature. We just call it the Jacobian of the map. All right? Which is what it is mathematically. Okay. Now again, it's, it's, it's sort of the detail of a, a formal, or, or, or rigorous detail, to observe that, well, you can truly represent a tensor only if you have a basis. And if you have a basis, you can then represent tensors or squared matrices. Okay? That is actually a carefully constructed argument, but we don't to go into that argument, right? So we can represent it. As a matrix. Okay. All right? And truly, the fact that we represent it as a matrix comes from the fact that we have a, we have basis vectors in the physical domain as well as in the pairing domain, but we won't get into that detail. Okay. So, so J is simply, that matrix J is simply this. Right? All right. And, of course, there are terms here. All right. And you see this is just writing out what I had on the previous slide using coordinate notation. I'm writing it out explicitly here. Okay? So, we have this. Now, why should I bother with this? Because note that the map that we have is continuous, and it is smooth. It is actually what we call a c infinity map. Right? We are able to take an infinite number of derivatives of this map. Okay? So, the map, x of c from omega c to omega e is c infinity. Okay. All right, we can take, so actually a very smooth map. If it's a very smooth map and J is, a, partial of x with respect to c. All right, it turns out that, rigorously, its inverse exists. Okay? Okay, so there exists J inverse which is partial of c with respect to x. That is what J inverse is indeed by definition. All right? But that's easy to do now. We have G in front of us, we can compute that explicitly, because we do indeed have an explicit representation for each of those x1, xi's, x little i's with respect to each of the c capital I's. All right? And therefore, it's easy to compute J inverse. All right, so the code, so J inverse represents a partial of c with respect to x and, indeed we have J inverse is the matrix, the matrix representation of it, is partial of c1 with respect to x1. Partial of c2 with res, sorry. Z1 with respect to x2. Right. All the way to, to, to this last 3, 3 component. Okay, it's a 3 by 3, so that's not difficult to invert. It can actually be inverted exactly if we care to do that, okay? And we do indeed care to do that, right? But then you note what we've done, right? Essentially what we have is J inverse, right? We look at its components, capital I, little i now, okay. It, these components are indeed the terms we need or the factors we need in the application of the change rule. Okay? All right. So the key here is because of the fact that we are explicitly constructing this map, right, the vector to vector point map, X is a function of C, we can compute the tension map. It's just a 3 by 3, easy enough to handle, can be explicitly inverted. Right? In closed form, and the components of that inverted matrix are indeed the ones that we need for our chain rule. Okay, this is actually an excellent place to stop this segment."
Jaa7VVKo1_g,"All right, we'll continue. So what we did the previous segment was use this mapping, between the parent and physical element sub domains to observe that we have actually a convenient way of writing out our gradients. Okay. We'll continue now from we, from there and consider explicitly the three integrals that go into our weak form. Okay. So, with that background now, we are getting into, the assembling the integrals in the finite dimensional weak form. Okay. The integrals. In. The finite dimensional. Weak form. Okay, so, let's consider the first of our integrals, right, so. We are going to consider this one integral over omega e, w, h comma i, j, h, i, d, v. Okay? We first invoke our constitutive relation to write this as integral over omega e. W h comma i. We pick up a minus sign here, minus kappa i j, u h comma j. D, v. Okay, that minus sign just came from the fact that the flux is written as minus the conductivity tensile. Multiplying the, gradient of the field of interest. Right? The final solution field. Okay, we'll take care of that minus sign later on. Okay. So we have this and now, essentially, what we are going to do is use what we've just discovered about writing out the gradients of w h and u h. Right? Okay. So, over element e this lets us write it out as, integral over element, over omega e. Right. Of w h comma i, we will write as sum over a. I'm not going to write the limits on a. Right. We know the limits, they go from one to number of nodes in the element. Well okay, let me write them as always for the very first time and then I'll abandon writing that. Okay. Sum over the nodes in the element, n a comma i c a e. I put parentheses on that. Kappa i j, okay. Open another set of parentheses for a sum over b, going from one to number of nodes in the element, okay? N v. Comma j, d, a, e, d, b. And, we have a minus sign. Okay? And as we did in the one d case, perhaps it is useful just to remind ourselves that, that is w h comma i, and that is u h comma j. Okay, and now we, use our discovery of how to write those gradients of basis functions. Noting that those gradients as expressed here are readings with respect to physical coordinates. Right? Okay. So this is now integral over omega e minus integral over omega e, open the first set of parentheses. Sum over A. And I'm going to abandon writing the limits from here on. Now, n a comma capital i. Okay and recall that when I write n a comma capital i, we are talking of taking derivatives with respect to, what? Right. With respect to the, coordinates in the parent sub domain. Right. This times c I comma, little i. Okay? Multiplying c, a, e, okay? Capital I, j. Open parentheses again, sum over b, abandoning limits, n b comma capital J. C J comma little j d, b, e, d, v. And then note now that I was not quite as careful when I wrote that. That's not a D, A, E. That's a D, B, E. Okay. Right, note that what I'm doing here is just, partial of c i, with respect to x, little i. Okay. Same thing here. Okay. Partial of c capital J with respect to x little j, okay? And this is. Partial of n a, the a basis function. The basis function for node a, or the basis function which is one at node a. With respect to c capital I, and this is Okay. Right? Fine. So we know how to compute these quantities now. Right? Okay. So. Let's move on, and actually let's make our expression here a little simpler by observing that the degrees of freedom of course are independent of position, and therefore they can be pulled out of the integral. Okay. So we take the step that we have taken before in the one d problem. This is equal to minus, now, sum over a and b. Okay. We have here c, a, e. Integral over omega e, n a comma i, c, capital I comma little i kappa i j n b comma capital J c capital J little j. Okay. All of this integral d v. And just for complete clarity I'm going to put parentheses on this. Right, on the integral. And note that the whole thing is then multiplied by d b e. Okay, some things to note. Though, perhaps one way to think about this is that we have an integral, right, and we have a complicated looking integrand here. Okay. Each integrand, right? Has indices a and b, right? And these indices a and b come from the particular degrees of freedom over the element that we are talking about. All right, so this would be, so the picture here would be, let me draw it here. The picture here would be In the physical domain, we have our element and we have a node a. All right? Or b. Okay? The index a or b runs over those nodes, right? It runs over the degrees of freedom that correspond to the nodes, right? And over the others of course. Right? It runs over all of those. Okay. So having chosen a particular degree, a particular combination of degrees of freedom for a and b, right? Okay. For that particular combination of a and b we have the functions n a and n b, okay? Now, inside of the, inside of our integrand. Observe that there is another comp well there is another set of multiplications happening and I'm trying to look for a different color here. There we go. Okay. Using green. Okay. Inside the integrand there is Einstein summation convention applied all over the place okay so there's a Einstein summation convention here. Okay? Here here, As well as here. Okay? Again, if you've studied continuum physics and done this in three dimensions, you've probably seen this sort of thing. Often. Okay? So, whereas, we have Einstein summation convention for summing over indices that correspond to either physical or parent domain coordinates. That's what the little i j and the capital i j refer to, right? We have used an explicit sum. There, to run over the degrees of freedom of the element. Okay? I just want to make this little bit clear. Okay, so if that part is clear, we'll proceed and in order to proceed, we observe that. The way I've written it out so far, even though we've actually moved over to writing our you know computing derivatives with respect to the barren sub domain coordinates. Our integral is still over the physical element, right? And that elemental volume here really defers to the physical element. Okay? The next step we are going to take is to convert this also into an integral over the parent sub domain. Okay, so now we are properly changing variables into the parent sub domain. Okay? So we are really completing change of variables. To c. All right? And what that means is that we need to essentially convert the elemental volume. Okay. Now, here is our map. Okay. Okay. That's omega e. And this is constructed of course from omega c. Okay. And we have our map. X of c, right? So for an arbitrary point, physical point little x here, and an arbitrary point c in the parent sub domain. Okay, what you want to think now about is how do we represent the elemental volumes, right? And in order to do that consider the fact that we have here. An elemental volume. Okay, that's my D, capital V. And here, we have an elemental volume, that I'm going to represent as d, v, c. Okay? D v sup c, just telling us that that's an elemental volume in the parent sub domain. Now, a another result from the fact that we have this mapping and that we also have the tangent map, J of c. The result that follows from this is the following. One can show that d v equals determinant of J. D v c. Okay? Again this is a standard result. If you've ever played around with these sorts of configurations, can be proven and it's conventionally done if you're studying kinematics of deformable bodies. Okay. But we are just going to use this result. Okay. We are essentially going to use this result in our, integral, right, that we're working with. This one, right? We are going to try to replace that, elemental volume with this expression. Right. That is however, perhaps best done in a separate segment. So we'll stop the segment here."
wfje8-ALRlE,"Okay, so we have everything that we now need to write our integrals in terms of coordinates in the parent subdomain. So let's just go ahead and write that. And in particular the integral that we are working with is the following, right? So, so actually let me title this. Integrals in the finite dimensional Weak form. And in particular we're working with this integral All right. And where we are is a realization that this integral can now be written as minus. And that minus comes from the constitutive relation. We've agreed to pull the sum out, and write it as a sum over A and B, and for brevity we are not writing the limits that A and B run over. Right? In the general case, they run over one to number of nodes in the element, each of them. All right. We have an integral now. Okay? And inside this integral we have terms of the form, out here we have C A B, right? We don't want to forget that. Okay. The integrand is made up of terms of the following form. N A comma i C capital I comma little i kappa i, j, N B comma capital J C Capital j comma little j. And now, previously we had d, v, being the, the elemental volume in the, physical subdomain. And we just agreed at the end of the last segment, to write it as in terms of an elemental volume in the parent sub domain. But that does involve our including the determinant of j here. And let's recall that it depends upon c. Okay? D V, c. The integral here now can properly be written as an integral over omicron c. We recall that the, this is the integral which we enclose in parenthesis. And we have here, d, d, d. Okay? Alright now, if we stare at this integrand, it looks a little, a little daunting, doubtless, but we observe that there is a The Einstein summation convention is at work. And so, really what we have left is going to be an expression which takes on values depending upon the specific element degrees of freedom, capital E and capital B. Okay, the, the capital I, capital G, little I and little G are going to be so to speak contracted out, to use the terminology of densers and vectors. Okay right. So, so when we carry out this, this integral and let's assume that we do know how to carry it out. First of all observe that this integral can, can be is, is, is easy to carry, is relatively easy to carry on because our domain omicron c is a nice regular sub domain. It's a nice regular domain, it's a bi unit domain. So let me just write that little bit here. So the integral of omega C is the one thing that I want to make a difference with the way I wrote, so, so this is now a triple integral, okay? Where, C, 1 equals minus 1 to 1, C2 equals minus 1 to 1. And C3 also equals minus 1 to 1. Okay. Everything that we have in here, I'm not going to explicitly write them as depending upon c, but we do recognize that indeed, n a comma i will depend on c, because n a is trilinear. All right, and the xis. Okay, so derivative with respect to any one of those coordinates is going to leave us with something that's bilinear in general, it'll depend on two of the xis. All right? If you look at xi i comma i, and you recall how we are going to compute it It too will depend upon C. And why's that? Right. That's because we are going to compute it or we are going to identify it as being the components of the inverse of the tangent variable. Right? And the tangent map, consists of first derivatives, right? So, this thing, this, too, will be a function of c, right? Kappa ij. Well, it depends. If we have something with, uniform conductivity or uniform diffusivity, it would be a constant, right? But that does not res-, we are not restricted to that. We could have something that's a function of, this thing could. Potentially be a function of, position. Right? Could be a function of position and because we know how to write the physical position as a function of z that too would be a function of z. All right, the kappa Okay. And then we have NB, which of course, we've all ready discussed, right? We know how to write its derivatives. It's going to depend upon c as well the derivative of c itself with respect to little x little j, right? And then again, we have the determinate here. Okay? And this again is an integral over c 1, c 2, c 3. We close our parenthesis. Right, in the places multiplying d B element e. Okay? So everything in that integrand is properly a function of the xi 1, xi 2, and xi 3. That integral can be carried out. Alright, for simple cases this integral is not that difficult to carry out, analytically, and by simple cases I mean things where situations where Kappa is indeed independent of position. Okay, however we're not going to get involved right now with evaluating it analytically, because we already have a way to evaluate this, right? Do you remember how we're going to evaluate in, in the general case? Right. Using numerical integration. Okay. So we'll come back to that later We'll come back es essentially to how to do that in three dimensions, okay. But the point I want to make now is that if one looks at the integral here, okay? That can be evaluated. It's going to give us a scalar value, right? Because all the little i's, capital Is, the little js and capital Js are going to be contracted away. It's going to give us a scalar value, however. That is going to be indexed by the, by the capital A and capital B corresponding to the local degrees of freedom, okay? So I am going to denote this K, AB, sub e, okay? The AB is obvious, why it needs to be indexed by AB. It's also obvious why it needs to be indexed by little e. Okay, because this is for a specific element, okay? And so we have it. Our integral on the left is now just minus sum over A comma B, c Ae KAB, for element e, dBe, all right? And now, we take a step that we took in the one d case as well. We are going to well actually, what are we going to do? Do you recall what we do next in, in setting up our finite element equations? We get rid of our some, our explicit sum over a and d by going to matrix vector notation. Okay, let's do that on the next slide. So what we're doing is using matrix vector notation. We're using matrix vector notation for local degrees of freedom. In order to keep track of local degrees of freedom, we're going to use matrix vector notation, right, and really for their numbering. Okay? And what this means is that for element e, if we have c 1 e, c 2 e, up to c number of nodes in the element e, all right? These are the degrees of freedom. As you recall that are used to interpolate the weighting function using our basis functions, all right? Okay, so using this, right, and of course the same thing for d's. D 1 e, d 2 e, all the way up to d number of nodes in the element e, all right? Using this, what we observe is that we essentially are able to write our our integral, right? This implies that we can write integral over omega e. As now, minus we're going to write the c vector in a, as a row vector, right? And you recall how we did this in the one d case as well. Okay? We get a big matrix here. Which consists of entries K 1 1 e, all the way up to K 1 number of nodes in the element, e, okay? And, the last entry here is k number of nodes in the element time number of nodes in the element, e, okay? That is the matrix that we get. And this is multiplied by a column vector using the notation that is sometimes employed in linear algebra. Okay. All right. And, for, even for the gravity we could write this as minus c e, right? Where ce, or ce transpose is that row vector. All right, this, Ke, right, Ke is the matrix, de, okay? And by, by reference to what was done in the one d case, we would call this the element conductivity. Or diffusivity matrix, right, depending upon what problem we were solving. Depending upon what physical problem we were solving, okay? All right, clear enough hopefully. We can now follow exactly that same approach to look at the first integral on the right-hand side of our finite-dimensional weak form. Okay, so now consider integral over omega e w h f d v. With everything we've gathered together this should be relatively quick, all right? We're going to write this as integral over omega e. W h, you recall, we will right as a sum over A N A. There are no derivatives here, right? So, we have N A, C A e. I put parentheses on this to remind us that this represents w h, multiplying f, d v, okay? We can take several steps all at once now. Let's do that. Let's, pull the summation out, c, A e. We have an integral over omega e, but right, but let's plan to write that with a change of variable as an integral over omega c. All right. Inside here we continue to have N a because it does depend on position. Multiplying f and let's recall that f could be a function of position but through our mapping we have that. Okay, and any, of course, is the function of position. Okay. We're writing everything as a function of coordinates in the parent sub domain. That leaves us with just d v, right, but we know how to handle that. We know that it's a determinant of the jovian of our mapping. D, V, C. Okay. Sorry that's a little to squeezed in there. So let me write it out here more clearly. Okay. That's what we have. Right. And, of course, this is now, again, we can take several steps at once. So we can write this now as we can abandon our explicit Writing of the, of the sum over A. Right. And we know how to do that. That base, that can be done by going to this rho vector notation. With a c degrees of freedom. The integral now explicitly is an, a triple integral. C 1 equals minus 1 to 1. C 2 equals minus 1 to 1. And z 3 equals minus 1 to 1. Right? Our integrand here consists of a column vector of the shape fun, oh sorry, the basis functions. It's all, they're often called shape functions, but a term that I've been avoiding. Okay, we have the basis functions. All right, we have this. We have f, right, which could be a function, which could depend upon c through x. We know how that dependents comes about. There's an abuse of notation in, in using the same symbol f, but I will, just dock that abuse of notation. All right. And then, we have the determinant of j, function of back position. Okay. We have this and and yes, and we observe that this integrant is essentially, integral, sorry, it's integral is over D C 1. D C 2. D C 3. All right. Okay, we can compute this integral in general. Again, we will look at how we handle these types of integrals using numerical quadrature, numerical integration. And we observe that we essentially get what I will write out as being c 1 e, c 2 e, c number of nodes in the element e. Times, the column vector which I will denote as F internal. Okay? Degree of freedom one in element e. F internal, degree of freedom 2, element e. Coming down as far as f, internal Degree of freedom, in an E, element E. Okay. And finally, we could write that as C, E, transpose F Internal for element e. Okay? All very nice and concise. All right, and the, the advantages that having done it with, with the, in excruciating detail for the, 1D case we know you don't immediately have the sense to work on it. All right, we'll stop this segment here."
kIiF9Mn7OXA,"Welcome back. We'll continue with our development of the matrix vector weak form, for the linear elliptical PDE with scalar variables in three dimensions. So what we are going to do today is essentially not only completed assembling the matrix vector weak, the matrix vector weak form. But also talk a little about associated issues which arise mainly from the fact that we are looking at problems here in 3D. So so the topic of this segment and the next couple at least is the matrix vector weak form. Recall that we at the end of the last segment, we were working with the local element level integrals. Okay, And we had developed again the local element level matrix vector representations of these integrals. In particular we worked with the left hand side integral we, the, the bilinear term. We worked with the, the forcing function from the right hand side. And now that brings us to the, to the integral that imposes the Reimann boundary condition. Okay. So, let's start with this one, that one. So what we're doing now is to consider the integral integral over partial of omega e sub j of minus w h j n d s. Alright? And it's useful right away to recall for ourselves the sort of term we're working with here, right. So the, the situation we have is the following. We have our bases. We have our domain and we're looking here at an element that has an edge. Or in this case, a face really, that coincides with that, that coincides with an, with the face of the, of the body, of the body of interest itself. Right, so we have an element of that type, okay, and the whole point is that in this case, so this is omega e. Okay? And we may think of that face as being partial of omega e sub g. Okay? And we recall that partial of omega e sub g. Is the intersection of the boundary of that element omega e with the alignment boundary of the problem. Okay, so that's really the face of the element upon which we're imposing the Reimann boundary condition. Alright, using our finite dimensional basis function it takes on the following form, it is now minus integral over that of okay, a sum of NA CAe Jn dS. Now, one thing we've got to be careful about here is that we can indeed consider the sum to be a sum running over the entire set of element nodes. Right? And, and let's start out with writing it in that fashion. Okay, but now we recognize that in this picture that we've drawn here, not all the nodes have corresponding to themselves shape fun, basis functions. That are non-zero on the surface of interest. Right? On the interface of interest. If we say for instance, that this is the interface of interest that we've, that we've indicated, then let me highlight in a different color the nodes that lie on that Boundary. Let's suppose that those are the four nodes lying on that boundary. Okay? It should be pretty clear to us that it is only the basis functions that are one at those nodes that will contribute at all to this integral. Okay? So, having made that observation, there are. Th, th, there are at least a couple of ways in which we can process and this, the, the, the, the different ways of proceeding simply correspond to different ways of doing bookkeeping here. Okay? For one thing, one may say that one may define., Okay. When we define a subset, right? When we define a subset of nodes, a, right? Which A sub n shall we say okay? Which consists of all nodes A such that x a e, right? Which is the corresponding nodal point, okay? Right? Using the local node numbering such that x a e belongs to partial of omega e sub j, okay? Right. And then what one can do is simply restrict that sum to a lying in this set. Okay? In, in this case, I've used the subscript n, to suggest that this is the, set of nodes or degrees of freedom corresponding to the Neumann boundary data, okay. So let's use this approach, right? So what we see then is that minus integral over this surface, w h j n d S equals. Now again, I'll take several steps all together, right. I'll do this, I'll pull our summation out and I'll just say a belongs to the set we introduced, right, script A, sub n. Okay? We have your cAe, and I apologize, that c looks too much like an e. C a e integral over over partial of omega e sub j Na jn dS. Okay? Now we make another note, which is that we can of course convert from here to our parent sub domain. Right? The parent domain from which we construct every single element, okay? So in that setting, let's suppose that we are still talking on the same element here and so let me go to the next slide to get this done. Now we're going to elucidate a manner in which you carry out this integral, right? So for that purpose, let's suppose that we are now working with an element, right, some general element. Right. And on this element, let's suppose that. Surface of interest to us, is this one, right. Let's suppose that this one is the surface partial of omega e sub g. Okay, now this element of course is always constructed from the same parent domain. The nice, regular element in this volume domain. Okay. Right, we have that sort of a mapping. All right. So, let, let's suppose, just for the purpose of argument, that, the, that the face partial of omegas, omega e sub j is, is the mapping of that face. Okay? And the way I've, drawn things out, the face of interest here which I will mark just for the purpose of argument again I'm going to mark this as partial of omega c sub j. Okay? It is the face in the bi-unit domain in the parent subdomain that gets mapped onto the face of interest to us, which is the face on which we are imposing the Neumann boundary condition, okay? Essentially all we need to do here is now recognize that if we construct a sort of, a lower dimensional mapping, right? Which is the, the mapping that converts the area of this particular face, right, the one marked as partially forming e sub j, here, which, which obtains the, the, this particular face, from omega c sub j. Okay, what we observe here is that the integral that we need to carry out, okay, which is coming from the previous slide, it is minus, sum, AE belongs to A sub j, cAe integral over omega e sub j, NA jn dS. All right? Let's observe that this thing can essentially be constructed as. Integral over partial of omega z sub j NA jn, right. And, what we will do here is write determinant of let me just write this as J sub s, dS c. Okay? Well, what I'm talking about here is the idea that, for the mapping of the the faces, we can, we actually need to worry just about, we, we need to worry only about how to map from this phase, partial omega c sub g, to omega, to partial omega e sub g. Kay? And Js is that mapping. Okay, so Js, insert Js is what we may define as the area coordinates, right, that correspond to a that correspond to a, to a new set of variables. Maybe x tilde one comma. C2. X tilde one comma c3. X tilde two comma c2. X tilde two comma c3. Okay? And what we mean by this is is the following. If what we're referring to here is the following. If we look at the surface that we are working on in our physical domain, it is this one. Okay, so this is the surface partial of omega e sub j. Okay, what I'm suggesting is that we can now define local coordinates on the surface. Right? And these local coordinates are what I'm referring to as x tilde one, x tilde two. Okay? All right? And they, these ones are obtained from a mapping of. That face in the parent subdomain which is c2. C3. Okay? And this is easy enough to do because in terms of c2, sorry, in terms of c2 and c3, we can indeed express the local coordinate x tilda one, x tilda two. Okay? Okay, what we need to do here is define the map x tilda, as a function of c2 and c3. All right? Okay? And then the determinate that we're talking of is simply the determinate of this particular mapping, right? The detail of how to construct this can sometimes seem challenging, especially if the surface partial omega e, as I tried to represent here or here, is not a plain surface. Okay? That takes a little more work, but it can essentially be done. Often they will indeed be plain surfaces. And then the, the, the, this mapping is, is straightforward. In particular if partial of omega e sub j is a coordinate surface, the mapping is actually very, is, is almost trivial. Okay? So this is how you would we would go about doing it, right. So"
yNGlwbXLPA4,"So as an, as an example, a particular sim, a particularly simple case, right? As an example, what we may have, is a situation where our domain may be such that. This may actually be one of our, we may actually have, in these, in these two directions. Our coordinate axis may be our, our physical coordinates maybe x 1, and x 2, right? And then, if our phase omega e sits nicely, in here. Right? Sub g sits nicely, here. It's easy, to construct it, as a mapping, x 1, x 2 functions of C 2, C 3. Right? As I'm suggesting here. Sorry, that's stupid. Okay? In this case, the mapping is very easy to construct. Okay? All right. So, so at any rate, what we get then, is is the following. We get minus, sum A belonging to The set C A e integral partial omega e sub j, N A, j n, d S equals minus sum A belonging to the set. Sorry I, I realize that at some point I, changed this notation to say, A sub j, it should be A sub N here. Okay. A sub N, C A e. Now, this integral over the parents of domain, simply becomes an integral over say, C i equals minus 1 to 1. And some other C j equals minus 1 to 1. Where in the example that I was constructing, I was saying i equals 2 and j equals 3, for instance. All right? So once we have this, we have an A here. J N, we have the determinant of this lower dimensional map, All right? The determinant of the Jakobian effect map properly, and the And the, the elemental quantities here over which we're  I just d C i, and d C j. All right? Now, when we carry this through, what we get is the following, right? We can now, abandon the explicit sum over the local degrees of freedom A, and instead, we go to a vector. Right? Which now consists of I guess C A 1 e, C A 2 e, up to C a 4 e, right? Because any phase will have four nodes, right? In this case of four degrees of freedom, right? In this case. And we have here when we carry out this integral, we end up with essentially A set of Of components of a vector, right? And we will call this thing f I forget what notation we used earlier. But let's just call these f sub j, to indicate that they are coming from the, the boundary condition. Okay? And f sub A, sorry, f sub j A 1, f sub j A 2, f sub j A 4. Okay? And this holds for A 1 to A 4, right? Belonging to that set A N. Okay? Now, this is just an attempt to be systematic with giving us a way to think of the set of degrees of freedom, that correspond to the Neumann boundary condition. Okay? And also, an attempt to point out, that yes indeed, this integral that needs to be carried out over one phase of this element, right? Can actually be constructed in a fairly straight forward manner, from a phase of the parent subdomain. All right. The setup may look abstract, but when one has actual degrees of freedom, actual elements, it actually turns out to be relatively, straightforward, in most cases. Okay, there are some special cases where for instance, the surface is indeed inclined, or doesn't coincide, it is, is not a plane. Or does not coincide properly with a with a with a coordinate plane. Where the evaluation can get a little, more complicated, okay? But the approach is the same, fundamentally. All right. As, as a very last step, one could actually, you know, one could go away from this representation of a reduced vector here. Consisting only of the entries corresponding to the degrees of freedom belonging to this set. 'Kay? One could simply, expand it out, right? Okay. Right? So, expanding out All  expanding out to include all the degrees of freedom, in element omega e. Here is what we get. Okay? We get minus, this is what we'd started out with. So, we're going all the way back to the integral, that we're trying to impose here, right? Minus integral over partial omega e, sub j w h j N d S. Right? This can now be written as minus Let's suppose, the first minus C A 1 e. Let's suppose the very first degree of freedom from this element did indeed, belong to the set A sub N. Okay? Let's suppose that the next two are, maybe C 2, C 3 e. Okay? So, what we have is A 1 is 1 2, and 3 does not belong to this set if, okay, let's see 2, and 3 does not belong to the set. Maybe 4 does belong, A 2 e 5 does belong. Right? Maybe 6, and 7 don't belong. And 8 belongs. All right, I'm taking a particular case where, where here. These degrees of freedom. Okay? So considering the case where local degrees of freedom 1 4, 5 and 8 are A 1, A 2, A 3, A 4, it's this set of degrees of freedom, which basically make up the. Okay, this is the special case, I'm considering. Okay? Then, what this thing would multiply here, would be a vector, okay? Which would have a contribution, f j 1, okay? It would have 0 contributions, for the two, and three positions. It would have a non-zero contribution ,for f j 4, also f j 5. 0 for 6, and 7. And a non-zero contribution for f j 8. Okay? So this is, this a particular example I'm considering, and actually pre, constructing this The final matrix vector form. Right? In this case, we use only vectors. All right. You will also note, that as I was going along with this example I kept changing actually the particular degrees of freedom, which la, lay on this phase. But hopefully, that actually helps to, because it, it, it, it will hopefully, force you to think about exactly, which degrees of freedom, do lie in it. And, and how to write it in each case, okay? By constantly changing the, case that, that I said was laying on this so, sorry, the degrees of freedom that laying on the phase. All right. So at this point, we are done with assembling all of the element level integrals. When we come back in the next segment, we're going to talk about how to put them all together. Right? How to assemble the final matrix vector equations. All right."
J0wvkiF9U7k,"Okay. Welcome back. We'll continue with now assembling our element level matrix vector representations into our global matrix vector weak form. All right at the the end of the last segment we had obtained a representation 'kay, so for the following integral, right? You probably have this on you notes, and just taking it down. I'm just copying it down from my saved slides here. Right. We said that. We, we considered a, a specific example where we said that that local degrees of freedom one, four, five, and eight were the ones that lay on the face that coincided, the face of the element that coincided with the Neumann boundary, okay. So those, so we wrote that out in the following form. And for the resulting vector that we have here, the, the column vector. The first element was non zero, the next two would be zero, the fourth would be non zero, the fifth would also be non zero. Right, the sixth and seventh would be zero and the eighth would be non zero. Now I am going to do is so, so this is, this is an example, right? This is an example where local degrees of freedom 1, 4, 5, and 8 are the ones that belong to that set, okay. What I'm going to do is just to have common notation. Observe that this can, this then can of course always be written as minus c e transpose, right? The vec, the, the role vector of all those degrees of freedom F j e. Okay, all we need to observe is that in this column vector F G, sub e, some of the contributions are 0, right? These are the ones corresponding to the degrees of freedom that do not lie on that particular element face coinciding with the Neumann boundary. The rest of them are in general non zero, okay. So the reason I'm doing this is, because this is what we want to use, when we pull together all the other integrals that we've developed over previous segments to represent sorry all the different representations we've de, developed for the, for the other integrals in the weak form. Okay, so what you want to do is use in the following, right. We now have a way to write the left-hand side, okay, as a sum over e of terms of the form c e transpose K e d e. Okay? We derived this a few segments ago, equal to sum over e, c e transpose F internal e. And now, this new term that we've developed here, right. This is again an, a, a sum over elements. But, I'm not going to write, I'm, I'm not going to I'm not going to say that this sum for the traction term extends over all the elements, right. Instead adopting the notation that we had on the, in the previous segment to designate the degrees of freedom lying on the Neumann boundary, I'm going to say that in here belongs to a set E sub n, which I think we actually already have defined, right. The elements corresponding to elements which have some part of their surface, of their faces, lying in the, lying on the Neumann boundary. Okay, so we have e belongs to this script E n c e transpose F j e, okay. The other sums run over all the elements. E going from one to nel, e going from one to nel. All right. Okay at this point I want to point I, I want to bring to your attention the fact that we can just multiply this through by minus, right, by minus 1. Changing all the signs so that we get something that we may feel a little more comfortable about. Okay. So we basically transfer the minus sign onto the forcing function. And this brings us to a, another little detail, which it is useful to bring up at this point before we go ahead to assembly, okay? And I'm referring to the fact that the way we've set things up here that term has a negative sign, okay. So I'm going to refer to that and, and I'm going to do that, in an aside. I need to make a, an, an, I need to make an Aside here. Okay. Essentially the form of the equations that we have, okay, the form of the equations that we have. Is obtained. From the strong form, right, going all the way back to the strong form. Where the strong form had the following appearance. Right. Minus divergence to flux equals, a force in function. Okay, now this particular form had been adopted by me, keep in order to be consistent. Right, to be consistent, with the way heat conduction or diffusion equations are typically written in the field of numerical methods that treats these types of problems, okay. However, it's probably useful to also recognize that this is a steady state equation. Okay arrived at from an equation of another type, right? Now, let us suppose that whatever quantity that is undergoing transport like for instance, the temperature, right? If, if one were to include the rate of the temperature, right. Let us go away from steady state but really look at how the steady state problem arises, one would get an equation of the following form. One would have c derivative time derivative of u equals minus divergence of j minus f, okay? Where this would be if you were thinking of the temperature problem for instance, right? The heat conduction problem. This left hand side would be the rate of change of temperature. All right, and the statement here would be that rate of change of temperature was driven by this term which we all ready have previously observed to be the total influx of heat into a little elemental volume. Right. Instead of total let me call it net heat influx. Okay. Right, so that is one way in which temperature rises, simply by heat being transported into a little volume. And then this term would correspond to the local heating. Local, maybe distributed Okay? What we did was, start out with this you know, what, what we did could have been arrived at and probably is arrived at, by starting out with this time dependent form, right. So this is, time dependent form, right. And seeing that well, we have steady state, and therefore we set the time rate of temperature equal to zero. Okay? And then of course that we would have been left with the right hand side equals zero, which we would have rearranged to form presented here. Okay? All right. And, and this is the form that we worked with, okay. But you observe that when viewed in the larger setting of a time dependent or transient problem. The local distributed heating or alternately, if you're looking at a diffusion problem, the local supply of mass, okay. For the way we're writing things out is properly minus of f. Okay. Right its just a matter of the way we we took the signs when we started out with the PDE in this form, all right. So what this suggests, is that if we look back to the equation that ended the previous slide, 'kay. It makes sense for us to simply flip the sign on the forcing tone, okay. And you know, we give it a different symbol, right. And we'll, that will be properly the forcing that is obtained from considerations of how the Steady State PDE can be arrived at from the transient problem? Okay, so so what we are going to do is redefine f bar equals minus f, right, and if you go back, and redo all our derivation with just this little change, what we will see then is sum over e integral w h f d v integral over omega e of this. Right? With the minus sign, 'kay. Is of course integral sum over e, integral over omega e, w h f bar, d v. Okay, and accordingly the minus sign that showed up when I wrote out the the weak form as a sum over elements, right, at the minus sign of forcing term would be absorbed, okay. So, what I'm proposing is also to write now, sum over e, C e transpose, K e, d e equals, wha, what we have so far is this sorry, I noticed I flipped the positions of this transposing element here, okay. c e transpose, K e sorry, c e transpose. F internal e plus sum over e belonging to e n, c e transpose F j e, right. Right, this is what we had at the end of two slides ago. What we're, what I'm proposing now, is to replace this with sum over e, c e transpose, F bar internal e. Okay. Right. Where the association that we're making here is that now. That that this is essentially equal to integral over omega e w h, f bar d v, okay. All right. When we do things this way, f bar turns out we be properly the forcing function that would make sense even in the case of a time dependent problem which is something that we will go to. Okay. So for our purposes there are no big changes. It's just a matter of flipping a sign on that term. Okay, and it actually makes everything consistent now, with the time dependent case. Okay. So, if we agree with if we agree about this little change we can now go on, and essentially we will now go to our step of finite element assembly. Okay. So, the assembly of global finite element equations right, in matrix vector Okay. All right and, and, you know, very well how this precedes essentially it's a matter of looking at each of our element level contributions such as this one. Right or, or, or this one and this one. And understanding how they map into the global vectors? Okay. This is relatively straightforward to do in the case of the 1D problem. It turns out to be a little more complicated in the case of the general three dimensional problem. And what we have to grapple with here is the idea of mesh connectivity? Okay so that is, what we are going to look at now? Okay."
2daVbA9fB8Q,"When I went to the time dependent form of the of the strong form of the equation, in order to from there derive the steady state form of the equation, I wrote out the equation in this form. What I omitted to mention there is that this coefficient c is the specific heat of the heat conduction problem. Right? So let me write that here. Right, so it really is a specific heat of the particular material that we are working with. So let me call that the specific heat of the material or the specific heat of the medium. Okay, with that clarification, that equation should make complete sense now."
HHtenbozn28,"And in order to do this I am going to draw a section of the mesh. I'm going to sketch a section of the mesh. And let's suppose that we're talking of a node there. Now that node lies in general, that is just an interior node. It lies at the intersection of a number of elements. It's important to draw this one carefully, so I'll try to be a little more careful. Okay. That is where things start and to complete this picture, I'm actually going to go to other colors. So, I hope that'll make things clearer. Okay. This, I'm going to extend out to there. Okay. We have that and I will, okay. I'm going to also do the following. I have that. I realized that I drew a line which actually should have been dotted. Okay, I'm going to redraw that line, and make it dotted. Oh dashed. Okay. Then this line also is dashed. It is joined by this one. Okay. This one comes out, comes down on the way here, joins up there. What's this, then? This thing and that should also be dotted. Okay. This one is complete. This one is complete. And right I need to have this, this, that, that. Okay, and back here, I have another one, here. Okay, and this goes all the way down here. Remarkably enough, I think I've completed it. Okay. What I'm trying to show you here after a few minutes of hard work, is the fact that I have here, if you look at it carefully a node. All right? I have here a node, which I'm going to denote as A bar because it's a global node. OK? So what I have here is a global node, global degree of freedom A bar. OK and that's the node corresponding to the global degree of freedom. All right? I want to emphasize here that it belongs to how many elements? I know that may look like a messy sort of figure but I think the basic idea is conveyed. It belongs to 8 different elements. Right? And Let's label the elements as, to which it belongs as, the different color again. Let's label the elements to which it belongs as the element, that is sort of in, front, bottom, and to the left. Is e1, the one next to it is e2, the one behind it is e3, and the one all the way in the back is e4. And I'm going to try to, to, to label it, okay? We continue. I'll call this element, which is to the front, up, and to the left, as e5. The one that's front, up and to the right, as e6. The one that is behind, to the right and up, is e7. The last one is e8. Okay? So what I'm trying to show here is that a, a, general internal node belongs to 8 different elements. Okay? Since we are working with hexahedron. So, global degree of freedom, A bar, in general. Belongs to 8 elements, okay? They are marked e1, up to e8. Okay? All right? So, Now what one can observe is that if we follow a, numbering system in which, for a single element we have a numbering system of the following type. Right, the local degrees of freedom of this element, right? We've been talking of numbering as 1, 2, 3, 4 down there. 5, 6, 7 and 8, okay? This is A equals 1, okay? So, this is local degrees of freedom numbering. Okay, and the big question here is, and this is an important one, how does one represent the way our global degree of freedom, A bar, shows up as a local degree of freedom in each of the eight elements to which it belongs, okay? This the onset of this question is, is what we call or what, what, what generally describes this sort of information is what we call mesh connectivity, okay? The way this is typically provided in, in the context of a finite element. Problem, right? Is to provide enduring input, right? This is provided, provided typically in an input file.  Right? It maybe an input file or in some cases, if the meshes a fairly regular the, this sort of thing can be just generated on the fly. K, typically provided otherwise in an input file. Okay? And the way that, that file is specified is the following, all right What you will have is a listing of elements, right? So, you may have your element, the list, listing of nodes for element e1 or the degrees of freedom for element e1. You may go on to other things and then you may come to element e2. You may have e3, e4, e5. Maybe, they're not all sequential. You have e6, e7. Sorry. E7. I think I'm running out of room here, so let me just give myself more room. E1, e2. That is how I indicate that they need not be sequential in numbering, right? E3, e4. So the elements don't have to be sequential. E5, e6, e7, e8. Okay? So The elements surrounding node A, the, right? The elements to which node A bar belongs do not have to be sequentially numbered in the global, in, in, in our global mesh, okay? All right. Now, but then let's look at things now. What we do for element e1 is simply list the global degrees of freedom that correspond element e1's local degrees of freedom, one through eight. Okay? So let's see how thing work out for this. In this particular case, and I realize I did indeed miss a couple of inches. So let me just provide that in the figure. I did miss this edge. No, I think things are complete. All right. Now. With this setting. Lets look at element e1 as listed here, as drawn here. And ask ourselves a question. What is the position of global degree freedom A bar, relative to element e1? Following the numbering, the local degree of freedom numbering that I have here, it, appears to me that A bar is local degree of freedom seven. Okay? As I speak, I see even more, one more place where I've missed out an inch. Let me get that in here, it's this edge. Okay. All right? So for element e1, A bar is local degree of freedom number seven. So, what one would have here is that you would have other global degrees of freedom in the positions one, two, three, four, five, six. In position seven, you would have A bar. In position eight, you would have something else. Okay? So we go on with that. For element e2, the way I've drawn things out here, A bar would be degree of freedom eight. Okay? So, for element e2, all these positions would have other global degrees of freedom. It's position eight would have A bar. Right? The number a bar, right? Okay. Where a bar would be the global degree of freedom number for that particular node. Okay, things would go on. I just completed for this case for e3, it is five. Okay? For e3, it's five. So one, two, three, four. A bar. Which is five, six, seven, eight. For element e four which is the one on the bottom layer behind and to the left, right? The one I didn't mark. A bar is degree of freedom six, okay? So for e4 is one, two, three, four, five. A bar, seven, eight. Carrying on, now for e5 it is a degree of freedom three. So one, two, A bar, four, five, six, seven, eight. For element e6, it is right. For element e6, it is number four. So it's 1, 1, 2, 3, A bar, Five, six, seven, eight. For element e7, it is actually degree of freedom one. So we have a bar, two, three, four, five, six, seven, eight, and finally for element e eight, that is local degree of freedom two. So, we would have something else in position one, A bar in position 2, three, four, five, six, seven, eight. Okay. So, this information is the critical bit that carries all we need to know about mesh connectivity. All right, and now as you have more and more elements in the mesh, you would have a similar array, if you like, for each element. Some some authors, and some books I guess refer to this as a local destination array. Okay? But that's, that's just a very specific name for it. Anyhow, the important thing is that we need to provide this sort of information. And then, where would we use this information? Well, we would use this information now in recognizing that for As we set about assembling our, matrices and vectors that come from our element, level integrations, we assemble them into global matrices and vectors as global matrices and vectors are going to have components that are numbered by a bar. Right? So any place where a, an element has a local degree of freedom corresponding to the A bar degree of, global degree of freedom, the corresponding component in the stiffness matrix for instance will be added or the corresponding component in the force vector will be added on. Okay? So we'll stop this, segment here. When we come back we will take this particular case and, try to construct, the contributions, to the stiffness matrix and force vectors. Okay."
fgi9UyFJvnk,"All right. So, welcome back. We'll continue with assembling our global finite element equations in global matrix vector form. All right, and we will do this by using this idea of mesh connectivity. And in particular I'm going to try and to use that example that we constructed to illustrate how, how this proceeds. Okay, so assembly of global, Matrix, Vector, Equations, which really is just the vector matrix weak form. Okay. Essentially, what happens is that the expression that we had on the, on at some point in the previous segment, which is sum over e ce, transpose Ke de, equals sum over e ce transpose f r with our redefined forcing function. F bar internal e plus sum e belonging to the set of elements that had one of their faces lying on the Neumann boundary. C e transfers f j e, right? We have this. Now, this will be, will be replaced, as we know, with a set of global vectors right, vectors and matrices which I'm going to write now as as follows. I'm going to write this as c transpose. K K bar.  D bar equals c transpose F bar. Internal, okay? Plus c e transpose F j. Yeah, just Fj. Okay. The reason I introduced bars here is something you may recognize from our treatment of the one dimensional problem. We know that we still need to account for the proper handling of Dirichlet boundary conditions, and when that is done you will recall that the matrix in place o k bar as well as the vector in place of d bar are going to be a little different, right? They'll have different sizes. Okay? It's just to, leave us that freedom that I'm calling this k bar now. Okay, let's first look at what k bar is itself. K bar is defined then as, through the sort of abstract representation of the assembly over all the elements of the individual element stiffness matrices, right? And likewise, F bar internal is the assembly over elements of F bar internal over each element e, okay? Let's now talk a little bit about K bar, okay? So, so essentially, this process of assembly is something that we go through by looking at mesh connectivity, and simply putting together elements the components of K bar, by accounting for the local and global degrees of freedom. Okay. So let's suppose we were looking at K-bar. Okay. And this will be a big matrix. Okay, let's look in in general, let's suppose we are looking at the at the contributions to the particular row where the row and the row, and column, right? Corresponding to the A bar degree of freedom. Okay? This is where the A bar degree of freedom arises. And likewise perhaps here. Okay? All right? So, what I'm going to give you is a single expression here for all the contributions to the, to the A bar, A bar component of the stiffness matrix, okay? All right. Now what we see is that in terms of the elements stiffness matrices, okay. We have contributions from Ke1 plus Ke2, Ke3, Ke4, Ke5, Ke6, I need more room, as often, plus Ke7 plus Ke8, okay? All right, now, if you look back at the way we described our global degree of freedom, A bar as being related to the local degrees of freedom of elements in 1 through e8, okay? What you will note is that in element e one A bar occupied the seventh local degree of freedom, okay? So the contribution that comes from that element to the A bar A bar entry here would be Ke 1, 7, 7, okay? For e2, it would be Ke bar 8, 8. For e3, it will be Ke bar 5, 5. For e four, it would be seven, sorry six, six six. For e5, it would be 3, 3. e4 it would, sorry, e6, it would be 4, 4. e7, it would be 1, 1, and e8, it would be 2, 2. Okay, note also that because I'm looking at only the A bar A bar component, right, of the global stiffness matrix, or the global in this case conductivity matrix, sorry, we have also the diagonal contributions from each element. Right? If instead of looking at the A bar A bar contribution, I were looking at maybe the A bar B bar contribution, right? Where B bar was some other global node. Okay? All right. Then, we would have contributions to this A bar, B bar component, of the global conductivity or global diffusivity matrix. Okay? Where here, we would have in general, I'm going to maybe write one or two of those contributions. Right? Let's suppose this came from some element E i, okay? All right? Now, let's suppose that in element E i, the A bar global degree of freedom corresponded to the local A degree of freedom. And the B bar global degree of freedom corresponded to the local B degree of freedom. Okay? In element E i. In that contribution, K A B from element E i, right? From the local conductivity matrix of element E i, would show up in the global A bar, B bar position. If in a different element A bar and B bar also showed up as local degrees of freedom. Let's suppose, an element is subject. Okay? And let's suppose, here, in element e j, the global a bar degree of freedom corresponded to the local c degree of freedom. All right? And the global B bar degree of freedom corresponded to the local d degree of freedom. That's where they would show up. Okay, so that would be an off diagonal contribution, okay, from different elements. And, and of course, there could be different elements in here, right? In general, contributing to that, to, to the global A bar, B bar, component of the Stephanos matrix. Okay? So this would, of course, continue. Right, and you would, you would essentially populate the entire Stephanos matrix by doing this. But note in particular that once you have the mesh connectivity information and you have the, the, the, the conductivity, the, the local element conductivity matrix from each element. It's actually a fairly simple matter to slot them into their global positions, their global conductivity matrix, okay? All right, and essentially the same sort of approach holds for the F bar internal, and also the F j, okay? All right, so let's just complete that that bit of assembly as well, okay? It's essentially likewise, right? Let's suppose I'm looking at F bar internal. The global matrix. Sorry, the global vector. Okay. It'll be this, create long vector, right? Because it has all the global degrees of freedom in it. And for essentially, for the same sort of situation that we had, let's suppose that we were looking again at the A bar contribution to this, right? And the A bar would show up maybe somewhere along here. Okay? So now this would be fairly straightforward. What we would do is we would look at F bar internal from elements e1 to. All right? Okay, and now this is fairly straightforward, right? For element e1, we know that the global A bar degree of freedom corresponded to the seventh local degree of freedom. For e2, it was the eighth. For e3, it was the fifth. For e4, it was the sixth, e5, third, e6, fourth, e7, first, and e8, second, okay? That's how things would add up, right? And of course for any other global degree of freedom for B bar actually, if I just continue with the sort of approach that we'd used previously. What we, for, for the global conductivity matrix this is what we would get. We get F bar e i, B plus F bar E J D, okay? All right, and this would be because the, we concluded that, or rather we set it up, so I said the B bar global degree of freedom corresponds in element e i to the B local degree of freedom and in element e g to the d local degree of freedom. Okay? And you will do the same thing for, for, for all for all the other entries, all right? And essentially the same thing with F bar. Sorry, not F bar. Here. Essentially the same thing with our F j global vector, which represents the contributions from the tractions. Okay? All right? If, again, let's just do it for B. For global degree of freedom, B bar, right? So b bar and the global degree of freedom, were, lucky enough to find itself, on, on one of the Neumann degrees of freedom, right? The degrees of freedom in which we have no known boundary conditions specified, then we would have here F j element e i. Contribution from local node B plus F e j contribution from local node, D. Just note that the j that I'm using here is just a superscript to indicate towards that it that this contribution to the forcing comes from the Neumann boundary condition that we're denoting as j, with that we're writing as jn, okay? So the same thing would carry on, okay? All right, so that's how you would assemble these global matrices and vectors that go into our global finite element weak form, right? In, in matrix vector form."
frxjuCGpVck,"There was a minor error in board work on this slide. It's something that you may have picked up yourself, and it occurred right here. I used ce transpose on the right-hand side of this equation. As you would probably have realized by just examining the equation, that can't be ce transpose, especially because everywhere else in that equation, we have c transpose itself. So. What I'd written here as ce transpose, should be just c transpose. And I can do it by just crossing out that e, okay? Now, when you look at the equation, we have c transpose on the left-hand side. As well as on the right-hand side, and that equation is consistent."
yUAs8V1t1IQ,"So, let's return, then, to what we had for our global equations, right? So we return. To c transpose k bar d bar equals c transpose F bar internal plus c transpose F j, all right? On the previous slide we looked at how we use our mesh connectivity information to construct these global matrices k bar and global vectors F F bar internal and F j. The next thing we do here is account for Dirichlet boundary conditions, okay? Now, suppose. That, global degrees of freedom. Right? A bar, sorry, B bar. And so on, right, belong to a set that I'm going to call A sub D, right? Or maybe I should be calling this A bar sub D, okay. Which is the set of global degrees of freedom on which we have Dirichlet boundary conditions specified, okay. This is the set of global degrees of freedom on which Dirichlet boundary conditions are specified. Okay, well we know that if that if A is the local degree of freedom in some element, if A is the local degree of freedom in element. Say e i, 'kay, corresponding. To global degree of freedom, say A bar belonging to the set, right, the set of Dirichlet degrees of freedom globally, right? If this is the case, then we know that in that element, w h sub e i, right. Is constructed as a sum B equals 1 to number of nodes in the element. B not equal to A, NB CB e, all right? This we know very well, right? Okay, which essentially corresponds to the fact that when we look at the right, we, we see that this implies that now when we look at the c vector, c transpose, right. Right, it will have you know, c whatever, c D bar c E bar, right? Corresponding to various global degrees of freedom, except that what will happen is that there will, there will not be, you know, sorry. It will, it will just go on right length, right, like this, right, F bar and so on. What we'll, what we'll be missing here is A bar degree of freedom will be missing, okay. All right. And this will hold for all degrees of freedom that have Dirichlet boundary conditions specified on them, all right. So if this is so, we know that now the dimension of c transpose is going to be somewhat reduced, right? So, now if the measure of set A bar D, right? Which is, which in this case is just the number of degrees of freedom belonging to this set, right? If measure of A bar D, which is number of degrees of freedom belonging to A bar D, right? If this is equal to ND, okay, what we know is that we can say something about the size of c transpose, right? What we are seeing, then, is c transpose is a row vector of dimensions the following, right? Number of spatial dimensions times total number of nodes in the problem, right, minus N sub D, right? This is the dimensionality of the row vector c transpose, okay? But our vector d, our global vector d, so your global vector d bar, okay? As we've written it in our global vector matrix equations has all number of spatial dimensions, n sub s d, times number of nodes. Okay, of course number of spatial dimensions because you are developing a problem in three dimensions, is 3, okay? All right, but then, we also observe that d bar consists of entries which are the form d1, d2 and so on, okay. But now let me see. We said a bar and d bar belong to the set of global degrees of freedom in which Dirichlet boundary conditions have been specified, right? So when we come to d A bar, d B bar, right, going all the way down to d n s d times number of nodes, right? What do we know about d A bar and d B bar? We know them, right? These are known, okay? So, what this also tells us is that our global matrix K bar has dimensions, number of spatial dimensions times number of nodes in the problem minus the script, N D, right, those many rows, okay. Times the full number of columns, okay? So K bar is not a square, is not a square ma, matrix, right? It, it, it's a rectangular matrix, okay? Right, so the, the situation that we have now is in c transpose K bar d bar equals c transpose F bar internal plus c transpose Fj, right? Well, some of the degrees of freedom in d bar are already known, right? >> Hm. >> Those are the ones that are known, okay? So what we do is we simply move those, you know, the columns from the K bar matrix, right? That correspond to the known degrees of freedom from the d bar matrix, sorry, from the d bar array over to the right-hand side, okay? So what we will do now is, is the following, right? So what we say is that c transpose times a reduced matrix, K, times a reduced vector d, okay. Equals, c transpose F bar internal, plus F j, minus c transpose times K bar. Now, for K bar you will have the A column for the A bar column, all right, times the dA bar degree of freedom. The dA bar degree of freedom here is just a scalar, all right? Likewise, you will also have c transpose, times the K bar B bar column, times the scalar degree of freedom, d B bar, okay? Now you specify that this as well as that are A bar and B bar columns of. The full K bar matrix, okay? Likewise, the d matrix now, is obtained by, removing. A bar and B bar degrees of freedom, right? And any others, degrees of freedom from d bar, okay? All right, now this is done for all A bar, B bar. Belonging to our set, A bar D, okay? All right, so what are we left with then? We are left with a system, right, in which we have a system of the form c transpose K d, right? Where the dimensions of K now is N s d times number of nodes minus the size of this set of Dirichlet degrees of freedom, essentially squared, 'kay? And so we'll be using notation a little here. And of course c transpose and d are vectors, also, of the size N s d times number of nodes minus N D, right? Okay, this minus F bar internal, minus F j, minus K bar, the A bar column of the original full K bar matrix. Multiplying d A bar minus the B bar column of the original rectangular matrix, K bar. Multiplying the d B bar degree of freedom, okay? All right. And this entire vector, now, right, also, is a dimension, N sd times number of nodes minus N D, right? It has to be, right? And, indeed, the way we've set up things, it is that, right. It does have that dimension, okay. So all of this has to be equal to 0, all right. All right, so then, this has to be equal to 0, and this has to be equal to 0 for all c. Sorry. All right, let me write it down here. This has to hold for all c belonging to R with that dimension, right? Number of spatial dimensions times number of nodes, minus ND, okay? And you remember where that requirement comes from, all right. It comes from the requirement that c degrees of freedom are the ones that interpolate our weighting function, okay? Since this must hold for all c transpose in that space, the only way this can work out is if we have K d equals everything on the right-hand side, everything, that makes up, that vector, right. F bar internal minus F j, minus K bar A bar d A bar, minus K bar B bar and d B bar, okay? And now, of course, we're going to call all of this our final forcing vector, okay? So, we've arrived at the same matrix vector form of our final finite element equations, right? K d equals F for linear problems, okay? So this is our final matrix vector equation. All right, we are going to stop this segment here."
1G6uOV5SY6c,"There were a couple of errors in sign that crept into this slide. The first error appears at the matrix vector equation at the top of the slide and the error, errors are that both of these last two signs should be plus. And then, when you come to the very last equation, this one, which, as you will recall, was obtained by just moving the terms that are braced. This equation to the right hand side. When we do that, everything is correct, except for this term, which should also be plus. With that the equations are all consistent and everything works out just right."
lb5gmDZbPD4,"Welcome back. Today we are going to start and hopefully get through most of a new unit. And that unit is going to address basis function in an in, in an, in, in a unified manner, okay? The idea is that the most standard basis functions, which are based upon Lagrange polynomials. Are constructed in two and three dimensions. Essentially through a process that is called a tensor product, representation. Right? And this is done by first constructing basis functions in 1 D and then essentially extending them to multiple dimensions. So, we're going to take that approach, and, and, and doing so present a hopefully unified picture of basis functions that we have already been using. So, all right. So we're going to look at a I call it just that, we're going to look at a unified view Of basis functions. In 1 to 3 dimensions. All right. Okay. The the approach is to start with 1 D and and, and work up from there. I should also mention that we'll start the segment by looking at this approach for Lagrange polynomials. Okay, so we're going to look at Lagrange polynomials and let's start by recalling 1 D right. In 1 D let's look at what our linear basis functions look like. And that's our element. In 1 D, we're talking about linears, so this is what we have. Of course, we construct everything in our bi-unit domain, okay? So, in this bi-unit domain we have C. I'm going to call it C 1, just to prepare ourselves for smoothly moving on to two and three dimensions, okay? So this is C 1 equals minus 1. C 1 equals 1. OK, and of course this is 0. All right? In this setting we know very well what our linear functions are. Okay. That's meant to be linear okay? So this is N 1, and we have N 2 Okay and of course they satisfy the usual properties Kronecker delta and the fact that they evaluate to one when added up at any point C 1 in the domain, okay? These are linears and you recall how they were written? They were written as N 1. Of C 1 equals 1 minus C over 2, and N 2 C 1 equals one plus C 1 over 2. Okay and then we went on to develop quadratics. Right. We have our element. C 1 is minus 1. C 1 is 0. C 1 is 1. Okay? And in this case. Our first basis function looks like This. Right? That is N 1. N 2 looks like. That, right. That's N 2 and N 3 as you will recall is that. Okay. Allright, and the expressions for them were N 1 function of xi 1 is 1 minus xi 1 times xi 1 over 2. N 2 equals 1 minus. C 1 squared, it's just one minus, C 1 squared, and then N 3, is 1 plus C 1 times C 1, divided by two. Okay all right, so we know these ve, well enough, and, the reason I put them down here is just to get us started. You recall that when we developed the finite element formulation in one d, we also presented the general Lagrange polynomial formula. Right, for, arbitrary order, right? And that formula was the following, right? So. Right? So when we develop Lagrange polynomials of order. So we going to write this out for Lagrange polynomials of order k Right? And you will recall from our treatment of problems up to now and actually from just looking at this at the sketches of our basis functions, that the order of the polynomials bears a relation to the number of degrees of freedom on the element, right? And what is that relation? Relation that the, that the order of the polynomial is the number of degrees of freedom minus 1. So, we're going to write that out. And, we, we previously called this number of nodes in the element. I'm going to use that sort of, idea. Except, I'm going to call this number of nodes in 1 D. Okay. So it's number of nodes in 1 D minus 1, which gives us the order of the polynomial. The reason I am bringing in this reference to 1 D is as you will see, we will use the one d construction to build our constructions in two and three dimensions, okay? All right. So so the polynomials of order k are the following, right? So we know by looking that at, at, at what we've written out there that we can, we have N A, right, function of C 1. Equals the product index b, running from 1 to number of nodes in 1 D, okay? Except that B. Is not equal to A. Okay. Now the product that we need to have here is the following, C 1 plus C 1 at A, divided by Z 1 at A, minus Z 1 at B. All right, and if you test this out for any of these functions I put up there for linears of quadratics, you should see that it works. Okay? So this is the setting we have. Now, we are going to generalize this right? Essentially what we are going to see and what we have seen already is that when we go to two and three dimensions, and we did look at two dimensions briefly, we're going to see that we essentially construct those bi-linear and tri-linear or bi-quadratic and tri-quadratic functions as we will also see, by forming what we call tensor products of these functions in 1 D. Okay, also what I'm going to do is that because I'm going to now use this to construct our functions in, in higher dimensions, I am going to put a tilder on everything I have written here. Okay? This is just so that I can use N itself for the actual functions in higher dimensions. Okay? Same function. I'm just putting tildes on all of it. All right, so this is what we have. So let's go, let's move on now to tensor product functions. In let's look at 2 D. Okay. Now, in 2 D, we already did see the bi-linear shape functions. Okay, so recall the bi-linears All right, the bi-linears were the following. I'll sketch one of them all right? So because we want to make use of this perspective view. C 1, C 2, okay? And we label the nodes here. Or, degrees of freedom here as A equals 1, 2, 3, 4. And, I will draw just a, and 4. Okay? And you recall that is bi-linear, so it, it, it slopes down linearly to 0 along each coordinate direction, okay? And, it also slopes down to 0 in this dimension, all right? And this dependents actually, not quite linear but looks something like that. Okay? All right, so, so this is N 4. Okay so this shape, this basis function I am showing you is N 4. Okay. And, okay, so this is what we have. I'm going to directly just write out the, the, general Lagrange polynomial formula using the stance of product, using the denser product approach. Okay. In order to do this let's note that the kind of bi-linear shape function we are constructing here has n n e degrees of freedom. Right, we're constructing four basis functions because, because this element has four degrees of freedom. And in this case, n n e equals 4. Okay? Note of course that n n e equals 4 is a number of nodes in one dimension squared for, the linear case, right? So if you take the linear basis functions, right? And well, the, the two more degrees of freedom, the bi-linears in for, for the 2 D case involve four degrees of freedom, right? So that's n n 1 D squared. What we are going to do here is, is observe that we always have some NA, Z 1, Z 2, right, are functional Z 1 and Z 2, is going to be written as a product. Of N tilde B. C 1 times N tilde C, C 2, okay? For B comma C equals 1 up to number of nodes in 1 D. Okay? This is the general formula, alright. And A of course here, A which is on the left-hand side equals 1, 2, number of nodes in the element in 2 D. Okay. It's really as simple as that. Right, and it is, and, and this sort of formula where we're really taking those basis functions we developed on the previous slide that we labeled N tilde, right? We're taking them in each dimension, right? Along each of the dimensions the, the C 1 and C 2 dimensions and simply forming a product with it, right? This sort of a product is what we mean by the denser product formula. Okay, with this sort of view, what we observe now is that N 1, C 1, C 2 is a product of N 1 tilde. C 1 N, I'm sorry, N tilde and N It's a product of N, N 1 tilde in the C 1 direction and and, two, two three four. N 1 tilde again in the C 2 direction. All right. And that becomes clear by just looking at it. So this goes on. Well actually let me write all of them. N 2. N 2 is what? In the N 1 for, in the C 1 direction. Look C 1 dimension it is N 2 tilde C 1, N 1 tilde C 2, right? This goes on and we finally write N 4, okay? N 4, C 1. C 2 is, can you work it out? It's N 2 tilde, sorry, it's N 1 tilde again. N 1 tilde, C 1. And. 2 tilde. C2. Okay, it's as simple as that. Right? Okay and you recall we did indeed look at the bi-linear case, right? And now if, when we go on to higher dim, when we go on to basis functions of higher order, all we need to do is to go back to the polynomial sorry, to the 1 D. Construction. And, figure out what order we need to order. And it send the order in 1 D that we need in order to construct these functions in 2 D is simply the square root of the degrees of freedom that we want to have, right, in 2 D. Okay, so we construct those 1 D basis functions, and then form the sort of products with it."
MNonArg5EZs,"So I introduced a couple of errors in the formulas on the segment, and both errors appear in the very first slide. So, if we go to the bottom of the slide, and look at the numerator here. If you try to evaluate it and see whether it gave you the sorts of basis functions you'd expect, it doesn't because it's not right. Instead of C1 plus C1 sub A, we should have C1 minus C one sub B. The numerator's all right, but with this correction things work out. However, if you look to the right, there is another error. And that is fixed by just introducing a minor sign on the right hand side of the expression for N1 tilde. With those corrections, everything should work out."
Pu-E1i2S3Ro,"And now you immediately know how we are going to for our basic functions in 3D, right? Already written about previously, you written out our trials. At three forward to just write out the formula now, all right? So we say likewise. In 3D, right? In this case we have number of nodes in the element equals number of nodes in 1D cubed, okay so it won't definitely be constructed down the trilineals we observed that we had eight degrees of freedom. Right? Two cubed. Okay? So it is the same approach. So, we have you recall that the tri-lineals, right? The tri-lineals were. Right, for the trials and errors we had this sort of element. In the by unit domain. Okay. Right. So in the setting once again, we simply have N A phi 1 xi2, xi3 equals N-tilde B C1, N-tilde C C2 N to the D C3, right? Where B, C, and D belong to 1 up to number of nodes in the 1D element. And A belongs to the set 1 up to number of nodes in the element. Okay? So it's completely straightforward to see how we construct these, and then it's just a matter of figuring out the numbering. Okay? The numbering in this case, of course, is A = 1, 2, 3, 4, 5, 6, 7, 8. Okay, if we were to go to triquadratics, there are various numbering systems that I use, depending upon the particular preference of the people consulting these functions. But typically for traffics, let me show you what sort of numbering is followed. Okay, and we're going to draw that. Again, and I'll try to draw it a little bigger. Okay. So these would be our vertex nodes. Or vertex degrees of freedom. I'll switch to a different color to draw the mid-side nodes. The mid-side nodes would be these ones. Okay? And then there are the mid-face nodes, which are these. Let's see, one, two, three. I guess I need to have one on and that sort of does it okay. And there's one final which is a mid body node for which I'm sort of running out of colors here, but maybe I'll use black. Okay, this one would be in the very center of the cube. All right so the numbering that's typically used here is the following. Often the vertex nodes are numbered one through eight. And then the mid-side nodes are numbered next. Sorry, I began numbering the, some of the mid face nodes instead. So you have nine, ten, eleven, twelve. Need to erase this one, as well. Okay, 9, 10, 11, 12, actually, maybe I won't use that number in system. What I'm going to do, instead, is this one, 1, 2, 3, 4, 5, 6, 7, 8. Okay. Okay, so I have one, two, three, four. Sorry. One, two, three, four, five, six. Seven, eight, nine, and then continuing on, we get 10, 11, 12, 13, 14, 15, 16, 17, 18 and we get the mid body node. 19, 20, 21, 22, 23, 24, 25, 26, and 27, and with the mid face node on the top. Okay. So there we have it and other numbering schemes are possible, now what one has to do in constructing this triquadratic basis functions is pick any one of these nodes. So let's suppose we decide to construct a basis function for node five, all right a degree of freedom five. Okay, all we have to do here is observe that that basis function could be constructed from the n third of By choosing for the xi1 direction, something, something else for the C2 direction, and something for the C3 direction. What I'll do now is to come back and say which ones. All right, let's look at this. The xi1 direction is this. That is the C2 direction, and that is C3. All right, so that's now, that is the five, right? That is the basic function we want to control. So, in the C1 direction, right? When we look back at the way we number, the basis functions in 1D. And you see in one direction we are using the N2, N2 tilde. Okay, in the C2 direction we are using N1 tilde and in the C3 direction also we're using N1 tilde. Okay? So that's the sort of check one would make in writing out these basic functions in three dimensions, and of course the same approach would be used in 2D as well. Okay? The nice thing to observe is that it is just the very simple product formula that one has to use. So we construct these basic functions and then generalize them to two and three dimensions. Of course, this works out as long as we are looking at elements that exploit the product instruction. Right? So it is indeed that when one can go from simple 1D segment in 1D to quadrilaterals in 2D and hexahedron in 3D. Okay. In a couple of segments we will look at a variation on that idea. All right, but we'll end this segment here."
uIRkSoONw_g,"Hi. In this segment, we'll begin looking at the coding template for the second coding assignment which is homework three. All right? So, to begin with, let's look at the source file, main.cc. All right? Now, I've actually included two sets. We have a source files and header files for the 2D problem and for the 3D problem. Okay? So, let's look at the 2D problem first. You can see, it's basically the same structure as the previous homework assignment. One difference is that now I'm setting the dimension equal to 2, which we're then inputting as the template parameter for our finite element method class. The other difference is that, when we're generating the mesh, instead of passing in a single integer, since it's 2D, we're passing in a vector with two integers that defines the number of elements in each direction. So here, the name of the vector is num of elements, or num_of_elems. And for an example, I've set up four elements in the x direction and eight elements in the y direction. A four by eight element mesh. Of course, you can change that as you're doing your programming, but that's the set up. Okay? Other than that, it's pretty much the same. We have generate mesh, set up system, assemble system, solve and output results. We aren't calculating the L2-norm of these problems. It's just you can, you can do it for a 2D problem, if you have the exact solution. However, for the problems we give you, we aren't giving you the exact solutions as well. So, you don't need to bother calculating the L2-norm. If you did however, you would still be able to look at the rate of convergence as you increase the number of elements. Another difference or two other differences are that in this case, our constructor for problem object for a finite element class. We don't have any constructor. We don't have any inputs to the constructor because we are only doing linear basis function, so we don't have to input the function order. And since they've already split up the problem into a 2D and a 3D problem, there aren't any further sub-problems after that. There's only one 2D problem, and one 3D problem on this homework assignment. So, there are no inputs to the constructor, okay? So, let's go over to the template. This is FEM homework 3a template. We have the same header files here. Again, using namespace deal two. And if we scroll down to the declaration of the functions and objects, let's look at that. All right. You'll notice that I didn't include the function C at node, whatever. Because since we're doing linear basis functions, it's a lot easier to keep track of what the value of C is at, whatever node you happen to be at, okay? So I deleted that. If it was useful to you, you're free to go ahead and write such a function yourself. Now let's look at basis function and basis gradient. There are a couple differences here. First off, we have more inputs. We still have the node number, but now we have C1 and C2. Obviously we have two Coordinates since it's a 2D problem. Also, with basis gradient instead of returning just a double, since we are 2D the gradient is a 2D vector. And so I'm returning the standard vector of doubles. Okay? The solution steps however are the same generate mesh to find boundary condition setup system. Assemble systems solve an output results. Except of course that generate mesh accepts as a vector of integers instead of just the single integer. The D02 Class objects are the same. The Gaussian quadrature rule is the same. All the data structures in solutioning those are all the same in this assignment. All right, so let's scroll down here, to the constructor. Again we still have the same method of calling the constructor of our class variables fe and dof_handler just as before. But since we don't have problem and order as an input, we don't need to worry about storing those. We have the same destructors before as well. All right? Moving on to the basis function and the basis gradient. Again, we talked about the differences in input, where I have node xi_1 and xi_2 node. But you'll still do it in the same way. And probably in this case it wouldn't make sense to do a generalized functional, though you can certainly go ahead and do that. Since it's a lot simpler. Since we're just doing linear basis functions and that end. So we don't necessarily have so many notes as before. You can just use a simple if statements. If the note equals such and such. Evaluate the basis functions using C1 and C2. All right? And then again, you'll return value. Similar with basis gradient. You notice here, when I declared this standard vector values, which is what will be returned, I've already set the length, or the size of values to be dim. Of course, dim we've defined in main.cc, we've defined it to be two in this case. So this creates a vector of length two. If it initialized the contents to be 0, of course, they would have been 0 automatically, anyway. But you will need to go ahead and evaluate the basis gradients at the given nodes for both components of this gradient vector. All right? In some ways, simpler than before because we're only dealing with linear basis functions or rather, bilinear basis functions I should say, because it's 2D. But of course since they are in two directions, that is something else for you to consider. All right? At this point I should point out that DL2 again had some different node numbering from the class. All right? So let's go to the board here. In the class, or rather in the lectures.  We went around the nodes, since we're only dealing with bilinear, we only have nodes that have the vertices here. But there's something like this, one, two, three, four. All right? In deal two, it's a little bit different. Again as before as with the linear case. We'll be starting at 0 in our numbering but we're always going left to right. We go left to right, bottom to top. So 0, 1 and then up here it's 2, 3. Okay, so that's something for you to consider in your if statements. What would've been basis function one according to the lectures is now basis function zero according to DL2. What would've been basis function two according to the lectures is now basis function one according to DL2 and so on. Okay. Okay, so in this case remember we need to use the DL2 basis function node numbering. All right? Because we are using D02s mesh. We're using D02s connectivity, matrices, and so on. So remember to use D02s known numbering as you're creating your basis function and the gradients of the basis function. All right. So moving on to generate mesh. Again, it's very simple, there's not much for you to do. You just have to define the limits of your domain, the left and right, top and bottom. So x min, x max, y min, y max. And again, we'll use DL2s functions to create that mesh. So not a lot for you to edit there. And now we come to define boundary conditions. Again, this is defining the Dirichlet boundary conditions. In this homework, homework three, you won't have any knowing boundary conditions, and we actually don't even have a forcing function or a body force at all. All right, sorry, in this case a forcing function since it's a temperature problem. And so these Dirichlet boundary conditions are the only Dirichlet, are the only boundary conditions we're applying. Of course there are Neumann boundary conditions of zero, which we don't have to do anything about in our coding. Now I have, you actually have to create this function yourself but it's going to be very, very similar to this same function, to find conditions in the previous homework, alright. You'll be looping over all of your notes and you'll have an if statement to check to see what boundary you're on. Now the difference is, that now you have both x and y coordinates. And so, you'll need to check. In order to check your boundary out, you'll have to specify, are you checking the x value, or the y value? The other difference comes in that, node locations whereas before it was a vector, now it's a table. Let me scroll back up to the top and so you can see, table is a DL2 function or a DL2 data object. And the two here just specifies that it's a two dimensional table. All right? And it holds doubles, okay? So let me just sketch out what that would look like. Two dimensional table, the row index, is going to be the global degree of the global node number. So it's just like the index and the vector of node location before. So this will go up to, just the largest node number, global node number. Okay? So that specifies which node you're at. The column just tells you which component of the position looking at, you want to know. Really it's telling you, do you want the x coordinate or the y coordinate. Okay? So for example if this is our mesh. And I'm actually, not exactly sure how DL2 does the global node numbering in 2D. So I'll just do it this way, but again, DL2 does it in this, and they do it in a different way, so. Anytime you want the global node number, you'll actually need to use local DUF indices to go from a local to a global node number. But let's say the value here is, say this coordinate is 0,0 and just for simplicity I'll make this 1,0, 2,0. This would be 2,1. This would be 2,2. And so on. Okay, so this table over here would look like this. At Node zero, the x-coordinate is zero, the y-coordinate is zero. At Node one, x is one, y is zero. At Node two, x is two, y is zero again. If we come down to Node five, for example, x is still two, the y has a value of one and so on. Okay. So if I wanted the x coordinate at node five, I would use node location. At node five. If I wanted the X location I would take 0. And then that would return to me 2. Okay? So again, this is the global node number. And the second component, the row, or sorry, the column number, is the component. Again, essentially X or Y or in the case of the 3D problem that we'll look at, Z. Okay? All right. So with that, you should be able to fill in this defined boundary conditions function pretty well. Now, let's stop there in this segment and in the next segment, we'll move on to looking at setup system and assemble system."
jrrrApF4s_0,"Alright, we'll continue with our development of basis functions in multiple dimensions. What we did in the previous segment was to, observe that the stensor product of idea allows us to very conveniently construct, the higher order, the functions in, in higher dimensions, using the functions in one D. Okay, in order to continue and, and provide us with, ourselves with everything we need to complete formulations in two and three dimensions, there is one thing we haven't yet done. Even for our 3D formulation of the linear elliptic PDE for the scaler variable, and that thing is integration. All right, we haven't talked about how we do numerical integration in multiple dimensions, all right, that's what we will aim to do in this segment. So the topic of this segment is numerical integration. N again. One through three dimensions. Well, let me write out the word. The approach we will use is conceptually similar to what we did in previous segment. Right? Side of forming the extensive product formulas. Okay, and to start out let's go back to one d. Okay, in one d this sort of situation we are confronted with is the following. We need to integrate from minus one to one. Some function, say G of C1, D, C1. Okay. Every single integral that we need to evaluate in our finite element formation reduces to this. Okay. And of course this is the form that's applicable to every component of, of a matrix. Whether it's our stiffness matrix, or conductivity matrix, or, or our forcing function. And so on. Okay? And you recall how we did this in one d? Right? How did we go about doing numerical integration in one d? Remember? Yes we did it with numerical quadrature, right? Which basically says that okay let me just do a sum instead of the integral. Okay? I'm going to do a sum with l going from one to number of integration points Okay? I am going to evaluate g xi 1 at all these different values of l, all right? And once I do that. I am going to multiply each of those function evaluations with the, a weight, w L, okay? So, what we have here are quadrature points. P-T is short for point and that is a weight. Okay, that's the general approach for numerical quadrature, of course we use Gaussian quadrature. Okay? And Garrison Quarters said that well if you choose to do it with a single integration point. Okay? You would pick c l equals zero and w l equals two. Right. Okay and the reason for that weight of w l equals two is something that we understood from how we would integrate constants. All right? And this went on, right? So, for instance, if you went as far as n int equals 3. What we would see, it. What we would observe is that we would get sorry. I should have paid attention to my notation here, this will be c one, here right? Okay. So if we were doing an n equals three, then c one, one at n equals one would be minus square root of three over five. W one would be, five over nine. C one two would, by symmetry, be the point zero. W two would be eight over nine. And, c one three would be the point, root three over five. And the weight, you may remember, or you may figure it out from symmetry, would, again, be five over nine. Okay? And we saw that you know, Gaussian quadrature we said that a rule of that n int rule, right, or n int point rule integrates.  A polynomial of order two, n, int, minus one exactly. Okay? So, we'll recall all this. What we're going to see is, that, when we go to multiple dimensions, it is essentially a tens of product idea. Right? And already, tens of product rule that calls on this basic idea. All right. So let's go ahead and see how that works out. Okay. All right, so in 2D. In 2D as you may imagine you need to integrate terms of the following type. Need to integrate, okay? Minus 1 to 1. Minus 1 to 1. G, generally a function of c1 and c2. d c 1 d c 2. Okay, and these limits, let's say, for c 2, and this is for c 1. All right. It's really very straightforward. Let's suppose we first do the integral over c 1, okay, and we're going to do numerical quadrature, okay? So, right, so I have numerical integration or quadrature. Right? So what numerical integration says is that well I can just break this up. I can write this as an integral c2 equals minus 1 to 1 and now for the inner integral. Right? Over c1 I will first use quadrature. Okay? So what this says is that I'm now going to talk of doing a sum let's say L1 equals to number of integration points along that dimension. Okay? All right, so we have L1 to number of integration points. We have G, C-1, L-1. Alright, you pick quadricant points only, along the C-1 coordinate. And, you leave the C-2 as the continuous coordinate. All right. So, you, you retain, therefore, the elemental d x e 2, okay? Actually, let me write that. We're first doing the integration for expectancy 1, right? So then, we just multiply this by w and 1, okay? So, what I have here in parentheses is the result of having it integrated over C 1. Right, yah we did numerical integration over at c 1, but nevertheless there we have it. Okay, d c 2. Right. And then we come back and do the same thing over at c 2. All right. So numerical integration would give us this, and then as another step we would get now Sum l 2 equals 1 to n int. Sum l 1 equals 1 to n int. G c l one. Sorry. C 1 l 1, c 2 l 2, w l 1. Wl2, and we're done. Okay? Now, of course, this is, based on an assumption of symmetry in c1 and in c2, right? Symmetry in the sense that we are, considering the case where we have a polynomial of, perhaps, the same order in c1 and c2. All right, that's why we've chosen the same number of integration points along the two directions. Okay, of course, this does not have to be the case, right. We are always free to choose different number of integration points in the different directions. Right, if we know something about our polygon, right? So I, I'm going to make this more particular by changing this to n one int. Right, and this to n 2 right, it just says that we can use different number of integration points in the different directions, right, so the remark here Can use different number of integration points. Along c1 and c2. Right, if you know something about the polynomials that we are working with, okay. And of course we would be, we could use Gaussian quadrature in each direction, right? And everything would work out. The, the integration points along c 1 and c 2 would be the same as the Gaussian quadrature points and so would the weights, all right? All right okay, and here, what you will see here, of course, I'm not, I'm not going to write it down, it's something that you can conclude easily by going back and looking at the formulas. What you will see is that, in, in the two d case, the weights add up to what? Consider what would happen if you had to integrate a constant. Okay, so what happens here is that I just write it out as a sentence, here. Sum of. Weights. Equals 4. Okay and this comes simply because we're integrating over bi-unit domain, right? And the area of the bi-unit square in 2D is of course 4. Okay, so that's where that's from. Alright, now we can extend this to 3D, right? And this would be completely clear how to do it in 3D. Right? In 3D we're trying to integrate something of this form, right? We're trying integrate c 3 equals minus 1 to 1. C 2 equals minus 1 to 1. C1 equal minus one to one G of C1 C2 C3, D c1, D c 2 d c 3. All right? I'm straight away going to just write out the formula. Our numerical quadrature formula is sum l three equals one to n three int, right? Number of integration points in that c 3 direction. L-2 equals one to number of integration points in the, C-2 direction. L-1 equals one to number of integration points in the C-1 direction. G, C-1. l 1 xi 2 l, sorry, xi 2 l 2, xi 3 l 3, right? Times. wL1, wL2, wL3. Okay? And again, we could be using Gaussian quadrature points and weights where the points are again simply determined as the For the points as chosen in the one d case. So this is how we would construct our, we would, we would actually evaluate all our, our integrals. Alright, so, so this is for the generally case, sorry, sorry, not the generally case but the case where we are looking at either a bi unit domain in one d. Two d or in 3 d. Okay, and that bi unitness is reflected in the limits on these integrals. All right. We'll end this segment here. When we come back, we will look at a, different type of basis function."
LVbv05-l-0Y,"Hi, now we'll move on to looking at set-up system and then assemble system. And we'll probably finish up with the homework 3 template in 2D in this segment. All right, so lets look at the code. Here in setup system it looks very, very similar to problem one, or rather to homework two, which was our first coding assignment. Slight differences here, is that no location, again we have two indices. Again the first is your global node number, just as before. But again, the column number now is which component of the position vector you want. Do you want the x-component or the y-component, or in 3D, the z-component? Okay, we again call the define boundary conditions function. Resize our vectors and matrices, we define the quadrature rule. Again, you'll need to decide what quadrature rule you need. Is two enough, do you need three? Or more, or less? Not a lot to do here in setup system, not much different from before. But now we'll move into assemble system Okay we have the same setup before k=0, f=0, same constants, data types. We set up our element loop the same way. Again we update the local DOF indices vector with element. So that we can again relate our local degree of freedom numbering, or local node numbering with the global node number. But, now we have introduced these other objects. We have a full matrix called Jacobian, and then a double detJ, which will hold the determinant of the Jacobian. The Jacobian you can see is of dimension dim by dim, or in this case a two by two matrix. It's the Jacobian for the mapping between the real domain and the bi-unit domain. Remember, in the 1D case we actually did have this Jacobian, but it was just a 1 by 1 matrix. Essentially was H, E over two. All right, so that's how that showed up there. In this case we have to create the matrix itself. Now in the general case, elements aren't always going to be the same. And they aren't always going to be square. Or have nice right angle corners like they are in this problem. And so, I've kept the Jacobian general for that case. However in our case, as you're checking your code you may notice that the Jacobian is actually the same at every quadrature point in every element. And that it's actually a diagonal matrix. That's because of the uniformity of our mesh. Okay, but in general with an unstructured mesh, you wouldn't have the Jacobian being the same at each quadrature point. And it wouldn't necessarily be a diagonal matrix. So on this first part, I have the same structure for defining Flocal. Again, we don't have Neumann boundary conditions. Or at least, we don't have non-zero Neumann boundary conditions. And we don't have any forcing function. So Flocal is actually zero for this problem. However I've left this structure there in case you wanted to include a forcing function. So it's there for you if you'd like. Again, all the structures there you would still have to define what the eighth component of Flocal was. Just as you did in the 1D problem. All right. Now for Klocal, I also created a full matrix called in the Jacobian, which will store the inverse of the Jacobian. I'll go into that a little bit more on the board in a second. I've also created this full matrix Kappa, which will be our connectivity tensor. In the assignment, it's already defined for you that it's a diagonal matrix with the value of 385. Okay so now let's look at K local itself. And actually before we get into that. Again Klocal used Jacobean, uses the determent of the Jacobian. Which actually comes into play with the quadrature, numerical quadrature. And So that's where it will come into play. So let me look at the Jacobian for a second. Look at it in 2D, but it easily expands to 3D. Okay. So here's the Jacobian, it's in 2D. It's just the partial of x 1 with respect to dPsi 1. Partial of x 1 with respect to dPsi 2. Here of course x 1 is x, x 2 is y, if you're thinking of it that way. Partial of x 2 with respect to x dPsi 1, partial of x 2, with respect to dPsi 2. Another way to put a more generally. We could say that the Jacobian of  is equal to the partial of x, sub i with respect to dPsi sub j. And, of course, that easily transfers over to the 3D problem. Okay, now where do we actually need this? Well of course, we need the determinant of the Jacobian to do our numerical quadrature. Again we saw the HE over 2 and we did quadrature in 1D. The HE over 2 was the determinative of the Jacobian. So that will show up again, and we'll see that in a second. Now the other place it will show up is when we're doing our basis function gradients. Remember when we define the function basis gradient. It was the derivative with respect to dPsi, okay. Now for Klocal, let me just look at Klocal . Something to point out, first is we have this negative sign. I include the negative sign in the Klocal. So that when we're solving the actual system we can still do, D is equal to K inverse, F, okay? So, I include that, minus sign here. We have this integral over dPsi domain. In this case it's a double integral, so I'll just include that here. We have our basis function A. Now it's the derivative, I'm going to write it out here. The derivative with respect to X, I. Times kappa, I, J. And then a partial of B with respect with X,J Determinant of j. Sorry, the determinant of the j isn't necessarily with the numerical quadrature. It's with the change of domain. The change of indices from our real domain to our bi-unit domain, just to clarify that. dPsi 1, dPsi 2. And I think this notation we used in the lecture. When you have this repeated index there's an implied summation over I and J. Over I and J. And it's going from using our C++ notation would be 0 up to dim-1, okay? So in this case, I will do a summation with I equals 0,1. And a summation with J equals 0, 1. All right, but now this term here, or rather both of these terms. These are the gradients with respect to the real domain, but we only have the gradients with respect to the bi-unit domain. So if we look at this first term, of course it applies to the second as well. Partial of NA with respect to dPsi i Is equal to, using the chain rule, partial of NA with respect to dPsi 1 times the partial of dPsi 1. With respect to xi plus- Partial of Na with respect to dPsi 2. Times the partial of dPsi 2 with respect to, again, xi. So you can see that these are very close to what we have up here. Of course the little I and the big I is just an index. Except that these terms come from the inverse. So, this term here would be the Jacobian Inverse. And it would be component  for example. Okay? So that's why in the code over here, after I've created, after we found the Jacobian I. And you can see that we've done that using an interpolation here of the node locations of x, times the basis_gradients. Once we've defined the Jacobian, we come out. Notice this is all within our quadrature loop here. Okay, we are at, since it's 2-D, we're looping over the quadrule in two directions. And at each quadrature point, we evaluate the Jacobian, then take the determinant. This Jacobian.determinant, that's a function related to the DL2 full matrix class. So we can do that. And also associated with it, there's this function dot invert, so we can get our inverse Jacobean. Okay? Now this term here I could actually write as a summation as well. Let me write that on the next slide, just so we have a little more room. So the partial of A, N A with respect to X I. Actually be, the product N A with respect to dPsi. I'll do a little i. Times the partial of dPsi little i with respecting XI. And again, there is an implied summation here. It's the same as summing over I equals zero up to less than ten. Okay. Writing that summation in again wasn't necessary, because there is the repeated index here. There's an implied simation over the index i. Alright, so now if we look back at our code here, I've set up several different for loops. We have the for loops over A and B. Which of course correspond to the components of Klocal. We have a for loop over capital I and capital J. Which of course are these capital I's and capital J's. And then, a for loop over the little i and little j's. Which are the i's and j's that we saw in this notation here. But now again, you will have to decide what is integral going to be, so you'll set that up. Using, of course, the quadrature rules. Including the quadrature weights. And, of course, we're using the inverse Jacobian and the determinant of the Jacobian. Because of the bi-unit domain. Okay? You will still need to do the assemble system portion of this function. But it will be exactly the same as assembling the system in the previous homework template. We apply Dirichlet boundary conditions the exact same way DLT will do it. Solve is exactly the same as is output results. Again, the biggest portion that you'll have to be working on in this template, is again with an assemble system. And the basis functions themselves, as well as defining your boundary conditions. Okay, so that wraps it up for the 2 D template. In the next segment we will quickly go over the coding template for the 3 D problem on homework 3."
_AhH81aVr1Y,"In this next segment, we will quickly go over the coding template for the 3D problem on Homework Three. So we'll go straight to the code, there actually isn't a lot It's different. Just a few small changes. We'll start with the source file. You can see, of course, we've changed dimension to equal three as you would expect. Now the vector defining the mesh size, so the number of elements in each direction. Of course, we added on a third dimension. So in the z direction, you can specify the number of elements. So here, for this example, it's a four by eight by two element mesh in 3D. I've also included the .h file, the header file for the 3D problem. So just those three small problems. Lets go over to the template file itself. No changes in the hair file. Here in the declaration of our objects and functions, you can see that for the basis function and basis gradient. We now accept as in input C three, since we have the first dimension now. Same for both. Of course, the outputs are still the same, double and vector of doubles. And the rest of these functions, the declarations are exactly the same and the same with all these dead objects. So just that one small change in the function declarations. We'll scroll down. And the constructor, you can see is the same. I don't know if I pointed this out though in the TD problem. Here, this is one, because we're using linear basis functions always. This is one, because we're using linear baseless functions always. And this is one, because there is one degree of freedom per node. The destructor is also listening. Moving back down to the baseless functions and baseless gradient, we've already covered the difference here. It's just that you have a third input. However, again, we have to watch out for the deal to know the numbering. So let's go to the board quickly. In the lectures, we then went around in a more sequential manner. For Deal II, we follow the same method of going in the same direction. So let me write out the, get some axis here. So this is C1, C2 is going back into the screen. I think C3 is pointing up. If you were to look at just the bottom half, so just looking in the C1 and C2 plane, you would see that Deal II is consistent going from 2D to 3D. We go zero, one, two, three. You can see that's the same as the numbering in the 2D problem. Now we shift up to the plane XC3 equals 1. We just continue going left to right. So we get four, five, six and seven. Again, you'll need to use this node numbering as you're defining your basis functions and your basis gradients. And again, you can do those with the next statement or you can even set it up as a tensor product of the 1D basis functions. That's up to you.  Now again, for the basis function gradient, we are storing the value in this values vector. The declaration is the same as 2D. But again, here, dim is three instead of two. So that automatically updates for the new dimension. I scroll down to generate mesh. Again, very similar as before to what we had before, except now we have z_min and z_max that you'll be defining. The top and bottom boundaries of your domain. Defined boundary conditions will be essentially the same as the 2D problem. Note the node locations in this case is still a vector, where the row index corresponds to the global node number and the second index corresponds to the component of the vector. So is it your x, y or z. So in this case, there are three columns in this table and 0, 1, and 2 correspond to the x, y and z components of the location of each node. Setup system is exactly the same, any changes that would need to be made in setup system are taken care of by the fact that it's a template class and so we use dim a lot. For example, in this forward, we use dim. And so now, we're leaping up to three instead of two. Assemble system is again, very similar. The main difference here is that since it's 3D, we have three loops over the quadrature rule. So again, this is that sort of tensor product idea with the quadrature points. Each quadrature point, technically now becomes a 3D object. But by leaping over the 1D rule three times, we find all those combinations of quadriga points. Now remember, since we are dealing with, in the 2D problem two quadriga points and in the 3D problem three quadriga points. We also need to multiply by respectively two quadriga weights or three quadriga weights, depending on the dimension. When you're doing your numerical integration. Kappa, I've also updated to be a 3D or a three by three tensor, which we'll use in creating Klocal. However, other than that, everything else is the same. So, it should be very straightforward to go from the 2D problem to the 3D problem. The main challenge will be creating your 3D basis functions and basis function gradients. Going to 3D means you have twice as many options there. Twice as many nodes, so twice as many if statements, if you're doing it that method. So, hat should cover it for the Homework Three, coding templates both 2D and 3D."
NvlTcojBFck,"Welcome back. In order to wrap up this unit on basis functions, and as it turned out, on integration as well, I would like to say a little bit about slightly different basis functions that are still polynomials but are maybe not quite as clean and as, as the ones that we've been working with so far. Okay? So the, the, the motivation for these types of basis functions comes from the fact that, especially in two dimensions and three dimensions the quadrilateral in two dimensions and the hexahedron in three dimensions are actually not the simplest space filling figures, right. In 2D, there's a simpler figure that fills space or simpler sort of structure that fills space, and do you recall what it is? The triangle, right? And in 3D, it's a tetrahedron, right. In fact for that reason, partly for that reason they're also called Simplices in the corresponding spaces. Okay, so what we are going to look at here for completeness is what I may call, in general simplex elements. Right, and this, sort of by definition is in 2D and 3D. Okay? So let's look at 2D first, right. So what we look at here are triangular elements. Triangular elements. With linear basis functions. Okay? Now everything is exactly the same except that you know, I'll find, find element formulation and all of that is exactly the same except that, we just don't work with quadrilaterals, right. Instead, we're interested in looking at subdomains omega e which, in general, has that sort of arbitrary scalene triangle shape. Okay? So this is our omega e. We could have A equals 1, 2, and 3. Okay? And everything's exactly the same. So we have three degrees of freedom here if we you know, think of something like our heat conductivity problem, right or our mass diffusion problem, right. Essentially linear elliptic PDE in three dimensions with scaler variables, right? So we'd have in this case, have three degrees of freedom, we'd have n n e equals 3. All right, and we take the same approach that we took in the case of quadrilateral elements, right. Where this triangle is thought of as being constructed as or, or as being obtained as a mapping from a parent domain. Right. So, since it's a triangle we, we obtain it from a triangle also in this parent domain. Okay, it's also two dimensional so we have our coordinates there. As before we have c1 and c2, okay? Except well here's one of the differences. In the case of triangular elements, this parent domain is not bi-unit. It is simply a unit domain, okay? So then this is the point 0 comma 0. This is the point 1 comma 0, right. C1 equals 1, c2 equals 0, and that's the point 0 comma 1. All right? It's a unit domain. Okay? Now, all we really need to do is define the basis functions. Once we've define the basis functions, everything proceeds just as before. The way this is done is, it is conventional to label this posi, this node in the parent domain as node 1. This is node 2. And the origin is node 3. Okay, right? So we have three basis functions, right? And the way these things are set up, is that although we have a two dimensional domain and so we truly need, of course, only two coordinates in that two dimensional domain, we do go ahead, and define a third coordinate. Okay? And the third coordinate is the following, right. So what we do is also define. C3, which is simply 1 minus c1 minus c2, all right? The reason for doing this as well simply because one can, one doesn't really need to do it. One can work without it, but once one does this, the definition of basis functions becomes very convenient. Okay? When we do this, we what is done is that the, the collection of coordinates nodes c1, c2, and c3 are called are often called area coordinates. Okay? The reason is that if, I redraw this triangle here. Okay? And you recall that those are our 3 degrees of freedom, our 3 nodes and recall that these are the points 1 comma 0, 0 comma 0, and 0 comma 1, okay? And okay. If you look at let's consider c1, okay? If you now draw little strips, right. Parallel to the c1 equal zero axis, right? That c1 equals 0 axis, you remember that this is c2, and that is c1, but c1 equals 0 axis is, is, is indeed the c2 axis, right. So if you draw these little strips parallel to that axis, what you see is that as we draw large as, as we come closer to the point c1 equals 1, c2 equals 0. Right? And if we just add up the area included in all of these strips right? As we, as we move towards c1, we essentially come closer to the area of the triangle. Okay, all right. So it's as if, the area of you know at any time, if I now look at at the area. Enclosed by these strips say, all the way up to here. Okay? So area of the strip. Okay? Is is essentially equal to c1, okay. That's the basic idea here, right? Right. Sorry, it's equal to one-half c1. Right. One-half of c1. Okay. Right. So it's essentially for this reason that that we take this sort of approach. And you know, of course the area of this entire triangle is a half, and so on, that explains why we call these, why we call these coordinates also, area coordinates. Okay, but that's just a detail. The fact is it's convenient to use c1, c2, and c3."
21xRLnJcXhQ,"Okay, so with this in hand, how do we sh, how do we write out the shape function, the, the basis functions, right? So again, let me and, let me now draw this. And, again, because I'm going to draw the basis functions here, I'm going to draw this triangle. Though, it is of course, the same triangle I had on the previous slide. I'm drawing it now, in perspective view. Okay, all right, so in this in this view. Okay, I'm gon, I'm going to first write out the basis functions and then sketch them out. Remember this A equals 1, that is A equals 2, and this is A equals 3, okay? So, in that setting, we, it's, it's, it's really easy. N1, re, is the function of xi 1, xi 2, xi 3 is xi 1, N2 is xi 2, N3 is xi 3, okay? And in particular it is the convenience of writing out these basis functions in this manner, in this so to speak completely symmetric manner that also motivates the definition of the code of these coordinates, xi 3 even though the span of the space is just two, right, consider this with the rank of this the, the dimensionality of the space, sorry, is, is just two. Okay right, so, so let's sketch these, right? So I'll sketch just I'll sketch one of them to begin with. And so I'm going to sketch N1, okay? As you can see from that, N1 goes to 1, right, at xi 1 equals 1, right, at xi 1 equals 0, which is the xi 2 axis, it drops to 0, right? So to 0 all along the xi 2 axis, right, and it goes down linearly, okay, along this, along the, the line, right, along this line here. Right, along that line, right, the N1, of course is always equal to xi 1. Right, along that line it slopes down linearly from 1 to 0, okay? That is xi 1, and likewise, we can construct xi 2 and xi 3. Xi 2 is one there slopes down linearly to 0 at the other two, at the other two nodes, right? That goes down to 0. Right, and likewise xi 3 or, or N3, sorry, is one at that node goes down to 0, linearly, okay? So that is N3.  we should write, okay. N2 is that basis function, and N1, N1, we've, we've already  and one and two in green. Okay? So those are are basis functions and one thing to note, of course, is that. You will have observed that I referred to these basis functions as being linear rather than bilinear, 'kay? And it should be completely clear why, right? They are indeed just linear not bilinear. Okay, so that's it. Once we have these basis functions, everything proceeds just as before. We just need to pay attention to the fact that the number of nodes in our element is not four, 'kay, even for the simplest of these of this class of elements, it's not four, it's three, and we can go ahead and construct our, our entire finite element formulation. One can go on to higher order triangular elements also. There are quadratic triangular elements which involve midside nodes as well and, and it's in ex, one, one can define the, the basis functions for them also and as, as, as an extension of what we've done here. I'm not going to get into it because really if you're, if you do need quadratic triangles, you, you are probably better off just using quadrilateral, bilinear quadrilaterals anyway, okay? So, so try thi, this, these elements have certain advantages. The, the primary advantage of, of, of them is of course, simplicity, okay? So simplex elements in general. They're, they're simpler than quadrilaterals in and hexahedra, right? Of course I haven't put down the simplex element in 3D but I'll do that pretty soon. They're simpler but there's a drawback, right? There is an obvious drawback. And what do you think that could be? Just the basis, right, I mean, you're using a lower order basis, the, the, you, you start out with the linear basis instead of a bilinear basis. Your representation of functions is already a little poo, is, is a little poorer. They also are there, there, there are, there are issues of incompleteness of polynomials and so on which, which arise, so in general they don't work as well as the corresponding bilinear, biquadratic elements and so on. Okay, but nevertheless they're simple to use and they are convenient they also of course, for certain problems of generating meshes they do present an easier task, 'kay? But that is largely been been overcome in the finite element community. All right, so we have this, right? Let me just put down the simplex element in 3D and then we'll be, we'll, we'll have got this covered as well, 'kay? So, in 3D, right, so we have tetrahedra. 'Kay? So a general element in the physical domain may look like that. Right, it'll have four nodes, right? The simplest linear tetrahedron. Right, I'm showing you the simplest linear tetrahedron. So this would be our omega e and it would be constructed as you may imagine from a parent domain, right, which is also in 3D, but it is a tetrahedron in 3D, right? This sort of a right tetrahedron in 3D. So it is. Okay, and oops. Okay, I guess I, in order to make it completely clear I should make some of these lines dashed. Yep, that's really the only line I needed to make dashed, okay? So, so the, the coordinate directions would be xi 1. Xi 2. Now we would properly have a xi 3 coordinate dimension, right? Our nodes, and here in, in the physical domain we may have numbered them as whatever, A equals 1, 2, maybe 3 and 4, right? What happens here is that we have four nodes of course. Right, so we will label them here as A equals 1, A equals 2, A equals 3 and here we'll get A equals 4, okay? For convenience again we will define now, xi 4 equals 1 minus xi 1 minus xi 2, minus xi 3, okay? Right, using the same sort of idea, in that case, we use this idea of area coordinates. Now we can I guess effectively call them volume coordinates, right? But, but the idea here is this, is the same. Now all we have is N1 function of xi 1, xi 2, xi 3, and xi 4, right? It is just xi 1, right, until we come down to N4. And that's it, right? We just go ahead, we construct our, we now know that the simplest of our basis functions is, is linear. There are four nodes to the element, four degrees of freedom. We just go, go ahead and construct our entire finite element formulation just as before. Okay, calculation of derivatives and everything is straightforward. One remark I  I could make is that now that when we work with linears linear triangles and or linear simplex elements. Lead to what is the order of any gradient now of our basis functions, and therefore the order of our representation of any gradient field? It's constant, right? Right, so if we take any of these linear basis functions and compute its derivative with respect to any physical coordinate, right, what we will see is that it leads to constant gradients. All right, and this in some cases can be an advantage, for, for simplicity of calculation, but of course if, if your best representation of gradients within the element is a constant, you are obviously losing some fidelity of representation of functions and so forth, okay? You're just not you're just not able to represent higher order functions. Right, so, so there are, so there are those sorts of drawbacks as well. So, so, there, there's also a, when it comes to integrating of with with, with these type of simplex elements of course there are special, the, there are special numerical quadrature rule, numerical integration rules. Just as we did in the case of triangles, we can also define higher order tetrahedra. Okay just as we do for for triangles, right? And one can construct the triquadratic so sorry, not triquadratic, quadratic tetrahedral, cubic tetrahedral and so on. One can also define the, the, there are numerical quadrature or numerical integration rules defined for them. The optimal the optimality of Gaussian quadrature however does not hold for triangular elements, right? There are other numerical integration tools that are, that are used. So Gauss quadrage, Gaussian quadrature is no longer optimal. Nevertheless, these can be defined, these have been defined, and they're actually  available in module, the finite element literature. Okay, but we can stop this segment here, and indeed, this unit. Hopefully, it's allowed us a unified representation of basis functions of definitely of the linears, bilinears and trilinear and therefore, you know, quadratics, biquadratics and triquadratics and so on, based on Lagrange polynomial functions. And we've also seen how simplex elements are defined, the simplest versions of simplex elements, the linears. We also saw in the course of this unit the extension of quadrature to three dimensions of, two, two, two and three dimensions for basis functions that are constructed from Lagrange polynomials. All right, we'll end this segment here. When we return, we'll start up a new unit."
8vV_ENhex_c,"Welcome back. What I'm going to do in this segment and maybe one or two more is  almost take a step back. And outline for you the main steps that are involved. Only slightly different steps that are involved in setting up two-dimensional problems. Okay? So, we're following somewhat of a deductive approach. Where we go from the more general 3D structure of, of the problem, to a 2D structure which is, of course, is a particularization of a 3D. Okay, we're going to do this in the context of the linear, lytic problem in two dimensions now, with a scalar variable. Okay? So we will call this, of course two dimensional. Linear. Elliptic. PDEs in scale, in a scalar variable. All right. So, since we are talking of linear elliptic PDEs with a scalar variable. The physical problems that this is applied to remain, heat conduction and mass diffusion in two dimensions. That's all. Okay. There's actually very little to do. Almost nothing to do. And I'm going to do just that, all right? So, here is my version of doing nothing. So, I'm going to first, we, we start out with a strong form of the problem. And, and really, all we have to observe is that everywhere in the 3D formulation where we observe that the spatial dimensions could the spatial dimension index ran from one to three. It now runs from one to two. And we need to recall for ourselves how we would construct the basis functions from over, over two dimensional elements. All right, okay. So, I'm, for, just for completeness, I'm going to put down fairly quickly, the strong form and the weak form. Okay? And in fact because I'm going to do that fairly quickly, let me straight away say that we're given the data. Okay? We're given u g. We are given j sub n. We are given f. And we are also given the constitutive Relation  -j i. Sorry, +j i = -Kappa i j. u, j and here, we have i, j = 1, 2. Okay? This is what we're given. So, with this background, here is the strong form. Okay, the strong form is. Find u such that j i, i =, now we'd use f in our first all sort of problem. And later on, you will recall that I said, that there was reason to replace that with minus f bar. Okay? Right? This is our PDE. All right? In omega. And, and we remember that now omega is a, sorry, it's a subset. Omega is a subset of R 2. Okay? So, the domain that we're looking at would be this. Right? With the difference that instead of drawing a three dimensional basis, or three basis vectors here, I have just two. V-1, V-2. Okay. So, this is our PDE. With. Boundary conditions. U = u g on what we understand to be the Dirichlet boundary. Okay? And -j, sorry. I'm doing this all with coordinate notation. So,- j i n i = j sub n for the influx condition on, and now, I'm in boundary. And n, now, is a unit vector in two dimensions. All right? Okay, that's it. That's our strong form of the problem and I've already stated up here that i and j run over one and two. Okay? I'll go ahead and write the weak form and I will in fact write the finite dimensional weak form, right, essentially skipping a step here. So, I will right now the finite dimensional weak form. Okay. So that is find u h. Sub, sorry, it's not sub i. Find u h belonging to s h subset s where s h consists of functions u h, belonging to h 1, over the two dimensional domain omega, and satisfying the traditionally boundary condition. Okay? So find of, of this type such that for all wh belonging to Vh subset of V, okay, where now Vh consists of all weighting functions. Also, to be picked from H1 functions, and satisfying the homogeneous Dirichlet boundary condition. Okay, find such that for all wh of this type, the following holds, right? Integral over omega wh,i, jhi. Now we are in two dimensions, right? So, what we are working with here is is an integral over an area, okay? So we get dA, right? So, wh,i ji jhi equals integral over omega wh f bar dA, okay? And what we note here is that okay, that in order to get things right we will pick up a minus sign there, okay? This is, I'm just working through all those changes I made with the signs later on in the 3D problem, okay so we have this. Plus integral over the Neumann boundary wh j sub nd. Okay, now we need to be a little careful here. In two dimensions the boundary is a curve, okay? So the boundary integral here I'm going to write as dS, all right, where for us S is now a curve. Okay, and dS is the elemental curve, okay? All right, so this is what we have. And the only thing we need to recall, if you're, is as we did on the previous slide, just remember, here, that i, remember that i equals 1,2, okay? And in this goes jhi, we also know will eventually be written as capital kappa ij sorry, minus kappa ijuh,j, okay? So j and i both run over 1 and 2. All right, and you see that now those signs will cancel out, right, plus, minus and minus will cancel out and we get back everything as positive, right, the way we write it down. All right good, so, so this is what we have and you know how everything works from here, right? We are going to use elements which are subdomains. I'm not going to write all the technical stuff about the, the union of elements you know, with closure applied to them giving us the total domain and all that, right? We don't need all of that here. Let's just look at the element sub-domains, right? So, we may work with quadrilaterals here, right, of that type, okay? And just to fix idea, let's suppose that we're working with bilinear quads. All right, so this would be omega e, right? We would have A equals 1, 2, 3, 4, which we know would be constructed from a mapping from a bi-unit domain, and we studied this process. Okay, we could have then, have it constructed from a bi-unit domain, which is now expressed in terms of coordinates xi 1, xi 2, all right? And here our nodes are we, we know exactly how, what these are, right, these are minus 1, minus 1; 1, minus 1; 1, 1; and minus 1, 1, okay? Those are our nodes in the bi-unit domain, okay? Now we also remember that this mapping gives us a way to write out the geometry, okay? So we know that the geometry in a the physical domain is a function can, or can be parameterized by our coordinates in this bi-unit domain, okay? All right, so we may have this or we may choose to be working with triangles, right? And we do that or. Triangular elements. In the case of triangular elements, I just drew the very simple so then, it's the one that we looked at in a previous segment. All right, that's omega e. Once again we have nodes at the vertices, and I'm going to label them as A equals 1, 2, 3. We know this can be constructed as a mapping from a unit domain, which we also express in terms of coordinates xi 1, xi 2, and in this setting We get that point is 0, 0; 1, 0 and 0, 1, okay? And right, we also recall that in this setting our no, our degrees of freedom here are labeled 1, 2. And three. And that reminds me that I did not label the degrees off freedom in the bi-unit domain here. So let me call this one, two, three, four. Okay, but we now that already very well. Okay. And, again, we will get from here a mapping of the geometry as well xi 1, xi 2. Okay? All right, so in this setting we will get our basis functions, right? Right. There's two ways in which we would write our basis functions, depending upon whether we were working with the bilinear quad or the linear triangle. Right? In the case of the bilinear quad right? We would get bilinear basis functions right? Bilinear polynomials. Right? These would be written as n a parametrize by c1 and c2. And we know what the formula for them is right, I'm not going to rewrite it. We've written it a couple of times at least now these are the standard bilinears. We've also looked in one of the previous segments at the Generalized Lagrange polynomial formula for this, right? So these are bilinear Lagrange polynomials, right? In this case, A runs over 1, to number of nodes in the element and their number of nodes in the element is 4. Right? Okay, or we would have linear polynomials. All right? If we were working with the triangles. Right? Linear polynomials would again we written as a n parameterized now by c1, c2, and remember the way that these things are written often Using area coordinates, we would write Z, include 8, Z3. Okay. And remember that Z3 is defined, in the case of triangles as 1 minus Z1 minus Z2. Right. And we saw this as recently in E segment. In this case again A equals from 1 to number of nodes in the element. And for the case that we're considering up here on this very slide. Number of nodes on the element is equal to 3. All right? So this is what we have and and with this we go ahead and of course construct our finite dimensional functions, right? So we will go ahead and write u h over e is now sum over the number of nodes in the element. N A, right? I'm not going to write the parameters of N A because they could be different depending upon whether we are working with bilinear or linear polynomials, all right, but we're working with bilinear quads or with triangles. So I'm just going to say it's n a times d a e. And for the waiting function, right, we have w h of e equals the same sort of expansion. Okay. All right, we just need to remember that in the two cases, the number of nodes in the element could be different. All right."
P3NHmrT9EC4,"And next we have to construct our gradients, right? So, for gradients of our trial solution and weighting function, we have u h comma i in general, 'kay? This is now again, sum over A 1 to number of nodes in the element, NA. Okay, now we're going to write N A as a we're going to write the gradient of N A, right? And the way we can do it here is using the chain rule, right? So, what I'm going to do here is write it as N A comma xi1, okay? Xi1, oh, I'm sorry. N A comma xi1 times derivative of xi1 with respect to x i. Plus N A comma xi2, derivative of xi2 with respect to x i, okay? Again, remembering that here that i equals 1, 2, all right. Now something to observe here is that I claim that this way of writing it works for bilinear quads or for linear triangles, okay? Even though we said linear triangles have an additional, can be written as, depending upon an additional coordinate, xi3, right? Because, of course, what we're doing in here is sort of rewriting, xi3 as depending upon xi1 and xi2, right? That's what's going on here, okay? So let me say here that rewrite. NA xi1, xi2, xi3. Using xi3 equals 1 minus xi1 minus xi2 for triangles. Okay? All right, basically there are only independent coordinates, of course in 2D, so that's all that we need to compute. All right, so that should be straightforward. Okay, that brings us then to to the question of how do we get how do we compute those derivatives of xi1 and xi2 with respect to x i? All right? How do we do it? By exploiting the mapping of the geometry, right? So what we do is we use the, the fact that we have an isoparametric mapping. All right. And you recall what that means, right. That means that x, i in element e, all right, can be written as a function of xi1 and xi2. Always by writing it as sum over A, NA x A i e, all right. Just as we did before, okay. And in fact we've even drawn on the, in the first or second slide of this, well on the first and second slides of, of this segment, we've, we've shown that mapping, okay? All right, what this lets us do then is write out partial of x i with respect to xi1, and partial. All right, so, well, let me write it in general, write it in this fashion. Partial of x i, with respect to xi capitalize I, all right. And we know exactly how that works. It's now sum over A, running from 1 to number of nodes in the element, NA comma xi I, x A i e, okay? And here, too, we have i equals 1, 2, and capital I also equals 1, 2. Here, too, I've used the fact that even if we were working with triangles, xi3 would be written in terms of xi1 and xi2, right? There, there are only two independent coordinates. All right, and this is what then allows us to write J, all right, the Jacobean of the mapping, okay? So J is now, of course, just partial of x1 with respect to xi1, partial of x 1, with respect to xi2, partial of x 2 with respect to xi1, partial of x 2 with respect to xi2, all right? So, now this is a little different from 3D. Well, it's, it's different from 3D simply because we work in two dimensions, right? But it is what you would expect, which is that the Jacobean of the mapping is a. Tensor that takes two-dimensional vectors to two-dimensional vectors. Alternatively viewed as a matrix, it is a two-by-two matrix, okay? And this implies for us, then, that J inverse, which is actually very easy to write because it is just a two-by-two. And we know there exists an analytic formula which is actually very easy to write up, right. We know that we can, we cannot therefore explicitly compute the inverse, right. But that inverse by definition is this. All right. And, why this matters is that these are the terms that are going to be used in our chain rule to compute derivatives, right, of our functions. Right. I just rewrote the formula for computing the gradient of u h. There is a sum implied over capital I, and you remember the capital I runs over 1 and 2, okay? And having written out the inverse of the Jacobean gives us those terms, okay? So that's it. Now we can go back to assembling all the integrals that we need to compute, right? Now what we can do is compute. Element integrals. Okay? All right. And remember that the way we go about that is to say that integral wh comma i jh i with a minus sign d A is sum over e, integral over omega e w h comma i j h i d A, right? Well, we know how to compute everything inside there, okay? When we write this out in detail, what we get is integral over omega e, w h comma i, j h i, dA. In one fell swoop I am going to write the, the final expression, okay? So, we know that this can be written as minus sum over A and B, okay? A and B running over 1 to number of nodes in the element in the general case. All right. We get here c A e. We get an integral. Now, I'm going to straight away write this integral as an integral over our parent domain, okay? So, it will be xi1 or, or, I guess xi2 equals minus 1 to 1, xi1 equals minus 1 to 1, okay. Inside here I'm going to get NA, all right? I will get NA comma xi I partial of, well actually, we don't even need to write it that way. We can write this directly as N A comma i, right? Where N A comma little i simply means that we know how to compute the gradient with respect to physical coordinates, right? We looked at that on the previous slide, right? We know how to compute those guys, right? That times right, we get a minus kappa i j. Now to compute u h comma j we will pick up an N B comma j here, okay? We will be integrating this over d A where if you look out here, we have d A as the elemental area as it, it is an element of omega e in the physical domain. But here because we're integrating over the parent domain, we pick up here, determinate of J, right, which also depends upon xi, upon the vector xi, right, upon both coordinates. This now with an, multiplied by our elemental area in the parent domain, which is that one. Okay, I'll put parentheses here, and coming out here is d A e, sorry d B e, all right?  Simple as that, okay? Okay, we know now that this is going, when we carry out this integral we know how to carry out this integral. We've looked at numerical quadrature, okay? So we know how to carry out this integral. It is going to give us, finally, we know it is going to give us K bar A B. The AB entry of the K bar matrix for element e, right? And if we were doing heat conduction, we would call this the conductivity matrix. Okay? Just like that, we go ahead, to compute now integral over, omega of, w h f bar, okay? Integral over, integral over omega of w h f bar d A, right? Okay, integral over w h f bar of d A, which is again, sum over e, integral over omega e, w h, f bar, d A. Okay, where, now, integral over omega e, w h f bar d A equals sum over A, c A e, integral minus 1 to 1, minus 1 to 1. We may call this one xi2. Call this one xi1, okay? And here we get N A, no derivatives f bar. Now we need to integrate over d A but over the elemental area in the parent domain. So once again we just get the determinant of J. D xi1, d xi2, all right? Okay. So, one thing that I should point out perhaps on the previous slide. Our, our tier is that when we look at these at the summations over i and j, just remember that i comma j here equals 1 comma 2, all right? Okay. Okay, so we come back here. So that's how we know, that's how we need to compute this particular term. All right, and what we will, what we know also is that when we look at that term, it is what we will call, believe we call this F bar internal A for element e, okay? And the very last one is the now, the, the integral over the Neumann condit, over the Neumann boundary, all right. So we have integral over partial omega sub j, w h, j sub n, d little s, okay? This one actually takes a little work simply because we need to be careful about how we do the how we pick the degrees of freedom that contribute to that particular boundary here, right, from each element. And also just recalling for ourselves what we will do with doing the integral along the curve that ds implies, okay? So we'll come back and do that in the next segment."
naLrrzB2Np0,"All right. We will continue with, just looking at the critical points that go into setting up our equations in two d. And I hope what you're noticing is that there is really nothing to do. Nothing new at all just paying attention to the fact that our spatial dimensions aren't over one and two. Okay? So let's get on with this. All right so we got as far as the boundary integral. Okay, the boundary integral which is this one. Integral over the Neumann boundary. W h, j n, d s, right? And here, we need to pay attention to the fact that d s is a curve, right? An elemental. All right. Elemental curve. Okay, again, this is a sum. Now over e, belonging to the set of elements whose boundaries coincide with a Neumann boundary. Right and we'd use that notation, script e sub n. Integral, now over partial omega e sub j. All right? And then the, the integral over that part of the element boundary that coincides with the Neumann boundary, okay? W h j n d little s. Okay? And then, we said that what we need to do is worry about this integral. Okay? And the way we do this is to once again do a sum over a. And now, we observe that. We, essentially, we recall things that we've noticed before, right? Which, and, and one of the things we'd noticed before, was that, if this is our, domain. And this were the element that we were concerned with. Right? What we note is that only some of the nodes lie on the relevant boundary, right? Of this part of the boundary with the, with the Dirichlet boundary, this part with the Neumann boundary. What we observe it is, is that it's only this elem, this node a or that degree of freedom. Which belongs to the set of Neumann boundary nodes which we call a sub n, okay? And likewise, if right next to it we were to have a triangular element, and this is possible. In a finite element mesh, right? If you had triangular element right here, then it would again be, so that would be a, maybe that would also be b, also belonging to a sub n, okay? All right, so we have a belonging to a sub n, c a e integral over. All right. Now. We need to go to the element boundary but in the parent sub domain. Here we would have n a, j n. Okay. And what we would do is to recall for ourselves that. If our edge here. Right. If our, if our edge here were this one. Right? Partial, omega, e, sub j. Right? We would need to just remember that, well, it always arises as a mapping from one of the. Coordinate plane, coordinate lines now in the parent domain. Right? So what we would observe is that we have this map. Okay? Right? And that map, you remember, is something that we denoted as j s. Okay. Okay, now in this particular case, the way I've written it out, j s is the function only of c two. All right, so okay? Given that we know the positions of all the relevant degrees of freedom. And actually, the nodal coordinates in the physical domain. The relevant nodal coordinates in the physical domain are that one, and this one. Okay? And the relevant, relevant ones in the parent domain are minus one comma minus one. And one comma minus one. Okay. So we know very well how to construct that map j s. All right? In particular we did talk about, in, in, in the case of the three d problem we said that if the edge were not in, in the physical domain if the edge were not aligned with one of the, coordinate directions. Right. One of the basis vectors in Two dimensional space, such as this case, right. That edge, partial omega, e sub j, is not quite aligned with the e one vector, okay. And I suggested back then, when we were doing this in three d, that alright, maybe we need to find a new direction there, right? Right, we said that maybe we need to define a new vector along that direction. Okay? If, if that were the case, it would take a little bit of an effort to construct that, but, but not too much. If it did align perfectly with a coordinate direction, it would be a much simpler construction, right? To get this math. J sub s. Okay? All right. So, what one would get in this case is here we would get the determinant of j s, right? And we would be then integrating along for c two as shown in that particular, in this particular example. Okay? And the same sort of thing would happen if we had a triangular element. Right? And that with the triangular element. Right? And that were the direction that we needed to integrate along. Right? If this were, sorry the edge that we needed to integrate along. It would come from a regular unit domain. Okay. Those will be the nodes that will be relevant in this domain. If that were a, b, and if a were the first order in the physical element and so on. Right? So then it would be mapped along zero comma zero, and one comma zero. Okay? Just realized that with the way I've drawn things this, this elemental curve length is d x c one, not d x c two. All right so this what we would have to construct. Right in either case we would note that that determinant, determinant of j s is simply. Derivative of x tilde with respect to c one. Okay. If x tilde were one of the coordinate directions only, it would be a very simple calculation. It would be a little more complicated if it were aligned in some general direction.  Right, in some intermediate direction. Okay, but nevertheless, it can be constructed. All right. Okay. And then this figure is pretty, is quite busy now, but let me just do one more thing here. That integral is what I believe we were calling f a sub j. Sorry, F j sub a. F j a element e, okay? All right, so we have all of these, and what we would then have to do is put these things together, okay. One thing to point out is one useful thing to point out is, that I will do in a remark here. Okay. Is that for the case of linear triangles. Okay? The matrix components of j c, right. J x c is for the case of linear triangles is the map that allows us to go from, go to the physical domain from the parent domain, right? And if you're working with linear triangles. J x c takes on a very special form. All right, so, the map itself is x, of c. And j x c is the gradient of that map, right. So, the matrix j x c here, which is now, x one comma c one, x one comma c two, x two comma c one, x two comma c two. If you go ahead and compute this for linear triangles, what you find is that this matrix is a constant, right? The components of that matrix are constant because of the linearity of the map, right? There's no bi-linearity in the independence of x one and x two point in, in c one and c two. So when you take those derivatives you get constants. Right? And in fact, in fact not only that but a further, simplification is the fact that the determinant of j x c in this special case, right, only for linear triangles turns out to be. Can you deduce what it's going to be? It's always twice the area of omega e. Okay, the measure of omega e is the area of omega e in this case. Okay, so that's a special case for linear triangles. It's something that's useful to know and to be able to use when one is developing these formulations. Okay. All right. So let's return then, to assembly. All right. For assembly, here's what we have. Sum over e, of, let me see, we have here. Sum over a comma b. C a e. K bar, a b. And d b e equals sum over e. Sum over a. C a e, f bar internal a e, right this is what we get from the forcing. Plus sum e belonging to the set of elements that include. Whose boundaries coincide with the Neumann boundary. Sum a belonging to a sub n. Right, the set of nodes within each of those elements that also lie on the Neumann boundary, c a e, f bar, sorry f j a e. There's no bar on that one. Okay? Assembly is what lets us go from here to a single matrix, right? And now, actually, everything's just the same. Nothing is any different. We first of all can get rid of the sum over degrees of freedom over in each, each element by essentially writing each of these sums using matrix vector notation, right? So we here introduced the idea of the row vector of waiting function degrees of freedom. Right? We get k bar, the rectangular conductivity matrix for this element. Multiplying d bar, and I don't need that superscript. Multiplying d bar e. Right? And you remember the reason that I'm using a k bar and a d bar is recognizing that, Dirichlet boundary conditions have a say here. Right? And the K bar is a rectangular matrix because of the fact that the, the existence of Dirichlet boundary conditions says that the size of the vector c e, right? Is not the same as the size of the vector d bar e in every single case, right? For the elements lying on the Dirichlet boundary they are not the same, right. And then like Y is for this we get c e, transpose f bar internal e. And here we get, the same thing. Right? Remember, in this case, I'm going to the full, row vector c e. We simply say that we construct f j e, such that the degrees of freedom that don't lie on the Neumann boundary from that element e. We'll simply have zero entries in the corresponding f j vector. Okay. This is exactly the same, all right. We don't, oh, the way we did this in three d and the way we would do this in two d is exactly the same. In the case of two d, we wo-, like in the case of three d, the, the sum over the degrees of freedom a and b. Right? Would run over the number of nodes in the element, right, or the number of degrees of freedom that were used to interpolate each field in that case. The fact that that number is greater in three d and lesser in two d has no effect. Right? So we just run over that sum. Okay, so that process is no different okay, in constructing these element matrices and vectors."
M96mlUdkaCk,"Okay, so, just recall however that K bar e has dimensions, number of spatial dimensions times number of nodes in, in an element minus ND, which is the number of degrees of freedom with Dirichlet conditions on it, right. This times N s d times N n e, okay? This is no different from before except for the fact that now N s d is 2. That's all. Right, otherwise it's exactly the same thing, okay? All right. And then assembly is what let's us say that assembly over e of K bar e equals K bar, all right? And this assembly process is just like we did in 3D. Again, nothing different, right. We just look at every global degree of freedom, look at every local degree of freedom from any element that has the same global degree of freedom and add up contributions in the matrix. Right, and the same way for our other, for our vectors, F bar, internal e, right? The assembly of all of these gives us an F bar. This assembly process is no different, right? Just look at global degrees of freedom that, correspond to local degrees of freedom in every single element that they belong to, right, and just add up the contributions. Right, and in doing this, we may choose to say that e here runs only over the set of degrees of freedom, that. Sorry, it, it just runs, we need to only run over the elements that have a boundary coinciding with a Neumann boundary, okay? 'Kay, I just realized that what I did here for the, for the dimensions of K bar e actually apply to K bar. This is corresponding the K bar, okay. All right, so we've observe that K bar is a rectangular matrix. It's not square. And we know why it's not square, because there are fewer degrees of freedom corresponding to the trial solution vector, okay? So that last bit is exactly the same. So if you look at K bar, what we will see is that, right? In general, it has fewer rows than columns, right? These are number of spatial dimensions times Nne, right? Whereas, these are number of spatial dimensions times Nne minus ND, right, as we observed above, up there, okay? Now, some of these are some of these degrees of freedom that are in the d bar vector are known, right, okay? We've observed that fact before, maybe some of these degrees of freedom are known, right? And what we do then is go back here and essentially eliminate the corresponding columns right? So for instance if this were an, and if this were A bar and that were B bar, right? This column would be the A bar column here, right? And then you would get the B bar column. Right? And corresponding to it, we, what we would see is that d A bar would be known. And d B bar would also be known, right? And d bar, A bar, B bar, those degrees of freedom would be known. We just moved them over to the right-hand side because they're known degrees of freedom, right? So, that is known. Right? And likewise that one would be known. And why would it be known? Sorry, because it corresponds to a Dirichlet boundary condition. Right, likewise, this one. D-i-r for short, for Dirichlet. Okay? All right. And this column is what I've been calling before K bar, A bar. And this would be K bar B bar. Should be using that as a super script. Okay? And just as before, we move this over to the right-hand side. Okay. So what this gives us then is a, the full c vector here now multiplying K and d where K, where properly now has dimensions of number spatial dimensions times number of nodes in the element minus ND. Square. All right? And therefore c transpose ND are also of the same dimension. Right, N sd times N ne minus ND. This is equal to c transpose F internal plus F j minus dA bar, right. Just that degree of freedom, just that scalar multiplying the K bar A bar column, minus dB bar, multiplying the K bar B bar column. Okay. And this finally is our full F vector. Also with dimensions Nsd times Nne minus ND, okay. And this has to hold for all c belonging to that same, to space of the same dimension. All right. And what that implies again is that K d equals F, okay? All right, so as you see, there's really nothing to do here. Just had, it was just a matter of observing that our spatial dimensions run over 1 and 2. Just recalling for ourselves how we would construct the relevant gradient fields, using our basis functions. See that, observe that it doesn't matter whether we're working with quadrilaterals or triangles, there's something special in the case of linear triangles which is that that the Jacobian of the mapping is a constant. But other than that, it's, it's, it's all completely standard, especially once we form the elemental matrices and vectors. And in fact the process of forming the elemental matrices and vectors itself is exactly as we followed in 3D. And then final global assembly also follows the same process, including the treatment of Dirichlet boundary conditions. Okay, so we'll end this segment, in fact, this this treatment of two dimensional problems here, 'kay?"
nK0sQuS3s5A,"So, there is an error in writing out the dimensions of vectors and matrices in two slides of the segment. Starting out with the slide that I have before me here, if you look at what I have up here. Let me get my pen working. I have n s d times n n e here. That should be replaced with just n, sub n p for number of nodes in the problem. That, however, has to be applied over the rest of this slide, as well as the next one. So let me put a line through that and call it n sub n p. Let's see, where else do I do it? Here n sub n p. Here, too, n sub n p. I should emphasize that the n, n sub d is correct, that does stay. That looks like it for this slide. Let me advance to the next slide. And here again, we are. That should be n sub n p as should that n sub np and that n sub np. With that, we have corrected the slide, or the segment."
ew2w_UzSQbg,"Welcome back, at this point we are ready to take a fairly significant step forward. We're going to begin a new unit where we look at a linear elliptic PDE but with a vector variable in three dimensions. And a model problem for this is elasticity, specifically we're going to look at linearized elasicity, okay? Because there are some technical issues well, not just technical issues but it turns out that non linear elasticity is not necessarily elliptical. In case, we'll do linearized elasticity. Okay, so, we begin now with the linear elliptic PDE in three dimensions. For a vector variable. And the model, like I said is linearized elasticity in 3D. Okay, I'm going to start by writing out the strong form problem. So the setting is one of 3D again, so we have our basis, our three dimensional basis set. Okay, we have our domain. We have let me just show the position vector of some point here. Okay, that point is x. Okay, so maybe I should write it on the, at the point. This is x. 'Kay, so that's the position vector of x. Our body as before will again be denoted omega. And okay. Let me now start out by fine. Setting up the problem, okay? Let me first state what data we have. Were given some data, now remember, were doing a vector problems so all the data actually are vectors, okay? Or vectors are, are tensors in fact. Okay, so what is the, what, what are the data we have? We are given we are given a vector set of vectors and allow me to put the g as a super script here, okay? Unlike the subscript that we were doing earlier because I need to use a subscript to denote quadrants, okay? So, that little i there signifies our spatial dimensions, right? Okay, so we're given this as part of our data. Given u g sub i we're also given, what I'm going to denote as t bar sub i, okay? We are given f sub i, okay? These are all data and right here phonetically what I'm going to say is that what this really means is that u g, t bar, f bar, now using direct notation. All belong to R3, okay? So truly all I'm saying is that those data that we, that I just spoke about are all vector data, okay? So when I use back here I've used coordinate notation and back here I've used direct notation. Okay, so these are the, the, the data we have. Furthermore, we are given a constitutive relation. We're given a constitutive relation which I'm going to write first in coordinate notation, okay? The consideraitve relation we are given is for the stress. A quantity that we call the stress, okay? That will be denoted by sigma. It is a tensor so it has, in coordinate notation, it has units, it has it has two indices, i and g, okay? I'm going to put down here stress. Okay? The stress is given to us as a, through a tensor con, constitutive relation as equal to c i j k l epsilon, okay? And what we have out here is the strain, okay? I'll just state here that because we're doing linearized elasticity the strain here. I'll write it out actually, the string is what we call the infinitecimal strain, okay? Infinitesimal strain, okay? Sometimes people like to specify that the stress that we call, that they are using here is the Cauchy stress. But it turns out that if you're doing linearized elasticity in infinitecimal strain. There is only the Caushy stress this out in it, okay? This quantity here is what is called the elasticity tensor, 'kay? It's a fourth order tensor and it is what we call the elasticity tensor. Okay? Now all of this, I, I'm not going to write it but since I'm using indices this is coordinate notation. Direct notation for the same thing, which I'll write in parentheses like I did above. Direct notation would be sigma as a tensor and like I said once before, we will do, I'm using an under bar for tensors and vectors. We just distinguish them by context, okay? Sigma in the direct notion is c elasticity tensor, okay? And the fact that two in the c's are being contracted out here. K and l with k and l is represented by a colon or two dots, 'kay? Just think of it as a generalization of a dot product at symbol is sometimes called a contraction symbol. Okay, an epsilon, all right, in direct notation also, okay? So let me just state here that, that symbol is sometimes called is sometimes called a contraction. I suppose the idea comes from the fact that you're taking a second order tensor and which is, which is, epsilon. Epsilon so, so the stress and the strain in our second order tensor try to, and to take second order tensor which is a strain and sort of contracting it down to a different second order tensor. Or alternatively taking the fourth order elasticity tensor and getting rid, rid of two of it indices led a contracting down its the order of the resulting tensor, okay? So that's the contraction symbol. All right, so we have we have all these things here. So to this, the constitutive relation, okay? We need more."
mw1wsbSm-vc,"We also need, some something called Kinematics, all right? We need the kinematic relation Again, with coordinate notation. This kinematic relation for the stream. Epsilon kl is one half derivative of another vector, u with respect to x plus the symmeterizing term. I should mention that, I should make it clear that that is l, not i. Okay? Right. So we have this kinematic relation. In direct notation this would be epsilon equals the symmetric part of the gradient of u, okay? All right. So we have these components of our problem, okay? So let me just go back here for a second. So we're given data in terms of these vectors ug, t bar, and f. We're given a constitutive relation, relating the stress to the strain through the elasticity tensor. Okay? And we're given this kinematic relation. Okay, so given all this, what we are faced with is finding u. Okay? Which is, I'm sorry. We need to stick with coordinate notation here. Find ui, okay. And again, ui is a vector, right? It's a component of a vector, so this really is saying that u belongs to R 3. Okay again, parenthetically, we have the direct notation there. Okay? Find ui, right? Which, s, so ui is our solution field and you probably know if you, you know some elasticity, you know that this is the displacement field. Okay, so, find ui such that. The following hold, okay? Let's first write out the PDE itself. The PDE is the following. Sigma i j, sorry, sigma i j, j plus f i equals zero in omicron. Okay? And, if you all ready know it that simply means that the stress, the stress divergence, the divergence of the stress plus f equals zero, okay. This is sometimes called the stress equilibrium equation. Okay, so. Sometimes called the quasi-static stress equilibrium relation. All right? Okay. So that's the PDE. Boundary conditions. U i equals u given sub i on the Dirichlet boundary. But here comes the rub We have a vector problem, right? So if you look at the equation that we put down the PDE, this PDE. What we need to recall is that in all of this, we need to recall that we need to recall that i, j equals 1, 2, and 3, right? So when we look at the PDE, we actually have three PDEs there, right? For each value of phi. Not a, not a big surprise, because we are talking of finding a field which is itself a vector, right? Which has three components. So we're really solving three PDEs. Okay? So vector PDE. What this means is that we need boundary conditions for each component of our field, of our solution field, all right? But then each component of our solution field could have a Dirichlet boundary condition defined on a different Dirichlet boundary. Okay, so on this Dirichlet boundary itself we can have a subscript. What this means, when I go back, oh, well, l, let me draw it right here. What this means is that we have our basis We have our domain. It means that for each component, right, I could have a certain tertiary boundary. Okay. Right? Let me actually give it, give this a specific component. Then we suppose that this is the Dirichlet boundary for the component one. All right. However, when I come to component two and three, I am free to take on, to, to give them different Dirichlet boundaries, right? So it, and these could overlap or not. All right, they, they can be completely arbitrary all right? No implied relation with each other. This simply comes about from the fact that we're solving a vector problem. All right, and of course, it's elliptic so we do need boundary conditions everywhere. Okay? So that's for the Dirichlet boundary. Well, the same sort of thing happens for the Neumann boundary. Because each of those Dirichlet boundaries has as its compliment a Neumann boundary. And, I think I'm going to denote this as omega. Well, it's obvious, right? Omega, partial of omega t bar sub 1. We have partial of omega t bar sub 2 and Partial of omega t bar sub 3, okay? Of course, in each case, each set of Dirichlet and Neumann boundary conditions are complements of each other, okay? So, here we have, okay. So where does the Neumann boundary condition come in? It comes in to say that sigma ij, nj equals t bar i on partial omega t bar sub i, okay? What we have is that the boundary, okay? Is always equal to, partial omega u sub i, union partial omega t bar sub i. And that each Dirichlet boundary for a given component of the solution is, disjoint from its Neumann boundary. Okay? This holds for i equals 1, 2, 3. Okay? Here's what this means.  Three dimensions. This is our continuum football now, all right? When it comes to boundary conditions, what we're seeing is that for each component of space, right? For each compo, for each, f, for each, direction as defined by our basis vectors, we have, a Dirichlet condition for the corresponding component, so u 1! So maybe u 1 is specified on the maze bar of this football And the corresponding traction component, right? Which would be obtained through this relation. All right? The corresponding value t bar 1. Right? When you're talking of this. Okay. So if you want to specify on the maize part t1 is specified on the blue part, all right. And then again if you too maybe specified on the part of the football that you can see, right? And t2 is specified on the opposite part. Okay. You u 3 maybe specified maybe on some I don't know, some combination of the maize and blue. Maybe that maize part, this blue part. On it's compliment is where u3 is specified. Okay, what this corresponds to, is if you've studied problems of elasticity or mechanics is the fact that on a given boundary we may often not be controlling all components of the displacement. Okay? So it's common in, problems of elasticity to do, so called union axial tension problems, right? So supposing this were my, this were a bar and at this end, I decide to hold things fixed. So there are different ways to do this. I may choose to specify that on, that on this surface, on this edge of the, this sort of LEGO pieces, I control all displacement components, right? U1, u2, u3, I may choose to say all of them are zero. All right. And then I may choose to load this thing up. Right? Alternately, there is another version of ten, of tension. That it, what, what I've just described is not true in the axillary tension. It turns out it develops stresses at this end in, in, in, in directions other than along the longitudinal direction. Instead, I may choose that at on this boundary I specify only this component, let's call it the e 1 component, the u 1 component, to be zero. Okay? And the other two components are allowed to move around, to relax, so that now, when I pull this bar. Because of something called the Poisson ratio, in linearized elasticity, this bar will tend to shrink in size. It will tend to contract in, in, in, in the other two dimension I pull it one way. Then I'm allowing points on this surface to also move in the, the vertical direction and the direction towards you. Okay? So elasticity gives us the freedom to do that on a single boundary. There are many ways in which we can combine the Dirichlet boundary conditions on, on the, any given surface and indeed for the traction conditions too. All right, so in the problems I just described here, in this uni axial tension, often the lateral boundaries are free of traction. Okay? Which means there are traction on those surfaces is zero.  Sorry, zero. Now when I pull on this, on this end of the bar I may choose that but also specifying the Dirichlet condition, right? I may specify that I'm, I'm, I'm going to pull this end. There we go. I'm going to pull this end to some distance, right? That's a Dirichlet condition. Or I may choose to say that on this surface I'm specifying only that the traction along this direction, this longitude, this, this longitudinal direction is controlled, only that component of the vector. Okay? I'm a, I'm free to actually to specify the displacement components in the other two directions. That's they're all kinds of ways in which we can combine them. All right? And, and these are actually highly relevant for physical situations and lead, they're, they're completely different boundary value problems. They lead to solutions that look That look, that are different in, in important ways, okay? But the important condition to always be satisfied, is that the union of the Dirichlet and Neumann condition, boundaries for each component, always essentially, give a union that gives us the entire boundary. And that each Dirichlet boundary and Neumann boundary for a given component direction are disjoint. Okay so this is just a beginning step an early step towards the strong form of elasticity. I'm going to end the segment here, but when we return, we are going to say a little more about this. Because it's important to understand what elasticity's trying to do here. I will also write up the direct notation for this, problem. And say much more, actually about, many of the, the ingredients of, this problem. And it's important to understand that, as we are, setting up numerical methods for it. The comp, the complexity comes mainly, entirely from the fact that it's a vector problem. All right, we stop here."
7sY8JnZHeko,"I neglected to write something down on this slide and it happened when, I was talking about, the nature of the boundaries here. In particular we talked about how we decomposed the boundary into a and a subset for each spatial dimension. And we also mentioned there that in each spatial direction, the and subsets are mutually exclusive, right. So their intersection is the empty set. I said that, I didn't quite write it. So, let me put it down here. Let me try and squeeze it off to the right here. So that would be, partial omega U, sub I, intersection partial Omega T sub I = the empty set. All right so with that everything works out."
FA4VWUXqOu4,"In the last segment we've seen 3D linearized elasticity using coordinate notation, in strong form. What I'd like to do to start off this segment is for completeness put down the direct notation version of the strong form of 3D linearized elasticity. Okay? So, let's start then with linearized elasticity In three dimensions. Okay. Direct notation Okay, the setting is as before, we have our bases, e1 e2, e3. We have our body, right? Omega. We have our position vector of a point there. Right? And as we've done before, we observe that for boundary conditions, we have Dirichlet boundary conditions for each spatial dimension, right? So this is where we have u sub i equals u. G sub i for a specific i, right? And in the complimentary part, we have attraction boundary condition, right, or the Neumann boundary condition for a specific i. Okay, and remember that the whole business with the fact that we are now looking at a vector problem, right. Our, our solution the solution that we're looking for is a vector solution with three components. This is what specifies that when we look at the boundary of the body, we need to specify boundary conditions separately for each component. So, it being linearized elasticity, being an elliptic problem, it means that we do need to have boundary conditions at every point of the boundary but what can happen is that perhaps over the maze part of this boundary we specified maybe the u1 component of the, of displacement over the blue part of the boundary, which is complement to the maize part. We may be specifying therefore, the T1 component of the traction. All right? However over a different part, maybe this part of the boundary, we specify the u2 component of the, of the displacement, and on its compliment, which is facing me, we specify the t2 component of contraction. Observe that the way we've partitioned the boundary for the two different cases for spatial dimension 1 and 2 are overlapping, all right? Okay? And, and likewise for, for spatial dimension three, it may be a completely different sort of partitioning. Okay. Whoever, on the partitioning for each spatial dimension must satisfy the requirement that the Neumann and Dirichlet boundaries are disjoint. Okay, as I've indicated in the figure here. Okay. So we have, of course, that that the boundary is equal to the Dirichlet boundary union, the traction bond, the Neumann boundary and furthermore ,for each such decomposition, the intersection is disjoint, right? Sorry, the intersection is empty, right? Those boundary subsets are disjoined. Okay. Given all this, here is what we want to do. What we want to say, is, given, ug sub i, t bar sub i, the vector f. Now, I'm using direct notation for the vector f, okay? And the cosiderative relation sigma equals c, contracted with epsilon. Note that I'm using direct notation here before. The contraction of, the fourth order tensor, e is c, and there is, and there, the stream tensor epsilon. Okay, given all this, and the kinematic condition the kinematic relation. Epsilon equals the symmetric part of the gradient of u, okay? Given all of this, find. U, okay. Such that. Okay we have divergence of sigma plus f equals zero vector in omega, okay? And then we also have, e sub i equals ug sub i on that Dirichlet boundary, okay? Note that we cannot write out the Dirichlet boundary condition in direct notation simply because the boundary subsets are different and each component is specified. Possibly in a different boundary subset. Right, and the same thing happens for the traction condition. For the traction condition, we can however write sigma n, which is a vector. We're seeing that its ith component equals t bar i on partial omega t bar i, okay? All right one thing to note here is that This notation, delta sigma, is sometimes written as divergent of sigma, right? And, it is, sometimes also interpreted as taking this operator, the gradient operator, and dotting it with sigma. Okay, which is also the same thing that's expressed here. Okay. Alright, this is the end as far as direct notation is concerned. Perhaps the one other thing to remember, is that, when we write in direct notation, C contracted with epsilon, this is, a second order tensor, okay? This is a second order tensor with free indices i j, okay? And this is, by definition, this is C. I j k l epsilon k l. Okay. All right. So this is direct notation. This is an explanation of direct notation using coordinate notation. All right. Now, however, in order to continue, in order to develop our, formulation, I'm going to prefer to use coordinate notation because it makes all the manipulation of indices much more transparent. Right. Long before we get into that however, we need to see a little more about the constitutive relations. Okay, so let's do that. We have sigma, I'll, I'll use coordinate notation. Sigma i j equals C i j k l epsilon k l. Okay. Let's say a few things about this. C i j k l is a fourth-order tensor, it's called a fourth-order elasticity tensor. Okay, for linearized elasticity, it is constant with respect to. Wrt is short for with respect to. Right. C i j k l is constant with respect to epsilon. All right? With respect of the epsilon tensor. All right? So this is what we mean by linearized elasticity. And the context of linearized elasticity, this is what we get. Essentially what that means is that the relation between sigma and epsilon is a linear one. Okay? The relation on this, in this equation is a linear one. When we have linearized elasticity, and that's translated by the fact that C is a constant with retrospect to epsilon. Okay. I want to say a few more things about the properties of C. C has what we call as major symmetry. 'Kay? And it's important that we understand these properties betw, because we are going to use them we, develop our formulation. Okay. Major symmetry. What this means is that C i j k l equals C k l i j. Okay, and here's why. It follows. From. The fact. That. When one is dealing with linearized elasticity, linearized elasticity itself is built upon the idea that, there exists a function. That backward looking E is, the symbol for there exists, okay? There exists a function, psi, okay, which is a mapping from S3. S3 being the space of symmetric, second order tensor. Okay. There's a mapping from that space S3 to the space of positive release. Okay? All right? So S3 is symmetric second order tensors. It's, it's that space. Okay. Now, there exists a function psi which is mapping from S3 to R, it's in fact call, written as psi function of epsilon. Okay? And this is the strain energy density function. All right. We saw an explicit form of strain energy density function when we developed our variational methods for linear elasticity in 1D. All right, you consider it a quadratic form, there. We'll get into, into the particular form for psi in a, in a little bit, but let's just, for now, all we need to know is that this exists, okay? It can be shown then, that, right, one can show that C i j k l is the second derivative of psi with respect to epsilon i j, and epsilon k i. Okay? In fact, linearized elasticity, since it says that C is a constant, also says that, since C is constant, with respect to Epsilon, okay? It implies that psi is, at most, what? In fact, psi has to have a certain form. Can you think of it? Psi of epsilon is quadratic. Okay? So this second derivative of, psi with respect with epsilon, can be taken and, in general, it's not zero. Okay. Because C i j k l has that relation to the second derivative of psi with respect to epsilon, it follows that. C, I, J, K, L, which is equal to second derivative of psi with respect to epsilon I, J, epsilon K, L Is also secondary root of psi with respect of epsilon kl epsilon ij. What condition do we need to have that to be true? It is that in general psi is, let's just say here smooth enough. Okay, with respect to Epsilon. It can be more precise but for our purposes that's all we need to do. All right, if psi is smooth enough we can interchange the order of derivatives of epsilon, I, G and epsilon, K, L, right? But then if you look at the second, or rather the third member of this equation, this by definition is just Cklig. Right, and what we've called our sum major symmetry has just been proven. Okay? So, so the fact that C has major symmetry, it really follows from the existence of the strain energy function, strain energy density function, all right? It emerges however, that C has further symmetries, okay? C also has minor symmetries. Okay, which is that C, ij kl equals C, j i k l. Okay? And this follows because the stress Is symmetric. Okay, it turns out that the stress that we are dealing with here is properly called the Cauchy stress, but actually that's a detail that is somewhat lost in the context on linearized elasticity. Okay, it gets more important if we we're doing non-linear elasticity which we are not. Anyway, the Cauchy stress is symmetric. Okay? Which is that sigma i j equals sigma j i. Can you think of why that last relations holds? It follows from the balance of angular momentum, for this version of theory of elasticity. Okay? Well, if that is the case what we have is sigma ig which from our constitutive relation to cig kl epsilon kl and note that the kl and the Cs have been contracted out with the kl and the Cs of epsilon leaving free in the Cs ig. Okay? But this is also sigma ji which from definition is C ji kl, epsilon kl. Okay? These two conditions then imply that C ijkl equals C jikl, right, which is this condition of minor symmetry that we'd put forth, okay? C has the other kind of minor symmetry as well. Okay? So the other kind of minor symmetry is that C ijkl equals C ijlk. Okay? And why does this come about? This comes about because epsilon kl is one half, by definition, it's one half uk comma l plus ul comma k, right, and remember what that is, that is just one half derivative of uk with respect to xl, which is the first element of the parenthesis, plus derivative of ul with respect to xk, okay? Well, if this is the case, it follows that epsilon kl equals epsilon lk, okay? But then what that says is that sigma ij, which is C ig kl epsilon k l, is also C ij kl epsilon lk. Right? Okay? Now, if you look at, the order of the indices here. Because of this contraction, right? Okay? And recognizing that l and k are just dummy indices. Okay, it follows that this last term can equally be written as C ij, okay, lk, epsilon kl, all I've done there is to switch the order of the, switch the position of the key and l indices. Okay, it doesn't matter because they are dummy indices and they are being contracted out anyway. But then if you compare this and that, okay, it implies that C ij kl equals C, ij lk. Okay. Which is what we'd set out to prove for the second of our minor symmetries, right, this one. Okay. That sort of sums up what we needed to see about the symmetries of our elasticity tensor c, we will use, these symmetries in a very important way when we develop our formulations for finite elements. All right, we'll end the segment here."
TNpV5IJy5Bc,"Welcome back. We continue with our study of the constitutive relations of linearized elasticity. What we've seen so far is the fact that our elasticity tensor c is symmetric, right, it has major and minor symmetries. We saw what these mean and why they hold. Continuing. Right. In the context of linearized elasticity, there is a, another very important property that c owes, okay. So C is, positive definite. Okay? And, here is what we mean by saying it's positive definite. For all tensors and, I'm thinking of what I should use as a general tensor up here. Let me use theta, okay. I hope that doesn't disturb you, any of you too much if you're, gotten used to seeing theta as a, as temperature, or angle or something. Theta is a tensor for me. So theta is a general tensor, so it doesn't need to be symmetric, so I'm going to say it belongs to that space. That's not notation I've used before, GL3 simply means the group of general linear, I guess I should capitalize them, right, so, okay. General linear transformations. In R3, 'kay, that's what GL 3 is. It's just a fancy way of saying matrices in 3D, if you choose to look at heavy tensor as being a square matrix, okay. Anyway, so supposing we have this, right? Then what we're saying is that for all theta belonging to this space, we have the following result. Theta contracted with C, contracted with theta again, right? So we form this quadratic product of C with theta. Okay? This is greater than or equal to 0, right, the scalar, for all theta. Okay? In coordinate notation, this is what it means. Theta ij, Cijkl, theta kl is greater than or equal to 0. Okay, that's what we're seeing. It's useful to write this in coordinate notation just to be sure that this quadratic product is indeed a scalar. Okay, so it's greater than the real number 0. Okay. This holds for all theta belonging to gl3. In fact, what we can also show is that actually, the equality, right, so theta contracted with C contracted with theta, equal to 0, right, which is one of the cases of the, of the inequality on the previous line. The, the, this special case, the equality holds if, and only if, iff to mean if and only if, theta itself is the 0 tensor. Okay? All right. This is the condition that holds, okay. Now this is very important in the context of linearized elasticity. It essentially translates to the condition that linear elasticity or linear rise elasticity talks about materials that are fundamentally, not subject to material instabilities, okay. And this is important if you can imagine as you may imagine if you're going to compute with this model, right, we're going to put stresses and strains. We're going to put loads on it. It's important to know that material instabilities within the context of linearized elasticity are inadmissible, okay? So what this means is that linearized  elasticity theory,  has no material instabilities. Okay. If you're wondering what a material instability could possibly be, fracture is a material instability, that's not covered by linearized elasticity. Okay, you need to do special things beyond linearized elasticity to admit fracture. Likewise slip bands or shear bands, which form in the context of, of extreme plastic deformation are not covered by linearized elasticity well. You have to include plasticity theory for that, okay. That's always the same. It's important, however. It's actually very important, and we're going to see another result which , which, which brings out that importance. Okay.  what I'm going to do next is, actually give you a very actually, a very special form of the elasticity tensor, but one that is used very widely, okay. So we are going to look at, we're going to write out C, for materials that are isotropic. For isotropic materials. 'Kay, of course, this is for isotropic materials, long as we want to treat them, with, linearized elasticity. And of course, we're talking mechanical isotropy, right? Okay. Right, let me first write it in coordinate notation. Okay. In the context of linearized elasticity, the requirement of material isotropy also translates to the fact, and this can be proven, that it, that the material can be entirely specified elastically with just two constants. All right, and those constants are what go into C So with that being stated, here is what C looks like for an isotropic okay? One of those constants is lambda, I'm going to denote it as lambda. So, C i j k l is lambda times delta i j delta k l, where, can you image, can you guess what the delta i j refers to? Kronecker deltas. Right, and of course this one too. Okay? So we have that plus, 2 mu times one half delta i k delta j l plus delta i l delta j k, all right? If you wonder why I put, put in a 2 mu and then reintroduced the half, it is because what I have here is, also often recognized as a fourth order tensor, okay? This is often denoted as sort of a bold faced I with indices i, j, k, l, okay? And this is what we call the fourth order. Symmetric identity tensor. Okay? What it does is the following. If you take I as defined there, okay, and, feed to it any second order tensor, right? So let's use theta which we used in the previous, slide, right? And remember, theta is just any old second order tensor. Doesn't need to be symmetric or anything, right? It turns out that I acting on theta spits back for us theta, but the symmetric part of theta, all right? Which is, since I'm using coordinate notation, that is properly written as one half theta i j plus theta j i. Okay. So it sort of gives us back theta, but the symmetric part of it. All right, so if theta were already symmetric, well it would just give us back the same tensor. However, in general, when we feed it a non-symmetric tensor, a non-symmetric tensor, this identity gives us back just the symmetric part of that tensor. Okay. All right. Right. Now, in writing direct notation, so I'm going to use this I in writing the direct notation version of this relation, in writing the direct version notation of this relation, delta i j, the Kronecker delta, I like to write as, what I call a bold faced one. Indices IG. Okay? If you come from the world of matrixes, that's just your three by three identity matrix. Okay? All right. So, direct notation for the same relation is the following. C equals lambda: one dyadic one, okay, that's the dyadic product. Again, to know what it is, well just look at the direct notation, that's what it means, okay? I often call this the tensor product, plus 2 mu times the fourth order identity, symmetric identity. Okay? Note that because each of these, tensors, each of these ones is a second other tensor then they're dyadic or their cancel product gives us the for, a fourth order tensor. Okay? So this is direct notation. All right. What about these constants? Lambda and mu are what are called Lame constants. OK? And if they seem unfamiliar to you, they will become familiar in just, a few seconds. And here's how they're related, right. If now, you use something that you're probably really more familiar with if E is the Young's modulus. And if nu is the Poisson ratio. Okay? Then, the relations are the following. The Lam constants can be related to these guys. So Lambda is E nu over 1 plus nu times 1 minus 2 nu, okay? And mu is E divided by twice of 1 plus nu, okay? Chances are the Young's modulus and Poisson ratio are something that you've encountered almost certainly in, in your, second year of, your study of mechanics or, or, or, or engineering or something like that. Do, do you recognize what this is? Do you recognize what mu as defined as that also gives you? That should be a little more familiar if you recall these sorts of relations. It is the Shear modulus, right, all, of course, for isotropic materials. Okay? Lambda is not related to Shear or anything. It's just often just called along a parameter. A third, yet another modulus that you may be familiar with is kappa, which is the bulk modulus. Right, so kappa is related again to E and nu as E divided by 3 times 1 minus 2 nu. Okay? All right.  So, there are limits on, these, constants, and I'll talk about them in just a bit. It, it emerges that, mu can lie between minus one. And half.  All right, for the linearized theory of elasticity. Okay? Do you recall what this limit Corresponds to. No, this corresponds to elastic incompressibility. Okay? And as you may imagine from here, if you approach the limit mu equals minus 1, you get a material that is shear unstable. All right, so actually well, actually to use this theory properly, I guess I should, let's just do that, okay? All right, because otherwise, linearized elasticity tends to break down in those limits. So let's, let's just leave those actual limits on, okay. Right? Also from here you, you may be able to see why mu equals half is the limit of incompressibility because it makes the bulk modulus get unbounded, okay? So you have a material that's very, very stiff to, to compression, right, like that. Okay, fine. So, let's get back to talking about, positive definiteness, all right? So having put down this form of the elasticity tensor for an isotropic material, the, the condition of positive definiteness. 'Kay, it translates to a requirement on our on the elastic constants that we were working with on the previous slide, okay. In particular, one can demonstrate the following. Positive definiteness implies the following conditions. It implies that lambda plus 2 mu has to be greater than 0, okay? It also implies that mu itself has to be greater than 0, okay? If you come from the world of structural dynamics or acoustics in elastic materials, these conditions may mean something to you, okay? You recall it. No? Okay, well, let me see. Well, maybe some of you do know. Well, the reasons these are, these are important conditions is the following. In elastic materials waves propagate in at least two ways. In the bulk of a material, as longitudinal or sheer waves. It emerges that the longitudinal wave speed, is lambda plus, sorry, that lambda looks bad. The longitudinal wave speed is lambda plus 2 mu divided by the mass density under the square root, okay. So when we say that lambda plus 2 mu is great than 0, what we are saying is that we have real wave speeds. It allows longitudinal waves to actually propagate through the material, okay. And, the sheer wave speed, is square root of mu divided by rho. Okay, so these conditions on lambda plus 2 mu and mu essentially allow us to have, propagating longitude and shear waves. In the elastic material. In the elastic material that we are describing with this linearized theory of elasticity. Okay? Let me see, now. Now, it turns out that in most materials of interest, the longitudinal wave speed is greater than the shear wave speed. Okay, and that's a fact that is sometimes used in geophysics too, when people are, are dealing with earthquakes. Anyway, all right, that is probably the basic material we need to know about our constitutive relations for elasticity, for linearized elasticity. And there is more of course that can be said, but it's not, but, but we won't, we don't really need to get to it, I think. Okay, so we're going to end the segment here. When we return, we will work with the weak form."
gJZg7YLUw_c,"All right, welcome back. We continue and we will get down to business now. And we so by writing out the weak form . The weak form for linearized elasticity. Right so. I'm going to take the approach we, adopted when we worked with linear elliptic PDs with scalar variables in 3D. Okay, which is, and I'm going to put down the weak form. And then, I will demonstrate how we obtain it from the strong form. I won't go back the other way to just avoid just, because those, those arguments are, identical to what we did in 1-B. But they are, you know, they're more subtle and, and so on. Okay, but however we know how they, how they operate and, and they hold. Okay, weak form of linearized elasticity. And, and, and from here on, what I'm going to use is called notation only, okay? Because there are details, as you can see, which become most transparent when we use coordinate notation. Okay, so given the usual data. Given ug sub i t-bar: i, f, i, the constitutive relation. Sigma: i, j Equals cijkl, epsilon kl, and the kinematic relation, Epsilon of kl, equals one half. Partial of u k with respect to x l plus partial of u l with respect to x k. Okay? Given all of this, find ui, OK? Which, belongs to some space S, OK? Which consists of all u i such that u i equals the Dirichlet boundary data on the corresponding Dirichlet boundary reserved for that particular component. Right? Of course what's implied here is i equals 1, 2, 3. Okay? Find u i belonging to s in this, of, of this type. Such that. For all wi belonging to v which now consists of all functions, which satisfy the homogeneous boundary condition on the corresponding Dirichlet boundary, and corresponding Dirichlet boundary, right? Here, too, we imply that i equals 1, 2, 3, okay? Okay. So let me read this all over. Given the data u, t-bar, u g, t-bar, f and constitutive relation. The constitutive relation and the kinematic relation, epsilon equals that turtle's derivatives of u. Find u belonging to s such that for all w belonging to v, the following holds, right? Alright, so now let's write out what holds. Integral over omega, wi,j, sigma, ij, dv equals integral over omega w i f i d v. Plus, now, here comes the rough. Because of the fact that our traction boundary condition needs to be specified individually for each component, we need to straight away here have a sum i going from 1 to number of spatial dimensions. And of course, we're doing this in three dimensions, right? Of integral over the corresponding traction boundary. Okay? Of wi t bar i ds. I want to emphasize here that, do we imply a sum in that integrand over i? Think about it. Look at the last sum. There is in fact no sum. Ok, a sum is not implied there. A sum is explicitly being carried out here. But importantly we don't do that over the integrant, because, why do we not do it over the integrand? It's because that domain of integration could be different for each component i. Okay. So we can't quite do it as an implied sum. What we need to do instead is compute that product for each value of i w i t-bar i. Integrated over the corresponding Neumann boundary, right? Which could be different for each component, right? We get a scaler, right? And then, we sum up those scales, right? Over to three special dimensions. All over the special dimensions. Okay? That's important. So let me state here. No sum implied for, for just that term, wi t bar i, okay? Instead, we have that explicit sum. All right, this is our weak form. What I will do know is derive it from the strong form. OK? And I, I'm going to take a bit of a short cut. I'm not going to re, well, OK, maybe I will restart all the data very quickly. So, what we, what am, I'm going to state here first that the strong form Implies the weak form. And the weak form also implies the strong form, but like we did for the case of the scalar 3D elliptic linear equations, We'll do only the rightward implication here, right? The rightward equivalence just, just to see a demonstration. All right, and that is the following, right, once again. Given u g i, t bar i, f i the constitutive relation sigma ij = Cijkl Epsilon k l. The kinematic relation. Epsilon i, epsilon, let me write this as epsilon k l equals This. Okay. What we want to do is find ui such that sigma ij comma j plus fi equals 0 in omega and boundary conditions. U i equals u g i on the relevant tertiary boundary. And sigma i j nj equals. T bar i on. Okay, so this has wrong form. Alright, yet again. Alright, we start out from here and now we introduce as we did before, the waiting function, right? So what we start off by saying is now let us consider. W i, belonging to v which has the usual property right? Which is the v consists of all wi such that wi vanishes on that Dirichlet boundary. Okay. What we will do now, and this is something you may recall from our previous repeated development into weak form. Actually, think about, what do we do? How do we proceed now? Alright we multiply that pde with wi and integrate. Okay, so, what we do is, multiply the pde by wi, and integrate over our domain omicron. Right? Which is an open domain in r, r3. All right, fine. When we do this, here's what we get. We get integral over omega wi sigma ij, comma j, db. Plus integral over omega, Wi Fi d V equals zero. Okay, here sums are implied over i. Okay, so, I'm not going to state it which means sum is implied, right. Okay. So, do you recall what comes next? Right, we observe that this, divergence, right, we get rid of by, invoking integration by parts. Right, and integration by parts, you may recall, is nothing but a combination of, the product rule of differentiation and Gauss's Theorem of Divergence. All right, apply to tensors now. So, what we do here is integrate by parts. Right? And we integrate by parts in that term, okay? And in integrating that parts, remember that this is basically product rule of differentiation. Plus the divergence theorem. Okay? And the way we do that is the following, right? We observe first that that derivative on sigma can be rewritten if we just consider it to be the following. Integral over omega, a derivative of the whole product. Okay, minus an integral over omega, wi comma j sigma ij dv. And what I've done here is applied the first of my to two techniques here, right? I've applied the product rule here. Okay. And of course, along for the ride is, W, I, F-I, D-V, integrated over omega. The whole thing is equal to zero. Okay. The next step, we invoke off the divergence theorem on the first integral. And in doing so, we observe that since the i index has been contracted out here, sum being applied, that term in parentheses is essentially a vector, right? A vector with free index j. We recognize that we have properly here the divergence of that vector, right. That vector for our purposes is something that you may probably write as w dot sigma, okay. It means we just have the divergence of that vector. Right, so now we apply the divergence theorem on. That term, right? And in doing so, what we get is integral over the boundary, okay? Wi sigma ij mj ds minus, the other terms don't get don't change. Minus w i comma j sigma i j d V plus integral over omega w i f i d V equals 0. Okay. We have this. And, what we are going to do now is, is two things. I'm going to rearrange things a little. And the way I'm going to rearrange this is by observing that this term has a negative sign. So I'm going to move it to the other side. But I'm going to change my left and right hand sides."
vWcNQ5r5KJ8,"So there was a question on how the summations over spatial dimensions are handled and to answer it, it's probably best to start out with the strong form, okay? So I'm going to just talk about. Over i equals 1, 2, 3, right? Spatial dimension, three. Okay. So this is the question that I'm going to try and address. The, the way to start off with it, probably the best place to look at it is considered the, the strong form. Again, I'm, and I'm not going to write out the entire strong form because I think we are all a little tired of  seeing it too many times. Okay, strong form, just the PD, right? Sigma ij, j plus fi equals 0. Now, remember this holds for i equals 1, 2, 3, so really what this means is that what does it mean? It means that there three equations, right? Three separate equations, and those equations, I'm going to write them out explicitly, are sigma 1j, j plus f1 equals 0. Sigma 2j, j plus f2, equals 0. Sigma 3j,j j plus f3 equals 0. Now in each of these, when I say j comma j, there is a sum implied on j, right? Sum implied on j. That statement has got to be an irony because the moment you put it down it's used the implication is no longer implied, right? It becomes an explication. Anyway. All right, so this is what we have okay? So, so when we look at the strong form, we really have three, three PDs, right? Three equations that are being solved independently, right? Each of them have to hold. The weak form, however, is different in, and, and you remember how we get to the weak form, right? Now, now in order, in getting to the weak form, we say how do we get to the weak form? All right, we take that same equation, sigma ij, j plus fi, right, equals 0. We multiply it through by wi, okay? And now we're seeing there's a sum on i and j. Okay? In so doing, we have essentially collapsed the three, the three equations that we started out with into one. Right? The explicit form of that is I'll write it out just once, w1 sigma 1 j, j plus w2 sigma 2 j, j plus w3 sigma 3 j, j plus w1f1 plus w2f2 plus w3f3 equals 0, right? That's what we're doing and then we integrate this over the volume. Right? So here too, sum over j. Okay? And then, of course, we do integration by parts and so on, but observe that here too, even though we started out with three PDs, we have collapsed them all down to a single weak form, right? A single equation, right? A single scalar equation really, okay? And then, of course, from here when we go on, you know, we do integration by parts and so on. What we end up is a term where we have integral over omega wi, j sigma ij dv equals integral over omega wifi dv plus the sum over i, integral over this, wi, t bar i. Okay? Now, the form of this is, is the following, the explicit form. It is integral over omega w1, 1 sigma 11, plus w1, 2 sigma 12.  Right? And somewhere along the line, we come a lot, come upon w3, 1 sigma 31 plus, until finally we come to w3, 3 sigma 33. All right, there are nine terms in here. Sorry. Yeah. Nine. Okay? Nine terms in here, dv, okay, equals the first term on the right hand side has how many? Three, w1f1, it goes until w3f3. Okay, and then this meddlesome term involving the sum, right, over the Neumann boundaries. I'm going to, I'm going to write it over here, because I need a little room. Okay, so that is integral over partial omega t bar 1 w1 t bar 1 dS plus integral over omega t bar 2 w2 t bar 2 dS plus integral over, the third Neumann boundary subset, w3 t bar 3 dS. Okay, and the key in this case, that each of those boundaries' subsets could be different. Okay, so over each of them, I mean clearly there's no sum implied, right, because I've written everything explicitly. Okay, so over each boundary subset, you're only integrating its corresponding components multiplied. Okay? So, all right, so that, that's how it works the weak form is a single equation, unlike the strong form, which is three."
QtwjiOnHICE,"So, essentially, what I'm going to do in this process is the following, integral over omega wi comma j sigma ij dv equals integral over omega wi fi dV plus integral over the, the entire volume, right. Wi sigma ij nj dS, all right. Now, here comes a step that is a little different from what we've done in our previous application to a three dimensional problem, okay. What we are going to use is this special decomposition of omega into. Into subset spatial omega ui, which is tertiary boundary and, Neumann boundary, okay. This holds for i equals 1, 2, 3. Okay. So what this says is that the way we're going to apply this is by regarding this product as a vector with index i. And then observing that we essentially have a, in that integrant, a dot product of w with that vector sigma n, okay? So, the way I am going to write that is the following. And we right that as integral over omega, okay, so let me write everything here. Integral over omega on the left hand side, wi comma j sigma ij db equals integral over omega wi fi dv plus. Now, because that integrand when viewed as a final, dot product of w with the vector sigma n. Right? Because that integrand has three terms in it, right? For i equal 1, 2, 3. We're going to write each one of those contributions of the dot product, right, separately, all right? So I'm going to write that as each, so each, of these decompositions of, of the boundary, all right, are going to be used, to give us the domain of integration for each integrand. Sorry, for each contribution to this integrand, right. Okay. All right, so, so the idea is you fix i, maybe i equals one, right. So you do the integral by viewing our boundary as a union of these two subsets, boundary subsets, right, but in computing that integrand you compute only for a fixed i, okay. So you fix these guys, right, these are fixed. Okay, fix everything, right. So fix all those i's. Now that gives you, the integral with just one of those components. You need to have all the components, and you do the sum. Right? So far, we've just written that integrand differently, okay? That's all we've done. Sorry, all we've done is write that integral, differently. Okay, but now we can do something nice, right? And what is that? We invoke our boundary conditions, right? Now we invoke. Boundary conditions on wi and sigma ij nj, okay. All right. And remember the way we wrote those boundary conditions, okay? Essentially, the way we wrote those boundary conditions, say that, now that we're integrating over each, special decomposition of the boundary, what we can do is this, right. So let me actually go to the next slide. So, what we can say here is the following, right. So we have integral over Omega, wi comma j Sigma ij dv equals integral over omega wi fi dv. That term has got to, got to be the most tame term we've ever encountered, right? The, the forcing function pretty much does nothing. Okay, plus. Sum i equals 1 to number of spatial dimensions integral. Now, I'm going to break up that integral, right? Because we've written it on the previous slide as an integral over the union of tertiary and Neumann boundary subsets, right? So I'm going to write it as integral over partial of omega ui of wi sigma ij nj ds plus integral over partial of omega t bar i, wi sigma ij nj ds. And I close my parentheses here, 'kay? Actually, looking at, what I've saved here from my previous slide. I'm going to go back for just a second because I realized that I did not put my elemental area, that integral, okay, but you may have caught it. All right. Here we are. All right, now is when we invoke the boundary conditions on, on wi. And what is that boundary condition on wi over the corresponding tertiary boundary? It is that it, vanishes. Right. And what is our boundary condition on sigma ij nj over that part of the Neumann boundary? It is that sigma ij nj equals at t bar i component, okay? And so we have it. We have integral over omega, wi comma j sigma ij dv equals Integral over omega w i f i d V, plus sum i going from 1, to a number of spacial dimensions. Integral over the corresponding Neumann boundary only, wi t bar i dS, okay? This is our weak form, all right? Which we've derived from the strong form. Like I said, we can go back the other way. But, it's not something we get into here. I will make one remark though. Which is that, just as we did in our little treatment of variational methods in the context of the linear, linearized elasticity problem in one leap. All right? We demonstrated there that the weak form could be obtained through these variational arguments of extremization of a free energy functional in one day. Exactly the same things holds in three dimensions as well, okay? So, the weak form can be obtained as this so-called Euler-Lagrange conditions The Euler-Lagrange conditions of a variational principle Right? And what is the variational principle? On well it's not minimization necessarily but it's really, it's extremization, it's on extremization. Of a free energy functional in three dimensions. The problem we treated a few segments back, quite a few segments back, quite a few units back, was in 1D, but essentially all those arguments hold in three dimensions. And actually, this is a very powerful approach. While in the case of the simplif, the simplest stuff, formulation of linearized elasticity, it doesn't give us anything that we can't get through other methods. Something you may be aware of is that there are so called constraint problems, right, and a common constraint on linearized elasticity is incompressibility. So if you're dealing with a linear, with a linear elastic material that's also incompressible. It emerges that the methods we work with, the standard finite element methods we work with, and that we will be developing now, do not work well. Okay? And there are fundamental pathologies which arise, and these are very well understood. And methods to circumvent that difficulty, right, are based upon defining more generalized versions of these free-energy functionals, introducing more fields and working with them. It's the domain of what are called mixed-methods, okay, and it's a very powerful approach and which, which arises from this, from the variational basis of the method. Okay, but we won't be getting into that in this entry level series of lectures. Okay, the very last thing I want to do is just go from there to putting down the finite dimensional weak form. Okay, so what we have here is will be just derived is the infinite dimensional the weak form.  Okay. The finite dimensional weak form. Follows. All right, and now of course you're quite expert at doing this, and you know exactly what that's, what going to happen, is going to look no different except for this pacification spaces, right? But let's just put it down so that we will be able to move on with the formulation. Okay, the finite-dimensional weak form. As before, given Ugi, t bar i, fi, the constitutive relation sigma ij equals Cij KL epsilon KL. Right? That's the form of the constitutive relation. Let me state it That's a constitutive relation and the kinematic relation epsilon KL equals the neutral stuff. Right? Given all these, find, now, now here's where things become different, right? So actually let me put it on the next slide, so that we can, give it its due attention. Find sub i, right? And as, as always that superscript h reminds us that we're talking of approximations and h is going to be related to the element size. Okay,that's all it denotes. Okay, to find of i, belonging to sh, which is a subset of s. Okay. And what do we mean by sh here? sh consists of all functions. Now, we further require as we've done before that we are interested in functions that are H1 over the domain. Okay? And, of course, they must satisfy in this class of finite element methods that we're working with throughout the series of lectures. We do require that they satisfy the Dirichlet boundary condition on the relevant subset of the boundary. Okay? So, find your weight of this type such that for all wh sub i belonging to Vh subset of V, okay? And what is Vh? Vh consists of all waiting functions, also H1. Such that, Wh of i equals zero on. The corresponding Dirichlet boundary. Okay, so find Belonging to Sh such that for Wh belonging to Vh. What all is well, the weak form that we wrote before, except its called the relevant fills, replaced with their finite dimensional versions. Whi,J, sigma hij, dV. Right? And we recall here that sigma h i j is simply computed by a C i j k l epsilon h k l. Okay? And what is epsilon h k l? Well, it just obtained from this relation. With u h. Okay? All right. This is equal to integral over omega. Sorry this is over omega too. w h i f i Dv, okay? Remember that, as before, we do not consider finite dimension inversions of, f, right? The, the f is part of the data, and we take it as given. Plus sum over the spatial dimensions. Of the integral over the Neumann Boundary, of Whi, t bar i, ds. Again, because d bar is part of the data we don't try finite dimensional representations of it, okay? This is it. Just for completeness, let me squeeze in here that epsilon hkl equals that. This parentheses closes that one. And this closes that entire node. Okay. All right. We are done with this segment. When we come back we will just work with this to develop our finite element formulation"
kDfW0vYt_-g,"Welcome back. We have got as far as deriving the finite dimensional weak form for 3D linearized elasticity. What we're going to do in this segment is essentially press ahead with the formulation. And of course you know how this proceeds. We essentially have to define basis functions, construct, representations for the fields that we care about, and go ahead and compute the integrals that arise in the weak form. Okay, so I'm going to call this segment the finite dimension weak form and basis functions. Okay, and remember just the integral equation we are working with is the following. I am not going to put down the whole finite-dimensional weak form in all its glory, but just remind ourselves what we are trying to accomplish here. We are trying to solve the following equation. Integral over omega Whi, j sigma h ij dV equals integral over omega. Whi fi dV plus this curious sum of integrals which has this form. Sorry a w bar is not required. Whi, t bar i, dS. And I'm not going to state it now but just recall that on the last interbrand we don't have a sum implied. Okay we have this and of course there are constitutive relations and we've studied those and summed and summed it. As you imagine now, we, we essentially have to define basis functions, right? And, and in defining basis functions as before, we will do this by constructing a partition of our domain, right? We partition our domain as follows. Basis our, domain of interest which is really the body that we want to solve and the linearized elasticity problem over that's our domain Omega. Actually, let me leave that outside of the body because we will confuse it with other things, that's Omega, we have the usual decomposition of the boundary subsets. And right, the, the way we do the decomposition is essentially the way we did it in the case of the 3D elliptic problem with scalar variables. All right. ,the kinds of elements we considered there are admissible here as well. Okay? And to fix ideas, let's look at hexahedron. Okay, so that is an element omegas, omega e and, that part, the partition of the domain is as always given to us as follows. Union over e, of all these open element sub domains, closed gives us the closure of the body, okay. As always, we have omega e1 intersection omega e2 is the empty set, right there at this joint. Okay, we have this and we're going to go ahead and construct our basis functions using this sort of decomposition. Equivalently now that we've talked about it, we could also have had a tetrahedra, ED composition into tetrahedra. Okay. Alright. The way we are going to go ahead and construct our, basis functions is as we know, very well is to construe, to consider nodes on the element. And again, just a fixed idea as I'm drawing things out here, I'm going to consider bilinear, sorry, trilinear hexahedra. So those are our knowns. We only have knowns at the vertices. Ok and we know that Okay, we know that digit. Right. So now let me suppose that this is element omega e, right? This is the element that we started out with which is now being essentially expand out here, okay, that's element omega e, and that is our node A equals 1 for the element, right? I'm following here, the local numbering of nodes. Okay, that's two, three, four, and so on. We are quite expert at this by now. All right and we're going to construct our basis functions from these from these nodes. The way we do this is to, to essentially define these basis functions on the nodes and we are going to define, write the basis functions as before as NA. Okay, our basis functions. This is our basis function at node A, okay? Let me say here, local node A. Now, something you may have noticed is over the past minute or two, as I've introduced, the nodes and the numbering. I have not been calling them I've not yet started calling them degrees of freedom. Okay, whereas earlier, I was using the term, I was using A more often for, in the context of degrees of freedom. But here, I'm being more careful to call them nodes and, and, and here is why. Okay? The reason is the following. We're going to construct a, representation for our, trial solution, all right? And remember our trial solution here is the, displacement field. That's component i over element e. All right? Now, we are going to use a representation where the basis functions will be exactly the same basis functions that we are so familiar with, right? If we are doing trilinears, we know what these are, right? And they're the same basis functions. However, we, and the sum of course is over A going from one to number of nodes in the element. Okay? However each one of these basis functions is multiplied by a degree of freedom vector. Okay? What this means is that for component U hi I have dA, ie. Okay, and we need to recall, here are the i, runs over one, to number of spatial dimensions. Okay. So this is a little different because now if you think about it, how many degrees of freedom does this element have? Okay? So the number of degrees of freedom In omega e, right, or on omega e, right, is. What is it, is it just the number of nodes as it was in the previous problems we were doing. No, correct. So it is actually, in the general case, it's number of nodes in the element times number of spacial dimensions, okay? Whereas, earlier the number of nodes on the element was, same as the number of degrees of freedom in the element. Things are a little different now. Okay, so what we have here is, another way to write this is the following, we use direct notation, right? So now that is the displacement field over element e and this is sum over A NA dA e, as before, except that that d A e, whereas it was a scalar for our problems of scalar variables is now a vector, right, for our problem of vector variables. So and, and what we have here is u h e, and d A e both belong to R3, right, the three dimensional vectors, okay? So sorry for seeming to flog a dead horse, but what we have here are vector degrees of freedom at each node. Okay, and this is a particular approach that we take for this problem, it is not necessarily universal, okay. So we have here a representation for a vector field and what we're seeing is that the basis function is here scalar, just as we've been doing all along. Degrees of freedom here carry the vector information. All right, and this is common for the kinds problems we're doing. Definitely for, for elasticity of, of, of any kind, of mechanics of any kind. It is not the same, for instance however in the case of the problem of electromagnetics, 'kay? It's common in that case to construct finite element formulations where the basis functions are vectors and the degrees of freedom are scalars. Okay? It's, it's just a difference. It's, it's a, a difference into, to do with the requirements of each problem and so on, the mathematical requirements. All right, so, so that's something to know too. And, and then of course it's the same thing for the, for the waiting function. We have wh ie equals sum over A NA CA, ie. Using coordinate notation. Right. Where we know that i Equals 1 to number of spacial dimensions. Right, and using, direct notation that would be wh e equals sum over A NA cA e, okay? As we noted above, Whe and CAe are 3D vectors, vectors in R3. Okay. The, the next thing we need to do if we are happy with this is to just remind ourselves of what the basis functions are. And these are constructed in exactly the same way that we know. Alright, so what we are seeing here is that as before if we have an arbitrary hexahedral element. Okay? That's omega e. We constructed as, as always, really from a bi-unit domain. All right, and everything, everything proceeds just as before. Nothing new here. Okay, so, in this setting we would have, in this domain again, we would have A equals 1, 2, 3, 4 would be in the back, 5, 6, 7, 8, right, and this is a bi-unit domain, so this point is as always, minus 1, minus 1, minus 1, right? And so on that point, our number 7 would be 1, 1, 1, and so on, right? We know this very well now, okay? This is how we construct our basis functions having defined these coordinates, c1, c2, c3. What we see is that as always NA c1, c2, c3 will be constructed from you know, these, these one dimensional Lagrange polynomials, right? And NA tilde, and I forget what I call them, let me suppose I call them A bar c1 N tilde, E bar, c2, N tilde, c bar, c3, I may have used different Our notation for the, for the indices a bar, b bar, c bar before, but, but I think by now we understand this very well, right, each of these as we recall is a 1D Lagrange polynomial Okay? And we know exactly how to construct these, in the case of bilinear, sorry, trilinears, triquadratics, and so on. Okay, and since we are so good at this now, I, I don't need to belabor the points, so I'm, I'm going to tell you right away that as always, we also use the the same sort of basis functions to interpolate the geometry, right? That part, of course, continues exactly the same because even when we were solving the linear elliptic problem in 3D with scalar variables. The geometry still was fully 3D and the geometry was indeed defined by by position vectors and so on. Right? So that bit is exactly the same. Okay, so we have x e, as a function of the C vector. All right? That's exactly the same map that we had before. Okay?"
oCm0rxmWrxs,"Okay, so we have an isoparametric mapping. For geometry. All right. So that says, as we know very well now, x sub i sub e, all right, As a function of, c vector is again obtained as a sum over the nodes, n a. Right? And now, na we know is a function of, is parametrized by the coordinates, and this by unit of mean. And we have here, xA ie, right? this is exactly the same as before. Okay, and once we have this, we know what other use we can put it to, right? Do you remember what else we do with this? All right. We use it to write out our gradients, right? So, for gradients. And what are the gradients we need when we look back at our finite dimensional weak form, we, we already see this, that sort of gradient. Right? Whi comma j. Right? This now over the element is sum over A. NA comma j, cAie, all right? I'm, I'd probably add an e in there, as well, okay, just to keep directing it. All right. And, of course, the same thing for uhi comma j. All right. Uhi comma j, is over element e. Is again, sum over A. NA comma j, dAie. Okay, And we know straightaway where we expect to use this right of course this shows up directly in the weak form, when we look at it, right, just recall. Recall the term on left-hand side in our weak form, integral over omega, whi comma j. We have sigma h, but we know of course that sigma h is just Cijkl. Okay. Sigma ijCijkl epsilon hkl. Okay? But here is where I'm going to use the, one of those properties of our, elasticity tensor. And the property I'm going to use is the fact that where as the string is properly defined as the symmetric bar to the displacement radiant. Right? That's the stream. However, because we've talked about how the elasticity tensor is Is possessed of minor symmetry in those two indices, we're actually completely free, to not worry about the symmetrization of the displacement gradient. In, in using this constitutive relation. Okay, so what I'm saying is that we are completely correct, long as we make sure that c has minor symmetry, you're completely correct in writing this as Cijkl, u,k, comma l. Okay. And, the reason for this is minor symmetry. Okay? All right? And, and, if you're wondering, yes, it is indeed of course true that, for that term also, whereas, I've just written it as a gradient of the weighting function, we would be right if we want to go the other way there, and say that it's the, it's the symmetric part of the gradient of the weighting function. All right? But we just don't need it. We can leave it this way, a minor symmetry with respect to the ij indices, which we also have, takes care of things for us. Okay. But any, but at any rate, we see clearly what gradience we need to compute, right? You should remember that k and l are of course just dummy indices because they are being summed away, right? Or even otherwise the would be dummy indices. And so when, when I say I am going to compute i comma j here it's, it's really the same. All right, so, we do need to compute gradients. But once we have our isoparametric mapping, we know how to go about that as well, right? And what we are seeing here now is that NA comma j is just NA comma xi capital I, right? Using as before, the uppercase index for coordinates in the biunit domain. You have this times the derivative of xi I. With respect to X, J. Right, for all J. As before. Alright, that's one, two, three. Okay. And of course, we know how to compute this now. Right, using the Jacobian of the mapping. All right? So, what we do here is use the Jacobian of the mapping. All right, and that, when we write it out as a matrix, is the following. Right, J. Right, depending upon the coordinates and the by-unit to me, is You know this all very well now. X 1 comma c 1, x 1 comma c 2, x 1 comma c 3 equal 1. Until we come down here and we get x of 3 comma xi 3. Right? But then of course, we also know that J inverse of C. Is. C, one, comma, X, one. Under C, three, sorry. One comma X3, and we go on. Until we come to C3 comma X3. Okay? And once again because we have only a three by three to invert here, it's actually not that hard to do it analytically. Right? So in our element formulations, as I suggested before for the steerer problems, we can do this analytically. So we thereby get each of these derivatives that we need. Right, and therefore we know how to computer the gradience. Okay, well, if we know how to compute the gradience, we can go ahead and, assemble our problem. Right, our, weak form. We do that in the next segment. We'll end this segment here."
034gY2jXGnY,"All right. So, we'll move on. We've established our finite dimensional weak form. We've looked at our basis functions. We made the somewhat important observation. That for this problem, because we have a vector problem, the number of nodes is not identical to the number of degrees of freedom. Right? But then we made the identification and we know how to relate them. All right. Let's go on. We'll get into assembling the integrals in our weak form, in our finite dimensional weak form. So, we will call this segment the element integrals. And in order to get to the element integrals, we first need to make the observation that our finite dimensional weak form. Because it is an integral over the entire domain, right? Or the boundary of the relevant Neumann boundaries also allows us under this partition to write it as a sum over a, the, the individual sub-elements, right? Or the subelement boundaries. Okay, so the finite dimensional weak form can also be written as. Sum over e, integral over omega e, w h i comma j, sigma h i comma j, dv equals sum over e, integral over omega e, w, h, i, f, i, d v. Plus, now here, we need to be a little more careful, right? We are talking here of doing the following. We are talking here of doing a sum I equals 1 to Nsd. Okay? We're summing here over our spatial dimensions. And where as earlier we could directly put down the corresponding Neumann boundary, we are going to write that also as that integral also as a sum over sub-integrals, right? Each of them is a boundary. So we need to say here that we are going to take the sum, not over all the elements, but the elements belonging to let me see, right, En, right? Remember, E was a set of elements that had one of their boundaries coincide with a Neumann boundary. All very well except that that too needs to be indexed by the spatial dimension. All right? Because for each of the i's, i equals 1, 2, 3. We could have a different set of elements whose boundaries coincide with the Neumann boundary for that particular dimension. All right? Okay. So we have that and then the integral is over partial omega t bar, I, and I'll put the E up there. All right. And this, of course, is the same whi t, bar I, d of S. Right? And all right, so this is what we have. What we are going to do is just as before, assemble each of these integrals over the elements of domains, or in this case, over the special element boundary sub-domain. Right? And of course we are also going to use the fact that this is given to us by B, elasticity tensor, now multiplying U, K, comma F. All right. Okay. So, we're, we're going to proceed with this now. So, we've, we will as always start with the, with the left hand side integrals, consider first integral over omega e, w h i comma j I'm going to directly go to the representation including the elasticity tensor. Okay, so sigma h i g. Oh, if I'm going to use the sigma h, I need to have an h there, all right. Because that's where the finite dimensionality of this comes in. Okay, so we have here C, i, j, k, l, u, h, k,I dv, right? Now on a previous slide, belonging to the previous segment, we wrote out the, the expansions for w h, and sort of, for the gradient of w h and for the gradient of u h. We will now invoke both, right, and in doing so, I'm going to write it here. We have here again, allow me to skip, well, okay. Mm, I won't skip a step right away. going to write this as integral over omega e, the w h, w h i comma j, is written as sum over A, N A, comma j CA I E, okay? This is whi, j, and I put it in parentheses. We have here C, I, J, K, L. And again we have sum over B, N D. Now, the gradient is represented as A, using the coordinate of direction L. And of course it's, L runs over 1, 2, 3. So that means n b here has to take a comma l, right, that's where the gradient is computed. And we have d b k e d v, okay. Now if you were go back and stare at the way we did the corresponding step for our scalar 3D problems, you would see something very similar except the fact that we had no index on the degree of freedom, degrees of freedom for the reading function. Nor on the degrees of freedom for the trial solution for the displacement field, okay. The fact that we do have indices here, comes from the fact that these are actual vector degrees of freedom. And i and k here both run over one two three. And the fact that we have those indices also shows up in the fact that we have a couple of extra indices here, right. The coefficient there, if you want to think about it as that. The elasticity's tensor's actually coefficient of elasticity, is a full taut tensor unlike the conductivity tensor or the diffusivity tensor, which are second order indices, all right. So that's, that's really all there is. And once we take, once we understand that, and we are comfortable with that bookkeeping essentially, all is well, all right. So we proceed, and as you may imagine, what is the next step here? Or as, as you may recall, what is the next step? Right. It is to observe that the degrees of freedom, nevermind the fact that they're vectors now are still, after all, independent of position, all right. And so they can be pulled out of our integral. And in doing so, we can also because integration and summation in this case commute because of linearity and all that. We can pull those summations also outside the integral, right? Okay. What that gives us is. Now I am going to write this as a sum over A and B. And let's recall, let's remember just here that A and B run over 1 to number of nodes in the element, right? Integral, oh, sorry. We need to have here c A i e, okay? I have here, NA comma j, okay, C i j k L, right? And for our representation of the gradient of the displacement, from that representation of the gradient of the displacement, we get an NB comma L here. Okay? Our dV, all right, I'll put a parenthesis here, or pair of parentheses. And, all right. What we've done in the process is pull the d B k e out, okay. Right? Straightforward enough. Oh, this is an integral over omega e, right? To proceed, and remember that we know how to compute these gradients, right? We looked at that on a, at the end of the last segment, right? So we know how to compute these gradients, no problem, all right. Let's move on. Remember what the next step is, right? It is to convert it to, we, we carry out a change of variables and rewrite the integral here as an integral of the bi unit domain. So, once again, sum over A and B, and I'll forgo writing the limits of that sum. We get here c A i e integral over omega xi, all right. That's our parent domain. N A comma j, C i j k L, N B comma l, right. Now, just as before, d V can be written as determinant of the Jacobian of the mapping. D V sub, thing was putting it there, right? Okay, now, of course, I could rewrite this in another step by specifying that this integral over omega C really in, involves integrals of C1, C2, and C3. It's a triple integral, each one of them going between minus one and one, and the fact that this is essentially d xi, d xi2, d xi3, all right? And this integral basically becomes something like, it becomes actually a triple integral, all right? Minus 1 to 1, minus 1 to 1, minus 1 to 1, all right? So, that, that step is given, all right? So, so well we can go ahead and compute it. Now observe that, in general, depending upon the basis functions we've chosen, whether we've, you know, decided to stick with trilinears, or tri-quadratics, or, or, or rank higher order, these are functions of c 1 c 2 c 3. Now, the way I've constructed this, I've been thinking to myself, at least, that C is independent of position. It doesn't need to be, right. In general, it, too, can be a function of position, right. That, that would allow us to go to inhomogeneous materials. Okay, all of that will simply add a higher order dependence on position to this integrant. And of course, we know that in general, the determinant would also involve a dependence upon position, okay. But nevertheless, we not integrate this, right? We can be doing numerical quadrature. Right, so we do a numerical quadrature or numerical integration, which is quadrature. Right, and since we're working here with Lagrange polynomials defined on hexahedral elements, we have available to us the optimality of Gaussian quadrature. I ought to admit here though that if our elasticity tensor has some strange dependence upon position that is not just polynomial, we made, Gaussian quadrature may not remain optimum, all right? But, but, but still, it's, it's, it's an approximation, and that approximation is understood. Okay, so essentially we know how to compute this, right? Let's com, let's assume that we, we go ahead and compute this, right? If we go ahead and compute it. Let's, let's write it out, okay? Let's write it out. And let us write it out in this fashion, okay, so. All right? So, this thing is now equal to sum over A and B, c A i e. We've carried out the integral, okay? So we have something in there. So this entire integral here that we have here, this whole entire integral, Is done. Now we could realize now that I omitted to write my DBKE here. Okay, that integral with the raised bracket is done. So when that integral is done, we will be left here with a dBke, which is this one, right? Okay. I've left a, an amount of blank space there because I want to fill it in with something and I want you to think of what kind of object goes in there. In particular, is it a scalar, a vector, or a tensor? As a guide to getting to that answer, recall that the. The integral that we are trying to work with here is the one that I am now showing to you on this slide. Right? It's the one that is written next to the word consider. What kind of an object is that? Is that integral itself, once you valuate it, is that a scalar, a vector, or tensor? All right? It is a scalar. Because though the C has fu, free indices, I, J, K, L. Those are all being contracted out, right, and a sum over each of those dummy indices, I, J, K, L is indeed implied. Okay, so what we expect to see is that this thing is a scalar. But the way we've constructed it, we see that the c A, the, the, the c and d, right. By, by c here, I mean this c, not the elasticity tensor. Degrees of freedom have indices, free indices. With the final thing should be a scalar, there have to be other three free indices in whatever object is going to go into the black space. Which will be contracted out with INK. Okay. So, whatever object we have here, I'm going to denote it as K, of course. It will also have free indices I, K. Okay? Right. And it is essentially that integral. Okay? So the integral we have here is Kik. Okay? It is a particular component, the IK component of a tensor. Right? Thought of as a matrix, it is a 3 by 3 matrix, right? Because I and K run over. 1, 2 and 3, because we're operating in three dimensions here. All right. Now there's something else I haven't added there. And can you tell me what that well can you think of what it is? The indices A and B, right, because those indices are still here. So anything that we compute for the integral is um,specific to the combination of nodes A and B, which went into that integral. Right, that, that form that intergrand. So we have an object here which, which I'm going to write as KABIK. All right? Now there are indices all over the place here. And it's useful to think to just sort this out. So when I say I comma K go 1 over 1, 2, 3. I am going to write here the, the statement that's almost a contradiction in terms which is that a sum is implied.  Okay, over IK. So I do not have an explicit summation here, but over the A and B indices, I do have an explicit summation. All right, it's, it's just a matter of taste in terms of what I choose to write as an expressed summation and which I choose to invoke the Einstein summation convention on, okay? The fact that I'm invoking Einstein's summation convention on the spacial indices, spacial dimensions comes from a sort of hangover of continuum physics."
nDDY9xAGcs8,"There was a minor error in board work that remained on the slide I have before me here, and that you have on your screen. It appears right about here. That term should not be sigma h with subscripts i comma j, it should just be sigma sup h, subscripts i j. The comma would suggest that there is a derivative in the j direction, which is not correct. Notably, however, if you were to just follow what was written up here, for sigma h i j, all would have been well. Right? But nevertheless it's important to make that fix if ever you come back and look at this slide. That's it. And with that everything is, consistent."
JvFTOg_Csno,"All right, so this work we have. Let's just look at this object and understand a little more about it. Okay, so one way to understand it is to realize that we could also write that as follows. Sum over A and B of a vector, c A e, right? Transpose, right? K AB, 3 by 3 tensor, d B, e, okay? And for each of them what I have here is the following. C A e, and d B e, both of which we've encountered before, and we have had occasion to note that they belong to R3. Okay? Whereas this quantity K AB, right. It is a it is a matrix right? And it belongs to GL 3, right? It is a general tensor. It does not necessarily have to be symmetric. In some special cases, it's symmetric, okay. If you go through the process, you will, you will discover later that it is symmetric for A equals B. Okay? All right, but for now it is a general, it is a generally tensor in three dimensions. All right, so hopefully this, this helps. What I've done here is get rid of the implied summation over the indices that run over spatial dimensions, and replace those with vector tensor products. Okay? Now any, so each member of the sum is indexed by A and B. Okay? Fine, so this is what we have here. Let's go through and fairly quickly compute the one of the other integral. All right, so next consider The following integral over omega e W h i, f i dV. Okay, this is the one we want to integrate now. So, we have integral over omega e. Sum over A, N A no gradient on it, C A ie. That sum gives us our waiting function components. All right. This times fi, right or this actually multiplying fi and this is sum and glide over the i dV. Using our, using our usual tricks and here I will take the liberty of skipping several steps at once. All right, and still hope to get it right. Okay. So we get sum over A, c A ie, integral over omega C, N A, fi, determinant of J C dV c. All right, and of course now we know that dv c is dc1 dc2 dc3. The integral over omega C is just the triple integral minus 1 to 1 over the 3 coordinate dimensions. Right? In the bi-unit domain. Right, okay. As before, we are going to call this integral here. We are going to denote it as F internal. All right. Note that it has a an index A coming from the fact that it was computed with the basis function corresponding to node A. And it is each F internal A is a 3 vector. Right, and this also we could write as sum over A, using the notation we used to write out our left hand side, we have C A e, transpose F internal A Okay? All right, at this point we, we've assembled the two somewhat easy integrals. We will return in the next segment to finish up the the integral involving the traction terms."
puYCOSoTMrk,"All right, time now to grapple with the traction integral. So let's recall what it looks like. It is the following. It is sum over spatial dimensions. So sum now over e belonging to each of these ones, right? Of okay now and we do an integral here partial omega e t bar i w h i t bar i dS. Okay? So what that tells us is that it's really this integral that we need to consider. Okay? So we have this one Remembering that we do not have here a, a sum implied. Okay, that's straightforward. We need to do that as integral over partial omega e t bar i. For w h i, we have sum over A, NA, CA, IE. That's whi. T bar I, DS. Which is sum over A, CA ie integral partial omega E t bar I. N a, t bar i, d S. All right? And now we recall the, the cases that we've actually considered before, right? So we just need to recall them. I don't think we need to remotivate and re sort of discuss them in great detail ago, again. So the the situation that we tend to have in general is this. That is omega E. And let us consider the, the general situation where, straightaway let's consider the general situation where this face is the one of interest, right, partial omega t bar I. Okay? So what this says is that traction component i is being controlled on that surface. All right. And I've drawn it, hopefully it appears to you that it is at an angle. If that is our basis set. Okay. All right. So that is the general situation we have. Recall of course, the something we know and we have looked at before. Which is that we are somewhat saved by the fact that As I have drawn it, perhaps that one can think of that face as coming from this one. Right? So for the way I'm drawing things here, I am implying that the face of interest, is a is this face, right? It is a xi 2, xi 3 face, right? So this is partial omega xi t bar i. Sorry, t bar subscript i, for this particular element. All right?  And so it is that we know where to con, to compute this, this integral except that we also know that when it comes to constructing our mapping, we may need to to construct local coordinates just in order to, to get the, the, the Jacobian of the mapping right. Right? So, let's suppose that in this setting, I'm calling that x delta 1. And the x delta 2 direction. Right, actually those vectors really are, e, should probably be written as e tilde 1 and e tilde 2. Anyway, those are the directions that we have, okay? And from this phase to that phase, we may think of there being a map that I denote as J sub S. Okay? C. All right? Okay. So we have all that and of course, I okay. So, so let, so, so I will, I will, I'm going to use that in a second. Before I do that, I just want to point that I also need to recall that the sum over nodes may not be over all, may not need to be over every single node, right? It may need to be only over that set of nodes. Right? And previously we've used the notation that this node, A, belongs to the set A sub n. Right? To say that that is one of the nodes corresponding to a surface with a Neumann boundary condition on it. Right? So really the sum is over A sub n, as is this one. Okay? All right, so using this map, the way we would construct this integral would be this. Now, the integral here would be over partial omega c t bar i. And for the way I've drawn it here, I am suggesting that that is C2 C3. Okay? All right. That is the, that is the C2 C3. Right? So then we have here our NA, t bar i, right? And now we're going to do the integral over dS sub C, right, which is the elemental area on the partial omega c d sub d bar i. You know, face in the bi-unit domain. And I've left some blank space here because I know that I need to construct, I know that I need to put in here determinant of js. Okay? Because js maps a surface to a surface. What is the order of js? What kind of a tensor or what kind of if you think of it as a matrix, what kind of matrix is it? Js is a 2 by 2 matrix, okay? So jS, using the notation that we've used is g l 2. Right? It maps two vectors to two vectors. Okay. It's probably useful for me to point out in addition that for the way I've constructed this particular example, dsc. It's something I stated already, but what is dsc? It is dc2, dc3. Okay? And therefore that integral would be over C2 and C3 going from minus 1 to 1, all right? Okay. So we have all of this and what we need to do is essentially go ahead and construct our vector representation of this. Okay? So in constructing the, the vector representation I'm going to write this as, now, I'm going to write this as follows. I'm going to write it as a sum A belonging to A, A sub n. Okay? Okay. And we write this as CA IE. Now that entire integral once I evaluate it, I am going to write as F. I'm going to write a T bar here,which just is reminding us that this particular type of forcing vector come to us, this particular forcing, comes to us from the traction. Okay. It is the it is an I component. Okay. And it also involved node A, okay? So that entire F with all kinds of decorations on it now with subscripts and superscripts galore, actually it has just three of them. Is the result of that integral that we carried out, right? That the integral over the surface. Okay? All right, so this is what we have. Now where does this go? Remember where this all came from. And in order to remind you of it, let me just go back to the previous slide. This is where it came from. We have at the very top of our slide here. The original integral from which this came. Right? And note, in doing this, that really there is a sum over the spatial dimensions, as the outermost sum here. Okay? And then there is the inner sum over the element integrals. Okay? So, what this is suggesting is that when it comes to assembly, okay. We will first assemble the contributions from each element. And then account for the sum over the spatial dimensions. Okay? But, but before we finally do that, there is one thing we can do which is going to get us rid of the restriction to the special set A sub N. Okay, right? And what that let's, what, what that involves is the fact that okay, so let me now say let me say the following. Mm, actually let me do one thing. Let me put a bar on this F first, okay, because what I am going to do now is just rewrite that as a sum over A, okay? Now I'm going to say that this sum runs over all the nodes in the element, CA ie, F, t bar, A, i, okay? And what I mean here is that F, t bar A i equals the F bar that I calculated on, on the line above. If A belongs to AN, right? Right, and it's equal to 0 otherwise. All right and, and you would probably recall that this was the, the approach we used when we constructed the the contribution from the Neumann boundary condition, also for the, for the scale of problem. Okay? All right. So, what have we here? We are now in a position to look at our total finite dimension you'll be performing with all these element integrals accounted for. Okay, so.  Right? It is the following. We have sum over e. C e, so let me see, sum over e oh right, we have yeah, one more thing, yeah. We have sum over e, we also have a sum over A comma B, c, A, e transpose, right? K, AB, d, B e, okay, this is the contribution from the left hand integral, equals sum over e, sum over A, c A e transpose F internal A, okay? And plus sum over spatial dimensions. Then the sum over, elements belonging to the Neumann boundary condition corresponding to that particular spatial dimension, right? And all right. And then we have yet another sum here over A. Right? And this will over all the nodes c, a, i, e, f, t-bar, a, i. All right? Okay. Now, in writing all of these, I did notice a few minutes ago that when I introduced this matrix K, it really corresponds to a particular element, because it arose from a specific element integral, right? And it arose from the element integral for element e. So strictly speaking, I need to have it, an E there. It's just to remind us that it is the contribution from that particular element. Okay? Likewise here and likewise if there's any room here for more indices, for more subscripts. There we go. Okay. So this is where things stand. The next step that I'm going to take is one in which I am going to collapse, well not collapse actually. Expand out these vectors, okay. And in order to do that let's just let's go to a new slide."
9PtK8-ByJbQ,"Recall what we have here. Okay. So we have a general element, right. Now, now we are, we are, we are rid of our bi-unit domain, right. We are rid of that domain. So I have an element So I think I may have distorted a little too much, but we'll see. There we go. Okay. So, let's suppose that this is the node A. Okay? Let me look at them all. Because I do need to know. There is a node in the back here. All right. And up here we have. Okay. So this is node. Equals 1, 2, 3, 4, 5. A equals 6, 7, 8. Okay, and the situation that we have is that C A E is C a 1, C a 2, C a 3, right, for element e, right, likewise d. So we would have, look I would say like, d a e. Or d B e, that's a dummy index of course. Anyway, let's write it as d B e is equal to d B 1, d B 2, d B 3, element e. All right. Essentially what I'm going to do is to observe that now, in the case of the trilinear element. How many total scalar degrees of freedom do I have? All right. We've actually done this calculation, right. Which is that we have, Nne times Nsd degrees of freedom per element. Right? Okay, so if this is the case, what it tells us is that we can actually write a, c e vector, which we can define as basically being C 1 e. Now, note, C 1 e is a vector. It's a three vector. Okay, it is the set of, it's this vector or A equals one, and so on. C 2 e to C number of nodes in the element e. Okay? Likewise, d e, it's just the d vector. Right? It's the collection of all the degrees of freedom that are used to build or interpolate, if you want to use that term, to interpolate our trial solution, our displacement field over the element, right? Okay, so there's a collection of three vectors, so it's d 1 e, right? And, and at that node 1, we have d 1, the one direction, d 1 and the two direction d 3 and d 1 in the three direction, right? Those are the three degrees of freedom at that node, right? Okay, so we have d 1 e, d 2 e, and so on all the way up to d n, n. E and d. Right? Note that c e and d e belong to r, NNE times NSD. Okay? They are big vectors. In particular if you are doing trilineals, we have a, how many degrees of freedom on a trilinear element here, for elasticity? twenty four. Okay so what we are seeing is that twenty four degrees of freedom on trilinear elements. Okay? And this 24 has got, as a, is the product of n n e. Times N sd. Okay? All right, well, what are we going to use that for? What we're going to use that for is the following, right? We are going to use that in order to write out this first integral here, and actually the second one, too. Given the first integral on the left, it's not the integral, sorry, the sum on the left and the sum on the, first sum on the right, okay? And in particular, what that's going to let us do is get rid of which of the two summation symbols on the left hand side. Right. It's going to let us get rid of the sum over A and B. All right? And in this case it's going to let us get rid of the sum over A. We're not going to do anything different with the, the code the code just yet. Okay? All right. Okay. So doing all this, what we get is, the following. Okay. So. Now, the finite. dimensional weak form. Really, this is already a matrix vector form Right? We have, we, we, we are already talking from matrix vector form. It's going to be the same following. It is sum over the elements c, e. Transpose. Now, because we're getting rid of the c, e, e, and the d, b, e, we also can, dispense with the subset the superscript in b under K e. All right? But what that means is we now get a larger matrix, K e. Right? D e equals sum over e. C. Transpose. Now we're going to call this the F internal for element e. And like I said, we're going to wait a little to get this last stub. Done, taken care of. Okay? So, for trilinears right, I'm going to stick with okay, let me talk about this in general. What are the dimensions of that matrix, K e? All right? It's got to accommodate the C transposing D e, in general. Right? So that is going to be N, N, E. Cross, times NSD. Squared. For the generally case right, we know that we still need to account for the Dirichlet boundary condition and we come back and do that later. Okay, so this is the generally case. All right? This is also, therefore number of nodes in the element times n s d. Okay? Just to point out, our K matrix, K e matrix now is a block. So, sorry, it's constructed of many three by three blocks. Right? Because each of the K a b, sorry, K a d, is was a three by three block, all right. So we have a three by three block. K 1 1. E. Another three by three block. K 1 2. E. Because, if we were bi-linear hold with trilinears we would have eight such blocks but let me call this K, one, number of nulls in the element E. Okay. You could come down all the way, until you came down, you, you would have, N N E rows, right. And this is K, E, N, N, E 1. Right? The one there is for the column. Until you come all the way here, you will get K N, N E, N N E. Right? For a trilinear element we would have eight by eight blocks, right. Eight blocks along the column direction, and eight blocks along the row direction. Right, each would be a three by three. Right, each of these is three by three. Right. Which, well it's actually by N S D times N S D. Okay? And likewise we would have FE That's a three vector. All right, I think this is a great place to end this segment. When we come back, we will pick up from here. And, completed"
jzXr4-vG0q4,"Welcome back. We are now at a, very close to an end game stage of our formulation for 3D elasticity. What we are going to try to do in this segment, and perhaps it will spill over into one more segment, is to assemble a global matrix factor equations. And talk about Dirichlet boundary conditions, and the final solution step, okay? So let's get started on that program. What we will do now, in this segment is, write out the global matrix vector equations. And in order to do this, what I'm going to do is, just write in the first line here the actually, the, the, the final matrix vector weak form, but retaining explicitly the sum over elements and then we work ahead from there, okay. So this is an equation we developed toward the end of the last segment. It's, sum over e, c e transpose, k e d e, right? And remember that your k e is our, elements difference matrix, right? This is equal to sum over e. Ce transpose fe internal plus sum over i, right? Over the spatial dimensions. Sum over the elements that have some part of their element boundary as coinciding with the alignment boundary of the problem, right? And then for such elements we have the sum over the nodes that actually over all the nodes, right? because we worked out the bit about nodes belonging only to the Dirichlet boundary over such elements, right? We got past that point. So we have the c is our vector of degrees of freedom for the weighting function. And we have this forcing vector, which we've been denoting as f t bar node a, spatial dimension i element e. All right from here we go on to the business of assembly. Okay? And in order to see how that works out, we are guided by our global definition of the c and d vectors, right. So let's start out with the global c vector, right. That is c and the way it is defined is the following. We have we start out from the, we start out by following the global nodes. Right, so we have for global node 1, we have degree of freedom 1. Right? Which would be in our case the it would correspond to the spatial dimension one. And then we would have the same node. Degree of freedom two corresponding to spatial dimension two, and spatial dimension three. Right? This would carry on, and for the general node A, we would have the same situation. Right, until it came down to the very last of our c degrees of freedom. Alright, you note that I'm not, locking us into a situation where the very last degree of freedom, right? Or the very last node which would be sitting here. Those degrees of freedom would be sitting there. Degrees of freedom would be sitting there. I'm not walking us into a situation where that has to be the last node in the problem, or the last numbered node. Because of course we know that the definition of Dirichlet boundary conditions may very well eliminate such nodes from having weighted functions being interpolated off. All right? So, I'm leaving open that flexibility here. So this is the global c vector, all right, and likewise the global d vector, all right. And I'm going to now call this a d bar vector, all right. And you may recall from our previous treatments of 1D problems as well as the 3D problems. But for scalar variables that we are doing, we are calling this d bar. Because we know that there are some of those degrees of freedom that we want to later on move over in order to impose Dirichlet conditions. Okay? So okay. So the same thing happens here. We have d one, one, d one, two, d one, three. All right. Carries on to da1 da2, da3, right? That's for some general note, right. dA1, dA2, dA3 simply represent the displacement, degrees of freedom In the respective directions. One, two, three. Along with respective coordinate directions, one, two, three. For node A. And then this carries through all of them. D down to number of nodes in the problem. One, two Okay? In general, the D bar vector will be. Will be what? Bigger or smaller than the c vector? Will d bar have more components, or fewer than c, or all the same? What do you think? All right. If there are any Dirichlet conditions at all, and there have to be Dirichlet conditions for this problem, the D bar vector will have more components than the C vector. I should also mention that in setting this up, I'm assuming that the very first node here does not have Dirichlet condition set up on it, okay? So let me just say that. No Dirichlet boundary condition on the very first node. But yes, there could be Dirichlet conditions on the very last node. So I haven't specified which node we're talking of at the, as the last component of the c vector. Okay, so we have these global, so these are our global. C and d bar vectors. Okay? And then once we have this, the, the degrees of freedom that we have are, are now to be viewed as simply those corresponding to these entries, right? Each of these, for the whole problem, right, viewed as a vector problem, each of these is a different degree of freedom, right? Never mind the fact that they come from the same node, right? As far the problem's concerned, they're different degrees of freedom, right? Likewise these, right? And of course for, for, for general nodes as well, okay? All right, so in this sense, one, one would say that d bar has number of nodes in the problem, times nsd degrees of freedom, right? All components, right? c has number of nodes in the problem times nsd minus ND where this now is the number of degrees of freedom with Dirichlet conditions on them. Okay, and in calculating ND, it is not necessary. Well let me ask you, do you think it, it is necessary that ND, right, ND, is it necessary that ND should be a multiple of the spatial dimension? Right, I'm asking, is, da, does it have to be a multiple of a number of spatial dimensions? And in, in particular I'm, the reason I'm asking this question is bec, is because I want you to think about whether Dirichlet boundary conditions have to be applied to all three degrees of freedom at each node. 'Kay, so the answer to this question is that is no, right? Because we, we've, we've, we know that we could apply Dirichlet conditions on a particular coordinate direction at a point and not on the others, okay? So in ge, so, so this answer in general is no, okay? Right, all right. Okay, so we have these things in hand, and now what we will do is to go ahead and essentially write out the global form, right, from that contribution. So what we have here is that sum over e ce transpose, Ke, de equals c transpose, K bar, d bar, right? Okay? Now we already know what our c vector and our d bar vector are. The K bar itself, is obtained by this assembly operation over the individual element's stiffness matrices. Right, and, and note, of course, that here, because we have confirmed indeed that we are doing 3D elasticity, the term stiffness matrix is is, is relevant, right? It's, it's, there, there, there, there's no confusion there. Okay now this proceed, this proceeds just as before, right? What we want to, to realize is that any single entry in the K bar matrix, which corresponds to degrees of freedom belonging to different elements, right, but the same global degree of freedom will have the corresponding terms added on, okay? So this thing works just as before when we realize that we just carry out the assembly over degrees of freedom over global degrees of freedom. 'Kay? You just have to carry out assembly of a globally numbered degrees of freedom and forming K bar, okay? And when we do this, we get that K bar as units, sorry, has, has dimensions of number of nodes in the problem times nsd minus ND, okay, times number of nodes in the problem times nsd, okay? In order to sort of further explain this process, let's try to do it for a pair of elements. Let's me try, let me try and draw something that's doesn't have very complicated surfaces. 'Kay I suppose that this thing goes a little longer. Okay, this is sa, somewhat similar to the si, sort of situation we'd looked at in the case of the 3D scalar problem. Right? So, let me label only, or let me draw only the common nodes here. Okay. And let us suppose that these nodes have global, global numbering. A, B, C, D. Okay, and furthermore these elements are omega, E1, and omega E2. Okay, all right. So, let's, look at what the, the Ke1 stiffness matrix may look like, okay? Now, let's suppose that for the Ke1 stiffness matrix we have a numbering, which comes from the, from the, from the local ordering of nodes, right? We, well, we know that's always the case, but let me just label the the local number in with nodes, right? For the local numbering of nodes in element omega e1 we know that what I've labeled here is global node c, supposing we say that, that, that is local node 2. Right, and element e1, right? According to that, D would be local node 3, right? A would be local node 6 and B would be local node 7. Okay? Now, for element omega e2 exactly those nodes. Suppose that they are local node 1. D is local node 4. Right? A here would be global node, A would be local node 5 for element e2. And B would be local node 8 for element e2. All right? So, with this background, all right? Let me try and write our Ke1. Okay? Ke1 would be a matrix where I'm not going to write out all of the components, because I'm just going to write the blocks, okay? So we know that this going to be Ke1, the 11 block up to the, up to Ke1, the 18 block, okay. Because, of course, we're working with bilineals here to fix ideas. So we have Ke1 88, all right? And down here we would have Ke1 81, okay? And Ke2. Is Ke2, 11. Ke2, 18. To Ke2, 88. Ke2, 81. All right."
lcDP8zC6vrg,"Okay, with this in mind, I would like to go into our global stiffness matrix, and just talk about where the where the matrices are going to be added on, okay? I should, I should mention here, of course, that each of the entries in these K e 1 and K e 2 matrices is a is not a scalar, right? What is it? Each of these is a, that's right, a 3 by 3 matrix. Right? Okay? All right. Okay, right, and, and that's the same for K e 2. Right, so now let's look at our global K bar matrix, okay? And in setting it up. Okay, let me suppose that this is row A, row B, row C and row D. Okay. And in terms of columns also I have I may have A here, B, C and D. All right, with all of this in place let's look at which sub-matrices from the K e 1 and K e 2 which I wrote in previous slide, are going to make their appearance here, okay? So let's look at the A A contribution. Okay, for the A A contribution we will have, it should be here, right? We would have from K e 1, right, if you look back at the figure I drew of the two elements e 1 omega e 1 and omega e 2, it should be clear that from K e 1 we have the 6 6 component, right? And from K, and from omega e 2 we have the, which one, that's right, 5 5 component, okay? You know, if we filled out all the ABCDs here, we would have 16 entries, and I'm not going to try to do all of that, right? So let's look instead at what happens with the CC entries, okay? So the CC entry from G1 would be K e 1, 2 2 plus from e 2 which would be K e 2, 1 1, okay? And this would go on. Of course, neighboring omega e 1 and omega e 2 there may be other elements which also share our global nodes A, B, C, D, all right, and contributions from those elements, stiffness matrices would also be added in here and here. Right, so. Okay? Okay. Now look, let's look at what happens with a with maybe the AB combination of global nodes, okay? So if we look at AB, what we observe is that the, the contribution from element e 1 is going to be, will be from the matrix K e 1. What do you think? What, which particular block is going to contribute here? It would be the 6 7, okay? And from e 2. From e 2 it would be the, right, 5 8, okay? And so on. So let's do one more. Maybe we do the, what do I have room here for? Let me do the AD. Okay? So AD, from K e 1 is going to be 6 4, and from e 2 is going to be. Oh, I'm sorry. From A 1 it will be 6 5. Right, and from e 2 it will be I'm sorry. I can't, I can't read my own, own handwriting. It would be 6 3. Really sorry. 6 3 from e 1 and for, from e 2 it would be 5 4. Let me double check that. Okay, and of course, there could be other, there will be other contributions from other neighboring elements that share the same nodes. Okay? So this is our global K bar matrix. Right? With this in mind, let's also try to assemble our forcing vector, okay? I believe that with this in place, we can do this, we can assemble the forcing vectors fairly quickly, okay? So, what we see here is that when we look at our sum over elements, c e transpose F internal e. Okay? Now we have this written as c transpose F internal, right? Okay, where we already know what the c vector is we only need to say what the, what the F vector is, okay? And since I have some room here, let me try and write out the F vector here. Okay, and I'll try to do it for the same combination that we have, okay? So for the F internal vector. It's for the same, I'm, I'm going to try to do this for the same combination of nodes that I've written out for the, that I've used for the stiffness matrix. Okay, let's do this. Hopefully that gives me enough room. And let me say that the nodes A, B, C and D make their appearance at roughly those points of, of the, of the vector. Okay, global node A. What contributions does it get from element e 1? Right, from element e 1 it gets the, the F internal e 1 from which local node of element e 1? Right, local node 6. And from e 2. From e 2 it gets a contribution from local node 5. Right, this continues and let me try to do the same for global node C. So from element e 1, it gets F e 1 internal node 2. And from A two, it gets F internal, 1, okay? Right, and of course there could be more contributions here, right, because of the fact that there are more nodes, sorry, more elements off to the side, right? Okay, so all of those contributions would go into those entries for global nodes A and C. Okay. So that's how we go, set about assembling our global stiffness matrix K bar and our global force vector F internal. We'll end the segment here. When we come back, we will talk about how we treat the traction or the Neumann boundary condition terms."
x-PTHIibfSs,"There was an error in board work that I made on, this slide here in front of me. The error is in, where I put that block matrix in the larger K bar matrix. Properly, if you look at those terms, K e1 sup 6 7 and K e2 sup 5 8, and look at where they come from, and in order to do this, let's just go back one slide. Okay? So now if you look at the way the elements and nodes have been written out for, or have been sketched out for this little assembly of two elements, you will note that the K e1 6 7 component Is map, is one that maps on to the global K bar AB block of the matrix. And the same holds for the K e2 sup 5 8 components, which are those two, okay? They also map onto the, to the global D bar AB block of the matrix. When you go to the block matrix K bar, what you will you note that I've written then in the K bar BA position for, in terms of blocks. This just needs to be moved up here which is the K bar AB position. Right? It's the AB position because the rows are got from there and the columns are got from there. It's a bit of a squeeze the way I've written it because the  K bar AA component turned out to be a little wider than I should have written out, but, the important thing is that this, block should appear right there. Okay? With that correction, which is an important one of course, we also note that the same sort of thing may have happened with the block that we've written out here. Let's just check that. So we have K e1 6 3 and K e2 5 4. Let's go back and see where they come from. So K e1 6 3 would be the A d component, right? And K e2 5 4, when you look at this assembly of two elements, would also be the global A d component. That term however, that block term however, is in the right position, okay? And once we look at things in that light, we see that now this K bar matrix is correct with this one correction. Right? That block should appear in that position. With that, everything's consistent and you can go ahead and, program this into your code."
Nv22ARg6J90,"In this segment, we'll start looking at the homework coding template for homework four. All right, and that will be the 3-D linear elasticity problem, the steady state problem. Okay, so let's come over here to the code and we'll look first at the main.cc file. And as you can see, almost nothing has changed from the previous homework assignment. I, of course, changed the name of the include file but it's still 3D passing in the mesh size. This is, for example, 10 x 10 x 10 element mesh. Okay, create our object and then go through the same steps as before. All right, so we can go straight over to our header file. And we have our same header files here. But now, in this homework assignment, we are going to be using DL2's quadrature rules. We'll be using DL2's basis functions. And so that will make things a lot easier on the coding end of things, but then it does change the structure of our code a little bit. So we'll be looking at those changes here. First off, I'm going to declare order and quadRule as global variables. Actually not variables, they're constants. I've designated that here, that they are both constant integers. And I've defined them here, because I use both of these numbers in the constructor of some of our class objects, okay. So, let's scroll down and look at the declaration of objects in our class. Declaration of objects and functions. We have, of course, the same class constructor and destructor. Here I have a function called C, this is our elasticity tenser. Now, DL2 actually does have the capability of creating a fourth order tenser, which the elasticity tenser is. I've set that up here as a function where the inputs are just the four indexes, and then it outputs that component of the tenser. Okay? So you can use that when you're creating k local. The solution steps themselves are all the same as well as these first three class objects, but now we have, again, the DL2 quadrature rules. We have two quadrature rules, a quadrature formula for volume integrals and a quadrature formula for surface integrals, which is the face quadrature formula. We'll be showing you how to use those later on. Other than that, the k, d, and f matrices and vectors are the same. Slight change here, I've changed this object for the table. Instead of no location, it is dof location, or the location of the degrees of freedom. The reason I've done that change is because since this is 3D elasticity, we now have three degrees of freedom per node. And so, there are three times as many degrees of freedom in the system as there are nodes, okay. And, this table gives you the location of each degree of freedom. So, what that means is degree of freedom, zero, one and two, which all correspond to the same node, would have the same location. But they each have their own row within this table DOF location. Okay, course bound, you guys, map is the same. I'll scroll down to the construction destructor. Here in the constructor, again we're, we call the constructor for FEM dof_handler, but I'm also calling the constructor for quadrature_formula and face_quadrature_formula. All right, because they need to know what the quadrature rule is. And so that's what you've defined up at the top. The default value is quadRule of 2. Leave it at 2 when you turn in your homework. But if you want to you can easily change that to 3, 4, whatever, DL2 would automatically take care of that here. Also, you'll notice that I left order as a variable that can be defined. Again, I have defined it as one at the top, leave it as one when you turn in your homework. However, feel free to, on your own, to change it to a higher order of basis functions if you would like, d02 automatically takes care of that. Okay? But again, when you turn it in leave it at order equals one and quadRule equals two. Okay? So those are the small changes in our constructor again. Small change here with the names of the output vectors, because again it's a vector field instead of a scaler field. Let's scroll down and look at this function C, which is the elasticity tensor. This is the first part that you actually have to add that something. And it's simply to input the values for the Young's modulus and the Poisson's ratio. From those values, I can reconstruct the Lame parameters, lambda and mu. And with lambda and mu, I use the formula that you saw in the class, in the lectures, to create, define the particular component of the elasticity tensor. Now if you recall. C = lambda times your second order isotropic tensor, tensor product with, again, the second order isotropic tensor plus one-half mu times the the fourth order isotropic tensor. Okay? And what that means in indicial notation, is that we have Cijkl equal to lambda times delta ij times delta kl. Of course, delta here is the Kronecker delta, where if i equals j. Oh, let me write that down here. Delta ij is equal to 1 if i equals j and 0 if i does not equal j. Okay. Sorry, slight mistake up there, it should be 2 times mu instead of one-half. Okay, so we have 2 mu, and then here we have one-half times delta I K, delta J L plus delta I L, delta J K. Okay? Of course those can cancel out. And that's what you see in the code here. You notice I'm using these conditional statements here. If i equals j, if k equals l. If i equals j, then the condition is true and so it returns one. If i is not equal to j, then the condition is false or returns 0. So it's acting the same as a chronic or delta. All right, so it's a little bit cleaner or quicker than typing in an if statement, but it serves the same purpose in this case. All right? If we look at generate mesh this is exactly the same as our 3D heat conduction problem. We simply have to define our, the limits of our domain. And that creates a mesh for us. All right, for define boundary conditions, again this is something you'll fill in for yourself, and it will be very straightforward, similar to the previous assignments. Again, a slight difference here is that we're using DUF location instead of node location, but that's a small change. Another element that gets introduced here, though, is the fact that you may have different Dirichlet boundaries for each degree of freedom, for each nodal degree of freedom. And by that, I mean you may want to fix on a certain phase, the displacements and the extraction, but not fixed the displacements in the y or z direction. If that's the case, then your if statement, you would check not only the location of the degree of freedom, but you'd also have to find out what's the nodule, the corresponding nodule degree of freedom. In other words, is that degree of freedom a Dirichlet placement in the x, y or z direction? And so I've explained that a little bit here, in the notes, in the template. However, you don't have to worry about that in this assignment because our only Dirichlet boundary condition is to fix all degrees of freedom on the face where z equals zero. So all have to do is check is the z component of DOF location equal to zero? If it is, set that degree free equal to zero, whether it's in the x, y, or z direction, okay? But in the future, if you want to, you actually can distinguish between displacements in the x, y, or z direction when you're playing Dirichlet boundary conditions, okay? Let's quickly look at setup system. There's actually no difference here, other than the fact that we're creating dofLocation as a table rather than no location, but nothing that you have to change. Okay, so we'll stop this segment here, and in the next segment, we'll look at the single system, which again, will be the meat of this template."
k3pswnNaT-U,"In this segment we'll move on to looking at the assemble system function for the homework four template. So let's look at that code now. You'll notice first off we have two new objects. We have this fe_values and fe_face_values. These objects are DO2 objects that hold information about the basis functions, the basis function gradients, information about the quadrature points, the Jacobians, and all this. But we no longer have to calculate the Jacobians. We don't have to write out the basis functions themselves. We don't have to write out the quantiger points It does all that for us, all right? So if you'll look, the first two are just fe and quadrature formula so that it knows what basis function order we're using and it knows what quadrature rule we're using. The third input value is actually a series of input values, of flags that tell fe values what information we're going to be using and that's the information that it'll be updating. So it makes a little bit faster or saves on memory that we don't need to update information that we won't actually be using. All right, so for fe values, which is what we use for volume integrals. We're going to be updating the values, which are the values of the basis function. Update gradients, that's again the basis function gradients. And then, this JxW, that's supposed to be J times W. J stands for the determinate of the Jacobian, and W stands for the quadrature weights, so it's all three quadrature weights multiply together in 3D. All right, so it takes the care of all of that 4s. The gradients we use in Klocal, the values we would use in Flocal if we had a body force. Okay, and course J times W we would use in any volume integral. If we move on to fe face values, you'll see I'm updating values. Quadrature points in J times W values. J times W values is as before. Only now you'll note that since it's a surface integral, it'll be the determinant of the Jacobian mapping from the 2D by unit domain to the 2D surface domain, right? update_values is, of course, still the basis function value on that surface. update_quadrature_points, this actually will give you the position vector of each quadrature point in the real domain, all right? So we need that in this problem because the traction for a Neumann boundary condition depends on the X1 component. Okay, so it varies as X1 varies on that Neumann surface Z equals 1. Okay, so that's why we're updating those values. So, let's scroll down into our element loop. Now, you can see here the first thing is that we do fe_values.reinit(elen), so fr_values is reinitializing for this particular element. So it's getting all the correct values for the quadrature, for the Jacobians and so on. The first step that we're going to look at is defining Klocal. Now before you look any any further at the code, let's look at the board at what that general form is. Okay, so in class we, or in the lectures we looked at Klocal and we had four indices that we were dealing with. Klocal AB ij. And the idea was that AB run over the nodes in your element. Okay, so I will go from 0 up to just less than the number of nodes, In the element. Okay, I and J are nodal degrees of freedom. And so that we run it from 0, 1, and 2 in this case for the 3D elasticity. All right, and the way we pictured that is that we had our Klocal matrix and then inside, we had these little sub-matrices. All right. And so, again there are eight nodes in a hexahedral element and so, these went from 0, 1, 2, 3, up to 7, all right, the same on the side, 0, 1 to 7. Now within the submatrices, we had indices 0, 1, 2, 0,1, 2. Now those correspond to the degrees of freedom at that particular node, okay? Now of course Klocal is not a matrix or matrices in d0l 2. It's just a matrix that's 24 by 24, all right? So we can look at this instead. This makes it, as 0, 1 2, 3, 4, 5, and so on up to 21, 22, 23. Okay, so let me label these. These would be our element degrees of freedom here. Going from 0 to 23, then we have our element nodes, Here. Going from zero to seven. And then we have our nodal degrees of freedom. Here. Going from zero, one, and two, all right? So you will notice when you look at the code again that our loops are actually looping over the element nodes and the nodal degrees of freedom. However, the indices in Klocal will be in terms of the element degrees of freedom. Okay, so how do we do that conversion? Because the indices start at zero it actually makes it simpler in this case. So we would do it like this. Klocal ABij corresponds to . I guess I should do square brackets on both of those, right? And you can see that that's true. Let's do it for 22 here. So that would be A is equal to 7, so 3 times 7 is 21. The nodal degree of freedom would be 1 at that point. So 21 plus 1 gives us an element degree freedom of 22, okay? And we'll work the same way for Flocal. For F local, it'll be F local A sub i, would be the same as F local of 3A, 3 times 8 plus 9, we'll look at that again in a second. Now let's look at how we actually calculate Klocal ABij. Let me write out the formula here. So we have Klocal ABij = the integral over the domain of the element, basis function A, the derivative to with respect to X of j, Our elasticity tensors Cijjl times our basis function B, ldV. All right and notice, we have repeated indices here. So there's an implied summation over j and l. For j and l going from 0. One and two, okay. All right, so let's look at what that will be in our node, in our code over here. All right, so first off, in our loops, we have a loop over quadrature points. Notice it's a single loop over quadrature points. d0l2 has combined all three loops since this is a 3D case into a single loop, all right? And it's keeping track of that for us. Now we have our loop over A and i, which is the loop over nodes A and then loop over nodal degrees of freedom. B and k again looping over element nodes and element or nodal degrees of freedom. Now, actually, I need to come back here and make a small change that I noticed. That's actually not the way I've written it here. That's not for Klocal ABij. It's actually for Klocal ABik, and the reason you should be able to see that is because j has already been removed through the summation. ink are three variables here, and so that's what should show up in the indexes of Klocal, all right? So that's why I have A and i grouped together, then B andk. j and l,, we have a loop there because of the implied summation over j and l. Okay, and now in here you'll define Klocal, using of course this integral that we're written out. Now one thing that's important for you to know is that when we use fe_values to get the gradient, which we do using fe_values.shape_grad() It's actually given us the gradient with respect to x and the real domain. You'll notice in the previous assignments when we wrote the basis gradient functions, the basis function gradients we wrote the gradient with respect to c and the bi unit domain and then we had to find the Jacobian. Do the inverse and so on, right? That's all taken care of here. Okay, so we don't have to deal with the Jacobian in order to make that a gradient with respect to the real domain, that's already done for us. Okay, so in order to access that gradient, you do fe_values.shape_grad, and then you'd input the element degree of freedom. Okay now, notice that's a little bit different than what we've written on the board here. In the lectures we always use the element node number to designate what basis function we're using. Here in d0l2, it's the element degree of freedom, okay? So it will be the same as whatever index you're using in Klocal. So this wouldn't be, for example, this wouldn't be A, this would be 3A plus i. And this wouldn't be B, it would be 3B plus j, for d0l2, okay? Sorry not j, plus k, all right. Again, you'll need to use the elasticity tensor function that I created before. And in order to use det to get your dertiminate of j times the quadrature weights we'll use fe_values.Jxw(q)*/ for this particular quadrature point. Notice also that we aren't inputting the value of c at the quadrature point, we're just inputting this index q which tells you what quadrature point we're at in the loop. Okay, also, to clarify, fe_values.shape_grad gives you a position vector. It's actually a d0l2.object, but you can think of it as a position vector. Or sorry, it's actually a first order tensor, which is very similar to a point and d0l team. Okay, so if you want to use a particular component, which of course you will, you can use just these square brackets. i or j or whatever it may be, okay? So, that should cover it for creating Klocal. Let's move down to Flocal. Now we don't have a body force in this problem or any forcing function like that. But we do have Neumann boundary condition. And so that will involve an integral, Over the Neumann boundary, okay? So let's write that out here. So for Flocal A sub i, which again is the same as in our code, it will be 3A or times A since is equal to 3, + i = this integral over a surface and so I'm going to designate that with this partial omega. And I'll use T to specify its attraction. It's the Neumann condition, and it's for a particular element, okay? And we'll be integrating hiI, so h is the traction, i is the component of the traction, the traction is the first order tense of our vector, times NA, and again it's a surface integral. All right? Okay, so let's go back to the code. You'll notice first, I'm doing a loop over the faces of the element, the current element. Of course there are six faces. So we loop over those and we're going to update fe_face_values for this, not only this current element, but also for the current face. That will put this quadrature points on the face itself that we're on. Now we're going to check to see if that face is at our Neumann boundary, okay. So this lm arrow face arrow center, that gives us a position vector at the center of the current face. And since I'm interested in the z component, I use square brackets too. Okay so here in this if statement, I am checking to see the point at the center of this face at the boundary z = 1, which is our Neumann boundary, okay? If it is, then I'll perform this surface integral, okay? If not then I will just move on. Either to another face, and if none of those faces were Neumann faces I'd move onto another element, and Flocal would be zero for this element, okay? But once we are, once we have found that Neumann face and we are performing that surface integral, we'll move inside and loop over our face quadrature points. Okay, notice that, the number of face quadrature points. Here I've extracted for you the value of X, the x-coordinate of the current quadrature point, okay, and I've done that using this fe_face_values.quadrature_point(1). If I wanted the z component then I would just change that zero to a two. If I wanted a y, I would change it to a one, okay. But then what you need to take that value of x and specify what the value of the traction vector h is at that quadrature point, okay. Now, once you've done that we'll move inside our loop over the nodes and the nodal degrees of freedom, okay. And, once inside there you will again use this integral that we've written out on the board to define Flocal. Now you'll notice I am looping over all eight nodes of the element. Even though we're only doing a surface integral. How are the quadrature points themselves, D02, is placed on the appropriate, correct, surface the correct face. So, we are only integrating over the surface itself. All right, notice again, we'll be using fe_face_values to find the basis function value. Okay, and you are still passing in the element degree of freedom. So that's 3 times A plus i. Not just A, okay. And again when you're finding JxW, the determinant of J times the quadrature weights. Again use fe_face_values, all right. Everything involved with the service integral will be using fe_face_values, not fe_values at all, all right? So once you've filled that in, you'll have Flocal created as well, and once you have Flocal and Klocal. A symbol system will be very straightforward. It will be exactly the same as previous assignments, all right. Applied boundary conditions will be applied in the same way. Again, you've already specified your knowing your boundary condition. Fixing all degrees of freedom, X equals 0. And I'll scroll down a little bit more just to finish up quickly. Solve is exactly the same and output results again is the same. Outputting our displacement results as a .vtk file which you can then open up using para view or visit to look at the displacements. All right so that concludes our this segment and it concludes our discussion of the template for homework four"
H1aHhCENGmM,"Welcome back. We are aiming here, to complete our assembly of the global matrix vector equations. And, talk about the final Dirichlet boundary conditions. So, to do that let's get on to the contribution that we have not yet tackled for the global equations. And this is the contribution from the Neumann or the traction boundary term, okay? So, the contribution. To global matrix vector equations. From the traction, right. Remember the traction is simply our Neumann boundary condition for this problem. Right. So the term we are talking about is this one. Sum over i, right, i running over the spatial dimensions. Right. Sum over i, sum e belongs to E Neumann, right. Sum over A, that's all the nodes in that particular element, right. We have here CAie Ft bar. We have the A index there. We have the i index there and we have the e index here. Okay, what I'm going to tell you in one fell swoop is that this is going to show up as c transpose Ft bar, right, globally. And I will write down on the next line the detail construction of the F matrix. And in order to do that, I am going to take I am going to write here first the C transpose vector a, as a row vector. Okay? And I am going to get myself room for it here. Okay, and the idea is that multiplying it is our FD bar vector. Okay. And here too, let me get myself enough room. Okay. So let's construct C. Sticking with the same assumptions I've made before we have a contribution from the very first numbered global node, okay. Because I'm assuming that our, in considering the case, where we don't have the Dirichlet boundary conditions on any degree of freedom on the first node. So, we have C1. Okay. And we're working with the outer sum first, right? The sum over spatial dimensions. So I'm setting i equals 1, and that is the 1 that shows up here. Okay, and that comes from some element, right? The fact that we're doing a sum over elements is already accounted for in the fact that we have this global node here, okay. Right so that contribution would be. F t bar. Let me see. It would be local node number one for that element. Okay? And since we are talking of global coordinate direction one. We would have a one here. Okay? It's going to be some element, let's call it, let's just leave it as E. Okay? And let's go on now, with this. And actually in going on, let me also use the same numbering that we used in order to construct the stiffness matrix and the F internal force vector, okay? So I have before me here those two elements, omega e1 and omega e2 and you probably noted them down in your in your book, or your notebook. So I would encourage you to go back and look at those. Okay? Because what I'm going to do now is look at the contributions to global node A. Okay? So let's suppose that global node A shows up here. Wrong C vector. Let's suppose that B shows, shows up here. Global node C shows up there, and global node D shows up there, okay? So that's, A is going to be here, B, C, D, okay? Now we start out as as I did for, for this very first degree of freedom, we start out with the spatial dimension one, okay? So the global node here, right, is cA1, okay. And let's suppose that on this force vector A shows up here. B shows up there, C and D. Okay? Now the only we would have a contribution from the A node is what? Okay. What we need to have is that the local well what we need to have is actually we, we can talk of it, of it in terms of the global nodes and so global node A, right, lies in partial omega t bar one. Right? That's the only way we would have a non zero contribution to the F vector from global node A. Okay? So let's assume that this is true. Let's consider the case where A does lie in omega, in partial omega t bar one. Okay? Alright, so you could go back and look at that at that figure that I drew back there two two or three slides ago. What I am seeing is that node A from that figure does indeed lie on, lying on, partially on , partially on omega T bar one. Okay. So, we would have a contribution, then, from element e 1. And the contribution from element e 1 would be F t bar local node 6, right, spatial dimension 1, element e 1. Okay? Right, now element e 2 also would have a contribution here, right? So we would have F, t bar, right? From element e 2, the contribution would come from local node 5, okay? The spatial dimension would still be 1, right? And of course, there's element e 2, okay? So this would be the situation if I'll reproduce that figure of two elements, somewhat more defined. All right, so again we have omega e 1, omega e 2, right? And I'd lo, I'd label those nodes as A, B, C and D, okay? What I'm seeing now is that let us suppose that D surfaces. Right, that, those are the sort of front surfaces of both the elements, right? Belong to belong to partial omega D bar. Sorry. Actually let me, let, let me write this properly. Okay, bo, both those surfaces I'm saying belong to partial omega t bar, okay, 1. All right. Okay, right. And, and then in, in terms of local numbering, what I'm seeing is that let me identify the surfaces I'm speaking of here. Okay, for element omega e 1, that's, nodes are 1, 2, 3, 4, 5, 6, 7, 8. And for. Omega e 2, the nodes are 1, 2, 3, 4, 5, 6, 7, 8. All right? Okay, so we would have those two contributions, right? Let's suppose then for omega, for, for the Neumann boundary in the 2 direction, right, which means the degrees of freedom are being controlled in the 2 direction. Sorry, not the degrees of freedom, but, but the traction components in the 2 direction are being controlled on the other surfaces, right? So let's suppose that it, it's on this surface and it's on this surface, okay? So let's supposed that this belongs to partial omega t bar 2. And this also belongs to partial omega t bar 2, okay? So the, can you think of which surface, do you see which surface I'm saying belongs to partial omega t bar 2 from element e 1? It is the 5, 6, 7, 8 surface, right, of element e 1. On element e 2 also it's a 5, 6, 7, 8 surface. Likewise, on element e 1 the 1, 2, 6, 5 surface belongs to partial omega t bar 1. And in element e 2, which one is it? It's the 1, 2, 6, 5 surface, okay? All right, so if you underst, if we are clear about that, let me put in the contributions then. All right, so for for the contributions from the 2 direction to the, to this traction force vector, would there be anything from node A, from global node A? There would be, right? So we, we would have a C A 2, okay? And that would show up now right here, right? It would be F t bar global spatial dimension 2, right. We would get a contribution from element e 1, right, from its node number 6, okay. Right, and we would get a contribution F t bar global 2 direction element e 2 local node number 5. Okay? All right. Which other nodes would contribute? So let's look at node B, okay? What contributions would it have? It would only have contributions from the 2 spatial dimension, right? Spatial dimension i equals 2. All right? So, for this node we would get a contribution from C B. Let's put the C B 1 contribution, then let's talk about the C B 2 contribution. Since the C B 1 contribution, right, on that node is 0, right? There is no contribution to the traction there, right? We are not controlling the traction there, okay? But that's a free surface, right, because that, that is a surface on which a traction may be specified, right? We have just not specified the 2 component there. So when it comes to the B node, we get a 0, okay? The contributions that do come there are from the 2 direction. And we have then F t bar, we are talking the 2 dimension, so we have 2 here. We are, okay, let's look now at the contribution from e 1. From element e 1, which local degree of freedom contributes there? It's the 7, okay? From F. Sorry from element T2. Sorry element E2. The contribution along the two direction, right, would be from local node eight. Okay? All right. Now, let's suppose that no other boundaries, of these two elements correspond to Neumann boundaries for the global problem. Okay. That means that if we go ahead and look at ca 3, aas well as cb in 3 dimension, right? We would get 0s, right? So here we would get a 0, and here too we would get a 0. Okay? All right? Let's see what node c does as I have drawn it, what traction surface does global node c lie on? Right? Also in the global spacial dimension one surface. Right? So when we come back here we have C and unfortunately the global  number of that degree of freedom is also c but hopefully we can cope with that, with that repetition. Okay. There is going to be something from the cc 1 direction right, from the one direction there. So we move along, right? And then we come to the c node. We get a contribution of the form F D bar, along the one spacial dimension, from element e1, right? So, which node from element e 1 contributes? It's the local node two. From element e 2. Which one is it? Local node one. Okay? Now, as I've drawn it, for node c, right? Global node c. There are, no other, spacial dimension, no other, traction boundaries that contain that node. Okay? All right? So what that says here is that for C c 2 and C c 3, I would get 0 and 0, okay? So let's just complete this particular line that I wrote. Global node a lies in omega t, omega t bar one. C also lies in partial omega t bar one. B Lies in partial omega t bar two. Okay. But to note, the way I've drawn it, A lies in this and in partial omega t bar 2. Okay? There's a partition there. Okay? So A the way I've drawn it node A is the only one that lies on two traction surfaces. C lies on one, and B lies on one. Okay. And which one the line has been written, has, has been denoted here. So, and, and for the more, for the way I've drawn it here, let's suppose that D belongs to no traction boundary. D lies in none of the boundary subsets. Partial omega t bar, i where i equals one, two, three. Given that, what contributions would we find in this traction force vector for no, global node D? You're right, all zeroes. So you get a zero, zero, zero, and we would go on. So, hopefully this process has given us some idea of how to construct this global traction force vector. Okay? This is what we are calling it. FT Bar. Okay?"
y1kCV1v8b-Y,"With all of that, when we put things together, we now have our global matrix vector equations. All right, and they are the following. C, T, K bar, d bar equals C, T, F internal plus C, T, F, T bar. Okay. What's the last step you need to take? Dirichlet boundary conditions, right? Okay? And in doing that, we need to rec, we need to come to the fact that our K bar, probably due without that arrow. What are the dimensions of our K bar matrix? Right, they are number of nodes times nsd minus Nd, right, where the Nd corresponds to Nd is the total number of Dirichlet degrees of freedom total number of degrees of freedom where Dirichlet boundary conditions are specified. Right, and this could be you know this could draw maybe, the, you, the, the one direction on some node, the two direction on some other node and the three direction on yet another node, if we sum this up we get three, in that case Nd would be 3. Okay, so, this times the number of nodes in the problem times nsd. Right? So K bar, these are the dimensions of our K bar matrix. Okay. Right. Okay. So clearly K bar is a rectangular matrix right? And what we need account for is the fact that when we look at the c transpose vector here, all right, multiplying K bar, right. And here we have our global d bar vector, right. Let's suppose that now when I label the degrees of freedom for the d bar vector, right, that is d bar 11 d bar 12, d bar 13. Okay. Now let me suppose that some global degree of freedom, all right? So let's suppose d a, d bar a 1, d bar b 2, and d bar c 3. Okay? Let's suppose that these global degrees of freedom are known Dirichlet boundary conditions. Okay? What this says is that d bar a 1 is known. I'm now going to write d bar a2 and d bar a3 because those are not specified. Right? Okay? Well, actually, actually, let me write them and, and specify and, and so to mark other ones that were known. Since I've run out of room here  let me just say the d bar, let me just get rid of d bar C, 3. Okay. So those are the ones that are known. Okay, so d bar a 1 and d bar b 2 are the, are two known Dirichlet boundary conditions, right? And, and there, there will be more, of course, in this sort of problem. Okay, so, that means that that degree of freedom is known, and that degree of freedom is known Okay. All right. And when we carry out this matrix vector product, we know that this entire column Right? And another one, right, are going to be known. Okay? So, for the way we had, numbered it previously, this column is what I would call the K bar, let me see. The column number here is the following. Nsd times a, right?, where a is the global node number, all right, plus 1. All right, because it corresponds to d bar a 1, right, 1, coordinate direction of global node number a. Okay. And this column likewise is K bar. Column number nsd times b plus 2, right. Right? This is the column.  Okay? So, since d bar a 1 and d bar b 2 are known. We do just what we've done before which is to account for the fact that those, that these columns multiplied by those known degrees of freedom can be moved to the right hand side. Okay? So we do that. Right? And then we are left with a reduced system, c transpose K d equals C transpose, F internal plus F t bar, right, minus d bar A1, which is a scalar degree of freedom, multiplying the column that we identified on the previous slide K bar nsd times a for the global node number plus 1 minus d bar. B2 scalar degree of freedom, multiplying the quantum number of K bar. nsd times global node number B plus 2. Okay. And as we've been doing in previous problems, that is our final F, vector. Okay? So what this implies for us then is c transpose, K d Minus F equals 0, right? And the degrees of freedom sorry, the dimensions of this K matrix now, it's square, right? because we got rid of all the Dirichlet degrees of freedom which were known and moved them to the right hand side. Okay, so K finally is number of nodes times nsd minus the total number of degrees of freedom that have their Dirichlet conditions specified on the N square. Okay? That is, those are the dimensions of K bar. Now of course we invoke our waiting function condition that our weak form, our finite dimensional weak form must hold for all waiting functions in the appropriate space. And the fact that it must hold for all waiting functions in the appropriate space is enforced here. By the requirement that this matrix vector equation that we have as the last line of the slide, must hold for all c vectors belonging to a Euclidean space of dimension number of nodes, times number of spacial dimensions minus ND. Okay? Which implies for us finally that we get back the same matrix vector from the equations. We know Kd equals F. Right? And we solve this for D, which will give us our global displacement vector. Okay? I should make one remark here that since we are talking of d being defined as K inverse F. Right? Under what conditions does the solution exist? Okay? Solution exists. Or there exists a solution d if our K matrix is positive is, invertible of course, Right? Under what conditions is it invertible. What, what, can you think of what it is that guarantees invertibility of that matrix? It turns out that for the 3D elasticity problem, K is positive definite. What conditions make K positive definite? One of the things that makes a positive definite is because our elasticity tensor, C is positive definite. Is there any other condition? Yeah. If you have some experience with these types of methods and with solving linear systems of equations, you will probably recognize that it has something to do with our boundary conditions as well. All right? We need to have enough Dirichlet boundary conditions to eliminate what are sometimes called rigid body modes in the context of elasticity. Okay? So K is positive definite and C is positive definite. And if the Dirichlet boundary conditions eliminate rigid body modes. Okay. And the question of how to do that is a little more involved which we really won't get into here at this point. All right. At this point we are actually done with our treatment of 3D linearized elasticity. This was our example of a vector problem and we'll end this segment and this unit here. When we come back, we will move on to a wholly new class of problems."
Agvq4CxFTDU,"Welcome back. With this segment we are going to start a new unit, and this will take us away from elliptic problems. We are going to start looking at parabolic problems. Right? And we're going to, going to stick with linear parabolic PDEs in three dimensions, but for a scalar variable. The kinds of problems we are looking at, therefore, are very similar to ones we've looked at, we've already considered. They are the, the unsteady heat conduction problem in three dimensions, or the unsteady mass diffusion problem also in three dimensions. You will recall that previously we studied the steady state versions of these two physical problems, and, because we were looking at the steady state versions, those, particular PDEs, are what we call elliptic PDEs. When we bring back the the time dependence, and say they're unsteady problems, we have parabolic PDEs. Okay. So with that somewhat verbal introduction let's get on with it, right?. Linear. Parabolic PDE in a scalar variable, in three dimensions as well.  Okay? And, like I said, the physical problems we are considering here are unsteady. Heat conduction. Unsteady heat conduction and mass diffusion. In 3D. Okay? And just remember that unsteady here means that we're talking of time dependent. All right. So, what is the situation we have here? I don't have with me today my my basis vectors, but we don't really need them. We have our body, right? We have basis vectors here, three dimensional. Everything that we talked about, the, the steady state heat conduction problem holds. Okay, so we have surfaces on which we are going to specify Dirichlet and Neumann conditions for the temperature if you're doing heat conduction or the concentration, if we are doing mass diffusion. That's fixed, okay, that remains the same. We have a source dom. We have the notion of the conductivity tensor, or the diffusivity tensor. All the same. The additional component is that we are saying now that at every point in the domain. Either the temperature if it's the heat conduction problem. Or the concentration if it's the As diffusion problem, is changing with time. Because of flexes, or because of the source term. Okay? So at every point, we will have an addition, a time dependent term. Its going to be a first order time dependence, because that is the nature of parabolic problems, right? And that is indeed the nature of the heat con, of the time dependent on unsteady heat conduction and master fusion problem, right? Their first order in time. Okay, so with that setting, let's let's wr-, essentially write out the strong form. Okay, and as we've been doing, let's begin by drawing a picture. The figure, these are our basis vectors E1, E2, E3. Right, our domain, that, right? Three dimensional, of course. We have three basis vectors here. Omega. Right? Somewhat mercifully perhaps, we are back to a scalar problem. Right? So, we don't need to worry about the three different, decompositions of the boundary, right? So we have here, partial of omega u, right? Because u is now a scalar once again, and here we have partial, I believe omega j is how we denoted it, right? All right. Now point here has position vector X which we will use, right?. And at this point the picture here, the usual pill box argument that's given is the following, right? We look at a little elemental volume. Okay. What we see is that we have a flux is coming into it. Okay? And since we've already introduced the notion of a flux before, we can use it now, okay? So, The flux is coming into it, and, and for certain it could be exiting some part also, right? So next one. So this is our flux factor J, okay? Now, inside that little volume element, we have some source stone. And that source stone, you recall, if we're de, dealing with a heat conduction problem, would represent local heating, right, through some external source. Or for the mass diffusion problem, it would represent a local supply of mass, okay? So let me write an F if I can there. Okay? That's F. Now, what we are seeing in this unsteady description of the problem, the description of the unsteady problem is that the result of the fluxes, the net flux into that volume element and the effect of the source dom they're combined effect is to change either the temperature for unit time. Right? With respect to time either the concentration changes, or the temperature changes, with respect to time. Okay, so let's also add in here essentially a, just to portray this, let me say that there is a d u, with respect to d t dom also coming up, right? T of course is time, okay? So, the strong form is the following. As always, given the data, given, I think we were still calling it Ug back then, we had Jn, which is our influx condition. Our source f, right? And the constitutive relation that we are now very familiar with, All right, using coordinate notation kappa i j being the conductivity tensor, right? Given all of this, now, we have one extra piece of not quite detail, but it really is a coefficient that is relevant to the problem. And I want to put it down here to have relevance to make connection with the physical problems that we are trying to keep at the back of our mind. Heat conduction and as diffusion problem. That quantity is going to be denoted as rho. Okay. I'll tell you once we set up the problem what rho is. Okay. So given all of these, what we're trying to do, is the following. Find u, okay, such that the following holds, right. Rho, partial of u, with respect to time, equals minus ji comma i plus f in. Now, here is this, this part is important. When you are doing the steady problem you specify the, all our steady, all our previous problems were time independent. Right? They were all steady state problems. We specify the PDEs therefore only over a spacial dimension, over a spacial domain omega, right? Subset of R 3, in general, in the 3D case. But now we have time dependence as well. So we say that this PDE holds in a combination of the spatial dimension, or the spatial domain and the time interval of interest. And that is indicated by a cross 0 comma capital T, okay? So the closed 0 to capital T is our time interval of interest. 'Kay? And when we write omega across that time interval we are just saying that our PDE holds over a certain spacial domain omega. And over a time interval 0 to T. Okay? All right. As before we have boundary conditions. We have u equals ug on the Dirichlet boundary. We have our Neumann condition, minus ji, ni equals j sub n on partial omega j. Is our problem complete with specifying boundary conditions? No. We need initial conditions as well. Alright? Because it's a first order problem in time, we have a single initial condition. Okay? And the way we do that is to say that U, which can be a function of position. It is indeed in general a function of position and this is what we saw in our steady state problems. Right. So, we have U as a function of, as, as, as parametrized we have position and at time t equals 0, okay? Equal sum U. Not function of position only, okay? All right? And perhaps this is best clarified by also saying here that U is a function of position. And time. Okay? All right. That indeed does complete the specification of our problem. PDE boundary conditions and initial conditions. What I'm going to do here is just make one or two remarks. Okay? The first remark is that, we need to say something about this new coefficient we've introduced, rho. Okay? For a heat conduction problem And for heat conduction problems, can you tell me what rho is? Yeah. For heat conduction problem, rho would be the, rho is the specific heat.  Okay? And, and in the case of heat conduction, do you also know where our p d comes from? What, what physical principle leads to our PDE? It's actually the first law of thermodynamics. Okay? So in that setting rho is the specific heat, okay, and the way we've written it, rho would be the specific heat per unit volume. Okay? So it does as we went you know row with specific heat per unit volume. The specific heat also can be determined as a can also be defined per unit mass. Okay but in our in our setting for the way we've set up the problem. Loads the specific heat per unit volume. Right? It turns out that if however we were looking at the the mass diffusion problem. Rho is equal to 1. Okay? We don't need a notion of spe, of specific of anything like a specific heat in the context of mass diffusion. Mass diffusion just rises from a, physical principal, which is, the conservation principal. Okay, so that is the setting for, for, for, for this problem, sorry, that is the, sort of, setting of context for For, for the physical problems. Okay, so let me see. Is there anything else we need to really talk about here? Actually I believe not. So, we have laid down our strong form of the linear parabolic PDE in scalar varia, in a scalar variable in 3D. It connects up with our heat conduction, mass diffusion. Physical problems and  we'll end the segment here. When we return we will do the usual take the usual steps that we've taken before right? The weak form and then talk about the the finite element formulation."
AFS1OqCquQ8,"There was a minor error in board work on this slide. It appears just about here where I written out incoordinate notation. The flux with the conductivity and the, what was meant to be the gradient in the field on the right hand side. And what's missing is, in this term, the evidence of the gradient. And that should appear as U comma J. Right, the comma was missing there, which did not suggest the gradient with respect to spacial coordinant. With that in place this equation and the rest of the slide are both consistent."
1ZSooWlRUYM,"Welcome back. We're ready now to work on our linear parabolic problems in three dimensions in scalar variables. What we did in the previous segment was setup the strong form of the problem. We'll just write it down very quickly now and, proceed on to the weak form and other things, all right? So we write, start out here with the strong form. All right. And just remember we are still talk, we are back now to talking about scalar variables. All right. Or scalar unknown, really, let me call it that. Okay, so the setting, as before, involves our domain. We have our basis vectors, e1, e2, e3. That is our domain of interest. It is omega, a point on it is x, the position vector x. And we have the setting of our Dirichlet boundary subset and the Neumann boundary subset, okay? This is the setting, what we are seeing is now given for data, right? We are given ug, jn, f. We have our, our, our old constitutive relation for these problems in 3D as well, right? Which is that minus ji equals, sorry. Plus ji equals minus kappa ij, u comma j, right? These are all the data that we use when we did the steady state problem, right? We have in addition, another coefficient, which I'm calling just rho, okay. And we made the point last time that this would be the specific heat for, unit volume, if we were doing the heat conduction problem. If we were doing mass diffusion problems, rho would typically be one, okay. Given all these data, what we want to do is find u such that. Right? The following holds. Rho time derivative with a partial time derivative of u equals minus j i comma i plus f in omega, the domain cross the time interval of interest, okay? For boundary conditions we have the same boundary, the same sort of boundary conditions that we encountered when we did the steady state problem, right? U equals u g on the Dirichlet boundary. And, right, minus ji ni equals j sub n, the influx heat, the heat influx or the mass influx, on the Neumann boundary. Additionally, we made the point that we need initial conditions, right, or an initial condition here. We have only one initial condition, because because of what? Do you, do you recall? It's because our problem has a single derivative in time, right? It's first order in time. Okay. So we need a single initial condition. And that is specified as u. Remember u is in general a function of position and time, but now we set the time equal to 0. And we say that this is some given function, u naught, suggesting the initial value, right, of u over the domain, okay? So this is what we have. And what we are faced with in this segment is setting up the, the weak form. All right, we are going to take the approach for the weak form that we'd taken before, which is, we, we have the strong form. We multiply it by a weighting function and integrate over the domain. All right, so to get to the weak form. All right. And remember this is going to be the infinite dimensional weak form. All right. In order to get to that, we see the following, right. Consider. W belonging to V, where V consists of now all functions such that w, equals 0 on partial of omega u, right, our same old weighting function, all right? We consider this, right? And we, essentially we multiply and integrate, okay? And we will do that in the next slide, okay. So what we are doing is we multiply. So we say the following, right? We have w rho, partial of u, with respect to time, equals minus w j i comma i plus wf, right? So we multiply all of this, and we integrate over the domain, right? So we integrate this over omega. So here we pick up a dv, all right? And the same thing happens here. We pick up a integral over omega. D v, and here, too, we pick up an integral, okay? That's what we have. Right, now, we proceed just as before, which is that we integrate by parts, okay. From here we integrate by parts, and just as we did in the case of the steady state problem, we integrate by parts only to have a different way to pose that, divergence theorem, right? We want to transfer that divergence theorem into something else, want to convert it into something else. And so we say all right, we see it there, and we are going to integrate by parts. Okay, now we are knowledge experts in this, so we don't need to go through all the steps. Let's just jump directly to the final form that we get on integration by parts, okay? And that is the following. It is, that, on the left-hand side now we have integral over omega w rho time derivative of u, partial time derivative of u dv equals. Now, the way that integral works is the, the way integration by parts works here is to give us two terms. One is integral over omega w comma i, j i, dV. I'll write the second volume term, which is at this point just a bystander, and in fact, indeed is a bystander through most of our, deriv, our, our, the development of our formulation. And we have, of course, the boundary term, right? We get integral over partial omega w ji ni, right? And that's in an, an integral over the surface so we have ds, all right? So nothing happening with the time dependent term on the left-hand side or the forcing function, okay? And then of course we take the usual steps which is to observe that this term, right? Is equal to integral over partial omega u, w ji ni dS minus integral over partial omega j w ji ni dS. Right, we have these two terms and then we invoke our boundary conditions, all right, on the strong form, as well as our, our homogeneous boundary condition on the weighting function, right? Given the way we've defined the weighting function, the way we've always defined the weighting function, we know that w goes to 0 on the Dirichlet boundary. So that term drops out. And here, we know that ji ni on the Neumann boundary is minus jn. All right, so, making these substitutions we arrive at integral over omega w rho partial time derivative of u, dV equals integral over omega, w comma i, ji dV plus integral over omega w f dv plus integral over the Neumann boundary, w jn dS, all right. Now, let me do just one more thing and we'll have the final weak form. I'm going to invoke the constitutive relation here, right? And we know that ji is minus kappa i j, u comma j. So, we invoke this, and then also observing that we have a minus sign in front of it, I'm going to move it to the left-hand side, okay? Right? So, what we have finally is the following. Integral over omega, w rho, partial time derivative of u, dV plus integral over omega, w comma i, kappa ij, u comma j, dV. Equals integral over omega, w f dV plus integral over the Neumann boundary, w jn dS, okay. This is everything we have, right. What, let, let me just write out now the finite dimensional weak form and we'll be ready to go, right? The finite dimensional weak form from here is obtained by just observing that any attempts to solve the, the infinite dimensional weak form are not likely to be any more easy than the strong form. So we decide to go to an approximate representation of it, all right. And what this says, is now find u h belonging to S h, all right. Which is a subset of S, okay? And what is Sh? Sh now is a collection of all functions of the type of denoted u h which, as before, we will expect to come from h1 on omega. Right, so the spatial dependence is going to be the same. Right, even in the time-dependent problem we are assuming that the kind of approximations we are going to construct will have the same dependents that we know from, from before on the spatial variable, okay? So we have this. U h equals u g on the Dirichlet boundary, okay? So find u h given this. Now we'll, let's assume all the data, right. So we know everything about the data, all right? Find u h belonging to S h such that for all w h belonging to V h subset of V, right? Where again, V h is also drawn from the space of H1 functions. Okay. Now, for all w h belonging to V h, the above weak form should hold, except that every function that is obtained from either w or u is replaced with the finite dimensional version of it, right? So we say that integral over omega w h rho partial time derivative of u h dV. Equals integral over omega w h comma i kappa i j u h, sorry, comma j. I realize I got the sign pro, I brought in the equality too early, plus sign here. DV equals integral over omega, w h f dV, plus integral over the Neumann boundary, w h j n dS, okay? This is our finite dimensional weak form for the, for the unsteady problem, right? Now, lets just stare at this for a few minutes, right? Or maybe, maybe not that long, but for a few seconds at least. Right, so now when we go through the whole process, what we expect is that just as before, right? We are going to do everything as before, right? Let's we, we, we are going to now, how do we go to these finite dimensional weak forms? Well we are, remember that, that, that these are, at this point, at this point, this problem is still finite dimensional only in space. We've done nothing about time, right? Because time is still very much of a true derivative, right? That is, that is a time derivative. We haven't done spe, anything special about approximating it yet, okay? So, as before, we will construct our finite dimensional basis by, by a partition of the domain, right? So as usual we will say partition. Omega equals union over e of each of these omegas. So e's, right, we have all that, right? Okay. And the picture is, is also the same as before. If this is omega you know, let's suppose again since we are in 3D, let us suppose that we are using our hexahedral element subdomains and. That is one of our elements, all right. This is omega sup B. Okay, all of that is the same."
BnlrtvFcJRg,"'Kay? And in particular let's suppose that we have let's, let's suppose that we have bi-linear elements, right? Here. It, it, it really doesn't matter whether our elements are bi-linear or not. It will come in only in a few more minutes, but but, but whatever it is, once we know that we have this partition into elements of domains, we can then go ahead and construct our basis functions, right? So, just as we did before we will construct basis functions. Okay. And, and here's where things start getting a little interesting. So, let's construct basis functions. We know that the, the way that we set it up is to, to construct basis functions which are, defined over every element, right, that's how we do it with this idea of compact support of the, of the underlining basis functions that we are going to, that we are going to use. And I will specify that here is, is independent upon position, and time, okay? So we want to construct a representation for this sort of function, using our basis functions, alright? So we will as before expand over the nodes in the element, NA, okay? Now, here is the, here is where we need to really start paying attention. So, any of the func, is, is parametrized by x because after all it is a, spatial basis function. But we already know all about that. We know how we're going to construct our basis functions from our, bi-unit domain right? Which is parametrized by c. All of that is exactly the same. So, this is where the spatial dependence of Is, taken care of. The time dependents of Right, is done in, in this, in the sort of formulation that I am describing here. It is done simply by having the coefficients here. Right? Which we know to be the nodal values of our trial solution field. Right? DAe all right? We just make these guys time dependent. Okay? In this sense, once we've done this what we are doing is a spatial discretization. Right? So this thing, this takes care of our spatial discretization. Okay, we haven't done anything to discretize time, yet. For this reason, the type of formulation I'm describing where the time dependence is held in the coefficients. Right? This type of a formulation, this type of a finite element formulation is often called a semidiscrete finite element formulation. All right? So this thing is often called a Right. Okay. Because there is as yet no discretization of time. All right. Interestingly, for w h, let's try to understand what we have. We know that w h is a function of position. Look back at our weak form, and think about whether or not we actually need any time dependence in our waiting functions. Is there anything in that weak form, you could look at the finite dimensional or the infinite dimensional weak form, it doesn't matter. Let's look at the finite dimensional, because that's the one that we're, we are actually working with. Is there any need for time dependence in our waiting functions there? All right, and indeed there isn't any. Okay. Because, it doesn't, there's no time derivative on it. There also is no integration over time, right? So there is really no nn, you know, w is something we specified. We don't really need to worry about any time dependence there. So w h is only a function of position, right? And this we, we already know, right? We've got this one very nicely covered. All right, we have our basis functions which depend on x through c and the coefficients here are c, A, e. So this is no different in any manner from what we did with the weighting functions for the steady-state problems. Okay, all right with this in mind, observe that if you are still looking at, at if you still have in front of you the finite dimensional weak form right, I'll pull it up again, here we, here you have it. If you look at it, you will, if you look at the second term of the left hand side and the two terms on the right hand side, it should be b fairly clear to you that through the entire process of our finite element formulation, right, setting up the element integral, well, first of all calculating gradients of fields as needed, setting up the element integrals going to matrix vector forms, first over elements then doing assembly over the whole domain, none of that is any different. Right? For the second term on the left-hand side and the two terms on the right-hand side. Okay. So, what this leads to, what this allows us to do is save about three seconds worth of me talking, and we can simply say from here, it should be clear that from here integral over omega w h, i kappa i j u h, j dv, right, at the end of the whole process, right, when everything is said and done. It's going to be essentially a c transpose K d. All right, we know how this all works already. And nothing is different. Except, except for one thing. What is different here? The c is just as before, the K is just as before, the d coefficients are time dependent, okay? All right, we have that, integral over omega, w h f dV plus integral over the Neumann boundary wh jn dS is c transposed f, okay? Just as before. Right? Because here there is no time dependence in in c nothing is different,. Note, however, that, right one could allow a time dependence in the forcing term. Okay, and in fact even in our boundary terms. Okay, so let me make a remark here. Okay, one can have ug equals ug function of time, right? So the Dirichlet function that we use, the function that we use on the Dirichlet boundary, could be a function of time. In fact, so could your influx. Right? There is nothing preventing this. Right? And indeed, the formulation can take full account of it. It could also have f, our forcing term could be, of course, a function of position, but also of time. Okay? And so in general we could have f equals some function of time. All right. Okay, if that's all, good, really the only term we need to worry about, when you go back and look at your week form for this problem is the very first term. Okay. It's only this term that we need to worry about now. Let me go to a different color. Right, this is the only term that we need to really focus on here. Okay? And so we will when we return in the next segment."
MeRwljkxtIM,"So, we will continue, and what we observed at the end of the last segment is that the only contribution we need to really worry about and, and, and expect it to be any different is the new one, right? And, so we consider the time-dependent term, right? Right? And the time-dependent term is this one. Integral over omega, w h comma, sorry, there is no comma here, that's the whole point. W h rho partial time derivative of u, d v. Okay, this is the one we need to consider, sorry u h here. Okay. It should be completely straightforward now because, we, we already have our, expansions for w h n u h, right? And, in fact, in fact, let's just make one observation. In order to consider this, what we need to do is use derivative of u h with respect to time, the partial time derivative is after all nothing. So, and, and there, furthermore, restricted over a particular element, right? That partial time derivative in an element e is nothing other than sum over a N a, we know all about those, d A e dot, right? Where the dot denotes a time derivative. Okay? All right. Well then. The way this works out then is that, integral over omega w h rho. Multiplying the time derivative. Is. Sum over the elements. Right. Integral over each element of the following. Right? Now, let's, let's write out the, the usual sort of summation that we have. Sum over a N a c a e, right? And we know that this first parenthesis is what gives us the w h term, multiplied by rho, and for the next term, we have now sum over B, N B, d B, e dot d V. Okay? Simple as that. All right, so I'm just going to carry along this sum over e, as well all right, because we know, we've done this sort of thing so often in the past that we know how it all works out, okay, so let's just do that. So, this is a sum over e right, I'm, as before now I'm going to pull out the sums over A com, A and B. We have here c A e, integral over omega e, N A rho N B, d V. We have there the integral, and we get here d B e dot, okay? All right, so, if you stare at what we have here for the integral, that I've put in parentheses, it should be clear that for a given element, when you carry out that integral for a combination of, basis functions on node A and B, you get a scalar, right, in every case. Okay. This, resulting matrix, or, or, or these resulting terms, are going to be of a form that we've not, encountered yet, before, okay? We have encountered related, not really related terms, but if you think about the term that gives rise to our, conductivity tense matrix or our diffusivity matrix for this sort of problem, it involves derivatives on the N A and N B, right? It involves spatial derivatives, right, we don't have any of those in this term. Right? So, this is a different type of term. I am going to denote this integral M A B sub e. Okay? All right. And this term that I have just written out, M A B sub e can be further assembled into a, a matrix vector form, right? And we know how that happens, right. So then from here, if we just use, c e equals c 1 e up to c number of nodes in the element e. Right? And the same thing for d. E. D e we know is just d 1 e, up to d number of nodes in the element sub e, okay? However, we also know that in the particular form that we are dealing with here, we have a d e dot, right? Because it, we, we get time derivatives of each one of these terms. I'll make this dot a little bigger than the dots of the ellipses, okay? So, when we get all of that, what we observe is that, this integral that we started out with, this, mystery new integral, essentially reduces to, integral over omega w h rho. Right, so this integral that we're trying to evaluate is essentially now sum over e, c e transpose, right? By assembling all those you know, by replacing the explicit sum over nodes A and B, right, the explicit, replacing that explicit summation, which we had on the previous, slide here, right, this explicit summation. What we're able to do by defining these element level vectors we know is get rid of that explicit summation, right, and we are masters of this, as well. Okay, what that means is that our M A B scalar terms, slot themselves nicely into a matrix, right? They slot themselves into a matrix that I am going to call the M. Bar. A B matrix. Sorry, it's not A B. I, I just got rid of the A B. The M bar e matrix. Okay? And that is multiplying a d bar dot e, vector. Okay, you also know why I am using bars here, right? Rather than the final forms, because we know that we can have this issue of Dirichlet conditions there. Okay, so just because, after we apply the Dirichlet conditions the final matrix, matrices that we have are going to be reduced in dimensions.  Also, the final d vector, or, or the d dot vector that we have is going to be reduced in dimensions. Through exactly the process we've observed before, for handling Dirichlet conditions, okay? It's, it's, it's in, it's in sort of preparation for that, and in anticipation for that, that I'm calling these, this matrix M bar, and M bar e, and that, that vector d bar e, okay? That's all. Now, this matrix is what is called often the mass matrix, okay? In the context of finite element methods, any such matrix that's obtained by directly multiplying the basis functions, no derivatives, right, no spatial derivatives on the basis functions. Directly multiply them and integrate over the domain, maybe multiplying with rho, right, and you've that in some cases that rho could be 1, so that case is also covered, right? The important thing is that it is when we form a matrix of this type which involves a direct multiplication of the basis, functions over an element, right, and their integration, it's called a mass matrix, right, and in this case, of course, the element mass matrix, okay? All right. It's, it's a new matrix that we've not previously encountered, okay? Now, I should make a remark here. Which is, that if we have a general element that is far, that, that is not, that doesn't have one of its surfaces coinciding with the Dirichlet boundary, then of course we know that the, the, the c e vector for that element is going to be is going to be full. It's going to have as many entries as the number, as the nodes in the element, and likewise the d bar vector, right? For that case, all right, for that case what sort of a matrix is M bar? 'Kay. So. For a general element. Omega e, such that. The boundary of omega e intersection Dirichlet boundary is the empty set, right? So an element that does not have one of its surfaces coinciding with the Dirichlet boundary. For such an element we observe that the dimension of c e equals n n e, okay? Right? Right? Therefore, what that implies, that for such a, for such an element, what can you tell me about the M bar matrix? Okay, recall that the M bar matrix is now going to have this form. M 1 1 e up to M 1 n n e, e. M n n e, n n e, e, and here we will have M, n n e 1 e right, it should be obvious that this is a square matrix with, dimension n n e times n n e. What more can we say about it? If I have a general term here that is, M A B e and, we have a term here which is the. Through the transposed position in the matrix. What can we say about M A B, and M B A? Right? That's right. They're equal. All right, it just follows from the definition of one of those components, right? Each of them is just a mu, just the integral of n A times n B multiplied by rho integrated. All right, so they're symmetric, right? So basically we have M, M bar is a symmetric matrix. Okay. All right. One can also show using the properties of the of our basis functions that the M bar matrix is positive definite, okay? Right, and you recall what that means, right? So if we have, some general vector. Well actually, let well, it, it's true for M bar e, as well, but, but, hm. Yeah, that's okay. So M bar e is positive definite, so what we see is that, what that means as you will recall is that if we took any vector c, right, and we formed this product c transpose M bar e, sorry, if you too any c e transpose c e, right? And remember, c e can be arbitrary, right, could be any vector. This is, greater than or equal to 0 for all c e being in any dimensional vectors, right? In fact, it's equal to 0 only if c, the c e vector itself is equal to 0. Okay? This is what we mean by positive definite matrix, and indeed, M bar e is positive definite. Okay, all right, so this is one remark about properties of the matrix. There is another remark, which I am going to make, which is that M bar, as computed here, is what we call the consistent mass matrix. Okay. And. Okay. One can also do a lumping, okay, and, what a lumping involves is computing the consistent mass matrix and then summing up all the elements along a row, and putting that sum on the diagonal. Okay, so it's, it's, it's an approximation, it's not a, that's why it's not called a consistent matrix, right? So a lumped element mass matrix. Is the following, right, M e, say, tilde. Okay? Which is obtained as follows. On each diagonal, use sum over A. Sorry, sum over B M, in this case use sum over B, M 1 B e, okay? And put it there, and make the other 0. Okay. Likewise, you do that for all the diagonal elements. Okay. And, and every other element is 0. Okay. Right, which basically is telling you that M tilde e A B, right the A B component of the M tilde matrix, is simply sum over B. M A B e. Right. If, sorry. A equals B. If this is to be the case, sorry, I need to make a correction here. I need to have a new index for the sum. Okay, so if A equals B, which makes it a diagonal element, then you simply sum up over, you sum up the elements in that row. Okay? And put that, sum on the diagonal. Okay? And, it is equal to zero, otherwise. All right. It's just, it's just a sort of, quick and dirty way of diagonalizing the matrix, right. It's not a proper diagonalization through defining orthogonal basis functions or anything like that, right? It's just, just a way to simply lump it and definite a diagonalized matrix. And you also see why it's now called a lumped mat, element mass matrix, right? Because you're lumping all of the mass up at the, at the nodes. All right."
rbpJXpAoizA,"There remained a minor error in board work on the slide that I have before me and that you have in front of you on the screen. It appeared in the way I wrote out mass lumping for that diagonal component. It's an error that you may have picked up on yourself, especially by comparison with this other term, which is correct. The error here is that I wrote that sum as a sum over e. It should properly be a sum over B. So what we want to do is draw an arrow right through that e and replace it with a clear B. With that, everything's consistent."
cBN8hI2Cb_8,"Okay, so, so this is something to know, and we will also see what happens when we, you know, if we, we. From, from here we, I'm not going to consider the lumped mass, element mass matrix. I'm going to go back to the consistent one, when we finish the, the process, we will then redefine at the global level also, a lumped mass matrix, okay? But this, this, this was a very useful place to introduce it. Okay, so, with those two remarks in hand, let's just return to our formulation, which is the following, right? So we have we have now this, the following representation. Integral over omega w h rho Right? We've got as far as this. Consistent mass matrix. Okay? We're going to go from here to assembly. All right? Now we do just the same things the, the same things that we know from before, right? We say that okay, my c vector is you know, going back to our global numbering. Both nodes at c1, c2, up to c number of nodes in the problem. Okay? And this is. Global degree of freedom numbering. Okay, and the same holds for the d bar matrix, right, d bar again is d bar 1, d bar 2. D bar number of nodes. Okay? All right? Okay, oh, I, I should be a little careful here. This is not just number of nodes, right? I should be careful here. The size of the C matrix is less than number of nodes. It's number of nodes minus number of Dirichlet degrees of freedom, right? Number of degrees of freedom that have Dirichlet conditions on them, right? Whereas d bar is the full one. Okay, so we have this, what that let's us do is that we, we can go from here to saying all right if we have these vectors c trans, c and d bar defined. Then what we have here is c transpose n bar d bar, right, where n bar is the assembly overall elements of D n bar e matrix. Right, of the consistent element mass matrices. Now, this assembly process works just as before, right, just in the way we, just in the way we assembled our conductivity matrix or the diffusivity matrix, right, which is that we go, we look at every single degree of freedom, that global degree of freedom, that shows up on any element, right, and add the contribution from that element into the, into the global mass matrix. Okay, so that's it. So let's just recall with a very quick example how that works. Right, it's really no different from, it's identical to what happens with a K matrix. But it's probably worthwhile to just recall. Okay? All right, so as before element omega e1, omega e2. Let me label the global nodes as maybe A, B, C and D, and go to the local node numbering. Right, the local node numbering may be 1, 2, 3, 4, 5, 6, 7, 8 and on element e2 we have 1, 2, 3, 4, 5, 6, 7, 8. Okay, so with this setting in hand. Let's go ahead and look at how the m bar or, the global n bar matrix would be formed right? For just, for those nodes A, B, maybe AB, maybe CD as well. Let's see, okay, so So now we go to n bar, right, the global matrix, and let's suppose that the nodes we are interested in are A, B C and D. And those show up here as maybe A, B, C and D. Okay, so let's look at the contributions to the m, to the a a, position right, right, out here. So, from omega e1, we would get a cont, contribution which would be M e 1, 11, sorry, 22,  22 plus from m, from e2, we get indeed 11. Okay, let's do another one, let's do cc. Cc would be from e1, n e 1, we get 77, and ne 2, 88. Right? And of course, because node A, global node A and global node C will be shared by other elements besides e1 and e2, this, there could be more contributions to both of these, right? Okay. Let's do the AB term. Yeah, let's do the AB term. So, for the AAB term, what we see is that from e1, right? From e1, we get the 26 component, and from e2 we get the 15 component, okay, and maybe other ones too. Other ones from other, other elements that share nodes A and B. Just to demonstrate the, the symmetry let's do the B A term. So this was the, actually I think I already did the B A term. Let's do the A B term now properly. The AB term would be the a row and the b column. It would be the one here. Okay, so let's see. So, if we now we look at oh, no. I, I just realized that I, I actually did make a mistake here. Everything that I wrote here actually goes to the goes properly to the AB term. Right, so I was talking of the BA term but then I wrote all the AB contributions here. Right, so one needs to be careful about this, always, of course, okay. So the AB term means row A, column B. Okay, so let's do that. Row A, column B would be here M e1 26 plus M e2 15. Okay, and then let's do the row B, column A down which would be the one here. Okay? So row B, column A 10 would be from Me1 BA, right? So it would be 62 plus Me2, 51. Okay. All right. And so now we realize that because we've observed already that the element, the consistent element, of this matrix is symmetric. In fact, the lumped one is also symmetric. Anyway, because a consistent one is symmetric, we realize that the, that the global consistent mass matrix is all the symmetric, provided it is square, right? But, in general it is not yet square. We need to make it square, right? Which we will. Okay? All right. Now, let's, let's, let's go ahead with this with this right? And so, so, essentially this is how things will work. And, and what you're observing, is that the way you slot in the components of the same bar matrices are, is identical to the way we do it for the, for, for the, for the conductivity of the diffusive, diffusivity matrix. Okay? That assembly process is exactly the same. Okay this is a useful place to stop because all we do, really need to do when we return in the next segment is talk about how Dirichlet boundary conditions have an effect upon this term, this new term that we are considering. All right."
_ChO6q_8Udk,"So, I just noticed that when I wrote out the globally assembled form of this new time dependent term, I missed a critical dot on it, right? So this is what we have before us. And when I wrote out this formula. Right, what I missed was the fact that there is a time derivative on the d. And that dot is what I missed out. So let me make a nice big green dot on it, okay. And that carries over here. Okay, with that correction, we are essentially ready to move on."
Eza34lQzpK8,"Welcome back. What we did in the last segment, what we accomplish was a global matrix vector representation of the new time dependent term that arises in the unsteady form of the linear parabolic equation in three dimensions with scalar, with a scalar unknown. So let's so to speak, finish the job now. And we'll do it by carrying out the last remaining step in our finite element formulation. Which is accounting for our Dirichlet boundary conditions, okay? So the topic of this segment is, Dirichlet boundary conditions. Okay. So, for completeness, let's just and, and for connect, and for connections let's just write out our finite dimensional weak form, just, just the integral form. And then immediately after that, our matrix vector form for it, okay? So, what we have is the following. Integral over omega w h row, partial time derivative. Plus integral over omega. Wh,i, kappa ij. j sorry. U h comma j d V equals integral over omega Wh f dv, plus integral over the, the influx boundary, wh jn ds. Okay, these are the terms we have. Now, we've already written, so we've already written out the matrix vector form coming from the second term on the left hand side and the two terms on the right hand side. And we know very well that after accounting for the Dirichlet conditions on those terms we have a form that c transposed k d equals c transposed f. Okay, and this is all we would have if we were working with the steady state problem. What we've added onto this is the understanding that this term can be rewritten as c transposed M bar, the consistent mass matrix, d bar dot. All right, now, let's talk the Dirichlet boundary conditions on the Stein dependent term, only understanding that the Dirichlet boundary condition have already been accounted for on the remaining terms. Okay, so. So what we're seeing is that this form follows if the Dirichlet boundary conditions From the. From the integrals to be really precise about this, okay? For if this follows, if the Dirichlet boundary from the, integrals. Without time derivatives, Have already been accounted for. In the, in this matrix vector form.  Okay? In particular, in particular, note that that is what allows us to say that the K matrix is square. All right, otherwise the presence of Dirichlet boundary conditions makes K a, would, would make the corresponding matrix there a rectangular matrix. And that's what allows us to say that we have d here and not d bar. Right, the d here is only the final set of unknown Dirichlet conditions, and in fact, what I am saying here, what we are saying, is that F already has the Dirichlet conditions accounted for in there. Okay? So I'm sort of jumping ahead to that final step. All right. So this thing already has. The Dirichlet boundary conditions, from, the non time derivative term, right? The non time derivative integrals. Okay, it may seem like a piecemeal way of doing it, but I think we understand why we're doing that because we've already been over that part of the steady state problem. Yes in fact, you could view this as taking the steady state problem and adding on the time dependent term, right? How would you do it? Well, this is how. Okay, right, if that's what we were doing, then already, the Dirichlet boundary conditions would be accounted for in the f vector. Okay, so then we only have to worry about the Dirichlet conditions which are reflected in here. Okay, so let's do that. Okay, so, let's note, then, that C transpose. M bar, d bar dot, okay? Can be, or, or, or is indeed written as a C vector, right? C 1 up to some, C N N E minus N D. Right? We have that, multiplying Our m bar matrix and here on the right we have our big D bar vector. D bar dot vector in fact. Okay? So here we have, d bar 1 dot. Right? And let's suppose that here we have a d bar, d bar dot. Okay? Maybe I should make that dot in a different color. Well, never mind. That big dot is for, for the time derivative, right? The little dots are ellipses. Okay. So, it goes on and let's suppose that we have here a d bar, with a big dot, on a, on the B bar degree of freedom. And we, and with our, d bar n n e. Big dot. Okay? Now we know how this works. What we're talking about is that those degrees of freedom, have a Dirichlet boundary conditions. Okay? And we completed the Dirichlet boundary condition on degree of freedom a bar. Global degree of freedom Right on d bar, okay. And we know how that works out, because what, what we're seeing is that is that the, that this column, okay, that I'm going, that we denote as a, M bar, A bar. Right? That's a column. Right? And likewise I guess we'd get N, M bar, B bar column here. Right? And we know how that, what that implies, it just says that every element in that col, every component in that column is multiplied by the d a bar dot com-, component. And likewise, everything in the M bar B bar column is multiplied by the d B bar, sorry, t bar b bar dot, right. Degree of freedom okay, but those are known, right. So these are known, right. Not only are those, are those degrees of freedom known, but since they are time dependent, if they're known with every, at every instant in time, we can indeed compute their time derivatives as well, okay. So those components also are normal, all right. Those time derivative components, all those degrees of freedom, the Dirichlet degrees of freedom are known. So what do we do? Well, we take it, take them to the right hand side because the idea is that, that time dependence of those Dirichlet boundary conditions also drives the problems that we're looking at, okay? It's a time dependent driving of the problem with Dirichlet conditions. Okay? Right. So may, maybe I should just state that as a remark here. Right? So in the form that I've written things up in the previous slide. D bar, a bar, and d bar dot b bar drive the problem actually, let, let me qualify this further, drive the initial and boundary value problem Via time-dependent, Dirichlet boundary conditions. Okay? All right so well, what do we do? We know very well now, right? So, what this lets us do is to rewrite the whole problem now as C transpose M, d dot, plus C transpose K D equals C transpose F. Minus d A bar, dot M, M bar A bar, that column. Minus, d bar dot B bar, just a component, right? A time dependent or a degree of freedom, right, whose time derivative now, is dr, continuing to drive the problem, multiplying this column. Okay? Unfortunately, I started out by calling the F to the root earlier to pF instead of calling it F bar or F tilde or something. So, allow me now to simply redefine this is also F. Okay? So, redefine as F. Okay. So whatever F we caught from the earlier steady state problem has sort of been updated to account for the time-dependent driving with Dirichlet conditions, okay. All right. So what that lets us say then, is that we finally have C transposed M d dot plus k d, minus F with our redefined F, including these Dirichlet conditions, right. All of this equals 0 for all C now belonging to R of dimension n n e minus N d. Okay. And that of course comes from our, a specification in the weak form holding for all waiting functions. All right, and then we impose this, we see that the final matrix vector equations we impose to solve this linear parabolic problem in a scalar unknown in 3 D is this. Okay. So this is our semi discrete matrix vector problem. Semi discrete because we've really truly discretized only in space. All right. And what we really need to do now is account for how we, deal with the time dependent term here. And note that the all the time dependence has now been sort of put into this time dependent vector. Of degrees of freedom, right? That we need to solve for. So, so we'll end the segment here. When we come back, we will focus on the time integration."
-CA1T2d97JI,"Welcome back. We'll continue with our development of the finite element method for these linear parabolic problems in three dimensions where we have a single scalar unknown. What we've managed to do in the last couple of segments is starting from the weak form we've written out the stro, sorry, starting from the strong form we've written out the weak form and arrived as far as the matrix vector equations, okay? And if you have your notes in front of you, you can go back and observe that the form of these final matrix vector equations was the following. Right, the matrix vector equations. Right, it takes on the following form. We have this new matrix, the mass matrix Md dot plus Kd equals F. Now in this form of the equations, we know that because of the, there is no methods being employed. The methods are on the weak form. The the boundary conditions are already embedded in this set of matrix equations, right? What we do need to count for, however, is the, the initial condition. Okay, so whereas we have this linear system of equations, right, essentially an a, a first order of ODE, right? So this is a first order ODE. ODE being ordinary differential equation, so let me write that out. Right, in our vector unknown, d, okay? Now, because we've dealt with all this business about, you know, number of nodes in the problem and the number of Dirichlet boundary conditions and so on, I'm going to, as we proceed from here on just say that d is a vector living in an ns ndf dimensional space, right? NDF is simply number of degrees of freedom, okay? So, those are the number of degrees of freedom we are served. NDF is the number of degrees of freedom we are actually solving for. Now, this is the first order ODE, and what that implies is that we do need an initial condition which is something we had stated in the strong form of the problem. We just need to incorporate it here, and that ODE that we're working with i, is the following, sorry, that dif, i, initial condition that we need to work with is the following. d at time to equals 0 equals the vector of conditions which are obtained by taking u not at the particular location of the corresponding node, okay? And this is all the way down to u naught at x, now written in ndf, okay? Right, instead of continuing to number by node, we've forgotten all about nodes. Now we're just looking at this linear system of equations where d as an R belongs to to an ndf lucrative space and this is what we have for the initial condition, right? It's convenient sometimes to also offer, to, to refer to this vector as just d naught, okay? Right, so this is the setting we have. One thing I should mention is that whereas we sort of carried through our development of the method by considering a consistent mass matrix, one can also do global lumping, okay? So a globally lumped matrix, lumped mass matrix. All right, let's denote it as M sub l for lumped, okay? It can be defined as this thing can be defined. Okay? And the way to do that is to simply say that MAB lumped is equal to the sum over c of MAC, okay?. If A equals B, right, and it is equal to 0, otherwise. All right? Okay, and in some cases the problem is often solved with the globally lumped mass matrix. Okay, so this is this is what we needed to complete from the previous from, from the work, we set, set out in the previous segment. We move on from here and begin looking at the integration algorithms for this class of problems, okay? And this is going to depend upon time discretization, okay? The reason for it is that the the form of the matrix vector equation we're working with is what we call a semi-discrete formulation. And I explained why it is semi-discrete because we've gone through the discretization in space, but don't get in time. Okay, so now we do time discretization. Now, in a move that may seem a little disappointing to some of you the time discretization for this sort of problem is done with finite difference methods, okay? However, the, the nature, the structure of the matrices M, K, and and the vector F depend upon the finite element method. So even though we're going to use finite difference methods in this series of lectures for integration of time dependent equations, time integration of time dependent equations. Those methods are, do continue to be affected by the, by the underlying finite element formulation for the spacial part of b d e, okay. So this part is going to be finite difference. Finite Difference methods, 'kay? With that, that's what we're going to use. Now, I'll make a remark here, which is that a fully, space-time, Galerkin finite element method does exist. Okay. So, space-time. Finite element methods do exist. And they have remarkably good properties as well. These types of methods are based on, carrying out integration. Over, omega and over the interval 0, T. Okay? So these are based actually on a formulation which is now going to look, well, I, I, I, I probably shouldn't since, begin writing this formulation because one, we'll get sucked into the rest of the development, but anyway, they're based upon integrating over omega and over, the time integral. And, they, they tend to have very good properties. For instance, they have properties of the type where, the accuracy. The accuracy with respect to time. Is of higher order. Then what we will be developing here. Right? Accuracy is of higher order than, with finite difference methods, F D for finite difference. They do come with a, they do come at a cost, though. In particular, one has to develop a finite element mesh over space and time. And this can pose a challenge when one is doing problems that are 3D in space, right. Because then you have a forth dimension in time, which can get to be a bit of a challenge to visualize, definitely. At any rate the methods exist and they're actually very sophisticated, okay. So we get back now to our finite difference methods, right. So the nature of the time discretization that we are, we are seeking carry out is the following. We are going to divide our time interval into sub-intervals. So divide our time integral 0,T into sub-integrals. Okay. And these sub-intervals are, 0, let me do it this way. These sub-integrals are t0 to t1, t1 to t2. So on until we come finally to. TN minus 1, to t N, okay. So we have all of these sub-intervals, and what we also imply here, of course. Is that t0, or t nought is equal to 0, and tN equals capital T. Okay. So, if you look at this what we've implied is that we've divided our total time interval into capital N sub intervals, right. So clearly here we have, N sub-intervals. Okay? The basis of our, of the methods that we will develop is the following, okay. So what we are going to do here is consider. A, consider an interval. T sub little n, to t sub n plus 1. Okay? All right. Where, where, of course, little n belongs to the integer integral 1, 2, N minus 1. Sorry, 0 to N minus 1. Okay. So we consider an integral of this type. The whole, basis of our methods is going to be what is sometimes called time stepping, okay. So, time stepping implies. Right. Time stepping implies that we know the solution at tn, and we want to get to tn plus 1, okay. Time stepping is essentially, is the following. Knowing, the solution at t sub n, find it at tn plus 1. Okay. All right, so we're really, sort of, hopping along in time. Okay, so if you think about it as, if you think about it as follows. That's the time axis, right. We've started here at 0, we want to get as far as t, right. We have t0, t1, right. We have tn, and we will have tn, t little n plus 1, and here, of course, we have t capital N, okay. So the whole idea is that well, I know what, what, I know everything at tn. How do I get to tn plus 1? Okay, in order to proceed, and also actually in order to Later on carry out some analysis, we need to lay down some more notation. Okay? And here is what we will do. So some more notation. Okay? When I write d, at some time t n. What I implied by this notation is the. Solution vector d, if one were able to do exact integration in time. Okay. So this is the time exact solution, or the time continuous solutions. All right. At time at t equals t n. I am going to stick with the term time exactly. Time continuous gets, gets a little bit more, ambiguous. The time exact solution. Okay? And this simply means if we were able to integrate our ODE exactly. Right? This is our matrix of vector ODE. Right? If we were this with, with bound, with initial conditions. If we add a method to exactly integrate this, then the solution that we would get at a given time t n would be this, okay. In contrast, I will use d sub n, okay, to denote the, the algorithmic, solution at t n. Okay? So this is the algorithmic. Solution, okay? By which mean the solution that's obtained by applying some sort of discretized time integration algorithm based upon the discretization that we've just spoken of. Right? The algorithmic solution obtained. By the method to discrete, to integrate the discretized, the, the time discretized ODE, okay. All right, so we have this notation. What we need to do is actually first of all say what we mean by the time discretized ODE. And here's what we mean, okay? Now if I were to simply re write out our ODE in the time-exact form, here's what I, I, I would write it as m just as I have it up there, but let me, explicitly put in the time. Okay. Supposing we were looking at it time t n. Okay? This would be m d dot at time t n plus k d at time t n equals f which potentially could also depend upon time. All right. Now when go to a time discretized version of this ODE, what we will write here is the following. We will write this out in the canonical form m. Now instead of d dot t n, we will adopt the notation that, you know, that well if d is our solution, we can think of d dot as sort of the velocity of the solution, right. Just, just as a term, it's really just the rate of the solution, but, but we use the, the sort of canonical or the generalized velocity for a rate here. And so we would write this as v, okay? Indicating a velocity. Now, this is not a time-exact quantity. So we are going to denote it as v sub n, okay? Using this sort of idea. Okay? Plus k d n equals f at n. Okay. This is what we mean by saying that well here, v at n is an, a discretize approximation, It's a time discretized approximation again. Time discretized, discretized approximation of d dot right. D n is of course just as we've defined above. Okay, so this is what we call what I will refer to as the discretized version of the ODE, okay. So this is the time discretized Are the time discrete, ODE. All right? Okay."
1lT0ssVpNzs,"Now, in order to proceed from here, let me actually write this time discrete ODE. In ord, for, for, for, for purposes of argument let me write it at time, tn plus one. Right, so this of course is the time discretize ODE at tn. Right, because everything is at n. Okay. Let me know write this at time tn plus one. Right? So, at T n plus one, that becomes in canonical form, it becomes N V and N plus one plus K, d at n plus one equals F at n plus 1. All right. All I've done is, is write out the ODE at two different. Times okay two different times and as well. All right so this is generally what we would call the the time discrete version of the ODE. And, and when we write it in terms of the philosophy it's often sort of canonical All right now, this is where we need to invoke special integration methods, right? Our integration methods are based upon what we call the same inte- it's a family of integration algorithms. Okay? That are sometimes called the Euler family for first-order ODE's. Okay. And here is how they are posed. Supposing we're considering the following ODE. Suppose we're looking at, y dot equals f of y. Okay? This is the, algorithm we're looking at. So, what we will see here is that the time discretized version of this or the ARD or the, the, the, the solution as post by the Euler family of algorithms is the following. All right, the algorithm then is Is the following. It is that y n plus 1 minus y n over delta t, okay? Is equal to f at y n plus alpha. All right? This is the algorithm as defined by the Euler family. I'll have some explanation to do here of course right? So what we have is the following. Just the algorithm where, delta t is simply the what is called the time stamp. It's d n plus 1 minus t n. Okay? And this is the time stamp Okay? Alpha is a real number. And alpha belongs to the closed interval 0 to 1. All right, okay. So. We have the setting, and, since we're talking about the Euler family for the very first time let me, so to speak, introduce you to the members of the Euler family. So, we have the following sort of setting. Four alpha equals 0. Right, this is called the forward Euler method. Okay alpha equals 1, is called backward Euler. And alpha equals one-half is called various things. I will tend to call it the Midpoint Rule. Okay? It's also sometimes called the Crank-Nicolson method. Okay, so those are special members of the Euler family. Of course as as indicated back here you can one can design algorithms. One can develop algorithms and use them for any value of between zero and one. Okay, so, one one final step I want to take in this segment is to note that the way this can be sometimes rewritten is as simply. All right, this, this think can, is sometimes rewritten as y n plus 1 equals y n. Plus delta t, times f at y n plus alpha. Okay? Now, in order to complete this description I need to tell you what y n plus alpha is. Okay? Y n + alpha is constructed simply as a linear interpolation between values at alpha equals zero and alpha equals one. Okay? And that tells you that, that you construct it as Y N plus 1, right? Let me see, alpha times y n+1 plus 1 minus alpha times y n. Okay? All right. So really so when you sit back and look at these Euler family of At the Euler family of algorithms, what it is doing is to approximate our time derivative, right so we approximate the time derivative as a linear, as a linearly varying quantity over every time interval. Right? Why don't, we'll just sayinh well,let me just take a linear, approximation of it over the sub, over the time stamp of the time derivative. This is our approximation for the time derivative, right? And by saying that we are requiring it to be equal f at n plus alpha, this is where the design of our algorithm also comes in over the various possibilities in the Euler family because if we are trying to go from tn to tn plus one. Right? We're seeing that the rate in the average rate across the entity of tn and tn plus one depends upon f and we could, we are free to evaluate f or we're leaving ourselves the flexibility to evaluate f at any value inside here. Right, so at, at any point inside this. So that point is a gen, is generically n plus alpha, t at n plus alpha. Okay. By choosing different values of alpha, including alpha equals 0, or alpha equals 1, or alpha equals one-half, or, or really anything else, we get particular properties for our method. Right? So, so this is where the approximation of the time derivative comes in. We have this side of it, which is the linear approximation, and here we are approximating the ODE by evaluating the right hand side at some T equals alpha. Okay? So this is the basis of the Euler method and we are going to apply it to our time discretized ODE when we return in the next segment."
Eyh2ws5uZO0,"Welcome back. So at the end of the last segment, we wrote out our ODE in its discretized form. And we've also introduced the the Euler family. All right, we've met the Euler family, so to speak. All right. What you're going to do now is go ahead and apply it to our ODE. And the approach we are going to take is one of revisiting this discretized form of our ODE, okay? So go back to our time-discretized ODE. All right. We write it out as M v at n plus 1 plus K d at n plus 1 equals F at n plus one, right? Where what we can say now is that we can invoke our Euler family as applied to our definition of v, okay? So what this means is that we're saying here that d at n plus one equals d at n plus delta t v at n plus alpha, okay? Where v at n plus alpha equals v n plus 1, multiplied by alpha, plus 1 minus alpha times v at n, okay? And in writing out these two equations you can see that we have essentially applied this idea of the Euler family in there, right? Okay? All of this, of course, to complete our specification of an, of even our time-discretized form of the ODE. What we are saying is that of course with all this, we are given the initial condition, all right? And that initial condition is d 0. Okay? We know what we mean by d 0. Right, in the previous segment we explicitly said what d 0 is. All right? So now with you know, in general you, you really see this idea of time stepping. The idea is that given things at d n, right? Given quantities at t n, sorry. Given quantities d n and v n at t n this is a scheme for going from n to n plus 1. Okay. And it utilizes, it uses the, the initial condition here. All right. Now, there are two sort of canonical approaches to solving this time-discrete, discretized ODE. And we'll, we'll look at both of them in turn. The first is what is called the v-Method. 'Kay? Somewhat unimaginatively. All right. Here is the v-Method. In order to app, apply the v-Method and actually to apply the other method as well, we go back and look at this. Form of the problem and look at these two equations and do the following. Okay? We first note that. Then we can rewrite now d n plus 1 as d n plus delta t, times v n plus alpha but v n plus alpha is also written up for us here. So we'll write that as alpha v n plus 1, plus 1 minus alpha, v n. Okay, all right, so that is the first step, combining the two sort of the equations that define the Euler family. Now, in the next step what we're going to do is to rewrite this as rewrite the right-hand side as d n plus 1 minus alpha delta t. V n, plus alpha delta t v n plus one. Okay. Now what we've done here, if you observe, is rewrite d n plus 1, in terms of quantities that depend only on quantities at n. Okay? So let's denote this as d tilde at n plus 1. Okay? Now, we are getting into the realm of what are called predictor-corrector methods. 'Kay? And the idea is that if we want to calculate d n plus 1, right? We have this formula for it. We predict this, right? We say that well this is our guess for what it is, okay? And in general that will not be correct and we need to correct it with this term which is the corrector. Okay? So, written differently, d n plus 1, actually not so differently, d n plus 1 is equal to the predictor d n first 1 tilde plus the corrector alpha, delta t v n plus 1. Okay, so this is a predictor-corrector method. Okay. This v-Method that I'm talking about is one where what we do is return to our time-discretized ODE. Right, which is this one, Mvn plus 1 plus Kdn plus 1 equals F at n plus 1, right? We return to this equation, and we try to rewrite it only in terms of v, okay, and the way we can do that is to essentially use for dn plus 1 here, this predictor-corrector form. Okay, so substitute. Predictor-corrector form for dn plus 1, all right? What that does then is leave us with M v at n plus 1 plus Kdn plus 1 tilde plus alpha delta t Kv at n plus 1, okay? All of this equals F at n plus 1, all right? Now, I said that the reason we call this the v method is because we're going to try and rewrite our discretized ODE entirely in terms of v. Right, entirely in terms of vn plus 1. So that is indeed what we do, okay? So we get here this implies M plus alpha delta tK vn plus 1 equals Fn plus 1 minus Kd tilde at n plus 1, okay? We solve this equation for v, for vn plus 1 and we now have the corrector, right? Because then we can add vn plus 1 from here, up there, and we get the corrected state, all right? Right, so, so basically we solve. It's this inverse acting on Fn plus 1, minus Kd tilde n plus 1, okay? Right, now I'm going to make a couple of remarks. Okay, and those remarks are the following, right? Now, the first remark is that if, instead of M, we used the, the lumped-mass, okay, and, furthermore, we use alpha equals 0, okay, this will be forward Euler. Okay? 'Kay, have this particular combination, and what we see is that for vn plus 1, which is lumped-mass inverse Fn plus 1 minus Kd tilde n plus 1, okay? Because the lumped-mass is a diagonal matrix, its inverse is trivial, right? So as a result we don't really solve equations here, right? We don't solve, we don't have to solve with a, a, a linear system of equations by, by somehow effectively generating that inverse, okay? So this is what we call an explicit method. Okay? In contrast we have if alpha is not equal to 0, okay, all right, then we have in that case even if our mass is lumped, we still have equations to solve, okay? So this, this now an implicit method. All right? Okay, so these are two, two remarks to make and what we will do is move on and proceed directly to the other canonical solution method, which is the d-method. Okay? And the d-method once again, we could look at our predictor-corrector form. But now, we sort of turn this around to eliminate v from our discretized ODE, all right? What that implies then is we write here we get vn plus 1 equals dn plus 1 minus dn plus 1, tilde, okay, and divided by alpha delta t, okay? This of course, holds only if alpha is not equal to 0, okay? All right and in this case we get v from here and then we substitute it back into our discretized form of the ODE, okay? And. Okay? So, when we substitute in our discretized form of the ODE, M will be n plus 1, we had dn plus 1 minus dn plus 1, tilde, divided by alpha delta t, right? That is our approximation for vn plus 1. Well that's our expression for vn plus 1 using the Euler family. And here we have Kdn plus 1 equals F at n plus 1. And this again we reorganize by simply multiplying through by alpha delta t, okay? So we get M plus alpha delta tK. D at n plus 1, equals alpha delta t. F at n plus 1 minus Md tilde at n plus 1, okay? Now in this form of course you realize if we can get to the final form it doesn't matter if alpha is equal to 0. Therefore this works ev, even though I, I, I wrote that in, in, in defining vn plus 1, right? Alpha even though I wrote that alpha is set equal that, alpha cannot be equal to 0, right? We still get back our we will still able to actually solve the equation and move on, okay? So this would be the v-form and now what we get here is dn plus 1 equals M plus alpha delta tK, dn plus 1, sorry. Inverse here times alpha delta t Fn plus 1 minus Md tilde, n plus 1, okay? For this sort of method if we use a lumped-mass, right, we use a lumped-mass here. What is seen is that we require less effort to set up the right-hand side, okay? All right, if we use a lumped-mass here, there are fewer operations, not less, less that limit that fewer operations. Fewer operations to form the right-hand side. Okay? But the actual equation solving effort in the two methods, the v-method and the d-method is the same, all right? So here, here we have it our you know, this, the, these are the two methods by which we can set up the the, the solution of this problem you observe that you're, you're still solving exactly the same, sol, the same serof, the same set of equations so the methods are, are, are, are really equivalent. It's just a matter of which, which variable you attempt to eliminate first, okay?  right, so what we can begin to do now is to proceed to the to actual solution and well, sorry, we, we've already looked at the, at the solution techniques. What we can do now is to, is to set ourselves up to carry out an analysis of these methods, okay? And that may perhaps be done best in a separate segment. So when we return, we'll take up the analysis."
nL6u-s460qM,"In this segment, we'll start looking at the coding template for homework five. So let's turn and look at the code now. Again, homework five is the transient heat conduction problem. And you'll notice some differences here in the main.cc file. One is that I've created this constant alpha, which is an input for the constructor for our FEM problem object. Alpha is to specify what Euler method we're using. Again, alpha can range anywhere from zero to one. For the homework, you'll be doing it for alpha equals zero, one half, and one. But, other than that, the mesh generation is very similar, and setup_system and assemble_system. However, now we also have to solve functions. We have solve_steady, which solves our steady state problem, and then solve_trans which, obviously, is the transient solution. In your header file you'll have a function that calculates the L2 norm of the difference between the steady state solution and the transient solution at a given time. And so I'm also outputting to the screen here, some of those results of the L2 norms, there will be four L2 norms, It will calculate the L2 norm at time equal 0, 1,000, 2,000, and 3,000. Those are some of the results that we'll be using in the grading. Let's shift over to the header file now. Just as in homework four, I've defined order and quadRule as global variables at the top here. Again, when you turn in your assignment, use order equals one and quadRule equals two. But for your own edification, you can  change that order and quadRule if you'd like. The L2, again, will take care of any any order our quadrature rule. We'll scroll down and look at the declaration of functions and objects. The first four solution steps are the same, but then we have this solve_steady and we have apply_initial_conditions, since it is a transient problem or includes a transient problem, we have to include initial conditions. So you'll be defining those within this function. That function is called within the solve_trans function itself. I now have two output results functions, one for the steady state result and one for the transient results. Notice for the output_trans_results, I have as an input an index. So that's just to distinguish between the output results for, say time equal zero and time 1,000, it just puts an integer onto the file name, so that it doesn't overwrite the solution file. And again now we have another L2 norm function again, so if you've missed that from homework two, it's back. You'll notice here we only have the quadrature formula for volume integral; since we don't have any Neumann conditions, we aren't doing any surface integrals on this problem. You'll notice I have more matrices and vectors here than before. We have capitol M, which is our global mass matrix. There is again K and then we have a generic system matrix. That's because when you're doing the transient problem, you have to do a couple of matrix inversions to solve, for example, the VM plus one. And so, system matrix is a generic matrix that you'll be defining later on, in the solve_trans function. We also have D_steady, which is the steady state temperature profile. D_trans, which would be the transient temperature profile at Tn or Tn plus one. V_trans would also again be similar. It's the transient time derivative of the temperature at each node. Either for time Tn or Tn + 1, depending on what part of solve_trans you're in. And then again our vector F and RHS. RHS is a generic right hand side vector, similar to system matrix that you'll be populating within solve_trans to solve for your Vn + 1 vector. Okay? We also have this standard vector of doubles, called L2norm_results. This is going to store the L2norm at various time steps. And then alpha. Alpha's just the input, from our constructor, ok? Specifying again what Euler method we're using. So if we scroll down to the constructor, again you can see we're inputting alpha and storing that within our class and very well alpha And if we scroll down to generate_mesh, this again is exactly the same as previous assignments where again, you just need to define the limits of your domain. Define boundary conditions, would be very similar to previous assignments. Very similar to homework three, which was your previous heat conduction, your study state heat conduction problem, except now you'll notice that I have two maps here. Boundary_values of D and boundary_values of V. I actually skipped over those higher up. They are class variables, class objects. And since we have these two fields that we're dealing with, we're dealing with the temperature distribution and we're dealing with the field of the time derivative of the temperature. And so we have boundary condition on both of those. The boundary conditions on D are 300 at one end and 310 at the other. They are fixed temperatures, so what does that tell you about the boundary conditions on V? Which is the time derivative of the temperature. Tells you, of course, that the derivative of D would be zero at those boundaries, where temperature is fixed, okay? So you'll be doing a similar loop to check where your node is to see if it is at an Dirichlet boundary. If it is at a boundary, you'll put that information into these two maps, using the globalNodeIndex, which is the same as degree of freedom index for this scalar problem. And you'll passing the prescribed temperature value and the temperature time derivative, which again we said would be zero. When you have, at fixed temperature, alright? So that will take care of your boundary conditions. Again set up system is very similar to previous problems, there's nothing for you to change here. Let's look at a symbol system very quickly, before we end this segment. This is going to be sort of a mixture of the previous two homework assignments, your heat conduction problem and your, and using FEvalues in the linear elasticity problem. Again, we have FEvalues here, we're going to update the values, update the gradients, and the Jacobean determinative of the Jacobean times the quad digital weight values. I set up this variable row and that's the specific heat per unit volume. It's a constant that we'll be giving you in your homework assignment, that you'll use to define M local, and then to get your global mass matrix. All right so let's move inside the element loop. And first we're going to be populating M local. Let me write out that integral for you here on the board. MlocalAB is equal to the integral over the element's domain, of row, which again is your specific heat pre-unit volume. Now, potentially that could vary across the domain. For this homework problem we're keeping it as a constant, so I can pull that out of the integral. But then, it's simply the basis function BA times the basis function B. Integrated. Okay, so this is fairly straight forward, and from your experience doing Klocal and Flocal this should be pretty straight forward for you. Pretty simple. Okay, so you'll edit that there to find that here. We scroll down and we have to create Klocal. This will be the same Klocal as you've created in homework three. The only difference is you don't have to worry about the inverse Jacobian since, using deal two's basis function gradients. We already have the gradient with respect to the real domain. So that simplifies our quadrature loops a little bit. All right so we have looping over AB. The quadrature points, which again all three quadrature loops are grouped into a single quadrature loop with. Then we do i and j to perform the multiplication with 0ur conductivity tensor, kappa. Alright? We don't have a foreseen function in this problem, so Flocal will be zero, and so we don't even bother in this assembly to assemble F, our global vector. But you will be assembling K and M very similar to previous assignments. Right. The solve_steady function acts very similar as the solve functions in previous assignments. The small difference that I've done, is I've Instead of applying boundary values, instead of applying our boundary conditions within the assembled system function, I've shifted it down to solve_steady. Okay. And within this function, we'll solve for the steady state solution, and we store that result within the vector D_steady. Okay, so that's nothing new. Up to this point we haven't solved for anything more than what you already solved for in Homework 3. Which is just the steady state heat conduction problem. In the next segment we'll move on to looking at solving the transient solution as the time progresses"
iM-Ht1x7e2M,"Now in this segment, we'll move on to looking at solving the transient heat conduction problem in the hallmark five template. If we look over here, now that we're solving the transient problem, we have to plan initial conditions. And I've actually set up most of this for you. What we need to do is we loop through all of our nodes, and we check the position of each node. And, depending on the position of the node we'll define the initial temperature at that node. Pretty straightforward. You'll just need to insert those values, in some cases it actually depends on the x value of the note itself, as a function okay? All that information is given in the homework, you just need to calculate the value. Now that takes care of d naught. Storing the values of the initial values of the temperatures in D_trans. But now we will also need the initial values for v. We need v naught. And so to do that, we will solve a matrix vector system, okay. Now remember, this problem is based on this equation. We have M times V plus K times D is equal to F. F in this problem is of course zero, but in general, it wouldn't and it could be changing with respect to time. Actually, all of these objects could be changing with respect to time, potentially. So, we can also put the initial values in this equation, and if we need to solve for V naught, well first we move KD over to the right hand side This becomes our right hand side vector. And then simply do a matrix inversion to solve for v not. Now in this problem the mass matrix and the stiffness matrix don't change with respect to time, which is why I can get away with calling the symbol system only once. Okay, so let's look in the code at how I'm doing this matrix inversion, and how I'm doing these matrix vector operations. Okay, I've done all of these for you, because you'll be doing several of these on your own within the solve trans function. Okay, so first, I set RHS equal to K times D trans. And to do that I used this dot vmult function. Now with sparse matrices it will do a matrix vector multiplication. I pass in RHS and D trans. And it's acting on my sparse matrix K. But what it does is it stores within RHS the values of K times D_trans, okay? In the next step, I just take RHS times equals minus 1. Essentially I'm just changing the sign on all the components in my RHS vector. Okay, so this point RHS is equal to minus K times D trans or minus K times D naught. And then finally, even though F is equal to 0 and in this case I've included it for generality, I do RHS add 1 times F When is the scalar coefficient to F. And so that takes RHS and adds on to it one times F, which is, now, gives us a right-hand side vector of F minus K times D trans, or F minus K times D naught. Okay, so now my right hand side vector is set up. In this case, I'm only doing a matrix inversion on M, so I don't have to do any matrix operations there. But I did, up here, copy M into the system matrix, okay? So that's the same as saying system matrix equals M. Now that all these data objects are set up, I can apply my boundary values. Now note that, since I'm solving for V trans in this case, I will apply boundary values of V. The matrix that I'm inverting is system matrix, the solution is v trans through an outside vector is RHS. I create this sparse direct umf pack A, which copies over from system matrix. And then I do my matrix inversion. Now it may be a little confusing. I do A dot vmult here, but you notice it's doing a matrix inversion and then multiplying it by the right hand side vector. It's just the way the functions were named and so you just need to remember that with the sparse direct GMF pack, sparse matrix dot vmult does the matrix inversion. However, when you're dealing with a sparse matrix and a vector dot v mult, just as a simple matrix vector multiplication without any matrix inversion. Okay? All right, so with that background, let's scroll down to the solve trans function. Okay so the first thing we do in solve trans, is apply the initial conditions. So that's the function that we are just looking at. And the next is to define delta_t. I have delta_t set to equal one and that will give you a stable algorithm for all alphas, alphas = 0,1/2, and 1. You're free to change that on your own however, when you turn in your assignment, leave it at delta t = 1, okay. I declared this vector D_tilde, which you remember we use as we're updating. From DN to DN+1. And then we go into our time loop. Now I'm going to point out here that we are using the V method here. Okay. And so we'll look over those equations as we go through this code. All right, so the first step is to find detailed. Let me write that out for us here. So detailed, n plus 1, again this is a global vector, is equal to d at time n plus delta t times 1 minus alpha v n, right. These are equations taken from the lectures so you can look it up there. So this is step one. Step two is to use d n plus one tilde to find v n plus one. And we have this system maybe, equations here. We have M plus alpha delta tK, and I'm not going to re-derive these during the lectures, but here's the result. Sorry, we haven't quite inverted that yet. going to delete that. Okay, so M + alpha delta t K times vn + 1 is equal to Fn + 1, which in our case is just 0. Minus k times d tilde n plus 1. Which of course you would then to solve for vn plus 1 you would do a matrix inversion. So we take m plus alpha delta tk, inverse times fn plus 1 minus k. Detailed m + 1. This will be referring to as our system matrix in the code. And this we'll set up as the right hand side, the RHS factor. Okay, once we have the n plus one, step three is to find d n plus one, which is d tilde n plus one, plus alpha delta t, v n plus one. Okay? Note in the code dn at this point is d trans, vn is also v trans. Here we update v trans to be Vn+1, and at this step we're updating d trans Sorry that's a capital V in our code. Updating D-trans to be dn plus 1. Okay. So we'll be following these three steps in the code itself. Okay, so let's go back to the code. You'll notice I've given a list here. Of how to perform these matrix vector operations. A lot of these we've already seen above in the apply initial conditions function. You can use this same .add function with the system matrix, like we did with the vector. Again dot v mult will do a matrix factor multiplication. If you want to set m equal to or system matrix equal to a vector or to a matrix excuse me, you would use copy from. However with a vector you can just say RHS equals f for example. We already saw this. I'm operator times equals with a vector to change the sign. Okay, so that should be enough for you to be able to do all of these matrix vector operations. Okay, so the first step again was to find D tilde. And again, at this point D trans it does hold the values of D so then nv trans holds the value of V sub N. Okay so once you found D trans you will then create system matrix, as we wrote on the board and RHS. Again as we wrote on the board. Once you've done that, we apply boundary values of V, again because V trans or VM + 1 is the solution vector we're solving for. So we apply boundary values of V using the system matrix that you've defined and the RHS factor. Okay, and that matrix inversion happens here with these three lines. Again using the sparse strict plume of pact matrix, A. Okay, once you've done that matrix inversion and V trans is now equal to V n+1, you can use D tilde and V m+1, V trans, to solve for D m+1. Okay and there you have the solution for this time step. The last thing to do is to output results. Now I have it set to output results every 1,000 time steps. You'll notice I'm using this modular operator. So every time the current time step index, divided by a 1,000 gives a remainder of 0. Then we'll output the transient results as a VTK file. And also store at the L2 norm of the results in this L2 norm results. Or, sorry, I'll calculate the L2 norm of the difference between the transient solution at this current time step and the steady state solution. I'll store that within this vector here, okay? Alright. So the last step, after outputting results, or the last function I should say, after the output results functions, is to calculate the L2 norm. This is going to be very similar to the L2 norm that you calculated in Homework Two, except of course you can now use dl2 basis functions. You will still need to interpolate to find U steady and U transient in exactly the same way you interpolated U H, you will find that element solution in the torched point. And you will construct the intergrand of you know, the square of u-trans minus U study. Okay, and then we we return the square route of that value to actually calculate the l2 norm itself. Alright, so that should be pretty straight forward with what you've seen before, and that concludes our discussion of the template for homework five."
NLQd_oA5qqY,"Welcome back. We are going to take the first steps towards analyzing our ODE as written out using the Euler family. Okay, and in order to carry out this analysis there are couple of things that we have to do. The first is the following, let us go back to our discretized ODE and write it out in a slightly different form. Okay, the forms are completely equivalent to what we've seen before, the v and the d form. But, but, for, for the purpose of analysis, let's do the following. Okay. So, the, the title of this segment is analysis. Okay? And in particular, we are going to look at what I will call model decomposition. Right. In order to get there, let's do the formula. Let's write out the equation to these formulas, right? We're going to write it out as M. Okay? We are going to directly write a time discretized approximation, a finite difference approximation if you will, of our time exact time derivative, right? So, we're going to write d dot here as approximated as d n plus 1 minus d at n divided by delta t, okay? So that's our approximation of M d dot. Now the, where the Euler family comes in is when we say that everything else is evaluated at n plus alpha. All right? This is how we want to view it. Now, one more thing. We are interested in understanding what the basic properties of our integration algorithms are, what the basic properties are, and as often happens in the study of ODEs, we will take advantage of what is sometimes called a homogeneous form of the ODE, which is obtained by, by, by setting the forcing equal to 0. Okay so, so what we need to analyze here is we want to analyze the stability, stability and what we will define as the consistency of the time integration algorithms. Right? We will define what we mean by their stability and consistency. But in order to do this and especially with things like the notion of stability it makes sense to actually set the forcing equal to 0. Okay? Think about why this may be. Why may it make sense to set the forcing equal to 0 if we want to start out by looking at stability? Okay? Now, it's, it's simply because well, you know, how a solution evolves in time can depend upon the forcing, right? You, you give it a forcing which makes it keep growing in time, well it will keep growing. Right? So we want to get that out of the out of the picture. Right? And instead we want to understand you know, the rigorous way to pose it is to just ask the question of, ask the question that if there were no forcing, how does my time discretization and my, my introduction of this, of this Euler, Euler family of algorithms affect the evolution of the problem, okay? So we, what we will look at is the, we will consider the homogeneous ODE. Okay? Now, the time exact version of the homogeneous ODE that we are working with is this. M d dot plus K d equals 0, okay? With the initial condition d at 0 equals d naught, right? And the time discretized version of it is what we've written up here M d n plus 1 minus d n divided by delta t plus K d n plus alpha, okay, equals 0, with the 0 being given. Okay. So the first of those equations that I just wrote is the time exact homogeneous ODE and the next one is the time discretized version using the Euler family. Okay one thing you will recognize here is that in all of this what we're seeing is that dn plus alpha equals alpha and dn plus 1 minus alpha. Sorry, alpha dn plus 1 plus 1 minus alpha dn. Okay? That's what we have. All right. Now, in order to proceed, we are going to do the following. We will, we're first going to study the question of, we're, we're first going to look at stability. But even before that what we want to do is carry out what we will call a modal decomposition of the problem. All right. In order to carry out a modal decomposition, we are going to take a step which is to invoke a related eigenvalue problem. Okay. We will invoke the related, generalized eigenvalue problem. Okay, and the generalized eigenvalue problem we want to invoke is the following. I'm going to write it as M for a vector here. Let me not use d but let me use phi, okay? The, right, so the problem we want to look at is M phi equals lambda K phi. Okay. Sorry, let me turn that around. Let me write it as, sorry, let me write it as this. Let me write it as lambda M phi equals K phi. Okay, this is the generalized eigenvalue problem. And, and perhaps you know, in a very obvious step, let me just change the left hand side and the right hand side. So we have K phi equals lambda M phi. Okay? All right. Where, so when you look at it in this form, it if you haven't seen a generalized eigenvalue problem in the context of linear algebra before. You will, you have seen a standard eigenvalue problem probably where a standard eigenvalue problem is obtained by just setting N equal to the to what you would call the identity matrix in the corresponding dimension. Okay, so let me just state this. So a re, remark is the standard eigenvalue problem. Is, a standard eigenvalue problem would be of the form K phi equals lambda phi. Okay? And all of this of course, phi belongs to the Euclidean space or real space with number of, of dimension ndf. Okay so, what I've written down here will be a standard eigenvalue problem which underlies eigenvalue problem is where instead of having the the corresponding identity matrix here, right? We have sorry, that's the isotropic denser. Instead of having the, the identity matrix here, right, we would have the we have some other matrix here. Right, and in our case it's a matrix n which we know to be symmetric and positive definitely. All right? Okay, so this is the generalized eigenvalue problem we want to consider, right? So it is this one that we will be working with. Okay? Now what one can do from here is the following, okay? What one can see is that for that problem let say, phi sub m, right? Where m equals 1 to n d f. Okay? Let these be the eigenvectors. Okay, and let lambda m be the corresponding eigenvalue. Okay. All right so, so lambda m is the corresponding eigenvalue. Now what one can do is one can show that it's possible to con, to construct a so-called orthonormalization of the of the eigenvectors, okay? So, the eigen-vectors and eigenvalues satisfy the following equation. The eigenvectors and eigenvalues of course satisfy K phi m equals lambda m times M phi m. Right? Little m going from 1 to number of degrees of freedom. All right? Okay. So given this, it is possible to construct a, a, it's, it's possible to choose the phi n such that they satisfy a certain orthonormalize a, a, a certain property of orthonormality, okay? In particular the property for orthonormality that they satisfy is the following. The phi m, right, the set of eigenvectors, m equals 1 to ndf, right? This set of eigenvectors can be orthonormalized. Okay? They can be orthonormalized to a different set, psi sub m, okay? M again equal to 1 to ndf. Okay, they can be orthonormalized to a set psi m which are such that they are, this orthonormal, this orthonormality property is an orthonormality with respect to m. Okay, what that means is the following. If we take psi m and dot it with M mass matrix, psi k. All right. This product is delta m k. Okay? Where delta is our Kronecker delta, all right? Okay? So this is the Kronecker delta. Okay, and furthermore the set psi m. Right? That set of eigenvectors. I mean, I see they're orthonormalized. What I can further see is that they are M orthonormalized. Right, it's simply, it can be interpreted as being orthonormal with respect to M. Okay?"
DhhryQHWoLI,"The type of autonormalization we need here it can be constructed by what is called the GramSchmidt process, okay? It can be constructed by the GramSchmidt method I guess. Okay? And what this does is it lets us start out with a linearly independent set of eigenvectors which would, which maybe the ones that you originally solved for by solving that generalized eigenvalue problem. Okay? So this is a linearly independent set. Okay? Why do we know that the eigenvectors that we get by solving that generalized eigenvalue problem are going to be linearly independent? That's right. It's because k and m are both positive definite. Okay, that's what guarantees the existence of linearly independent eigenvectors. All right. In some situations, we may have repeated eigenvalues, in which case the eigenvectors are not uniquely defined, but nonetheless they can be picked to be linearly independent. Right? Okay, so so we, we are assured of linearly independent eigenvectors phi and the GramSchmidt process, GS is short for GramSchmidt essentially takes us to a si sub m. Right, which are now M orthonormal as we have defined. This is a completely standard process procedure that is available in many books on linear algebra or, or sometimes even books on partial differential equations because the method does exist to, does extend even to continues eigen functions. All right, so, so we have that. Well, what does that then say for our, for, for, for our problem, okay? Note that, what this tells us is the following. Right? So now, because we have the set psi M we can construct the following sort of the product right? Supposing we take M times, M acting on psi K and we dot it from the left with psi M. Okay. All right. Now. Okay. And then we also have a lambda k here. Okay. The reason we have a lambda k, is because I started out by writing M psi K. Okay, so what we get on the right hand side is psI M dot K psI K. All right. Okay, when we get, when we look at this sort of product now what we observe is that because we have this M orthonormal property for our autonormalized set of eigenvectors. We have here lambda K, delta mk. Right, on the left hand side. Okay? And here on the right hand side we have now a result for forming the sort of quadratic product between psi M, K and Psi K. Okay, so alternately what this implies for us is that if we form just as we form for psi M a product, if we do the same thing for, if we do the same thing with by, by using K in it, right. Right, this is equal to lambda m, okay? We get lambda M because of the fact that we have this Kronecker delta product acting here. All right. So we have this additional property. Okay? All right. Okay. So, with this background in hand to let us now move on to trying to understand how we analyze this how we apply it to analyze our problem. Okay? In order to proceed there, let us first use the, another property that is given to us by orthonormalization. Now, because we started out with a set of linearly independent vectors phi m, okay, and we proceeded to orthonormalize them. We also know that this set is linearly independent, okay, therefore it spans the space that we are working in Okay so, let's state that, all right now the set psi m, m equals 1 to Ndf forms a basis in R Ndf. Okay? What that means then that any vector, okay, say d, right? Any vector d can be written as sum over N running from one to Ndf of d sub m. Psi sub m, okay? We have this expansion of d in the basis. Okay, all right. Okay, and now if we ask, well how do we compute this, how do we compute such a, a representation? We can compute such a representation if we have the dm's. Okay? And in order to know what those dm's are, we observe that we can simply multiply this, equation from the left by our matrix m. Okay, so. To get the coefficients d sub m, okay, what we do is that we first full construct Md equals sum m equals 1 to Ndf. Dm, M, psi m. Okay? Just using linearity. All right, okay, now, once we have this we can now dot this vector equation on the left by psi k, right, our eigenvalue, okay, and we can do the same out here. Okay. Now, using the linearity of the dot product, what we get is the following. Okay? What we get on the right hand side is sum m equals 1, to Ndf. dm psi k dot M psi m. All right, but then what is this? This is just delta km. Where did that come from? From the fact that we've constructed the psi m's, that entire set of psi's to be m orthonormal, okay, but then this explicitly when we compute the sum knowing the delta km is the chronicle delta, we get d sub k. All right, so here we have a definition for what our coefficients are. These coefficients, dk, are uniquely defined, okay? And therefore this expansion, right, of d in terms of it's, of that basis is also unique. Okay. So, let me just write a few comments about this, about this decomposition, okay. So, when we say that d can be written as sum over m, going from 1 to Ndf. D sub m psi sub m. This is what we mean by the modal decomposition of d. Okay? Each psi m is what we will call a mode, right, so psi m is the mth mode. Okay, and then d sub m is the corresponding modal coefficient of d. Okay? And, because we have a, representation for d for, for the dm's or the dk's in general as we derived on the previous slide. This representation, okay, what is this representation? It is that the dm, let me get rid of this because it might look like a minus sign. The dm the dm's are defined as psi m dotted with M d. Okay? So we have a unique representation for the dm's. Okay right. We are going to end this segment here, thanks."
C6o1k5ZLDF4,"Welcome back. We have just embarked upon the analysis of our time integration algorithms for parabolic equations, right, as of, of, first starter in time. What we've accomplished along that direction in the last segment is a recognition that there exists this generalized eigenvalue problem that one can identify, which provides for us a basis on which to carry out a modal analysis of the problem. Okay, so let's just recall that aspect and charge ahead. Okay so we're engaged here in the analysis of our, of the time integration algorithms. All right for linear parabolic systems. Right and you will recall that this is based upon the generalized eigenvalue problem. Right, and that generalized eigenvalue problem takes on the form K, which is our stiffness matrix. Sorry, our conductivity or diffusivity matrix. Acting on a vector, that is psi M right? K psi M equals lambda M, M which is our mass matrix psi M, okay? This is the basis for it and we went ahead with this, right? What we found and I'm not going to go through the entire analysis I'm just going to recall the critical parts of it, is that one can orthonormalize the, the eigen vectors psi with respect to M. Correct, so we found that psi L dot M, psi M equals delta LM, chronicle delta, and, because of that, this result also lets us say that psi L dot K psi M equals lambda L okay? All right actually properly equals lambda M multiplied with delta LM. Okay, that's what it actually is. Okay, so it is zero, unless M equals L. Okay, the second equation, the right hand side of the second equation. Okay, and if L equals M, it's equal to psi L, which is the same as psi M. All right, and you recall that in this lambda M is our eigenvalue, right, corresponding to the eigen vector psi M. Okay, so this is orthonormalization. Okay. All right, so with this in hand, what we observed I think at the very end of the segment is that since these vectors psi form and actually span the space, they serve as a basis. Okay. So, what we have is an expansion In the psi m basis. Right, where m equals 1 up to ndf, okay? All right, and this letter say that a vector d, right? Such as the vector of global degrees of freedom that we're now working with can be expanded as sum over M d sub m psi sub m, okay? Right, and we saw that each of the d n's, the modal degrees of the, the, the modal coefficients each, right? Each d n is given by right it's equal to psi M dotted with M d. Right? This is everything that we need to know. Right and these are the modal coefficients. All right. What we are going to do now is with a summary of our analysis, of sorry, with the summary of our modal decomposition of the problem, right. Right, so this, this, this result is the modal decomposition. The modal decomposition of d. All right, so, with this, with these results, with this summary of results, we can go ahead, all right. And, what we are going to do now is, extend this idea of the modal decomposition to, the equations we are working with. All right. So, we're going to start here with a modal decomposition. Of. The time exact O D E. Right. Right, and remember everything that we were working with homogeneous equations, okay. So which equation is that? Think about it. It's this one. M d dot plus K d equals not the forcing term on the right, or well, the forcing term is equal to 0. All right we have this with this initial condition that d at times 0 equals the d 0 vector. Okay, so now we're going to do a modal decomposition of this equation itself. The way that works is the following. We already know the modal decomposition of d. Okay. That modal decomposition I'm going to write in slightly different notation, the notations not terribly different. I just want to move the m from being a subscript of the coefficient to a superscript. Okay, so that's not a power, that's a superscript. All right, and you'll see just in a bit what I want to do with the subscript. I need them in there, okay. This times psi. Let me move this also to the after the superscript. Right, the m there is also a superscript. Now remember that this d vector of ours is time dependent. Right, that's why we're able to take a time derivative. Well, we could always take a time derivative, but that's what makes sure that the time derivative isn't doesn't vanish, right. Okay, so d is a function of time. Okay, my question is where on the right hand side does that time dependence go? Does it go into the coefficient? Right, the d n's or the sines, the eigen vectors? Right, they go the time dependence shows up in the d n's. Okay, and more than just a convenience that it has to be that way. Why is it that the sines don't change with time? It's because the sines are, are the eigenvectors of the generalized eigenvalue problem. But the matrices that define the generalized eigenvalue problem, the K and the M matrices are fixed in time, okay, because we are doing a linear problem. Okay, so let's just state that, that's useful to remember, okay. So, this what, what, what's behind this particular decomposition, right, where the time go, where the time dependence is held in the, in the coefficients, right, is the fact that the set of psi m. Right? Are fixed in time. Because, our conductivity matrix K and mass matrix M are also fixed in time. Okay, that's what allows us to do this. Okay, what you see, what that also allows us to do, what that immediately lets us see is that. D dot, right? D dot is now sum over n, d dot n, function of time, psi. All right? Some of cos running from one to number of degrees of freedom. Okay? And we are going to make these, and you are going to substitute both these decompositions into our time exact O D E, right? So what this implies then is let me just say this as substituting. Okay? It implies now that M multiplying the sum over little m d dot m, psi m plus K sum over m d m, psi m equals 0. Okay what I'm going to do next is is apply linearity, right? So what this then apply, what this implies is that because M and L are matrices and because we're dealing with linear algebra there, what we can write here is that sum over M, d dot m mass matrix acting on psi m plus sum over m, d m, conductivity matrix acting on psi m equals 0. All right. Now I am going to dot this entire equation on the left, well it doesn't matter with left or right, I'm going to dot it because it's a vector equation finally. I'm going to dot it with a specific eigenvector lets say psi L, okay, so now I'm going to take psi L, okay? Dotted with the sum d m dot, psi m. Plus psi L dotted with the sum over m, d m, K psi m equals 0. Okay? Note that something has changed between those two equations, right? Something very obvious has changed. 0 here is no longer a 0 vector, it's a scalar 0. Right? Because I'm after all taking a dot product off a vector equation here, this was a vector equation, and I've taken the dot product of it with another vector of the same dimension right, psi L. Okay, but now we know how this all works out, right? Because we know exactly what happened with psi L oh I'm sorry I am missing here, sorr,y I'm missing an m here, so let me just clean this up. I'm missing here. That matrix okay, all right. Now its all right, okay. Well let's carry out these dot products right and and we know what we get from these."
kMwd1mVNq1I,"'Kay, so we know how this works out. We get in the, in the, for the first term, sum over m, dm, dot psi L dot M psi m, plus sum over m dm psi L dot K psi m, equals 0. All right, now this we know from the orthonormality condition is delta lm, okay? And this we know also from the fact that we have the generalized eigenvalue problem at hand. This turns out to be lambda m delta lm, okay? And then when we account for the fact that there is a sum over m in each case, the Kronecker delta does its job and leaves us with the following, okay? What it leaves us with is on the, for the first term, we get dl dot plus, from the second term we will get dl, well let me put the lambda in front, we get lambda l dl equals 0, okay? All right, and this came around, came about just by applying the properties of the Kronecker delta and using that sum, right, over m, okay? Now, we did this for an arbitrary mode, or an ar, an arbitrary eigenvector psi l, 'kay? So this holds for all, well, this holds for all l actually, where all l equals 1 to ndf, okay? All right, this is what we will call our single degree of freedom modal equation. Okay, produces to a really simple form. All right, okay so we have this and what I'm going to do is very systematically do essentially the same thing for our time discrete homogeneous ODE, okay? Okay let's first write out that ODE, right? We agreed that as far as analysis was concerned, we were going to work with it in the following form. Okay, this is the form that we're going to work. We're going to rewrite this by just expanding out the n plus alpha, and also multiplying through by delta t, 'kay? So in one step, I'm going to say that I have M dn plus 1 minus dn plus delta tK times alpha d at n plus 1, plus 1 minus alpha, d at n equals 0. Okay, just multiplying through by delta t and expanding our dn plus alpha, okay, so that in itself. Right? Okay, and let's go ahead from here and essentially introduce modal decompositions of dn plus 1 and dn, okay? All right, so the modal decompositions we are going to apply here are. D at n plus 1 equals sum over m, the modal coefficient, m, evaluated at n plus 1. Okay, this is the modal coefficient the nth modal coefficient of the d vector at n plus 1, right, the time tn plus 1, right? Psi m, okay, I don't need to really to write the same thing for n, because it's just replacing n plus 1 with n. Okay, right, so likewise with dn, 'kay? We do carry our those modal decompositions and then we are to substitute them back. All right, so now when we make we we go ahead and, and plug them into the last equation from the previous slide, we have M, okay let me do two things in one step here again if you don't mind. I am going to write, I'm going to combine the matrices multiplying dn plus 1 and the matrices multiplying dn, okay, in a single step, okay? So for dn plus 1, I have M plus alpha delta tK, right, multiplying dn plus 1, but dn plus 1, I'm going to write using this modal decomposition sum over m, d, right, that coefficient, the dm coefficient at n plus 1, right psi m, 'kay? And the thing's multiplying dn, I'm going to write as follows. Okay, I'm going to write this as minus M minus 1 minus alpha, delta tK, and hopefully I caught everything I need to, yes I did dn, but dn again I'm going to write using the modal decomposition. Right, all of this equals 0. Okay, so I've taken two steps here relative to the last equation that I wrote on the previous slide. I've combined the matrices that multiplied with dn plus 1 and dn, okay? The dn plus 1 and dn vectors as well as using the modal decomposition for dn plus 1 and dn, right? So this is And this is that modal decomposition, okay, that's what we have, right. And, you know what I'm going to do next, right? What I need to do is go from here, this is a vector equation of course, go from here to a scalar equation for a single degree of freedom, and we're going to do that by simply dotting this on the left, by dotting this with psi l, right, so I'm going to do psi l, dot it with all of this, 'kay? Likewise here, I have psi l, dotting everything, okay? And then using linearity and so on, what we get here is the following. We get sum over m, dm, n plus 1, multiply psi l dot m psi m, okay, all right, plus alpha delta t psi l dot K psi m, 'kay? All right, all of this minus sum over m, dnm, we open parenthesis here, psi l dot M psy n, sorry, M psy, M psy m minus 1 minus alpha delta t psi l dot K psi m, okay? All of this equals now, what is it, vector or scalar 0, scalar 0, okay? And then, everything works out just as expected. This thing gives us a delta lm. From here, we get a lambda m delta lm, okay? Here again, we get delta lm. And from here, we get lambda m delta lm, okay, all for orthonormality and from the eigenvalue problem, right? And then we account for the fact that we do indeed have a sum over m and what that lets us say then is that because of the action of the Kronecker delta we are left here with essentially the action the, the what the Kronecker delta does because of the sum over m, is that it effectively converts that index m into l, 'kay? So we have delta l n plus 1, okay, from the first term here, 'kay, we get 1 plus alpha delta t. Here again, because it's Kronecker delta acting he, acting on, on, multiplying the lambda m, we are left with alpha delta t lambda l, okay? Minus d at n for the l'th mode times 1 minus 1 minus alpha delta t lambda l. Again that lambda l comes from here in the action of the Kronecker delta, right, all of this equals 0, okay? This is our equivalent for the time discretized ODE of the single degree of freedom modal equation for the time exact ODE, okay? So this is the single degree of freedom modal equation, right, for the time discrete problem. All right? 'Kay, it is to be compared with the single degree of freedom modal equation that we obtained for the time exact problem, okay? All right, this is actually a great place to stop. When we return, we are going to analyze the two single degree of freedom modal equations. Oh, just one, one more thing. I should mention that this holds for all l equals 1 to ndf, 'kay? All right."
PGw-wgZgTn0,"All right we are ready to move on, and were ready to move on with the analysis of our single degree of freedom model equations. For both time exact as well as the time discrete problem. Ok? So, we are going to look at at these two model equations, right, single degree of freedom modal equations, okay. So, we have here our single degree of freedom. Model equations. Right, I've used an abbreviation for equations here. All right, we have for the time exact case. We have d dot l plus lambda l d l equals 0. Okay? Now an o d e is not complete without its without what? Initial conditions, right? So the initial condition that we have here is the following. Right? This is after all and o d e for our L coefficient to the model expansion. Right? So the initial condition that we have is simply the following. It is that dL at 0, okay, is obtained as. Psi L dotted with Md at 0. Okay? Which is of course psi L, dotted with Md not, okay? And just for convention let's call this, let's call this quantity, right? Let's call that quantity d0L. Okay? That's the initial condition. Okay? All right, time discrete. And time discrete case is the following. It is d L n plus 1, times 1 plus alpha delta t, lambda L, minus d l n, 1 minus 1 minus alpha delta t lambda L equals 0. Okay? And again, this holds how do we bring in the initial condition in this case? We just say that we're given that that quantity we had identified for the time exact problem. All right? Okay. Now, one thing I want you to note is that the way we write at thi, at this time discreet single degree of freedom model equation is by starting from the, from the right. From the time discrete matrix vector equation, and applying model analys, modal decomposition to it. Right? We could equivalently have gone the other way around, right? So, we could have equivalently taken the, the model single degree of freedom model equation for the time exact case, right, which is this one, and then time discretize this. Okay, if we do this, and it's a very s, very useful exercise, it's actually a very simple exercise, you would end up with this. Okay, so this is your mark, I'm just going to give it to, give you in the form of a, sort of flowchart. Okay? So, what we have done here is the following, right, we have at one end, the time exact. Matrix. Vector equation. Right? Where this is Md dot plus Kd equals zero. Okay and I'm going to put a box on it here. Here that is Md dot. Plus Kd equals 0. Okay? Now one way of looking at what we did was, we went from here down to our time discretize case, right. Right, and this is the following. M d at N plus 1, minus d at N. Okay. Divided by delta t. Alright. Plus K dn plus alpha. Right? Equals 0. Okay? So this is the time discrete. Matrix vector equation. Okay? What we did then, is two things. From Md dot plus Kd equals 0, we went to our single degree of freedom SDOF, for single degree of freedom, model equation. Alright, this is the time exact version. Alright. Alright, which is essentially Dl dot plus lambda L, dl equals 0 plus initial conditions, right? Everywhere we have initial conditions. Okay? And then what we also demonstrated is how we can go from here to our single degree of freedom model equation. Right, but this is the time discrete case. All right? Which takes on the form that we've written just above here d L n plus 1, 1 plus alpha delta d. Lambda L minus dn at L, 1 minus 1 minus alpha delta d. Lambda l equals 0. Okay? That's where we are. The one step we haven't taken and which is very easy to take actually is this one. You could start out with our single degree of freedom model equation, the exact form and essentially do a time time discretization of it. And indeed you do get this. Right? Okay. We've done everything but that last dashed the step cor, corresponding to that l, last dashed arrow. Okay its very obvious why this works do you know why this do you recognize why this works? That's right because the process of time discretization, as well as the process of model decomposition are essentially linear operations, okay. And, and therefore in this sort of setting they commute, right, those two operations commute. Okay. All right. Okay. It, it, it of course not all linear operations commute. But, but because of the nature of the sort of operations we're carrying out here, they do indeed commute. All right."
1CMy5_PKQG4,"Okay. So let's get on. So what, what we're aiming to do in this segment is get some sense of the stability of the equations that we need to look at. Now if we want to look at stability we have to first understand we must first understand the stability of the time exact case, all right? Because that is the, the sort of behavior, the sort of response we are aspiring towards for our system, right? For our algorithmic system. Okay? So, in terms of stability, let's first understand the time exact case. Right, now, we've derived the single degree of freedom, modal equations, for a partic, for an arbitrary mode L. Okay? Now, everything we do holds for every mode, right? The, the, the, our, our analysis holds for any mode, because we're really working for an arbitrary mode. With that in mind, I can afford, I believe, to drop the explicit, use of the modal index, L. Okay? All right, so I'm going to drop that, right? So, from now on, the time for, for the time exact case, since we're working a single degree of freedom, for the time exact case and for the time discrete case, when we are working with single degree of freedom modal equations, I'm just going to write them as d dot scalar plus lambda d equals 0. Okay? But one more thing, I've got rid of one index L, but I want to bring another one. The one I want to bring back is the, I'm going to use h, right? And it's not an index, it's a superscript. Why am I bringing h back here? Remember h is our old friend the element, size. Right? It denotes the element size. The fact of spacial discretization. Why am I bringing it back? Right. It's because our n and k matrices depend upon our discretization, our spatial discretization. Right? So the ei, eigenvalues we're working with here are truly the spatially discretized eigen, the, the eigenvalues corresponding to the spatially discretized system. Okay? So lemme, lemme just state that here. Lambda h is the eigenvalue of a mode, or corresponding to a mode, that was, that is obtained after spatial discretization. Okay, so the factor spacial discretization, which is which shows itself up, which, which shows up in the finite element size, is indeed reflected in lambda h. Okay? Alternately, because these are partial differential equations, one could do a fully continuous analysis of them, and then one would have a eigenvalue corresponding to modes, but those would not be discretized modes, okay? Those modes or those eigen those eigenmodes would be actually eigenfunctions, not eigenvectors. Okay? So there would be a, so there's a difference. We're really working with the eigenvalues of the, of the spatially discretized system here, and that will show up, it's, it's for that reason that I'm bringing back our memories of h here. Okay? All right, so the time exact case is this, plus of course boundary condition. Sorry, the initial condition, right? So this in fact, d(0) equals d not, right? On the previous slide we've used the modal index L, but we've just decided to drop it here. Okay? Without, without risk of confusion, because everything we do holds for every mode. All right, so what is the stability of the system? How do we, how do we know what the stability of the system is? This equation is one of the simpler ODEs you're likely to encounter. Okay? We can directly write down the exact solution. All right? So the exact solution is d as a function of t equals d sub 0 exponent of minus lambda h times t. Okay? It's easy enough to check that that is indeed the exact solution. If you plug it into your, equations, you will find, and into our ODE, you will find that it satisfies the ODE, and it also, respects the initial condition. Okay? Just set T equal to 0 on the right hand side, since exponent or minus 0 is 1. You get back d at 0 equals d not. Okay? So this is the exact solution. All right, now, what about lambda h? What do we know about lambda H? It's an eigenvalue of the system, right? What is lambda h? Lambda h turns out to be greater than or equal to 0. Okay? All right? Why is lambda h greater than or equal to 0? We're not going to prove it, but do you know what properties give us this? It is the fact that M is positive definite. Right? And K is usually positive definite, but in the most general case, if we, if we want to also allow for insulation along certain directions, if you're talking of heat conduction or the possibility that there is no transport along certain directions if you're talking of mass diffusion, then k is a positive semi definite Okay, earlier on we'd used the fact that K can pretty much be taken to be positive definite, unless you really want to have insulation. We'd use that fact to make the observation that arriving that, that we are going to get eigenvectors that, that are linearly independent. Okay? All right. So, we have this, we have this sort of situation. All right, if that is the case since lambda h is greater than or equal to 0, what can we say about d? Say tn plus 1, right? And we know what this means. It just means that we're evaluating the solution at time n plus 1. What can we say about d at tn plus 1 relative to d at tn? Right? And, and here I'm using the fact that because of the nature of our time discretization and our choice of the progression of time instance, tn plus 1 is greater than tn. Right? So, let me just recall that also. We are, of course, using here the fact that we are progressing in time, so tn plus 1 is greater than or equal to tn, right? It's usually, it's, it's greater than tn. We nev, we never use it equal to tn, because that would mean we have a 0 times. Okay? All right, so given this, what should I use in this blank here? I've left a big blank spot between d and tn plus one and d and tn. What relational operator do I use? Right. Because the exponent of a negative argument is less than one, right? What we see is that this is a decaying function. Right, it's monotonically decreasing. Okay? Or another way at looking at it is, that d at t n plus one divided by d at tn is lesser than or equal to one, provided of course d at tn is not 0. Okay, but, nevertheless it's monotonically decreasing. All right, so we have monotonically decreasing, sorry, time dependent coefficient for our mode. Okay? All right. And this essential says that the nature of our heat conduction equation or our, or nature of our, mass diffusion equation, the kind we are looking at here, is for the solution to tend to be K. Okay? There is no tendency for the solution to tend to increase, provided we have set the forcing equal to 0. All right? And it is on in order to expose this characteristic of the equations we are working with that we are considering the homogeneous case. Because clearly, you could be supplying heat to increase the to, to, to raise the temperature, or you could be, you could have a local supply of mass, or, or, or, again, influx of heat or mass in order to push up the temperature or the mass concentration at, at any point. And therefore, these modes could be increasing in a problem that is in homogeneous. Okay? But we wanted to get to this fundamental characteristic of the equation, so we just assume the homo, we, we are considering the homogeneous case. All right, so why are we doing this? The reason we are doing this is because we want to understand what is the exact behavior that our algorithmic equations should aim to represent? Okay? All right. So, with this in hand, let's ask, ask ourselves what, what the same sort of analysis tells us for our time discrete equation. And this is really quite easy, because the way we've written out the time discrete equation, it is, it is in algebraic form. It just gives us dn plus 1 multiplied by co, by some factor minus dn multiplied by some other factor, okay? So the time discrete equation, if you go back and look at it in your notes, and I have it right up here in my slides, so let me just flash it up again, it is in the middle of this slide, right? Marked out  as time discrete equation, time discrete case. Okay? So I'm going to rewrite it using the same notation that we are now following, which is to drop the modal index L, and instead for lambda, bring back the superscript h. Okay? So the time discrete case we'll use in this notation is the following. It is d times 1 plus alpha delta t lambda h equals, I'm just moving it to, to the right hand side, equals dn 1 minus 1 minus alpha delta t, lambda h. Okay? Sorry, and here I have dn plus 1. Okay? All right, now, let's try to find that same ratio that we'd found in the case of the time exact problem. All right? So, the equivalent ratio to form here is dn plus 1 and d divided by dn. Okay? And this as we see is equal to 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Now, in the context of our time discrete algorithmic problem, we tend to call this a ratio, we tend to denote it as A. Okay? And A, very obviously is picked because we really want, because this is really an amplification factor. Right? Inasmuch as it is obtained as a ratio of dn plus 1 at, over dn, it essentially tells us how is our time discrete solution getting magnified from one time step to the other. All right? Or getting amplified from one time step to the other. Okay? And you note that A depends upon, if you want to think of it in that way, it depends upon alpha, it depends upon lambda t, and it depends upon lambda h. Right? Okay? So, what that says is that, yes, our integration algorithm matters, right, whether we're choosing one type of the oiler fam, one member of the oiler family or the other. Our time step matters for stability, that's not, that should not be a big surprise. But interestingly as well, the spatial discretization we've used does matter. Okay? And this is why I took pains to point out that the eigenvalue we're working with for any mode, is really a discretized eigenvalue in the sense that it, it reflects the spatial discretization. All right? And that too does affect our amplification factor. All right? When we, well let's just do one more thing. What do we mean now by a stable problem? Right? We wanted to reflect what we saw for the time exact case. Okay? So, for stability the requirement we want to have, right, is the following; we want to say that the magnitude of A should be lesser than or equal to 1. Okay? All right. Now, you may wonder why we are going with the magnitude. All right, what would happen, and why not just say, a has to be less than one? Well, what you're going to see, is that, this is a, after all, we're doing approximations here, right? We're constructing the, the reason we're the getting districtizations is we want to approximate the time dependent behavior. Well, one, result of that, approximation and of that districtization is that it possible for our solution to sometimes go negative here. Okay? So that is something that we will have to deal with. Right? And that's why we recognize that A could be, could, could be negative, dn plus 1 over dn could be a negative ratio. Solution can change signs, we're just recognizing that the algorithm may do that and therefore we are restricting ourselves further by saying that the magnitude of A has to be lesser than or equal to 1. Okay, we are not guaranteed, positive solutions always for our time discrete problem. Okay, so when we come back, we're going to end the segment here, when we return, we are going to, apply this, this condition to our, time discrete, problem and see what it tells us."
h2S5IFUpVuc,"All right. What we're going to do in this segment is complete our stability analysis of our Time-Discrete, Single Degree of Freedom, ODE. All right? Or a single degree of freedom equation. Okay. So let's recall where, where we got. We got as far as the amplification factor. Okay. And that amplification factor gives us dn plus 1 divided by dn equals A, this is the amplification factor. And it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? All right. Now what we want to consider is the fact that what we're going to use in our analysis is the fact that alpha lies in the, in the closed interval, zero to one, right? Delta t, which is our time step is greater than zero. Okay? And lambda h, we've agreed is greater than or equal to zero. Okay? All right. Now the stability condition. Or the stability criteria is that the magnitude of A is lesser than or equal to 1. Okay? All right. So this is what guarantees decaying response, right? From one time step to another. Okay? By the way, the fact that we arrive at this condition is actually the, is what we call a linear stability condition. Okay? And the linear aspect of it comes from the fact that we have indeed a linear problem here. And for linear problems, one can state the stability requirement by just saying that well, if your solution does not grow from one time step to the other, you have a, you have a you have a stable problem. Okay? When we go to non-linear problems, we can't quite use that. Because the physics of non-linear problems can actually lead to solutions growing, right? Over certain regimes, right? And that, that is the right physics for those sorts of problems. So let's get back to this. So, if this is what we have let's see how we can, we can extend this. What this implies, of course, for us directly is that minus 1 should be lesser than or equal to A should be lesser than or equal to 1. Okay? So, A has to lie in that interval. All right? Okay. So let's look at this. So what that means then is that minus 1 is lesser than or equal to 1 minus 1 minus alpha delta t, lambda h divided by 1 plus alpha delta t lambda h is lesser than or equal to 1. Okay? Now because of the conditions I put up here, right? We are guaranteed that our denominator is positive, right? So that says, if we are now what, what this lets us do is safely multiply through by that by the denominator and we get here. Minus 1 plus alpha delta t lambda h is lesser than or equal to 1 minus 1 minus alpha delta t lambda h is lesser than or equal to 1 plus alpha delta t lambda h. Okay? All right. Okay. So to move on, let's just go to the next slide and let's look at the right-hand side of that inequality. Okay? So consider in form, right? 1 minus 1 minus alpha delta t lambda h is lesser than equal to 1 plus alpha delta t lambda h. Okay? We can consider this, we see that these two cancel out. Okay? And we manipulate things and what we see here is that we get alpha plus 1 minus alpha delta t lambda h has to be greater than or equal to 0. Okay? And we do this by simply moving what remains on the left-hand side of the inequality on the first line over to the right-hand side. Okay? Now those cancel out, all right? And we are left with sorry. There's a delta t here. What we are left with here is the delta t lambda h is greater than or equal to zero. Okay? That's what the right-hand equalities says. All right. But this is always satisfied. Okay? All right? Let's look now at what happens. And, and in fact, this is always satisfied regardless of alpha. We satisfied for, for all alpha, all right? Because alpha doesn't even show up in this final form. All right. Now let's consider the left-hand side inequality. Okay? And that left-hand side inequality is that minus 1, sorry, minus 1 plus alpha delta t lambda h Is lesser than or equal to 1 minus 1 minus alpha delta t lambda h. Okay? All right. So now let's work with this a little. When we work with this a little, let's, let's, let me rewrite this by moving This term over to the right-hand side and, and, and rewrite them. So when we do that, we get 1 minus 1 minus alpha delta t lambda h plus 1 minus alpha delta t lambda h is greater than or equal to 0. Okay. That is the condition which must hold and this condition you note involves alpha, delta t and lambda h. Okay? Continuing to work with it, this implies that two plus 2 minus 1 all right. Minus two alpha, hm. What's going wrong here? Alex, you may need to just give me a pause here. I've got a two here. That over back, okay. Alex, I'm going to backup, I made a mistake here. Go back to the beginning of this last equation. Okay. I note that I need a plus sign there. Okay. Put that plus sign and go ahead and now, I get 2 plus 2 alpha, right? One alpha coming from here and another alpha coming from here. 2 alpha minus 1 times delta t lambda h is greater than or equal to 0. Okay? That is the condition we need. Let's see what else we can do with this now. I could write this out as 2 has to be greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? All right. All right. Now let us look at whether this can, under what conditions this may be satisfied. Okay?. Now let's save this and move on to the next slide, okay? Let's consider the following cases, okay? So case one. All right? And for case one, let us suppose that alpha is greater than or equal to one-half. Okay? What, when, when that happens, what that says, what that means is that 1 minus 2 alpha, delta t lambda h is always what? Right? It's always lesser than or equal to zero, okay? Which implies that yes, if alpha is greater than or equal to half, 2 is always greater than or equal to 1 minus 2 alpha delta t lambda h. Okay? Right? This thing always holds, holds unconditionally. Right? And by unconditionally, what we mean is that if alpha is greater than or equal to half, our stability condition holds for all delta t, right? For all delta t greater than zero, right. Okay. So we have a condition, we have a, a situation of unconditional stability of our method. Okay? So this is unconditional stability if alpha is greater than or equal to half. Okay. So, in particular, observe that alpha equals half, which is the Crank Nicholson method or the midpoint jewel as I sometimes call it, as a lot of other people call it. Right? Or even alpha equals 1, which is backwards Euler. Right? These are both unconditionally stable. You can take a time step as big as you like and you're okay. Okay? A solution may be terrible, it may be very inaccurate, but you're still stable. The solution will not blow up, right? The amplification factor will remain, the magnitude of the amplification factor will remain bound by, by one. Okay? Case two is the interesting one. Okay? Case two is that alpha belongs to zero comma half not including the half, right? Okay. All right. Which is basically zero is lesser than or equal to alpha is less than a half, okay? This is the other case. All right? So now what happens is that we have a situation of that's okay. So, so in this case, we have a, we must still have all right. So we, we are trying to see whether 2 is greater than or equal to 1 minus 2 alpha delta t lambda h. Right. This condition needs to be satisfied for alpha, such that zero is less than or equal to alpha is less than half. Okay? All right. This is what we're trying to look at, okay? Now observe that since alpha lies in this range that I've written on the right this parenthesis is always what? Is it all, what can you say about the quantity in that grid emphasis, in those parenthesis? Right? It's always positive. Okay? So now we get a condition, where we see that on, on, as far as delta t is concerned, delta t must be lesser or equal to 2 divided by 1 minus 2 alpha. We're able to divide through by 1 minus 2 alpha, because it is indeed greater than 0. Okay, so it is a positive quantity. 1 minus 2 alpha is positive. But then we see the delta t has to be lesser than or equal to 2 over 2, 1 minus 2 alpha, times lambda h. Okay? So this is a situation of conditional stability. Okay? So the algorithm is stable, right? It can be stable. But it depends upon the size of delta t, okay, conditional stability, which means conditional upon delta T. Okay? So delta T has to be bounded by something. Delta T cannot be too big. Sort of makes sense, right? You have an algorithm that is not entirely stable, that is not always stable. In order for it to stable well, take very small delta t's, right, take very small times 6. Ok, so, an example of this is of course alpha equals 0. Right, which is forward Euler. Okay. All right, now, to end this segment let us consider what else can happen here with delta t. Remember that lambda h, because of the nature of our Eigenvalue problem, lambda H if you ask yourself, what is the order of lambda h? Okay? And where does lambda h appear? Order of lambda h depends upon the Eigenvalue problem, on the Eigenvalue problem. K psi, where psi is the mode shape corresponding to this particular mode. K psi equals lambda h, n psi. Okay? Now, what this suggests, is that lambda h is of the order of, the order of lambda h is of the sort of terms that are like M inverse K. All right? Right. So it's with the order of the norm of terms in M, inverse K. All right. Now, I want us to think about how do M and K depend on the element size? Right, because these after all are obtained by the spatial discretization. Okay? So, recall M is of the order integral over omega e, and then we have the sum over e, sorry, we have the assembly over e and all that. We have the assembly over e, integral over omega e of terms of this type, right? We have, we have shape functions, right? So this is really of this is really like M A B in a particular element. Okay, so I can actually get rid of this, right. I'm just doing this element twice, sorry, I'm changing things around a lot. Okay, element twice, this is what we have, right, integral over omega E, N, A, N, B, D V. Okay, K AB from an element is of order, integral over omega e NA,X NB,X right? These are derivatives. Right? So there are derivatives involved in KAB and yes. KAB may also have a so this is xi, xj, K, kappa here may have a kappa ij. Okay? Dv, all right, and yes the mass matrix may also have a specific heat there. Right, or row could be one if you are doing diffusion. Okay? What is, what is the order of these sorts of terms? These sorts of terms, because their shape function derivatives, are of order one over h. Okay? As a result, what we're seeing is that MAB is of order may be 1, right. But KAB is of order h to the minus 2, okay. As a result, the order of lambda h, okay, is h to the minus 2, okay? In terms of the element size. Now what does that say for delta t? Delta t has to be lesser than or equal to 2 over 1 minus 2 alpha lambda h. Well, this lambda h is like h to the minus 2. Okay? As h gets smaller, our restriction on delta t gets more and more stringent. Okay? So the remark that I will make here is that restriction On delta t, gets more stringent. As h tends to 0. Essentially as we refine our finite element mesh, our spatial discretization, we see that it has an effect even on our temporal discretization if we're working with a conditionally stable method. Right? For the time integration, okay. So, this is how the finite element discretization also makes its presence felt in the time discretization when one is working with a semi discrete method and happens upon a conditionally stable problem, conditionally stable method. All right. We'll end the segment here."
12IQlVHX17o,"Welcome back. We are moving steadily ahead with our analysis of our Euler family of algorithms, or time integration algorithms for our parabolic PDE. What we've managed to accomplish in the last segment was an understanding of stability properties. And I'm going to start this next segment with very quickly summarizing that those results and moving on from there. Okay. So, the result that we had was for stability.  Okay? What we found was that for the Euler family, right? If I look at it, look at the stability results in terms of this parameter alpha. Right? What we find is that for alpha greater than or equal to half, we have algorithms that are unconditionally stable And what this means, is that it does not matter how big our time step is, okay? So unconditionally stable, also meaning any delta t greater than 0. Okay? Any of them will give us stability. Right? For the other cases when we have, alpha lying between, alpha less than one half, and remember alpha has to lie between 0 and 1, right? So, it can't get smaller than 0. So, if alpha is less than a half, what we have are methods that are conditionally stable. All right, and the condition that we obtain is one on delta t. We find in particular that delta t must be a lesser than or equal to 2 over 1 minus 2 alpha, times lambda h. All right? Now, I want to say something more about this result. Observe that for lambda h, which is the the discrete eigenvalue corresponding to the particular mode that we are looking at, right? Lambda h can take on different values for different modes. So when we are looking at the full matrix vector problem, what we need to ensure is that this conditional stability is satisfied for all modes. Okay? So this result must hold for all modes. Right? Consequently, what we want to look at, what we want to impose is this condition for the maximum eigenvalue. All right? This implies that really the kind of condition we must work with for our matrix vector problem, is that delta t is lesser than or equal to 2 over 1 minus 2 alpha lambda h max. Okay? The maximum I can value over all modes. Right? And that is the, something we obtain from an eigenvalue decomposition of the problem, which also we have looked at. The final result we noted, is that lambda h and therefore lambda h max also, varies as the elements size to the minus 2. Okay? All right? And what this implies then, what this means for our methods, is that the spatial discretization does in deed fact affect our, time integration, right? In particular, it, it affects our choice of time stamp. Okay? What this translates to then is that delta t is or, or delta d max really, if one is following this conditional branch of the algorithms, of, of the stability of the algorithms. It means the delta D max,uh, goes as, h square. Okay? Consequently as we refine the mesh, the spatial mesh, we are constrained to using smaller and smaller maximum time steps. All right? Okay, so this really a summary of the type of our stability results. What I would like to do to move on is revisit our amplification factor, okay? So let us recall, let's recall the amplification factor. All right. And that is something we denoted as A. And if I recall correctly, A is equal to 1 minus 1 minus alpha delta t lambda h divided by one plus alpha delta t lambda h. All right? This is the amplification factor. And we recall also how it applies to our algorithmic problem. We see what we have is that dn plus 1 is equal to A dn, okay? Now, what I want to do is look at the effect that this amplification factor has on the behavior of high order modes. Okay? So, we are going to use this fact to look at the behavior of high order modes. Okay? And for our implementation, what this means is, first of all what do high order modes mean? What is a mode of a high order? What characterizes the order of mode? It's not simply the number, right? It's not simply what number we ascribed to it, the m or the l that we've been using, right? We know that m equals 1 to ndf and the, where mappears is in labeling the mode, right? Psi m. Right? Just m having a large number does not make it a high order mode, that is simply an arbitrary choice we have made of labeling modes. Right? So what does make a mode high order? It is the value of lambda h. Okay? Essentially large values of lambda h are the higher eigenvalues, and in any linear system those are the higher order modes, right? For the system, it turns out that those are the modes that have higher order spatial frequencies. Okay? So so it's lambda h, but really we can we, once we have lambda h and we have a what we know about lambda h, a once we know we have a time step, this really implies lambda h delta t. Right? And it's doesn't need convenient to work in terms of lambda h delta t, because this is how it shows up in our problems, right? Okay? All right, all right, so let's look at what effect we had on our amplification factor. And to do that, I am going to plot up on the vertical axis, the amplification factor. And on the horizontal axis, I'm going to write, I'm going to plot this quantity, m, lambda h delta t. Okay? So the idea is that we are looking at the effect of higher order modes and, and, but, but the way we are plotting it up and the way we're, we're studying it we could get to the same behavior either by looking at lambda h large or just the effect of having a very large delta t. Okay? All right, so here I'm going to plot like I said I'm going to plot the amplification factor. Right? The formula for which we have on the previous slide. All right, it's, it's useful by the way, also to look at what the amplification factor is for our exact equation. Okay? And I'm going to write that up here for the exact equation, for the, for the exacting, the time exact equation. All right, the time exact single degree of freedom model equation we know that d of t equals d at 0 exponent of minus lambda ht. Okay? So effectively we have here a sort of time-continuous amplification factor. All right, we can just look at this quantity as being the amplification factor, right? And then if we apply again to the idea that we want to look at how it varies between one times step in the other, all right, from tn to tn plus 1, what you will see is that the amplification factor for the exact problem is essentially exponent of minus lambda h delta t. All right, the idea is that you could take this exact equation, simply write it out as a mapping from dn to dn plus 1, okay? And then you will see that the amplification factor you get is exactly the exponent of minus lambda h delta t, right? So, and, and, so that's just an exponentially decaying function, right? So let's write that one out first. We see that the amplification factor can have a maximum value of 1, all right as written here, right? The exact amplification factor. And I'm going to try to draw a an exponentially decaying function, and hopefully I can draw it reasonably smooth. Okay? So, this is what I'm going to call A exact. Okay? And in parentheses here I'm going write out the limit that A exact tends to as lambda h delta t tends to infinity. Okay? Clearly, as lambda h delta t tends to infinity, exact goes to 0. Right? Okay, now we're going to return to our actual amplification factor, right, our algorithmic amplification factor. And look at what value it takes in the limit for the different members of our, family, okay? So, since I have all this room here, I'm going to make use of it and I'm going to write the actual amplification factor here. And recall that it is 1 minus 1 minus alpha delta t lambda h divided by 1 plus alpha delta t lambda h. Okay? Because we want to look at it in the limit as lambda h delta t tends to infinity, I'm going to simply divide through by that quantity. Okay? So I get 1 over lambda h delta t minus 1 minus alpha divided by 1 over lambda h delta t plus alpha. All right? Okay. Now, so I'm going to look then at for various values of alpha, I'm going to look at what happens as limit lambda h delta t tends to infinity. Okay? Okay? Let's start at the top of the range of alpha. Say alpha equals 1. All right? For alpha equals 1, when lambda h delta t tends to infinity, if you look at that limit, you basically get 0. Okay, all right? So this function right so let me plot this out now. Let me use here's black. All right, so if it is as something like this. I should admit here that I'm not paying very careful attention to how it behaves relative to the, relative to the exact exponential at the left end of the limit. Right, I'm, I'm not paying much attention to that, so, but you can check that, okay? So, don't worry about the behavior on the left. What I'm interested in is the behavior for high order modes, right, which is 4 lambda h delta t tending to infinity, right? So what we have here, is that A, and instead of saying exact, I'm going to say, I'm going to write as a subscript here the, the value of alpha. Okay? So a1, and of course I'm using the colors, so a1 as lambda h delta t tends to infinity, which is the, the amplification factor now for the backward Euler algorithm, right, was alpha equals 1. This thing also goes to 0. All right? Let's change colors here again, go back to red. What I'm going to do is write them for all the others, okay? Let's write, and I'm going to write them only for, for the ones that we really care about here. Let me write alpha equals half, right? Which is the Crank Nicolson method. What we see is that for alpha equals half, substituting that value of half in there, we see that the amplification factor goes to minus 1. Okay? And for alpha equals 0, which is the fold Euler method, we have what do we have? We see that that limit is actually, the limit doesn't exist. Right? Because as we we simply set alpha equal to 0 that traction becomes unbounded, right? But let me allow me to write a unreal number, an unreal number, basically it's unbound, so it goes to infinity. Okay, that's not strictly to say that infinity's not a limit, right? But anyhow, you know what I mean. All right, so this is the situation we have. Let's let's look at what these things appeared like. So if that is 1, then this is minus 1. Okay? And okay, I'm going to show you what Crank Nicolson looks like on this, in green. Crank Nicolson essentially tends in the limit to minus 1. Okay? So a half, or, or the midpoint rule, as I used to call it, a half tends to minus 1. Okay? And finally for our backward Euler algorithm, what we see is that we actually get sorry for our forward Euler, we've already plotted our backward Euler. For forward Euler, what we see is that it is a it's a straight line. Okay, it's a straight line, and with, with a negative slope it falls to right, it leaves the plot. Okay, so this is a for 0, the forward euler algorithm, which is infinity. Right? It's actually it goes to actually minus infinity. Right? Okay? So that's how they behave, and you can plot up some of the others also in between here. So, what does this mean? If we stare at this plot, what you see is that when it comes to the high order modes, it is backward Euler, which has the behavior of the exact equation. All right? Alternately, we may state this as saying that backward Euler tends to dissipate with high order modes. This is numerical dissipation, right, in addition to the, to the physical dissipation that exists with the heat conduction or the master fusion."
dm_A1LpzZPA,"Right, so, remarks. Backward Euler. Right, which is alpha equals 1 damps out, well, I don't like the term damps, I'm going to say dissipates. Dissipates high order modes. Okay, and this is what we call a numerical dissipation. Okay, and because as I observed this has the same high-order behavior as the exact equation, it is often preferred. Okay? Numerical dissipation, right? And, let me also say here, it is similar to time exact equation. Right. The second note here is that there really isn't much to say about forward Euler by the way. It should come as no surprise to us as th, that that the amplification factor for forward Euler tends to minus infinity as lambda h delta t gets large. We've already seen some evidence of it. And in what way have we seen evidence of it? We've seen it in the fact that if delta t gets too large we know that the, that the forward Euler method is no longer stable, right? And that is also reflected in, in the, in the amplification factor going to minus, going to, getting unbounded in this case, minus infinity, as delta t gets too large, right?. Remember lambda h delta t can be can get large if either lambda h or delta t get large. If you have large times that, we know that there is a tendency for forward Euler to blow up, so to speak. Right. And we see that in the amplification factor, tending to minus infinity. Okay, so let me state that though. Forward Euler. Forward Euler, which is alpha equals zero. Has unbounded. Lambda has unbounded A. Okay, all of this, by the way, is in the context of high order modes, right. So I should say up here, this is all for high order modes. Okay, so everything I'm writing here applies to high order modes, okay? All right, so, this should come as no surprise to us, because we know that forward Euler has this tendency to, to sort of lose it, okay? Remark three, is that the surprising thing is that the midpoint rule. Or it may be a surprise. That the midpoint rule, which is alpha equals one half. Right has, has as we've seen A tending to mi, to -1, okay. What does this mean? What this means is that dn plus 1 for the high order modes, right if this is a high order mode I'm writing out, right? Kay, if dn plus 1 is the model coefficient of of a high order mode. What we're seeing is that dn plus 1 tends, is equal to minus dn. This leads to oscillatory behavior in the high order modes. And it's not uncommon to see solutions for from, from the mid-point rule which with respect to time, all right, if we're plotting certain modes with respect to time, what we will see is that if this is t1, t2, t3 and so on. Right? What we may very well see is that you get you get essentially oscillatory behavior. Okay? Something that's done is to simply form a time average, okay. And to take a time average you basically go damp out, well you don't truly damp out, but you, eliminate the effect of these oscillations in your post processed solution. Right, time average the, time average the solution, okay, to get around this. All right, this is important to know about the behavior of these methods in the high frequency limit, right, for high order modes. Okay. So we've looked at stability, we've understood high order, and the behavior of high order modes. What we are going to do next is essentially prepare ourselves for for talking about convergence. Alright? And the way we do that is by first looking at the notion of consistency. Okay? So, we look now at the idea of consistency. Alright. I order to get to consistency, let's go back to our discretized equation. Okay? And that equation, as you may recall, is the following. It is dn plus 1 times 1 plus alpha, well, no, let me back up a little here. Let me write it in fact in the following manner. Let me write it all the way back as dn plus 1 minus dn over delta t plus lambda h d plus alpha equals. Now, up to recently, up to a few seconds ago we were looking at the homogeneous problem, right? Which we got to by turning off the high order modes, sorry by turning off the forcing. We're now bringing back the forcing. Okay? And I'm going to write this as F at N plus alpha, on the right hand side. Okay? If you wonder what FN plus alpha is, FN plus alpha, right, for this particular mode, is got by looking at F sub n plus alpha, the whole vector, right? And essentially, right, dotting it with whichever mode we are looking at. Okay? If we we're looking at mode L we would dot it with the Lth mode, okay. But since we've agreed that we're going to drop the explicit mention of the model number, right, we will just write it as FN plus alpha here. Okay, alright, now, let's multiply through by delta T and see what we get. We get dn plus 1 minus dn plus lambda h Delta t. I'm going to expand out dn plus alpha, as alpha dn plus 1, plus 1 minus alpha dn equals delta tFn plus alpha, alright. Working with this a little more, what we get is dn plus 1 times 1 plus alpha delta t lambda H minus one minus, one minus alpha delta t lambda h, dn minus delta tFn plus alpha, equals zero. As a final step I'm going to divide through by one plus alpha delta t lambda h to give me dn plus 1 minus. Now when I divide this quantity by this I get back my amplification factor. So I'm going to jump a step and simply write that as A dn minus delta t divided by 1 plus alpha delta t lambda h, Fn plus alpha equals 0. Okay? Right, and I'm going to take the final step of calling this quantity here l at n plus one. Okay? Or I think I'll call it m, doesn't really matter, let me just call it l at n. Okay. All right? So this is how I want to write out my time discrete model equation, right, including the forcing. Now, the notion of consistency is the following. What would happen if you were to take the time exact modal equation, and plug it in here. Okay, so now let me now write that as d at tn plus 1 is the time exact mode corresponding. To lambda h. Okay? The particular value of lambda h gives us a certain mode, and that's it. Okay. Now the question we ask is what would happen if we were to take our time exact model solution and plug it back in here. All right? So what we are saying is that if we do that we get d at t and plus 1 minus A d at tn, right? This is also th, the time exact model value at time tn minus minus Ln. Okay, because the last term Ln does not depend upon on the, upon the modal values. Okay? Now, this equation is equal to zero. Correct, right? The left hand side on this equation is equal to zero. My question is, if we were to not plug in our exact solution here, right, as we, as I've done. Will the right hand, will what I've written here on the left hand side be equal to zero or not? In general, it's not equal to zero, okay? In general, however, one can write it out as delta t times some quantity tau. Okay? Tau which in general depends upon the time. Okay, right, the reason we are writing it out as delta T times tau is that, because we recognize that having started from this form of the equation, we've actually multiplied it, multiplied the ODE through by delta T. So we expect that whatever the right hand side is, it already has factor of delta T sitting in there. Right, so we choose to write it in this fashion. Okay, now. This is the general form that we would get for the exact equation. All right. It turns out that we can now make the following identification. Okay. What we say is that. Sorry. See, let me back up to the previous slide. Let me just say one more thing here. If we can show that our exact solution satisfies an equation of this type, okay, we had what we call a consistent method. Hour is no what no let me back up a little more its not quite a consistent method, this is a consistency condition. Okay? This is a consistency condition where we have a consistent method if we can show further. That tau, which I've written as depending upon the, the actual time. If you can show that tau is lesser than or equal to sum constant c times delta t, to some power K, okay? Where K is greater than zero. Okay? If we can show this, then we have a consistent method. Here's why. If we can show that this holds, one can say that as in, in the limit, as delta T tends to zero. Right? What we will find is that d at tn plus 1, minus Ad at tn, minus Ln, which is equal to delta t c delta t to the power of k, right? This thing also tends to zero. Right? This entire thing also tends to zero. Okay? All right, if k is greater than zero. Alright, and therefore what this suggests is that, yes the equation we're working with, the finite difference equation we're working with in time, is one that when we plug in the exact solution, though it does not immediately satisfy the exact solution, at least it does satisfy the limit of, of, vanishing time scales. Okay, so in the limit delta t tends to 0, the time discrete equation. Admits. The exact solution. Right, and which I'm sure you will agree is a useful property for a method. Right, for a numerical method. Okay? I should mention, of course, that in that, in that result t of tao N eq, is lesser than or equal to c delta t to the power k. C is a constant, and K is what we call, k is the order of accuracy. Okay? And it turns out that k equals 2 if alpha equals half, right. The midpoint rule. And it's equal to 1 otherwise. Okay. All right? So, the midpoint rule, as you may expect, because it does look at the, it does apply the algorithm between n and n plus 1. Gives you higher accuracy then any of the other methods. Right, so backward Euler and forward Euler are, are, are down here. And any other method also has Euler factor of c 1. Okay? Only the midpoint rule is second order accurate. Okay, this would be a great place to end this segment."
4Dj3U2ZSj0I,"So, we'll proceed. What did in the last segment was look at high, the behavior of high order modes and also introduce this notion of consistency of the finite difference approach to time discretization. I ought to mention that proof of that, those results for consistency, and order of accuracy are obtained by essentially looking at the time exact solution, carrying out Taylor series expansions, and finite Taylor series expansions, and then, and then manipulating them. It's a, it's a fairly straightforward exercise but somewhat tedious, okay? So, in this segment we are going to essentially move to the end, end game for this particular topic. We are going to look at convergence. Okay, so we are going to look at convergence. Of. Time discrete. Solution. All right. In order to talk of convergence, as you may recall from our early treatment of error analysis for the finite element method itself, we need first to identify what our error is? Okay. So we define the error, and as we've done before in the context of the finite element solution itself we will define the error as the time, as the discrete solution, right? But in this case, we look at the discrete solution at n plus 1, minus the time exact solution at n plus 1. Okay? All right. Now this is what we define as e at n plus 1. Okay? Now, e n plus 1 is of course, is just another vector in the same space. Right? Right, just as d and t are sorry, your d n plus 1 n d at tn plus 1 are vectors and this rndf space. But what that means is that because we have a basis our orthonormal basis, we can expand e as well in that basis, right? So we can essentially carry out the modal decomposition. Of e at n plus 1. Right. And that model of decomposition essentially is that e at n plus 1 equals sum over m, e m for the mode at n plus 1, right, those are our scalar modal coefficients times psi m. All right, just as for any other vector. Okay, now how do we pose the question of convergence, right? We say that something has converged, right? Or we say our solution has converged, have converged. Convergence Right, is the requirement that as limit as the limit of n plus 1, tending to infinity, right, as we take more and more steps, right. What we expect to see is that e n plus 1, dotted with M e n plus 1 equals 0. Right, that limit is 0. Okay. Can you tell me why it is okay to, or can you think, why it is okay to put M inside there? Normally one would say that well, the error has to vanish, right? But why is it okay to put M in there? It's because M is positive definite. Right? All right, so let me just state this here. Since. M is positive definite, all right. What we see is that e n plus 1 dotted with M e n plus 1, right, equals 0, if and only if e n plus 1 itself equals 0. Okay? Only if, it's only when that vector itself is equal to 0 that, that quadratic product is equal to 0, since M is positive definite. And so, and this is why it means that well, yes, if, if this particular limit is tending to 0, it has to be that e itself, e n plus 1 itself is tending to 0. Okay, so this is why it is okay to consider this, as our convergence criterion. All right. But a more importantly, we are not going to work in this form. Right, we are not going to work, with e dot with M e. We're going to work in terms of modal coefficients, because our entire analysis has been in terms of modal coefficients, right. But it's okay, because I'm going show that for this quantity tending to 0. Right. This quantity that I put a brace on, okay, that quantity tends to zero, only modal coefficients themselves go to zero. Okay. So let me, I've told you what I'm going to show you. But in order to write it out, let me say the following, but not the following. Okay, e n plus 1, dotted with M, e n plus 1 equals sum over m, e m, n plus 1 psi m, dotted with M sum over l, e l n plus 1. Psi. Right, and we're going to put parentheses here. Okay. Each of those sums is for one of those e's right, in the quadratic product. All right? Okay. Well, we know how this works now. This is equal to, let me see how I want to write it, right. It is the sum over m and l, right, of, e n, n plus one, psi m dot M psi l. All right, and for no particular reason I'm going to put parentheses around that, and here I have e l n plus 1. All right? However, we know that this is delta m l. Because of the order normality of the psis with respect to m. Right? Well, if that is the case, we know that, this product that we started out with, e, n plus one, dotted with m e, n plus one, equals sum over m, e m n plus one. E m n plus one. All right, essentially what has happened there is the chronicle delta, has been used to, to turn that l index into an m index, right? So this is basically just the Euclidean norm if, with respect to the psi basis, right, so this is sum over m, e m n plus one, the whole square. Okay, so now it's clear that if this quantity is standing to zero it means that the modal coefficients, that sum itself has to tend to zero. Right, which means the modal coefficients themselves have to go to zero. Okay? So what this implies, finally, is that, limit, n plus one, tends to infinity. E, n plus one, dotted with M e, n plus one, equals, essentially limit as n plus one tends to zero, sorry, tends to infinity. Of the sum. Okay? All right? So clearly, if this, has to be equal to zero, if this limit is equal to zero, it just means that, each, every single modal coefficient, tends to zero. Okay? All right, so limit n plus 1 tends to infinity. E n plus 1. Dotted with M e n plus 1, equals zero. If and only if, each E m n plus 1, equals zero in the limit. All right? So, it means that it's okay to just look at what happens with the modal coefficients. Okay? All right, so. Study convergence. Of modal coefficients, right, E n plus one. Right and now dropping the explicit, mention of the modal, index. Right I've just dropped n. Okay? All right, well that's what we're going to do. Okay, so, how do we set it up? We set it up by going back and writing out our time-discrete, discrete equation, our modal time-discrete equation in the form that we set up in order to pose the question of consistency. All right, so here is what I say. Consider. The time discrete equation in the following form. D n plus 1 minus A d n minus L n equals zero, okay? And consider the, the exact equation also forced into this form. We know, however, that the exact equation does not cooperate, and. Give us a zero right hand side, instead we get delta t, tau at t n. Okay, right, what I'm going to do now is subtract the second equation from the first, okay, so I'm going to change signs. Okay, and now when I add those two together, I get on the left hand side, right, d n plus one, minus d t n plus one, is essentially our e, at n plus one. Right? Okay? Right? It is the, error in the corresponding modal coefficient, right? And here I get minus A e at n. All right, the Lns cancel out, and I'm left with minus delta t tau at t n. I'm going to rewrite this as an expression that will allow me to write out a recursive formula for the error. Error at n plus 1 equals A times error at e minus delta t. Tao at tn, okay? I'm going to use recursion now, and write from here e at n equals A e at n minus 1 minus delta t tau t n minus 1, okay. What that implies is that on substituting e as written here in that expression, right? I get e at n plus 1 equals A square e at n minus 1, minus A to the power 0 delta t, tau at t n minus 1, okay. Right. Minus A to the power 1 delta t tau tn minus 1, and I just realized that here it is just tn. All right, I can take yet another step, okay. What, if I were to now rewrite an equation for e n minus 1. All right, I would get A e n minus 2, minus delta t tau at t n minus 2, okay? And making the substitution, right? You see how this is going, right? What I would get is that e, sorry. E at n plus 1 equals A cube e n minus 2, minus A to the 0 delta t tau tn, minus A to the 1 delta t tau tn minus 1, minus A squared, right. Minus A squared delta t tau at tn minus 2, okay. Now one can go on with this, and I'm sure you've all done it at one stage or the other with similar expressions. When you go all the way back to, the zeroth step, right, so the initial time, we get e at n plus 1 equals A, to the power n plus 1. Okay, right. A to the power n plus 1, e at 0, okay. Minus, sum i going from 0 to n, A to the power i, delta t, tau at tn minus i, right? You can check that this is what the recursive formula reduces to, okay? All right, let's stare at this. What is e at 0? What is the error at time t equal to 0? It's equal to 0, right? Because we've made sure that our initial condition is actually obtained from the time exact solution, right? I mean, at the initial time there is no error, right, because we fixed the initial condition."
HFNrmuZVVNo,"Okay, so what this gives us then is that. This tells us then that e at n plus 1 equals minus sum i going from 0 to n A to the power delta t tao t at n minus i. Okay. All right. Let's work with this. And we're going to work now, by invoking some inequality. Okay. In particular what we can say, first of all, is that the first con, the first step that we will take is not an inequality. It is to say that the magnitude of e n plus 1, right, is equal to essentially magnitude of sum i going from 0 to n, A to the power i delta t tao t n minus i, okay? Right, we're taking the magnitude of that sum, the absolute value of that sum. Now, is where things get really interesting. We, now's where the inequalities come up, okay? And, and remember when we write the inequalities, I'm going to start out by writing this. And you recall when we looked at the, our error analysis for the finite element method I made the point that when I write such an inequality, what I mean is that the previous right-hand side, this one is bounded from above by what I'm about to write now as the new right-hand side, okay? So, what I get is that, that is lesser than or equal to i going from 0 to n the magnitude of the, the absolute value of each one of those terms, and that's A to the power i, A to the power of i delta t tau add t n minus i. Okay? All right the absolute value of a sum is bounded from above by the sum of the absolute values. And this result is a very standard step in analysis. It is called the triangle inequality. Okay? But there is more. We can say further that that, that our most recent right-hand side itself is bounded from above by this expression. Okay? Essentially what we're seeing here that is that the that any absolute value, which is a product, right? So the product, the absolute value of a product is boundary from above by the product of the absolute values. Right, and this result is called the CauchySchwarz inequality. Okay? But now we have more. We have stability, right? If our method is stable, what we are able to say is that sum i going from 0, to n of 1 times magnitude of delta t times tau tn minus i, bounds from above our previous right-hand side. Okay? Essentially, what I've done is replace, A i with 1, right. And why, why am I able to do that? It's a property of the methods we are looking at. What property are we applying here? It's stability, right? Because we know that the magnitude of A has to be lesser than or equal to 1. The absolute value of A has to be less than or equal to one for stability, therefore, A to the power i has an absolute value also lesser than or equal to 1. Okay, so if you re-substitute that with one, we get a bounding from above of what we had as our previous right-hand side, okay. But now the story goes on, now we know that delta t is a positive quantity, right. So what we can do here is, we know that delta t's a positive quantity and, furthermore, we know that for tau, the condition of consistency allowed us to say something about it. Okay, so in one step I'm going to do this. I'm going to pull the delta t out. All right. And I have inside my sum. C delta t to the power of k. Okay. Why am I able to say this? It's because we said that we have consistency, okay? All right, we have consistency. Now one could introduce another step inside here, which would be to first say well, if all the tau's over all the n minus i's are take maximum of them. Okay and then I get to this point, right. So let me say that one can see inside of here, when they first, if one wanted to be really careful about this then I guess one should be really careful. So one would say, that first of all you would have a previous step inside here, which is to say that the, that this right-hand side, right? Using that, one could say that sorry, the entire right-hand side, not just that. One could say that this entire right-hand side, first of all would be bounded from above by sum i equals 0 to n delta t times the maximum over all i, t n minus i. Right. One could take that step, right? Which is to say, that well since each of the tau, you know you have a different value of tau for each time step. Let's consider the maximum over all i, okay. But then we know that that maximum has to be bounded from above by C delta t to the power of k, because we have consistency. Right, so then we get to this step that I have here. Right. Okay, all right. Okay now well, what do we get here? We have delta t times sorry, the sum here goes from 0 to n. Sorry, right. Okay, we have delta t times a sum of n plus 1 steps, because i is going from 0 to n, of that quantity c delta t times c delta t to the power of k. 'Kay, so we can very well say that it is just like multiplying delta t essentially n plus 1 times, right. So this thing is lesser than or equal to t to the power n plus 1. Sorry, t at n plus 1, not t to the power n plus 1, but t at n plus 1 times c plus delta t, to the power of k. Okay. All right, just summing over those n plus 1 steps. Okay, right right. Now and now from here, we see however, we see that as delta t tends to 0, right, because k is greater than 0, right? Then as delta d turns to 0, we see that the righ- hand side also turns to zero, okay? All right, but. Limit delta t tends to 0 of c delta t to the power k is equal to 0 for k, greater than 0. Right, and where do we get this? Why are we allowed to see a case greater than zero? Once again it is the result of consistency. Okay, so what we see finally, is that the absolute value of our modal coefficient of the error is bounded from about by, essentially by 0. Right. Which means it, it, it still tends at 0, right, and the limit as delta t tends to 0, right. What we are seeing is that, that quantity is lesser than or equal to 0. Okay, so indeed we have conversions. Right, and that's the end of our proof. Okay? Make a quick remark here, which is that what we've seen here is a demonstration of the use of consistency. And stability. Implies convergence. Okay, which is a very standard it's actually a tier in numerical analysis. Okay it's called the Lax theorem. Okay? All right we're actually done with this entire topic of methods for parabolic problems. And what and, and the approach we've taken if you just to summarize is to carry out a standard spacial discretization using the finite element method. But to the time discretization using finite difference methods, and since here we're looking at first order of problems. We looked at the Euler family. We analyzed its stability understood the behavior of its higher order modes looked at consistency, and looked at how stability and consistency give us convergence. Let me make one more remark now about about the be, about the use of the different algorithms, okay. In particular, let me do this, okay. Let me say, let me say that here I have alpha, okay, and let's look at alpha equals 0, alpha equals one-half, and alpha equals 1. Okay, and just for connection to other things. Let's give this, this algorithms their names, right? So this is forward Euler. Right, this is the mid-point rule, or the Crank Nicolson method. And this is backward Euler. Okay let's look here at the stability. Let's look at order of accuracy. And let's draw a straight line. Okay better, and let's look finally at high order modes, right. Okay? Stability of forward Euler is conditional. Right, and we've seen the cons, the stability condition. Midpoint rule and backward Euler are unconditionally stable. Okay? Order of accuracy, forward Euler has order of accuracy 1, midpoint Euler has order of accuracy 2, backward Euler has order of accuracy 1. For high order modes, limit as lambda h delta t tends to infinity. Limit of A in the tends to infinity not 0. Okay forward Euler is let me just see it tends to minus infinity right, because nothing can be equal to minus infinity, right. Midpoint rule. Limit lender each delta t, tends to infinity is equal to minus 1. Okay? Oscillatory behavior. For backward Euler we see that limit lender h delta t tends to infinity. Oh, sorry, limit of A here. Okay, here too, limit of A equals 0. Okay, so it damps out high order modes, dissipates them away. So this is the, broadly speaking, the behavior of the three main are the three most commonly used members of this family. Depending upon the problem you choose your method, okay. All right, we're done with this segment and with this unit here when we return we will take up the problem of Elastodynamics."
KtNJYj66s1E,"We're going to start a new unit, and a new equation now, we're going to go on to hyperbolic equations. We've looked at elliptic and parabolic equations, we now move on to hyperbolic equations. The canonical example here is elastodynamics, in whatever dimensions you wish. We're going to look at it in three dimensions, because we've left 1D far behind now. Okay, so, title of this topic of this unit is methods for hyperbolic. Linear of course. Pdes in vector unknowns. All right? And the example is linear, r i, linear elastodynamics. In 3D. Okay? So the setting is the following. I don't have my lego vectors today for my bases but I'll use my fingers okay. So that's my basis and this is the body of interest. When we considered linearized steady state elasticity. We were interested, of course, in how this ball would deform or how this continuum potato would deform. We didn't, however, consider the so-called dynamic effects, or the, or the, the effects that lead to wave propagation, all right? We couldn't, for instance, for that reason, also study the problem of this ball being actually tossed through space, right, and tumbling through space, and deforming perhaps at the same time. Right. We couldn't look at the time evolution of that problem. Right. Because we were looking at the steady state problem. So now we take away restriction, and look at the full blown elastodynamics problem. Okay. So Here we have the, the setting is essentially the same, as far as our pictures are concerned here, right. So we have our figures are concerned here, e1, e2, e3. We have a body of interest, right. This is omega. As we've done before, we have a decomposition of the domain into Dirichlet, into a Dirichlet subset for that particular component of the displacement field, right? And the corresponding Neumann subset. Okay? And this holds for i equals 1, 2, 3, right. X would be a point here, right. Which will be described by its position vector. Right, everything that we've seen from before holds, right? The decomposition of the, of the boundary into. The Dirichlet. Sorry, I got the union in the wrong position. It's the union of this Dirichlet boundary and the Neumann boundary. And, of course, those are disjoined, we know that. This is the empty set, and so on, right, we have all of this. This of course holds for I equals 1, 2, 3, right, three dimensions. Okay. I'm going to straightaway put down the strong form of the problem, right. The strong form of the problem is the following. Now, given data, ugi, t bar i. F i, right? In addition, we need some more data now. We need also other functions which I'm going to denote as u i 0, okay, and v i 0. Okay, we are going to use them for initial conditions. Alright so given all of this, and of course, the constitutive relation sigma ij equals C ij kl epsilon kl. We also know that we have the kinematics, right? Epsilon kl equals one half partial of uk with respect to xl, plus partial of ul, with respect to xk. Right? We have all of that stuff. Right, the only new things that you're seeing here are these two. Functions which I'm telling you right now we've been used in initial conditions. There, there, there is one more. We do need another coefficient, which I'm going to denote again here as rho, rho here is just the mass. Sorry, the mass density. Okay? Let me get rid of these arrows from here so that it's not confusing to think that they're pointing up from density. Those are the initial conditions. All right, we have all of this. The problem that we are trying to find is the fall, is to find u i okay, now it's a function of position and time. Okay. Right, and remember that i runs over one two three. Right, so everything that I've written on the first line, the first five functions is down here. Right. These functions are all just components of vectors. Right. Okay, so we want to find u i such that. Such that the following holds. Rho, second derivative of u i, with respect to time equals sigma ig comma j, plus fi, okay. In omega cross 0, comma T. Just as we did for the time dependent parabolic problem, right. We say that our pde must hold over the spatial domain and the time interval, 0 to T. Okay? Let me leave this here and then go on to the next slide to write out boundary conditions. Boundary conditions are no different. Right? For boundary conditions we have u i, at some position x and time t equals this given function ug for component i at positions x and t, and time t. Right. Now note that we're allowing here that Dirichlet data to vary with time. Okay this just allows us to have time dependent Dirichlet boundary conditions. Just as as we had for our time dependent heat conduction or time dependent mass diffusion. Right? We allowed the possibility that the Dirichlet conditions varied with time. Okay right at any point x belonging to a point in the Dirichlet boundary, right, for that particular displacement component. Our Neumann boundary condition or our traction boundary condition also is as before. Okay. Functional position and time for a point x belonging to the corresponding Neumann boundary. And note here that I'm continuing to use t bar for the traction function where as the t here is for time. Okay so it was, it was an anticipation of this final clash of notations that I have been using t bar for the traction. Okay, initial conditions. Our elastodynamics equation our pde for elastodynamics is a second order pde in time. And therefore, how many initial conditions do we need? Two, right? So we have u i at some position x but at time t equals 0, equals u, how do I write it, u i 0. Okay, which could be a function of position. We're allowing, we, we of course need to allow the possibility that well not just the possibility. We have to allow for initial conditions so, to be defined at every point. Right. So for every point x we have an initial condition of the displacement. Right. What it basically says what is the initial configuration of the body. Okay? So, this holds of, of for all x in omega. It is second order, so we need two initial conditions. Next initial condition is for u i dot x comma 0 equals the specified distribution of velocities. Okay, what this means is that at the initial condition we are saying that not only do we start out knowing where every point on this body is, right, that is the first of those initial conditions. What we are saying we must also know what the initial velocities are. Okay. That's the second initial condition. All right, this is it. This is our strong form. Okay. I'm going to straight away go ahead and write out the weak form. Right? The weak form. Of course, this is going to be the infinite dimensional weak form, but we know that going from there to the finite dimensional weak form is not such a big thing. The weak form. Okay? Given all the data that I have just put out there, right? I'm not going to repeat the data, okay? The weak form is find. U i belonging to S, okay. Where for our purposes here S consists of all u i such that u i equals u g i on. Okay. Find this such that for all w i, belonging to V where V consists of all weighting functions. Remember, w i is our weighting, our, our weighting functions. v belongs to w i such that w i equals 0 on. That Dirichlet boundary. Okay. Such that for all w i belonging to V, the following cond, integral condition holds. Now, integral over omega, w i rho, second derivative, second time derivative of u i. Plus integral over omega, w i comma j, sigma ij, dV, plus sorry, is equal to integral over omega w i fi dV. Plus as before, the sum over spacial dimensions 1, 2, 3 now. Integral over the corresponding Neumann boundary, w i t bar i dS. Okay, that's it. Now, if you stare hard at this weak form, you should observe that it is obtained by just adding one term to our weak form for the steady state elasticity problem. Right? And that extra term is just this one. Observe furthermore that this term requires no integration by parts. Right, it's literally obtained by looking at the left hand side of our strong form, which is right here. Okay, look at the left hand side of the equation at the bottom. Multiply that by Wi, the weighting function. Integrate of the domain, right? We know that the rest of the stuff on the right hand side is what attracts integration by parts, right? Especially the divergence of sigma, the first term on the right-hand side in the strong form. Well, I know directly how the weak form arises, nothing new here, right? Just add in that, this, extra term on the left hand side. All right, we'll end the segment here. When we return we will simply write out the weak form, sorry, the finite dimensional weak form and go directly into the finite element of matrix vector recreations. Good."
cTKU18RMjEM,"Welcome back. We are now on to, developing methods for, hyperbolic PDE's, in, three dimensions in vector unknowns. And specifically the the example problem we're working with problem we're working with is that of linear elastodynamics in 3D. What we've accomplished for this problem is a statement of the strong form and of the weak form. We'll pick up from there and write out the finite dimensional weak form and plunge directly into matrix factor equations now. So, we are doing, like I said, the problem of hyperbolic PD is, in vector unknowns. And in three dimensions. Okay? Of course, this is, these are all linear, right? So we're doing linear hyperbolic PDEs and so on. Okay? And also the canonical problem we're talking of here is linear, the last two dynamics in 3D. Okay? I'll direct you write out the weak form, but the finite dimensional weak form, which you know, really requires very little, extra specification over the infinite dimensional weak form. Also, I will, spare us the details writing out of the data here, right? We're very familiar with all of that now. All right. So, the task we have here is to find U i sup h. And remember the h now indicates that we're talking of a finite dimensional problem. Find u i sup h belongs to the space S h which is a subset of the larger space S. And in, and particularly we are thinking of s, h as consisting of all functions u, I, sup h. And we expect these to belong to h 1 over the domain. Such that u, I h, equals u i given on the corresponding Dirichlet boundary. All right? The, the part of the Dirichlet boundary that corresponds to that particular spatial dimension, denoted by i, all right. We have this. Such that. For all whi belonging to vh. Which is a subset of we where we H consists of waiting functions WI sup H, also H1. Search that. WI Sup h vanishes on that corresponding Dirichlet boundary. Okay, so find u i sup h such that for all w i sup h belonging to v h, the following condition holds, right? The usual integral equation, except now that we have the one extra term, right? The term that's second order in time. Okay, so we have integral over omega, w i sup h rho, second time derivative of u, integrated over the volume, plus integral over omega, w h i comma j, sigma h i j DV equals integral over omega. WH sup I, sorry, sub I. FI, DB, plus sum over spatial dimensions. One to three in this case, or n, s, d in general. Integral over the corresponding boundary. W, h, I, t bar, I D, s. Okay? That is our finite dimensional weak form. All right? And I didn't state that anywhere. So, let me just do this. All right? So, this is r finite dimensional Weak form. Okay? All right. Now we know how things play out from here. Everything works out just as before, right? We're going to use the same basis functions we're doing 3D so you can think of trilinear hexahedral as being the simplest of those elements into which we decompose the domain. Everything works out just the same, okay? Also note that each of these terms is going to give us our, our standard sort of contribution that we know so well. Right, when we account for the fact that sigma H satisfies the constitutive relation, right, which makes it. Truly linearized elasticity. Right? What we get from this term is a very standard one that we've worked with at least once in great detail. Right? So we recall that this term gives rise to c transpose Kd. All right, and these two terms here give us. C transpose F. Okay, also, now. When you recognize that. The second time derivative and the second time derivatives are in need of DT squared there. All right. When you recognize that this term on the left hand term, essentially enrolls the waiting function W multiplying the second time derivative of the trial solution, right? And you work through things just as we work things out in the case of, the parabolic problem in 3D. What you will see is that this gives rise to, what sort of matrix? We have coming from 'w' and we're going to have a 'd' coming from to u right? In particular, we get a d dot d double dot because we have two time derivatives on u. What is a matrix that goes between them? It's one we've encountered. It's the mass matrix. Okay. So essentially, this is the form that the matrix vector equations take in the case of linear elastodynamics. There's just one detail I want to point out here, which is something to do with the construction of M, because of the fact that our vector d has at each node three scalar degrees of freedom. There is one little detail about the construction of M, which I'm going to show you right now, okay? So let me just say here that we're going to look at one little thing to do with the form of M. For this problem, okay. And in order to understand that, what we need to do is consider the, element integral corresponding to that term. Okay, so we consider integral over omega. W, h, I, row second time derivative of u, I, u, I, h sorry there should be an h there and an h here. D, v okay, we recall this is simply a sum over the elements. Integral over omega E, WHI row second time derivative of. Right. Remember this ok. So. Okay, we we're going to work with just that element in a group to clarify things."
u5appqgEPzM,"Okay, we'll do it here. So, the element integral. Okay, integral over omega e. W h i, rho, second time derivative. Right, this one. I'm going to go straight away into, writing it out, with our basis functions and summing over basis functions and all that sort of thing. Okay? So, we know that this is integral over omega e. Actually let me skip some more steps. Okay, because we're now such experts at this that we don't really need to write every single thing. Okay, let's do it this way. Fine, sum over a comma b. C a e i. Okay? And you recall that c a e i is each of those is the i'th degree of freedom at the a'th node of element e. Okay? This. Integral over omega e. N a, let's have the rho there. N a n b d v. And outside of this integral. Come the degrees of freedom that, are used to, construct the representation for the trial solution, right? So that will turn out to be d b element e degree of freedom i, okay? And what is implied here is a sum over our, spatial dimensions i. Okay? Now, the form that I have in parentheses on the right hand side. Okay. The everything including the integral, is what we identified previously to be the a b component of the consistent mass matrix. Okay, same thing. The same thing happens here, except for the fact that there's one thing we need account for. Which is that there is a sum over i here, okay. So let me just mention this here. Recall sum on i, okay. And what that does is lets us write this thing out as sum over a comma b, c a e i, integral over omega e. Rho n a n b d v, but now what I'm going to do is, I wrote that d v too close, I'm going to slip in here a Kronecker delta, okay? So allow me to do this and I will show you why I'm doing this. Okay? Since I've slipped in the Kronecker delta and given it indices i j. I'm going to turn the d b e i from the previous line, this one. Into d b e. J. Okay. All right. As a result, now when we put this all together what you see is that what you observe is that though the contributions to the, from the integrand are the same as we had for the, for the linear parabolic. Problem with the scalar unknown, right. What we've done here to counter the fact that we have vector unknowns, is to include this Kronecker delta right there, okay? All right, now when we put everything together, we get the following. Okay. We're going, we're now, in what I'm about to write, I'm going to abandon the explicit, writing of the sum over, over a and b. Okay? And in fact, let me give myself more room here. Just before I go ahead and write that I know that I've forgotten the second time derivatives on the d. To account for the fact that we have a second time derivative right there. Okay? Now everything's fine. Okay. So the way I'm going to write this by dropping the explicit sum over a and b is the following. I get, c one e right? Transpose okay? C two e transpose. So on up to c number of nodes in the element, e transposed. Okay, this one for instance, c two e transpose, is simply c two e one. C two e two. C two e three. Okay? All right. All of that now multiplying some big matrix which makes up our mask matrix. Okay? And out here we get our vectors, or vector of d one e double dot d to e. Double dot all the way down to d n n e. Element e. Double dot. Okay? And like I did here for c two e let me just point out that this is the little vector d two, e one, double dot, d two, e two, double dot, d two, e three. Double dot. Okay? All right, now. Note that for each combination of nodes from the c vector and the d vector, okay? We get a little matrix, okay? And that little matrix is obtained by writing out here we have here let me show you what we get here. Okay. We have here a little matrix which is row n a. N b d v over the element and let me actually make it n one n one to show you the very first combination okay? But this matrix, that integral itself, multiplies what you may call the three by three identity matrix. Okay. That's because it's this matrix that I've just written, that little sub matrix that I've just written. Which multiplies, which, sort of intercedes in the multiplication between c one e transpose and d one e double dot. Okay, this continues until we get another such matrix. Right, each of these, each of these blocks is a little matrix, right. Each of these is an integral over omega e. Of, you know, n something, n something else, integral over d v. Okay? All right? All the way down. So we have how many such blocks? All right? We have n n e such blocks in the column wise direction and n n e such blocks in the row wise direction. Okay. All right. All right. And what's important to note is that each of these little matrices is itself an identity matrix, right? And this is what our Kronecker delta does for us. Okay? And so the general term in here would be something like integral over omega e rho n a, n b, d v one zero zero zero one zero zero zero one. All right, you'd get other such block matrices everywhere. Okay. All right. And this matrix that I've written out here is our element mass matrix. Okay, in consistent form, okay, the consistent mass matrix. All right so this is an important thing to note. This is slightly different from what we saw for the linear parabolic p d, where each degree of freedom was, was, is essentially a single scalar. Unknown. Right? And, and in that case, this, the, this identity matrix essentially collapsed to the scalar one. Okay? The fact that we are doing, dealing with vector unknowns here, just, has, has essentially expanded that scalar one into a, into an identity matrix. Right? Three by three identity, that's all. Other than that, everything is the same. Okay it's probably useful for us to ask, now what are the dimensions of m, e? Okay, what will they be? Right the dimensions of m, e are number of nodes in the element, times number of spacial dimensions, squared, right? Those are the dimensions of m e. Right. Okay. That's it. This is our element mass matrix. The only thing that's different with, with regard to what we've seen before. Assembly works just as before. Assemble over degrees of freedom, common degrees of freedom across elements. Belonging to the same global degree of freedom. Implies that those corresponding matrix contributions from every. From, from, from the neighboring elements add up. Okay? All right. So let me just state that here. Assembly proceeds as before. Over global degrees of freedom. Okay? What we are left with, finally at end of the process, right, after we, we, we say that. Well this has to, our, our weak form has to hold for all c belonging to the appropriate, Euclidean space, right. What we get at the end of it, is the following set of equations, right. We get m d double dot plus k d. Equals f. Okay? When we were not doing elastodynamics, but essentially elastostatics, this first term was missing. Okay? It's just shown up now, right. Everything else is the same. We know that boundary conditions are buried inside here, right, including time dependent Dirichlet boundary conditions. That also works just as we saw earlier, right, in the case of the time dependent parabolic problem, okay. If we have Dirichlet conditions, they are time dependent. Well we know exactly what they are at every time, at, at any time, right they will go to the right hand side. What we would have here in addition are initial conditions, right. D at zero. Equals the, the vector of, well it essentially equals the vector form by taking every Dirichlet condition. At degrees of freedom lying, corresponding Dirichlet boundary. And, putting them all in a vector d nought, okay? We did this for the parabolic problem. We need another boundary condition here, sorry, another initial condition, because we are second order in time. That's d dot at time e t equals zero, which we will denote as v nought. Okay? All right? And v nought, you remember, is simply, it's simply constructed of, u dot. Sorry, it's actually I think just the vectors, the value of the function v nought, right? At points x a, right? For every degree of freedom, a, that lies, sorry, for every single degree of freedom a. Right, for the initial condition has to be specified over the entire vector D. Okay? This is essentially it. Right, just as we specified initial conditions for our parabolic problem, we have two initial conditions now for this problem. All right? Okay, so that's really all we need to know about how the matrix, the matrix equations are obtained. The process is exactly the same. We get another mass matrix. There are some detailing mass matrix, it's at, at, for corresponding to every combination of degrees of freedom, it's slightly blown up. We have that little three by three identity matrix sitting there. And we have two initial conditions, right? And we know how to specify those initial conditions. So, so really, that's what we need to know. We'll stop this segment here. When we return, we will talk about time discretization."
m9wMmcEG71A,"Welcome back. What we've accomplished in the previous segment is a explication hopefully of the matrix vector form of the problem for lin, of linear elastodynamics in 3D. Now we proceed with the time discretization and understanding a little bit about the about the methods that are used to solve this particular problem, which is also a ODE now. Okay, so let's start with the matrix vector problem. Okay. What we derived at the end of the last segment was the following. Md double dot plus Kd equals F, right. This is the second orderal d in time and we have initial conditions. D at time T equals 0 is d naught, and d dot at time T equals 0 is that vector, V naught. Now, we can proceed from here, but it's useful to include one extra element in here. And that extra element is a sort of throwback to the times when to the times really even before finite element methods became very popular in structural mechanics when it was common to write out matrix equations of this sort for for structures, right? People would, would, the notion of using nodes and degrees of freedom had, had already been established, especially in the context of structures like crosses and frames and so on. And in that setting of structural mechanics, it was common to include in addition to the mass and stiffness matrix that we see here, matrices that we see here, a damping matrix, okay? So and, and here's how this was done, okay? So including the effect of structural damping. Okay. The way this will be done will be to include a damping matrix of C. Again, I'm just following the standard notation that tended to be followed in this business. So, we have yet another matrix. All right, so we have C which would often be modeled using what is called Rayleigh damping. And this was done in a very sort of simple, very empirical manner by taking some constant, a, multiplied with the matrix M plus some other constant b, multiplying the matrix K. Just like that. Okay? All right? Empirical, but it was found to work. And, there are other reasons why this works. We don't need to go into those reasons. This is the model for what is called Rayleigh damping. Okay. Where a and b are constants. All right? And that was it. There was no attempt to try and derive these this new damping matrix from any more fundamental partial differential equation. That can be done but, but, but it was not necessarily done in, in, in writing this out, which is just explicitly written out in this form. Okay. And then the form, the damping would be, would be included in, was the following. So now we would get the equations of, elastodynamics. With structural damping. Right. And that equation would turn out, would be Md double dot plus Cd dot plus Kd equals F, all right. Where the idea of damping was, that this is some sort viscous damping essentially that was being modeled, and you see the effect of viscosity. If you're familiar with that sort of physical phenomenon and the fact that you have a single time derivative on this extra term that's been introduced to the problem. Okay, that's just like a first, that is indeed a first time derivative, a single time derivative on the d vector. Right, plus boundary, sorry, plus initial conditions as usual. Okay? This is essentially the model. Now, from here, one can go on and write the time discretized form, just as before, right. From here, what we do is for time discretization. All right, we do exactly what we did before, which is to say that our interval, zero to T we write as the union of all these time intervals, of t0 to t1, so on all the way up to t sub N minus 1 to tN. Right? It's the union of all these intervals, right, of each of these subintervals where t naught, in the way I've set up the time interval, t naught would be 0. Tn would be capital T. Okay? We have everything just as we knew from before. All right? And then we say again that d At n is the time, discrete. Approximation. Write approx for short. The time discreet approximation to d at t n. Okay, just as we did for the parabolic problem. Okay, and then we get our time-discretized, our time-discrete matrix vector equation. Okay. Let me go back, just for a second to have you look at it. Right, we have it here. So, you see M d double dot plus C d dot plus K d. Now d double dot, because d of course displacement, d double dot is essentially the acceleration. Right, likewise d dot is indeed the velocity vector, right, at the global degrees of freedom. And d is the displacement factor. With this is mind, the time-discrete matrix-vector equation is often written as M at n multiplying a n plus one, where a n plus one now is the acceleration, right. The approximation to the acceleration at time t, n plus one. Plus C v at n plus one, v being the velocity, plus K d n plus one equals F at n plus one. Okay? With initial conditions now being that d not and v not are known. Okay? All right. Now, the family of algorithms that is commonly used to solve this equation, this time-discrete form of the equation is what is called the Newmark family. Newmark family of, algorithms. For second order ODEs, right? They're second order because they're second order in time. Okay? The way this family works is the following. For, now, we need to have some parameters for this family. Just as for the Euler family of algorithms for first order equations, we have our parameter alpha. Here, because these are second order ODEs, we need two parameters, it turns out, and one of those parameters I'm going to denote as gamma. And gamma belongs to the closed interval zero comma one, just like alpha did. And the other parameter I'm going to write as twice of beta, where twice of beta now belongs to the interval zero to one. Alternately beta belongs to the interval zero to half. Right? The closed interval zero to half. All right? Now, with this in place, here's how the Newmark family works. It says that d at n plus 1 equals d at n plus delta t times v at n plus delta t squared over two, times, 1 minus 2 beta, multiplying a n. That's the approximation at the acceleration at time tn, plus 2 beta, multiplying a n. That's the approximation to, to the acceleration at time t n plus one. All right? And then because it's a second order algorithm, we need something for v n plus one as well. V n plus one is equal to v n plus delta t times one minus gamma at a n, plus gamma at a n plus one. Okay? Those equations together with our time discrete matrix factor equations written above here, and of course initial conditions, right? Initial conditions here are just that d not and v not are known, okay? This summarizes our, family of algorithms for linear elastodynamics, okay? Now, let's talk about solution techniques, 'kay? The solution technique, that I'm going to talk about, I'm going to talk about a single approach, not two approaches as we did for the parabolic problem. The method I am going to talk about is what's called the a method, the a being for acceleration. Okay, and here is how it works, we again define predictors and correctors. Right? We say that d n plus 1, tilde, equals d n plus delta t v n plus delta t squared over two. One minus two beta a n. Okay? That's the predictor for d. The predictor for v is v n plus delta t. I don't need anything in the denominator, it's just delta t. Times 1 minus gamma a n. All right? And just as we did before for the parabolic problem, we've looked at the update formulas for d and v and simply extracted out those parts of the formulas that come from everything known at time t n. Okay, so these are our predictors. The correctors are. Right, those are predictors, and the correctors are the following, right. The predi, the correctors are obtained by simply writing d n plus 1 equals predictor Right, plus corrector. Now the corrector for d n plus 1 is delta t square beta a n plus 1. And the corrector for v is. This one, right? These are the correctors. Right, and what I've done now is write the corrector step for both of them. The a-method essentially, just like the, the methods in the case of the parabolic problems, the a-method is obtained by substituting these corrector steps in the original equation. Okay? So what we get is on substituting these corrector steps. Right, on substituting we get the following. M times delta t square beta plus C delta t gamma plus sorry. I'm getting ahead of myself a little, so let, let me just rewrite this line. I'm trying to skip a couple of steps, and I realized I was already making errors, so I'll just back up a little. Okay, so when, when we substitute these correctors, what we get is the following. We get M a n plus 1 plus C multiplying, C multiplies v n plus 1. So we get the predictor plus corrector. And for K, K multiplies d n plus 1. So again, we get predictor plus Corrector. Okay, this is the entire left hand side. All of this equals F at n plus 1. Now this is the so-called a-method, and by using that, those predictors and correctors, what we've done is to rewrite the equation entirely in terms of a at n plus 1 and predictors for d and v. All right? So this essentially then lets us rewrite this as M plus C delta t gamma plus K delta t square beta, all of this multiplying a at n plus 1. Equals F at n plus 1. And then the terms multiplying the predictors are just moved over to the right-hand side. And why can we do this? Right, it's because the predictors are known, right? They depend only upon the solution at n, which we always assume we know when we construct these time-stepping algorithms. So we get here, right, we get minus C d n plus 1 tilde minus K d, sorry. Sorry, it's C v n plus 1 tilde. C v n plus 1 tilde plus K d n plus 1 tilde. All right? That's our method. Okay, we can now go ahead and invert this, solve for a, once we have a at n plus 1, using our corrector steps we get back d n plus 1 and v n plus 1. Okay? The only thing we need in order to sort of start up this algorithm is a at 0. Okay? In order to get A at 0. Just use the equation at 0. All right, and by equation here I mean the time-discrete equation. And that is M a 0 equals F at 0 minus C times v at 0. This works because v 0 is known, right? It is just the initial condition. Minus K d at 0. d 0 is also known. All right? v 0 is known, d 0 is known. Okay? Okay. So here we have it. That is our standard solution approach for this problem. We can end this segment here. When we return, we will start our analysis, and that analysis also is going to be based upon our approach of modal decompositions. All right?"
gbzCrkea-2o,welcome back in the last segment we set up the time discretized equation for linear elastic dynamics and also looked at a canonical solution of technique for it right what I call the a method we get into analysis now as for the parabolic problem the analysis is based upon a suitably chosen eigenvalue problem and and the way we carry out this analysis is to use I can value problem to construct a decomposition of our OTE in two modes ok so our analysis is least on the eigenvalue problem space in the following eigenvalue problem on the good omega square m side equals case I alright where Omega square u we recognized to be the natural frequencies for each army goes the natural frequency at natural frequency of oscillation right in the context of our problem because each Omega would be a natural frequency of oscillation okay with this problem in as a basis and then as before proceeding to construct em or target can proceeding to consider I can functions are right conductors side that are M orthogonal right so the size r m or talking I convectors ok when we do this right we and we proceed just as we did for the for the parabolic problem right so we're right by doing this we can now do things like saying that any vector like the d vector can be constructed through a sum over l.d soup l side and right where each side L is an eigenvector again these vectors are mr Turner ok should probably site here right and say that l equals just as we saw before 12 total number of degrees of freedom in the problem ok we do this foodie and of course we can do this for every other up for other vectors that show up in the problem right we can do this will be an A and so on okay are we take this approach and what we see is that we get when we take this approach we get a reduction to ndf single degree of freedom model problems right or moral equations all right now people to go back and write this full-time exacto de rduction 2nds single degree of freedom more equations and we just continue here see of good time exact OD alright and the form of those equations is the falling right to the time exactly remember because it's pretty much the way we did for the parabolic problems right remember we wrote out the reduction for the for the time exactly d and then extended that to go time to strategize rudy is over the time exactly we would get dl double dot plus 2cl army girl each l arm d l dot plus Omega each square d l equals 0 if we consider the homogeneous case right remember the homogeneous cases when we have zero right hand side now r omega each l is just reminding us that that we have natural frequencies but those natural frequencies because they depend upon our matrices which are obtained by spatial discretization write those natural frequencies also reflect the effect of spatial discretization ok so these are what we will call the finite-dimensional or species specially discretized natural frequencies ok just as for the parabolic problem we considered lambda each sub L right which is simply the which simply was the effect of spatial discretization upon the eigenvalues it's something him right so that's America each l & 4 c sub L which actually properly should better be written as a see each subnet ok there's an H and agile because that also does reflect the effect of spatial discretization okay see each sub L is simply our remember the constants we use to define really damping it's those constants divided by the corresponding natural frequencies ok and this is what is called modal damping ratio ok all right now the way we proceed with our analysis is the following because we have a second-order de we are we rewrite our second order o de using a technique that's very well established in ordinary differential equations we write it as 2 first-order Cody's right so we rewrite the second-order OTE as to first order Cody's alright and in order to do this we say they're right we are now looking for the solution of a vector it was just the two vector where y is d and d dot ok alright it's not difficult to rewrite that single low second-order only in terms of this right arm and in fact what we will also do is that as we recall from the case of the parabolic problem ok for what we get for the time discretize problem is the falling maybe get a mobile be composition of the time discretized problem also write in model form ok and that problem is the following weekend II and +1 Modell plus 2c each sub L Omega H sub L the n plus 1 mode and plus Omega each arm and square d and +1 right where each of these is a moral coefficient the elmora coefficient of the corresponding vector ok this equals 0 is the homogeneous problem ok alright right and now the relations between the model coefficients between the DN plus 1 VN plus 1 and n plus 1 will be satisfied with those those model coefficient satisfy the same conditions that we obtained from numark family right for the full vectors okay right so we get new mark family equations relate dn1 at al v sorry it's not the vector right it's the mode v + + 1 @ l and he and plus 1 at ok alright those are the equations that involve the coefficient though are the parameters gamma and Vito
PQsLr4Gr-9U,"Okay? And then, when we do this reduction of a second order ODE to two first order ODEs, for the time discretized problem, okay, we get the following form. We get the following form. We get y at n plus 1, okay, equals A, which is now our amplification matrix, y at n plus L at n. Okay? All right, and let me tell you just once more, that Y at n plus 1, is d at n plus 1, v at n plus 1. All right? This is of course dn, vn. And A here is a 2 by 2 amplification matrix. Okay? It plays the same role as our scalar amplification factor for our parabolic problem, okay? And the definition of A, as well as the definition of this two vector Ln, reflects the reflects the Newmark algorithms right, with the gammas and betas and everything. Okay? We can work out all these details, but it's just tedious detail, which we are not going to truly use. Okay? All right, so then this is our time discretized equation, now written in, in the form of two first order ODEs instead of one second order ODE. Okay? I'm going to give you a summary of the stability results right now. Okay, stability. Okay? So, if 2 beta is greater than or equal to gamma is greater than or equal to half, we have unconditional stability. Okay? If on the other hand, gamma being greater than or equal to half, beta lies between 0 and gamma over 2. These conditions together give us conditional stability. Okay? Now remember, we are talking about a single degree of freedom modal equation, okay? Where, because of the fact that we have a second order ODE, what we are solving for are dn plus 1 and vn plus 1, right, both of those modal coefficients. The conditional stability holds when omega h, right, which is the frequency, corresponding to that particular mode, right? Remember, as before we're suppressing the modes, okay? Actually, even back here, we are already suppressing. Mode, number, index, all right, L. Okay? So that's going on here as well. So when, even though I've just written omega h here, it's really omega hL for each L, right? We need to consider this for each L, okay. So the stability condition also requires that omega h delta t, should be lesser than or equal to a quantity that I'm going to denote as omega critical, okay? Where omega critical. Equals the following. Ch, and again, this is Ch sub l really, but for every mode, okay? C h times gamma minus half, plus gamma over 2 minus beta, plus c h gamma minus half, sorry, c h squared gamma minus half, the whole square. All of that to the power one half. The whole thing divided by gamma over 2 minus beta, okay? That's the critical frequency, okay? What we see here is that there is the effect of damping. Okay? And, what we also observe is that the effect of damping, right? And, and we have damping when c h is, greater than 0. The effect of damping is to increase the critical frequency, okay? So we have the undamped critical frequency. Let's say omega critical u for undamped, okay? This is got by setting c h equal to 0, okay? And it is just gamma over 2 minus beta to the power minus half, okay? Right? I just want to point out that this undamped critical frequency is a lower bound to omega critical, all right? So what we're seeing is that the undamped critical frequency is lesser than or equal to the actual critical frequency, when you have some damping, okay? All right. Okay? So what we see as well is that omega h delta t, which needs to be less than the, than the critical frequency, is, it satisfies this sort of a condition. Okay? What I mean by saying this is that actually, let, sorry, let me not write this line. This is, is sort of attempt to state a condition. Let me not write that, that equation, it, it can be misinterpreted. Instead let me say this. The undamped critical frequency is a more stringent condition. On omega h delta t, right? It's really a condition on delta t. So what we are saying is that instead of saying that it has to be less than the actual critical frequency, right? Instead of using the condition that I have at the bottom of this slide, right? If instead of that, we were to say that, well, omega h delta t has to be less than this quantity, okay? Then we are actually imposing a more stringent condition upon our algorithm, our time integration algorithm, okay? Okay, with this in hand, I'm just going to list out properties of some sort of canonical, almost classical, members of this Newmark family, okay? And, I'm going to do this part in a table, where I'm going to list the method here. I will say what type it is. And by type I mean is it implicit or explicit? I will list here beta, gamma, let me see, what else do I need to list here, right? I will list here the critical frequency for stability for the undamped case. And finally, I will also write here the order of accuracy. Okay? So, the methods we are going to consider are the following, the first one we will consider is the, what is sometimes called the Trapezoidal Rule. We consider four methods, okay. I'll write them out first, trapezoidal rule, think linear acceleration. We have the average acceleration. These are all names of methods. And finally we have the central difference method. And, when I say central difference and trapezoidal method, trapezoidal rule, note that they, they will not be the same as what you may be familiar with from first-order ODEs. Simply because we are using terminology here that has been established for second-order ODEs, okay? All right, all of these methods are implicit except for the, except for the central difference method. Okay? Now, stability. They're all for, they all use gamma equals half, okay? Now, the trapezoidal rule uses beta equals one quarter. And because this combination of beta and gamma makes it unconditionally stable, there is no question of what the critical frequency is for stability, right? It's unconditionally stable, all right? So there's nothing to say there. Linear acceleration uses one-sixth. And what happens here is that the the critical frequency is 2 root 3, okay? Average acceleration uses beta equals 112, gamma equals half. And if I remember the undamped critical frequency is square root of 6. The central difference method finally uses beta equals 0. And the undamped, actu, critical frequency here is 2, okay? For order of accuracy, all of these are second order. Okay? One caveat though is that the explicit you get a truly explicit algorithm only for M and C being diagonal. All right? It's just a summary of some of members of the family. As you can imagine, because we are talking of a, of integration algorithms of second order ODEs, the numbers of this family are, are, are a few more, right? It's a fairly large family. Okay, we can afford to stop this segment here."
jR41sQ12ixE,"All right. What we saw on the previous segment was a fairly quick statement of the stability conditions for the Newmark family, right? And we saw the explicit results for some of them. We didn't derive any of them. What I'd like to do in this segment is outline the, the sort of analysis that leads to those, to those conclusions. We won't get into a completely detailed step-by-step derivation of the results. Unlike what we did for the parabolic problem, okay? But we'll sketch out the, the approach. Okay, so the stability analysis here, as in the case of the parabolic problem. Is based upon examining a particular object. Can you recall what object we examined in the case of the parabolic problem? We examined the amplification factor. Here, too, we have an amplification factor, but it is a matrix. Okay. So, the stability analysis is based on, an eigenvalue analysis of the amplification matrix, all right? Okay. And, the way we proceed with this is to, if I remember that the amplification matrix is what we denoted as A. Okay and you also recall that the amplification matrix is what showed up in this formulation of the problem as here, y n plus 1 equals Ayn plus Ln when we consider the full n homogeneous problem, okay? So what we're talking about is analyzing this, and all our detail of the Newmark family is sitting inside there, right? The particular values of gamma and beta we've chosen, and so on, okay. Here is the condition, okay? We define what is called the spectral radius. The spectral radius of A, right? And we denote that as we've used rho quite a bit, so let me use something else here. Let me just say r, okay, spectral radius A, r sub r function, okay, okay? This is defined as the maximum over i, okay? The maximum over i of lambda i of A, where those lambdas are essentially the eigenvalues of this two by two matrix. All right, and because it's a two by two matrix, of course, i just runs over one and two, okay? That's probably not even worth using an index there. Okay, so let's say that max i equals 1,2 right? Essentially it's the maximum eigenvalue, okay? Not just the maximum eigenvalue, but actually it is the magnitude of it, okay? Where we have accounting for the fact that the matrix A may not always be symmetric, and therefore, it could have complex eigenvalues, right? Accounting for that, we write our spectral radius as being defined as the max i equals 1, 2. Now, that magnitude that I wrote up there is properly the square root of the product of lambda i and its complex conjugate, which is going to be denoted as lambda i of A bar, okay? Where that bar implies the complex conjugate. Of lambda i A, okay? That is our spectral radius. Now the condition for stability requires that. All right, it requires that r, the spectral radius, should be lesser than or equal to 1. Okay, the spectral radius is defined there, should be lesser than or equal to 1. I'm going to say a little more about this condition. We have r can be lesser than or equal to 1, if lambda 1 and lambda 2 are distinct. Okay, it turns out, that if lambda 1 and lambda 2 are distinct the condition that we get that r can be lesser than or equal to 1 involves the fact that the eigenvectors of A. Are linearly independent. Okay, on the other hand r has to be strictly less than 1, right? Not lesser than or equal to 1, it has to be strictly less than 1, if lambda 1 equals lambda 2, right? We have repeated roots, okay? And in this case it turns out that the eigenvectors. Of A, are linearly dependent. Okay? I'm going to do very quick demonstration of why this is the case, okay? So, let's look at the two cases, okay. Let's first look at what happens if they are linearly independent eigenvectors. Okay. If they are linearly independent eigenvectors, then let's look at what happens for the homogeneous problem as we go from one time step to the other, okay? Essentially, what we see is that yn plus 1 equals A yn, right? But then yn is equal to A yn minus 1, and so on, right, yn minus 1 equals, tatatata, right? Goes on. So what we are seeing here is that with every step is getting multiplied by itself, right? If you just make these substitutions in here, we see that, right? Okay, so what we are seeing is that, after a certain number of steps, we're seeing that yn plus 1 equals A to the power n plus 1 times y0. All right? So, what we need to worry about is what is happening with A. All right, as it gets multiplied by itself, what are the powers of A? If we have linearly independent eigenvectors, one can show that A, okay, can be written as some matrix P, times a two by two matrix, which is lambda 1, 0, 0, lambda 2. P inverse, okay? As a result, we get from this, we get A to the power n equals P, lambda 1 to the power n, 0, 0, lambda 2 to the power n, P inverse, okay. So now you note that even if lambda 1 were equal to 1, all right this sort of a form stays well bounded, okay? All right, okay. This sort of a form stays bounded, right? So the amplification that is applied to the initial condition to get say, the nth time step solution does not get unbounded, okay? Things are different if you do have, if we have linearly dependent eigenvectors. In this case, the best we can do is write A as, let's say, some other matrix Q, two by two matrix, lambda 1, 1, lambda 2, Q inverse. Okay? This is the case of the linearly dependent eigenvectors, okay? Now, if you go through the process now, and calculate a to the power n. >> Okay, what you get is a form where you get Q, lambda 1 to the power n. You get lambda 2 to the power n, and all is looking good except for the fact that here you get n lambda, well lambda 1 is equal to lambda 2 here, okay. So it doesn't really matter. Let me just do this. In the case of linearly dependent eigenvectors, you have lambda 1 equal to lambda 2. So, you get n times lambda 1 to the power n minus 1, Q inverse. And now, do you see a problem as n gets large? Okay, what you note is that if you look at what happens here? And you have lambda 1 equal to 1. Okay? You see that the off diagonal term becomes n, okay? Right. In that case this term becomes n. Right? And then as you go to higher, and hard, higher, as you go, as you advance in time steps you have this sort of gradual sort of tendency towards unboundedness. Okay, so what happens in this case is that the off diagonal term. Diverges, as n. Okay? All right. Further analysis of this problem is based upon essentially solving for lambdas, right? So. Solving for the lambdas, right? The equation that we need to solve for the lambdas is the following. Lambda square, remember this is just the characteristic equation that you use to solve for the eigenvalues of a matrix, all right? So we will write it as lambda squared minus 2, A1, lambda plus A2 equals 0, okay. And here A1 equals one-half trace of A, and A2 equals the determinant of A. Okay? All right? With this form, it's just a simple quadratic equation, right? So we get lambda 1, lambda 2, are equal to what is it? A1 plus or minus square root of A1 squared minus A2. I believe, let me just look at that. Yeah, okay, so these are lambda 1 and lambda 2. All right. Now, here is the, sort of stability condition again written in terms of A1 and A2. Okay, because of course stability depends upon the values of lambda, but then since we have lambda 1, lambda 2 given by these conditions for A1 and A2, we can write it out in terms of A1 and A2. Okay? The conditions that we get are the following. We get minus A2 plus 1, divided by 2 is lesser than or equal to A1 is lesser than or equal to A2 plus 1 divided by 2 if the magnitude of A2 is less than 1. Okay? All right. And. Okay. And otherwise, we get minus 1 is less than A1 is less than 1 if the magnitude of A2, sorry, it's not just the magnitude of A2. It's A2 itself. Here too, it's just A2. If A2 is equal to 1. Okay, one can plot this thing up in this result up in an A1, A2 space, and here's what we see. If this is A1, and here we're plotting up A2. Okay? Let me see. I think, I'm going to mark some critical points here. These are the points 1, 1, minus 1, 1, and down here, I have the point 0 minus 1. Okay, in this, we have between 1, 1 and 0, 1, that line, and here we have that segment, okay? The first condition, this one okay, holds everywhere except for that line. Right, because that dashed line is A2 equals 1. Okay, so this sort of inverted triangle that I've drawn is the acceptable region. Okay for stability, if A2 is less than 1. Okay, if A2 equals 1, right, then what it says, is that on that line, it's got to eliminate, it's got to be, you've got to leave out those two points. Okay? That is the region of stability that we have here. Okay. Okay, we can end this segment here."
Nnmwcy0fQzs,"All right. In the previous segment what we looked at was, again, a sketch of the way we would approach the stability analysis for our modal equations for linear elastodynamics, all right. What I'd like to do here is actually take a step that we also took in the case of the parabolic problem which was to go from stability. Our understanding of stability to also very quickly cover high, high order models, okay? So, we've derived stability conditions on A. All right? Our amplification matrix. And in particular, what we said was that if A1 equals one-half of the trace of A. And A2 equals determinant of A. What we found was that the conditions are based upon lambda 1, lambda 2 equals A1. Plus or minus square root of A 1 squared minus A2. Okay? This is how we determine lambda 1 and lambda 2 from our characteristic equation for this matrix. And then we impose conditions at lambda 1 and lambda 2 can be, need to be lesser than or equal to 1. We've also understood when they have to be strictly less than 1. All right. So these are the stability requirements. Right, so we also said from here r, which is  the maximum of the square root of lambda i and lambda i bar. Right? Where the lambda i bar refers to the complex conjugate. Right? Okay, and then we say, finally, r has to be lesser than or equal to 1, right? And we've understood when it can, when it needs to be strictly less than 1. All right, so this is how we go about our stability analysis. Now you recall that when we looked at the parabolic problem, we looked at the, the amplification factor, and also used it to tell us something more about the high order modes. Okay. And we saw that the different members in that case of the Oiler family did different things to high order modes. Okay. It emerges that in the case of this problem, also the amplification factor plays a role. And in order to damp out high order modes, okay. In order to damp our high order modes, high order modes. Are decaying right? If if our eigenvalues become purely real, right? Sorry  it's the other way, sorry. High-order modes are non-decaying, sorry, are non-decaying, if lambda 1 and lambda 2 are purely real. Okay? And therefore in order to damp out the higher-order modes, what we need is that the discriminant of this relation, right, should be negative. Right? So what we need is that A1 square minus A2, is less than 0 for damping or digging, of high order modes. Okay, so this is really a sort of parabolic condition and if we go back and now plot out this parabolic condition on our, A1 A2 space that we introduced at the end of the last segment, here is what we'd see. That is 1, 1. This point here is minus 1, 1. And this point here is 0,1. Okay. What we did do last time was observe that our stability condition gives us this sort of triangular region. Okay. In addition to this, it emerges that our condition for damping out of higher order modes requires that there is a parabola which takes on this shape. Okay. So we need to remain within that parabola, not outside of that parabola. So, this is the region in which we get damping of high water modes and okay according to that condition. Right, so it is in that region that we get, damping of high order modes. Okay. Let me see if I can get back to my original color here. Okay, the effect this has is the following. You know, the stability condition for the condition for, for, for stability independent of time step series, right? So, so really the unconditional stability, okay, condition is the following. It is that beta is greater than or equal to. Gamma over two. Okay? So this is unconditional. Okay? It doesn't quite make sense to say stability condition and then fit an unconditional stability condition, so let me say stability. Requirement here. Stability requirement. All right, all right, so this is the unconditional stability requirement. It turns out, however that we need, beta to be, not only greater than gamma over two. But, this requirement of damping out of higher order modes, requires that beta should be greater than or equal to gamma plus half the whole squared to damp out higher order modes. Okay. And one way in which this manifests itself is in the effect on, the spectral radius. Here's what happens, right. So the effect of, of this requirement on the spectral radius is the following. Okay, turns out that if you plot out the spectral radius, here it's r, and here we look at, delta t times, 2 pi, omega h, okay, and this is just like we plotted in our, previous, study of the parabolic problem. We plotted up the amplification factor on the hori, on the vertical axis, and on the horizontal axis we plotted up lambda t times lamb, sorry, delta t times lambda h, which was the eigenvalue of that problem, all right? In this case, you're plotting up delta t times two pi omega h, which is effectively like delta t divided by the time needed of oscillation or something, okay? So for r, if this is the value 1.  Right? What tends to happen is that as delta t over two pi omega h increases, the entity times two pi omega h increases, if you have values of gamma or, or beta that are, you know, we satisfy the unconditional stability requirement but they have a, they're not as big enough as to satisfy the second requirement, right, of being greater than gamma plus half the whole squared. Okay, here's the sort of thing that happens with the spectral radius. And along this point is where you get, real, fully real eigenvalues. Okay? So you get real eigenvalues around here. All right, and this is for, gamma by two lesser than or equal to beta, lesser than or equal to gamma plus half of the whole squared. Okay? If on the other hand you have, beta exceeding gamma plus half the whole squared, then you tend to get, a behavior which looks like this. Okay? This is for, beta greater than gamma plus half the whole squared. Okay, and this is what leads to damping of high-order modes. Okay. So, that is what we need to know about, how high-order modes depend how, how our analysis of high-order modes follows from our stability analysis. Perhaps the last thing I want to say about this particular problem is how we go about getting to the, to a notion of conversions. Okay. And before we end, and, you recall that stability and consistency are the two requirements that lead to convergence. Okay? Now, in order to understand consistency of this problem, we go back to looking at this form of the, evolution equation for the time this could rise, modal equations, okay? A is our amplification matrix. Remember y is our two vector which has the displacement of the velocity in it. Right? And Ln is what we get from the forcing. Okay, so now we're going back to the inhomogeneous problem here. Okay, again, this is exactly the way we set things up for the parabolic problem. All right. Where Ln was the effect of our putting back the forcing. Right? We, we turned off the forcing when we studied stability. We turn it back on when we study consistency. Okay. So, this is the time-discrete problem. Okay, the time-discrete modal equation. Now, for consistency, we say that well, if instead of the time-discrete, solution, we were to look at the time exact solution, and just plug it into the equation we've written above here, right? We would get y of t n plus 1 equals A, our amplification matrix, times y and tn, right? Y of tn plus one and y of tn are the corresponding exact solutions. Okay? We get a plus Ln, right? But in general, the exact, the time exact solution does not satisfy our finite difference equation. Right? So we get, in addition, a term delta t times now a vector tau depending upon tn. Its a vector tau because of course y is a two vector, right? So tau also has to be two vector. Okay? So this is the time-exact, model equation. Okay? Now, consistency requires. That tau be written as again, a, a, a 2 vector, c. You can think of it as c being two constants, c1, c2, okay. c times delta t to the power k. 'Kay, where, or we could write this as tau1, tau2. Equals both functions of t n. All right? They are equal to c1, c2, times delta t to the power of k, where k is greater than 0, and c1, c2 are constants. Okay? Right? And, when I listed the various some of the members of this family, I said that, that the, their order of accuracy was 2. Okay? That order of accuracy is given to us by k. Right? So, when a couple of segments ago, I listed a, several members of this family and said that they all had order of accuracy 2, what it means is that in this consistency condition, the order of accuracy, k, is equal to 2 for all of them. Okay? Now, when we use consistency and stability to get a convergence, right, the so called Lax Theorem, right, which is that consistency and stability give us convergence of our time integration algorithms for this problem, right? That convergence condition is dependent upon writing out our error. E again is a 2 vector, right, because we're looking at the error in the displacement and the velocity, right, for every mode, okay? So, what we get is that e at n plus 1 equals A to the power of n plus 1, times the error at, at time t equals 0, all right. And, the error at time equals 0 is also a 2 vector because it has the error in the displacement and the error in the velocity. Okay? Plus sorry, it's minus sum i going from, from 0 to n. Right, you get i going from 0 to n, delta t, A to the power i, okay? The tau vector, which is the 2 vector I wrote just above here, at tn. Okay, this is the condition that we need to evaluate, we need to work with for convergence, and, everything works out just as we saw for the parabolic problem. Right? It's just that we have to deal with a matrix vector equation and then matrix vector inequalities here, but things essentially work out. Using what we know about stability and what we have written out for consistency, we can prove that limit of en plus 1 equals 0 as delta t tends to 0. Right, its limit as delta t tends to 0 of en plus 1, right, is equal to 0, all right? You get a 0 back to there, right? So you get the error in the displacement and the velocity tending to 0. Well, that's what we need to know as far as the analysis of this problem is concerned. And, that also concludes our rather quick study of methods for linear elastodynamics in 3D."
IzvVVTTSu6Q,"There were a couple of errors in both work in this segment. The first error appears on this slide where I talked about how for damping of high-order modes, we need to go a step beyond merely, looking at unconditional stability. The inequalities that we are talking about in this case, appeared here, and the error is in this very last, inequality. I wrote it as just gamma plus one-half the whole square. It should be gamma plus one-half the whole square, divided by 4, okay? That gives us the correct condition to be satisfied for damping of high-order modes. The next error appeared two slides later. There we go. And it appeared in the way I labeled that equation. I called it as it appears there, the time-exact model equation. That is not quite correct. The equation I've written out there is obtained by substituting the time-exact, or the time-continuous solution into the time-discreet model equation, okay? When we do that, we find out, we discover that the time-exact solution does not actually satisfy the time-discreet model equation. Instead, we're left with this extra term here, right? And it's on the base of this term that we talk about consistency and accuracy of the method. So, properly this equation should be labeled like it should be labeled just what I called it, which is the time-exact solution, substituted into the model equation, okay? So, let me call it that time-exact solution. S-O-L-N, short for solution. Okay? Substituted. In time-discreet. Model equation. Okay? When we do that, we get this extra term. Like I, reiterated, a minute ago, and that is the basis for our analysis of consistency, and accuracy of the method. With that things workout consistently."
UiZmpi9nTi0,"So if you've got as far as this particular video, I expect that you have also taken the time to watch many of those that preceded it and in the process have learned something about the finite element method. You may also have tried the quizzes and hopefully some of the programming assignments as well. If you've done a good proportion of all of those, you are actually pretty well prepared to go on and do other things with the finite element method, and also branch out to maybe learning about other topics which could then use the finite element method. Recognizing, of course, that what we've done so far is essentially meant to be an introduction at about the graduate level in any university. What I'd like to do with this last Lecture is point you to some resources out there, some of which you may already have known and maybe you know about all of these resources. But I'll just point you to some of them as things that you could go on to do having done all this work to learn the finite element method. So I'll start with this slide and this information in front of me. You see, I've talked about telling you a little bit about open source finite element codes and also about some related course material that's out there. As far as the open source finite element methods are concerned, you see these two links in front of you, The first is for Deal.II which of course many of you have probably tried out already, maybe for the assignments or maybe even on your own. And the other is for a different collection of softwares which I will also tell you about. So here we are on the Deal.II website and like I said many of you may already have seen it and tested things out on it before. Deal.II is essentially a collection of, as it says, open source code and libraries, which, with the background you have gathered, will allow you to go on and gradually build up more and more of your own repertoire in using finite element methods. You can go to this website yourself and browse around it. But let me tell you how you could very quickly start to be even more effective than you have been so far. If you click here on dev, you can see the drop down list and you go to tutorials. You come to this page, which actually sets you up to look at a number of examples with theory, the mathematics, and even code that will allow you to gradually go to more and more problems. There are a number of ways in which you can look at it and the way I like best is to click on the list here and you see there's an extensive list of examples of problems solved with the finite element method running all the way up to 50, 50 plus problems. So you could start out on with any of these, maybe you could even dive in if you think you've already accumulated some expertise and can afford to go on. So this is something that I would really encourage you to consider. And of course, you have been using Deal.II in a somewhat reduced framework as we were teaching you this introduction to finite element methods. Let's go back then and look at the other example of a software that I also have up for you, the FEniCS Project. So we click there, and it takes you to that website. The FEniCS Project is also a collection of free softwares, as you see right here and they explain what they mean by free software, and their philosophy and so forth. In this case you could go down and go along and look at applications. You'll see there's an extensive list here of different types of applications that you could use from this collection. If you scroll down, you see some things that you would begin to recognize, you see there is a solid mechanics library. There's something here for time integration, which you also know a little bit about. Is another kind of equation. There is this micromagnetics and so on, right. There's also software there about going massively parallel, and really many things. And the nice thing about Deal.II and FEniCS is the fact that they are open source, they are free, and they are also actually very advanced in terms of combining computational science with modern computer science. So I'd really encourage you to consider these two sites more closely. So let's go back then, to my slide here. So those are two of the open source softwares that I would really encourage you to consider. There are others out there of course, and by no means in mind am I saying that the others should not be considered. These are things that we use in our research group. Moving on then to courses. This course that you have been taking, the Finite Element Method for Problems in Physics, is going to continue in an on-demand format after it ends with this first iteration. And what that means is that will be available for you to start working with at any time that you like. You can start as you like and finish within a certain amount of time after that. You don't need to wait for a particular time when the course is going to be offered to start. These lectures are also available on YouTube, and in order to get to that let me do something slightly different. What I've brought you to is a page of a page on Open Michigan as it says, but let me show you how to get there. Well you can see how to get there, you just go to open.umich.edu, and that will bring you to a landing page where the University of Michigan has provided a whole host of open educational resources. Here, if you then go to the search bar and you type in Introduction to Finite Element Methods, I'd looked for it earlier. Do a search, shows you where the material is available. You click on Materials there and you see some of the lectures that you're already familiar with. If you click on the YouTube icon there, it will bring you to the whole series of lectures that you've been using on YouTube, right. So these lectures are already there, they actually were there even before our MOOC began, and I know that some of you had discovered these lectures, okay. So this resource is available. Also, with Open Michigan, we have provided a different series of lectures and this is a series called Lectures on Continuum Physics, okay? Search for that and there we go. Lectures on Continuum Physics. There we go. All right, so this is another series that we had recorded and provided, also about the same time that finite element lectures were provided. You click on Materials, and again you'll see all kinds of resources there. You see the video lectures. There are also assignments, which are available as PDFs. If you click on the YouTube link, likewise you go to this series on YouTube. Okay? Now, this series of lectures is also going to be shortly provided on a MOOC platform, okay, as we see here at the bottom of the slide. And this launch of lectures on continuum physics as a MOOC is expected to happen sometime during 2016. We're not certain exactly when, and this really depends on how quickly we can get all the pieces together. But the video lectures are already available, it's just packaging it as a MOOC that is going to take a little more time. So that's it really. I wanted to provide a brief indication of where you could go from these lectures that you've been following. And of course, there are many many other resources already available on the web, and we would encourage you to try as many of them as possible. That's it for now. And stay tuned. We will be back either with more in infinite elements or definitely, pretty soon, more on continuum physics."
